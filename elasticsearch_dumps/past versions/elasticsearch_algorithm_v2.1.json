[
{"_index":"throwtable","_type":"algorithm","_id":"the-algorithm-auction","_score":0,"_source":{"description":"The Algorithm Auction is the world’s first auction of computer algorithms. Created by Ruse Laboratories, the initial auction featured seven lots and was held at the Cooper Hewitt, Smithsonian Design Museum on March 27, 2015.\nFive lots were physical representations of famous code or algorithms, including a signed, handwritten copy of the original Hello, World! C program by its creator Brian Kernighan on dot-matrix printer paper, a printed copy of 5,000 lines of Assembly code comprising the earliest known version of Turtle Graphics, signed by its creator Hal Abelson, a necktie containing the six-line qrpff algorithm capable of decrypting content on a commercially produced DVD video disc, and a pair of drawings representing OKCupid’s original Compatibility Calculation algorithm, signed by the company founders. The qrpff lot sold for $2,500.\nTwo other lots were “living algorithms,” including a set of JavaScript tools for building applications that are accessible to the visually impaired and the other is for a program that converts lines of software code into music. Winning bidders received, along with artifacts related to the algorithms, a full intellectual property license to use, modify, or open-source the code. All lots were sold, with Hello World receiving the most bids.\nExhibited alongside the auction lots were a facsimile of the Plimpton 322 tablet on loan from Columbia University, and Nigella, an art-world facing computer virus named after Nigella Lawson and created by cypherpunk and hacktavist Richard Jones.\nSebastian Chan, Director of Digital & Emerging Media at the Cooper–Hewitt, attended the event remotely from Milan, Italy via a Beam Pro telepresence robot.","name":"The Algorithm Auction","categories":["2015 in computer science","Algorithms","Visual arts"],"tag_line":"The Algorithm Auction is the world’s first auction of computer algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"kinodynamic-planning","_score":0,"_source":{"description":"In robotics and motion planning, kinodynamic planning is a class of problems for which velocity, acceleration, and force/torque bounds must be satisfied, together with kinematic constraints such as avoiding obstacles. The term was coined by Bruce Donald, Pat Xavier, John Canny, and John Reif. Donald et al. developed the first polynomial-time approximation schemes (PTAS) for the problem. By providing a provably polynomial-time ε-approximation algorithm, they resolved a long-standing open problem in optimal control. Their first paper considered time-optimal control (\"fastest path\") of a point mass under Newtonian dynamics, amidst polygonal (2D) or polyhedral (3D) obstacles, subject to state bounds on position, velocity, and acceleration. Later they extended the technique to many other cases, for example, to 3D open-chain kinematic robots under full Lagrangian dynamics. More recently, many practical heuristic algorithms based on stochastic optimization and iterative sampling were developed, by a wide range of authors, to address the kinodynamic planning problem. These techniques for kinodynamic planning have been shown to work well in practice. However, none of these heuristic techniques can guarantee the optimality of the computed solution (i.e., they have no performance guarantees), and none can be mathematically proven to be faster than the original PTAS algorithms (i.e., none have a provably lower computational complexity).","name":"Kinodynamic planning","categories":["Algorithms","Automated planning and scheduling","Robot control","Robot kinematics"],"tag_line":"In robotics and motion planning, kinodynamic planning is a class of problems for which velocity, acceleration, and force/torque bounds must be satisfied, together with kinematic constraints such as avoiding obstacles."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bareiss-algorithm","_score":0,"_source":{"description":"In mathematics, the Bareiss algorithm, named after Erwin Bareiss, is an algorithm to calculate the determinant or the echelon form of a matrix with integer entries using only integer arithmetic; any divisions that are performed are guaranteed to be exact (there is no remainder). The method can also be used to compute the determinant of matrices with (approximated) real entries, avoiding the introduction any round-off errors beyond those already present in the input.\nDuring the execution of Bareiss algorithm, every integer that is computed is the determinant of a submatrix of the input matrix. This allows, using Hadamard inequality, to bound the size of these integers. Otherwise, Bareiss algorithm may be viewed as a variant of Gaussian elimination and needs roughly the same number of arithmetic operations.\nIt follows that, for an n × n matrix of maximum (absolute) value 2L for each entry, the Bareiss algorithm runs in O(n3) elementary operations with an O(n n/2 2nL) bound on the absolute value of intermediate values needed. Its computational complexity is thus O(n5L2 (log(n)2 + L2)) when using elementary arithmetic or O(n4L (log(n) + L) log(log(n) + L))) by using fast multiplication.\nThe general Bareiss algorithm is distinct from the Bareiss algorithm for Toeplitz matrices.","name":"Bareiss algorithm","categories":["Algorithms and data structures stubs","All stub articles","Computer algebra","Computer science stubs","Determinants","Exchange algorithms","Numerical linear algebra"],"tag_line":"In mathematics, the Bareiss algorithm, named after Erwin Bareiss, is an algorithm to calculate the determinant or the echelon form of a matrix with integer entries using only integer arithmetic; any divisions that are performed are guaranteed to be exact (there is no remainder)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"out-of-core-algorithm","_score":0,"_source":{"description":"Out-of-core or external memory algorithms are algorithms that are designed to process data that is too large to fit into a computer's main memory at one time. Such algorithms must be optimized to efficiently fetch and access data stored in slow bulk memory such as hard drives or tape drives.\nA typical example is geographic information systems, especially digital elevation models, where the full data set easily exceeds several gigabytes or even terabytes of data.\nThis notion naturally extends to a network connecting a data server to a treatment or visualization workstation. Popular mass-of-data based web applications such as google-Map or google-Earth enter this topic.\nThis extends beyond general purpose CPUs, and also includes GPU computing as well as classical digital signal processing. In GPGPU based computing where powerful graphics cards (GPUs) with little memory (compared to the more familiar system memory which is most often referred to simply as RAM) and slow CPU to GPU memory transfer (when compared to computation bandwidth).","name":"Out-of-core algorithm","categories":["Algorithms","Algorithms and data structures stubs","All stub articles","Computer science stubs"],"tag_line":"Out-of-core or external memory algorithms are algorithms that are designed to process data that is too large to fit into a computer's main memory at one time."}}
,{"_index":"throwtable","_type":"algorithm","_id":"chinese-whispers-(clustering-method)","_score":0,"_source":{"description":"Chinese Whispers is a clustering method used in network science named after the famous whispering game. Clustering methods are basically used to identify communities of nodes or links in a given network. This algorithm was designed by Chris Biemann and Sven Teresinak in 2005. The name comes from the fact that the process can be modeled as a separation of communities where the nodes send the same type of information to each other.\nChinese Whispers is a hard partitioning, randomized, flat clustering (no hierarchical relations between clusters) method. The random property means that running the process on the same network several times can lead to different results, while because of hard partitioning one node can only belong to one cluster at a given moment. The original algorithm is applicable to undirected, weighted and unweighted graphs. Chinese Whispers is time linear which means that it is extremely fast even if the number of nodes and links are very high in the network.","name":"Chinese Whispers (clustering method)","categories":["Algorithms","All orphaned articles","Orphaned articles from June 2015"],"tag_line":"Chinese Whispers is a clustering method used in network science named after the famous whispering game."}}
,{"_index":"throwtable","_type":"algorithm","_id":"edgerank","_score":0,"_source":{"description":"EdgeRank is the name commonly given to the algorithm that Facebook uses to determine what articles should be displayed in a user's News Feed. As of 2011, Facebook has stopped using the \"EdgeRank\" term internally to refer to its News Feed ranking algorithm, and in 2013, uses an algorithm that takes more than 100,000 factors into account in addition to EdgeRank's three. In 2010, the EdgeRank algorithm was described as:\n\nwhere:\n is user affinity\n is how the content is weighted\n is a time-based decay parameter.\nSome of the methods that Facebook uses to adjust the parameters are proprietary and not available to the public.\n^ McGee, Matt (Aug 16, 2013). \"EdgeRank Is Dead: Facebook’s News Feed Algorithm Now Has Close To 100K Weight Factors\". Retrieved 28 May 2014. \n^ \"EdgeRank: The Secret Sauce That Makes Facebook's News Feed Tick\". Techcrunch. 2010-04-22. Retrieved 2012-12-08.","name":"EdgeRank","categories":["Algorithms","All stub articles","Facebook","World Wide Web stubs"],"tag_line":"EdgeRank is the name commonly given to the algorithm that Facebook uses to determine what articles should be displayed in a user's News Feed."}}
,{"_index":"throwtable","_type":"algorithm","_id":"randomized-rounding","_score":0,"_source":{"description":"Within computer science and operations research, many combinatorial optimization problems are computationally intractable to solve exactly (to optimality). Many such problems do admit fast (polynomial time) approximation algorithms—that is, algorithms that are guaranteed to return an approximately optimal solution given any input.\nRandomized rounding (Raghavan & Tompson 1987) is a widely used approach for designing and analyzing such approximation algorithms. The basic idea is to use the probabilistic method to convert an optimal solution of a relaxation of the problem into an approximately optimal solution to the original problem.","name":"Randomized rounding","categories":["Algorithms","All Wikipedia articles needing clarification","Pages using duplicate arguments in template calls","Probabilistic arguments","Wikipedia articles needing clarification from May 2010"],"tag_line":"Within computer science and operations research, many combinatorial optimization problems are computationally intractable to solve exactly (to optimality)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hybrid-algorithm","_score":0,"_source":{"description":"A hybrid algorithm is an algorithm that combines two or more other algorithms that solve the same problem, either choosing one (depending on the data), or switching between them over the course of the algorithm. This is generally done to combine desired features of each, so that the overall algorithm is better than the individual components.\n\"Hybrid algorithm\" does not refer to simply combining multiple algorithms to solve a different problem – many algorithms can be considered as combinations of simpler pieces – but only to combining algorithms that solve the same problem, but differ in other characteristics, notably performance.","name":"Hybrid algorithm","categories":["Algorithms","All articles lacking sources","Articles lacking sources from May 2014"],"tag_line":"A hybrid algorithm is an algorithm that combines two or more other algorithms that solve the same problem, either choosing one (depending on the data), or switching between them over the course of the algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"medical-algorithm","_score":0,"_source":{"description":"A medical algorithm is any computation, formula, statistical survey, nomogram, or look-up table, useful in healthcare. Medical algorithms include decision tree approaches to healthcare treatment (e.g., if symptoms A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty.","name":"Medical algorithm","categories":["Algorithms","All articles that may contain original research","All articles to be expanded","Articles needing translation from Russian Wikipedia","Articles that may contain original research from October 2007","Articles to be expanded from September 2015","Health informatics","Knowledge representation"],"tag_line":"A medical algorithm is any computation, formula, statistical survey, nomogram, or look-up table, useful in healthcare."}}
,{"_index":"throwtable","_type":"algorithm","_id":"algorithmics","_score":0,"_source":{"description":"Algorithmics is the science of algorithms. It includes algorithm design, the art of building a procedure which can solve efficiently a specific problem or a class of problem, algorithmic complexity theory, the study of estimating the hardness of problems by studying the properties of algorithm that solves them, or algorithm analysis, the science of studying the properties of a problem, such as quantifying resources in time and memory space needed by this algorithm to solve this problem.","name":"Algorithmics","categories":["Algorithms"],"tag_line":"Algorithmics is the science of algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"smith–waterman-algorithm","_score":0,"_source":{"description":"The Smith–Waterman algorithm performs local sequence alignment; that is, for determining similar regions between two strings or nucleotide or protein sequences. Instead of looking at the total sequence, the Smith–Waterman algorithm compares segments of all possible lengths and optimizes the similarity measure.\nThe algorithm was first proposed by Temple F. Smith and Michael S. Waterman in 1981. Like the Needleman–Wunsch algorithm, of which it is a variation, Smith–Waterman is a dynamic programming algorithm. As such, it has the desirable property that it is guaranteed to find the optimal local alignment with respect to the scoring system being used (which includes the substitution matrix and the gap-scoring scheme). The main difference to the Needleman–Wunsch algorithm is that negative scoring matrix cells are set to zero, which renders the (thus positively scoring) local alignments visible. Backtracking starts at the highest scoring matrix cell and proceeds until a cell with score zero is encountered, yielding the highest scoring local alignment. One does not actually implement the algorithm as described because improved alternatives are now available that have better scaling (Gotoh, 1982)  and are more accurate (Altschul and Erickson, 1986).","name":"Smith–Waterman algorithm","categories":["All articles with dead external links","All articles with unsourced statements","Articles with dead external links from October 2010","Articles with unsourced statements from November 2011","Bioinformatics algorithms","CS1 errors: external links","Computational phylogenetics","Dynamic programming","Sequence alignment algorithms"],"tag_line":"The Smith–Waterman algorithm performs local sequence alignment; that is, for determining similar regions between two strings or nucleotide or protein sequences."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pseudocode","_score":0,"_source":{"description":"Pseudocode is an informal high-level description of the operating principle of a computer program or other algorithm.\nIt uses the structural conventions of a programming language, but is intended for human reading rather than machine reading. Pseudocode typically omits details that are essential for machine understanding of the algorithm, such as variable declarations, system-specific code and some subroutines. The programming language is augmented with natural language description details, where convenient, or with compact mathematical notation. The purpose of using pseudocode is that it is easier for people to understand than conventional programming language code, and that it is an efficient and environment-independent description of the key principles of an algorithm. It is commonly used in textbooks and scientific publications that are documenting various algorithms, and also in planning of computer program development, for sketching out the structure of the program before the actual coding takes place.\nNo standard for pseudocode syntax exists, as a program in pseudocode is not an executable program. Pseudocode resembles, but should not be confused with skeleton programs which can be compiled without errors. Flowcharts, drakon-charts and Unified Modeling Language (UML) charts can be thought of as a graphical alternative to pseudocode, but are more spacious on paper.","name":"Pseudocode","categories":["Algorithm description languages","All articles lacking sources","Articles lacking sources from October 2012","Articles with example pseudocode","Source code"],"tag_line":"Pseudocode is an informal high-level description of the operating principle of a computer program or other algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pidgin-code","_score":0,"_source":{"description":"In computer programming, pidgin code is a mixture of several programming languages in the same program, or pseudocode that is a mixture of a programming language with natural language descriptions. Hence the name: the mixture is a programming language analogous to a pidgin in natural languages.\nIn numerical computation, mathematical style pseudocode is sometimes called pidgin code, for example pidgin ALGOL (the origin of the concept), pidgin Fortran, pidgin BASIC, pidgin Pascal, and pidgin C. It is a compact and often informal notation that blends syntax taken from a conventional programming language with mathematical notation, typically using set theory and matrix operations, and perhaps also natural language descriptions.\nIt can be understood by a wide range of mathematically trained people, and is used as a way to describe algorithms where the control structure is made explicit at a rather high level of detail, while some data structures are still left at an abstract level, independent of any specific programming language.\nNormally non-ASCII typesetting is used for the mathematical equations, for example by means of TeX or MathML markup, or proprietary Formula editor formats.\nThese are examples of articles that contain mathematical style pseudo code:","name":"Pidgin code","categories":["Algorithm description languages","All articles lacking sources","All stub articles","Articles lacking sources from June 2009","Programming language topic stubs"],"tag_line":"In computer programming, pidgin code is a mixture of several programming languages in the same program, or pseudocode that is a mixture of a programming language with natural language descriptions."}}
,{"_index":"throwtable","_type":"algorithm","_id":"approximation-preserving-reduction","_score":0,"_source":{"description":"In computability theory and computational complexity theory, especially the study of approximation algorithms, an approximation-preserving reduction is an algorithm for transforming one optimization problem into another problem, such that the distance of solutions from optimal is preserved to some degree. Approximation-preserving reductions are a subset of more general reductions in complexity theory; the difference is that approximation-preserving reductions usually make statements on approximation problems or optimization problems, as opposed to decision problems.\nIntuitively, problem A is reducible to problem B via an approximation-preserving reduction if, given an instance of problem A and a (possibly approximate) solver for problem B, one can convert the instance of problem A into an instance of problem B, apply the solver for problem B, and recover a solution for problem A that also has some guarantee of approximation.","name":"Approximation-preserving reduction","categories":["Approximation algorithms","Computational complexity theory","Structural complexity theory"],"tag_line":"In computability theory and computational complexity theory, especially the study of approximation algorithms, an approximation-preserving reduction is an algorithm for transforming one optimization problem into another problem, such that the distance of solutions from optimal is preserved to some degree."}}
,{"_index":"throwtable","_type":"algorithm","_id":"polynomial-time-approximation-scheme","_score":0,"_source":{"description":"In computer science, a polynomial-time approximation scheme (PTAS) is a type of approximation algorithm for optimization problems (most often, NP-hard optimization problems).\nA PTAS is an algorithm which takes an instance of an optimization problem and a parameter ε > 0 and, in polynomial time, produces a solution that is within a factor 1 + ε of being optimal (or 1 - ε for maximization problems). For example, for the Euclidean traveling salesman problem, a PTAS would produce a tour with length at most (1 + ε)L, with L being the length of the shortest tour.\nThe running time of a PTAS is required to be polynomial in n for every fixed ε but can be different for different ε. Thus an algorithm running in time O(n1/ε) or even O(nexp(1/ε)) counts as a PTAS.","name":"Polynomial-time approximation scheme","categories":["Approximation algorithms","Complexity classes"],"tag_line":"In computer science, a polynomial-time approximation scheme (PTAS) is a type of approximation algorithm for optimization problems (most often, NP-hard optimization problems)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bidimensionality","_score":0,"_source":{"description":"Bidimensionality theory characterizes a broad range of graph problems (bidimensional) that admit efficient approximate, fixed-parameter or kernel solutions in a broad range of graphs. These graph classes include planar graphs, map graphs, bounded-genus graphs and graphs excluding any fixed minor. In particular, bidimensionality theory builds on the graph minor theory of Robertson and Seymour by extending the mathematical results and building new algorithmic tools. The theory was introduced in the work of Demaine, Fomin, Hajiaghayi, and Thilikos, for which the authors received the Nerode Prize in 2015.","name":"Bidimensionality","categories":["Analysis of algorithms","Approximation algorithms","Graph minor theory","Parameterized complexity"],"tag_line":"Bidimensionality theory characterizes a broad range of graph problems (bidimensional) that admit efficient approximate, fixed-parameter or kernel solutions in a broad range of graphs."}}
,{"_index":"throwtable","_type":"algorithm","_id":"chromosome-(genetic-algorithm)","_score":0,"_source":{"description":"In genetic algorithms, a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution to the problem that the genetic algorithm is trying to solve. The set of all solutions is known as the population. The chromosome is often represented as a binary string, although a wide variety of other data structures are also used.\n\n","name":"Chromosome (genetic algorithm)","categories":["Genetic algorithms","Pages using citations with accessdate and no URL"],"tag_line":"In genetic algorithms, a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution to the problem that the genetic algorithm is trying to solve."}}
,{"_index":"throwtable","_type":"algorithm","_id":"christofides-algorithm","_score":0,"_source":{"description":"The goal of the Christofides approximation algorithm (named after Nicos Christofides) is to find a solution to the instances of the traveling salesman problem where the edge weights satisfy the triangle inequality. Let  be an instance of TSP, i.e.  is a complete graph on the set  of vertices with weight function  assigning a nonnegative real weight to every edge of .","name":"Christofides algorithm","categories":["Approximation algorithms","Graph algorithms","Spanning tree","Travelling salesman problem"],"tag_line":"The goal of the Christofides approximation algorithm (named after Nicos Christofides) is to find a solution to the instances of the traveling salesman problem where the edge weights satisfy the triangle inequality."}}
,{"_index":"throwtable","_type":"algorithm","_id":"neuroevolution-of-augmenting-topologies","_score":0,"_source":{"description":"NeuroEvolution of Augmenting Topologies (NEAT) is a genetic algorithm for the generation of evolving artificial neural networks (a neuroevolution technique) developed by Ken Stanley in 2002 while at The University of Texas at Austin. It alters both the weighting parameters and structures of networks, attempting to find a balance between the fitness of evolved solutions and their diversity. It is based on applying three key techniques: tracking genes with history markers to allow crossover among topologies, applying speciation (the evolution of species) to preserve innovations, and developing topologies incrementally from simple initial structures (\"complexifying\").\n\n","name":"Neuroevolution of augmenting topologies","categories":["Artificial neural networks","Evolutionary algorithms","Evolutionary computation","Genetic algorithms"],"tag_line":"NeuroEvolution of Augmenting Topologies (NEAT) is a genetic algorithm for the generation of evolving artificial neural networks (a neuroevolution technique) developed by Ken Stanley in 2002 while at The University of Texas at Austin."}}
,{"_index":"throwtable","_type":"algorithm","_id":"genetic-programming","_score":0,"_source":{"description":"In artificial intelligence, genetic programming (GP) is an evolutionary algorithm-based methodology inspired by biological evolution to find computer programs that perform a user-defined task. Essentially GP is a set of instructions and a fitness function to measure how well a computer has performed a task. It is a specialization of genetic algorithms (GA) where each individual is a computer program. It is a machine learning technique used to optimize a population of computer programs according to a fitness landscape determined by a program's ability to perform a given computational task.","name":"Genetic programming","categories":["All articles with unsourced statements","Articles with unsourced statements from December 2014","CS1 Italian-language sources (it)","Evolutionary algorithms","Genetic algorithms","Genetic programming","Mathematical optimization","Operations research"],"tag_line":"In artificial intelligence, genetic programming (GP) is an evolutionary algorithm-based methodology inspired by biological evolution to find computer programs that perform a user-defined task."}}
,{"_index":"throwtable","_type":"algorithm","_id":"calendrical-calculation","_score":0,"_source":{"description":"A calendrical calculation is a calculation concerning calendar dates. Calendrical calculations can be considered an area of applied mathematics. Calendrical calculation is one of the five major Savant syndrome characteristics. Some examples of calendrical calculations:\nConverting a Julian or Gregorian calendar date to its Julian day number and vice versa (see the section on calculation in that article for details).\nThe number of days between two dates, which is simply the difference in their Julian day numbers.\nThe date of a religious holiday, like Easter (the calculation is known as Computus) or Passover, for a given year.\nConverting a date between different calendars. For instance, dates in the Gregorian calendar can be converted to dates in the Islamic calendar with the Kuwaiti algorithm.\nCalculating the day of the week.","name":"Calendrical calculation","categories":["All stub articles","Applied mathematics stubs","Calendar algorithms"],"tag_line":"A calendrical calculation is a calculation concerning calendar dates."}}
,{"_index":"throwtable","_type":"algorithm","_id":"defining-length","_score":0,"_source":{"description":"In genetic algorithms and genetic programming defining length L(H) is the maximum distance between two defining symbols (that is symbols that have a fixed value as opposed to symbols that can take any value, commonly denoted as # or *) in schema H. In tree GP schemata, L(H) is the number of links in the minimum tree fragment including all the non-= symbols within a schema H.","name":"Defining length","categories":["All articles needing expert attention","All articles that are too technical","All stub articles","Articles needing expert attention from August 2011","Computer science stubs","Genetic algorithms","Use dmy dates from August 2011","Wikipedia articles that are too technical from August 2011"],"tag_line":"In genetic algorithms and genetic programming defining length L(H) is the maximum distance between two defining symbols (that is symbols that have a fixed value as opposed to symbols that can take any value, commonly denoted as # or *) in schema H. In tree GP schemata, L(H) is the number of links in the minimum tree fragment including all the non-= symbols within a schema H."}}
,{"_index":"throwtable","_type":"algorithm","_id":"holland's-schema-theorem","_score":0,"_source":{"description":"Holland's schema theorem, also called the fundamental theorem of genetic algorithms, is widely taken to be the foundation for explanations of the power of genetic algorithms. It says that short, low-order schemata with above-average fitness increase exponentially in successive generations. The theorem was proposed by John Holland in the 1970s.\nA schema is a template that identifies a subset of strings with similarities at certain string positions. Schemata are a special case of cylinder sets, and hence form a topological space.","name":"Holland's schema theorem","categories":["Genetic algorithms","Theorems in discrete mathematics"],"tag_line":"Holland's schema theorem, also called the fundamental theorem of genetic algorithms, is widely taken to be the foundation for explanations of the power of genetic algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"premature-convergence","_score":0,"_source":{"description":"In genetic algorithms, the term of premature convergence means that a population for an optimization problem converged too early, resulting in being suboptimal. In this context, the parental solutions, through the aid of genetic operators, are not able to generate offsprings that are superior to their parents. Premature convergence can happen in case of loss of genetic variation (every individual in the population is identical, see convergence).","name":"Premature convergence","categories":["All stub articles","Evolution stubs","Evolutionary biology","Genetic algorithms"],"tag_line":"In genetic algorithms, the term of premature convergence means that a population for an optimization problem converged too early, resulting in being suboptimal."}}
,{"_index":"throwtable","_type":"algorithm","_id":"population-based-incremental-learning","_score":0,"_source":{"description":"In computer science and machine learning, population-based incremental learning (PBIL) is an optimization algorithm, and an estimation of distribution algorithm. This is a type of genetic algorithm where the genotype of an entire population (probability vector) is evolved rather than individual members. The algorithm is proposed by Shumeet Baluja in 1994. The algorithm is simpler than a standard genetic algorithm, and in many cases leads to better results than a standard genetic algorithm.","name":"Population-based incremental learning","categories":["Articles with example Java code","Genetic algorithms"],"tag_line":"In computer science and machine learning, population-based incremental learning (PBIL) is an optimization algorithm, and an estimation of distribution algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fletcher's-checksum","_score":0,"_source":{"description":"The Fletcher checksum is an algorithm for computing a position-dependent checksum devised by John G. Fletcher (1934-2012) at Lawrence Livermore Labs in the late 1970s. The objective of the Fletcher checksum was to provide error-detection properties approaching those of a cyclic redundancy check but with the lower computational effort associated with summation techniques.","name":"Fletcher's checksum","categories":["All articles with unsourced statements","Articles with unsourced statements from May 2012","Checksum algorithms"],"tag_line":"The Fletcher checksum is an algorithm for computing a position-dependent checksum devised by John G. Fletcher (1934-2012) at Lawrence Livermore Labs in the late 1970s."}}
,{"_index":"throwtable","_type":"algorithm","_id":"job-shop-scheduling","_score":0,"_source":{"description":"Job shop scheduling (or job-shop problem) is an optimization problem in computer science and operations research in which ideal jobs are assigned to resources at particular times. The most basic version is as follows:\nWe are given n jobs J1, J2, ..., Jn of varying sizes, which need to be scheduled on m identical machines, while trying to minimize the makespan. The makespan is the total length of the schedule (that is, when all the jobs have finished processing). Nowadays, the problem is presented as an online problem (dynamic scheduling), that is, each job is presented, and the online algorithm needs to make a decision about that job before the next job is presented.\nThis problem is one of the best known online problems, and was the first problem for which competitive analysis was presented, by Graham in 1966. Best problem instances for basic model with makespan objective are due to Taillard.","name":"Job shop scheduling","categories":["Combinatorial optimization","Mathematical optimization","Operations research","Optimization algorithms and methods","Pages using citations with accessdate and no URL","Wikipedia articles needing context from October 2009"],"tag_line":"Job shop scheduling (or job-shop problem) is an optimization problem in computer science and operations research in which ideal jobs are assigned to resources at particular times."}}
,{"_index":"throwtable","_type":"algorithm","_id":"robinson–schensted–knuth-correspondence","_score":0,"_source":{"description":"In mathematics, the Robinson–Schensted–Knuth correspondence, also referred to as the RSK correspondence or RSK algorithm, is a combinatorial bijection between matrices A with non-negative integer entries and pairs (P,Q) of semistandard Young tableaux of equal shape, whose size equals the sum of the entries of A. More precisely the weight of P is given by the column sums of A, and the weight of Q by its row sums. It is a generalization of the Robinson–Schensted correspondence, in the sense that taking A to be a permutation matrix, the pair (P,Q) will be the pair of standard tableaux associated to the permutation under the Robinson–Schensted correspondence.\nThe Robinson–Schensted–Knuth correspondence extends many of the remarkable properties of the Robinson–Schensted correspondence, notably its symmetry: transposition of the matrix A results in interchange of the tableaux P,Q.","name":"Robinson–Schensted–Knuth correspondence","categories":["Algebraic combinatorics","Combinatorial algorithms","Permutations","Symmetric functions"],"tag_line":"In mathematics, the Robinson–Schensted–Knuth correspondence, also referred to as the RSK correspondence or RSK algorithm, is a combinatorial bijection between matrices A with non-negative integer entries and pairs (P,Q) of semistandard Young tableaux of equal shape, whose size equals the sum of the entries of A."}}
,{"_index":"throwtable","_type":"algorithm","_id":"breadth-first-search","_score":0,"_source":{"description":"Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key') and explores the neighbor nodes first, before moving to the next level neighbors.\nBFS was invented in the late 1950s by E. F. Moore, who used it to find the shortest path out of a maze, and discovered independently by C. Y. Lee as a wire routing algorithm (published 1961).","name":"Breadth-first search","categories":["All articles needing additional references","Articles needing additional references from April 2012","Commons category with local link same as on Wikidata","Graph algorithms","Search algorithms"],"tag_line":"Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lin–kernighan-heuristic","_score":0,"_source":{"description":"This article is about the heuristic for the travelling salesman problem. For a heuristic algorithm for the graph partitioning problem, see Kernighan–Lin algorithm.\nIn combinatorial optimization, Lin–Kernighan is one of the best heuristics for solving the travelling salesman problem. Briefly, it involves swapping pairs of sub-tours to make a new tour. It is a generalization of 2-opt and 3-opt. 2-opt and 3-opt work by switching two or three paths to make the tour shorter. Lin–Kernighan is adaptive and at each step decides how many paths between cities need to be switched to find a shorter tour.","name":"Lin–Kernighan heuristic","categories":["Algorithms and data structures stubs","All stub articles","Applied mathematics stubs","Combinatorial algorithms","Combinatorial optimization","Computer science stubs","Heuristic algorithms","Travelling salesman problem"],"tag_line":"This article is about the heuristic for the travelling salesman problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"loopless-algorithm","_score":0,"_source":{"description":"In computational combinatorics, a loopless algorithm or loopless imperative algorithm is an imperative algorithm that generates successive combinatorial objects, such as partitions, permutations, and combinations, in constant time and the first object in linear time. The objects must be immediately available in simple form without requiring any additional steps.\nA loopless functional algorithm is a functional algorithm takes the form unfoldr step • prolog where step takes constant time and prolog takes linear time in the size of the input. The standard function unfoldr is a right-associative Bird unfold.","name":"Loopless algorithm","categories":["All stub articles","Combinatorial algorithms","Combinatorics stubs"],"tag_line":"In computational combinatorics, a loopless algorithm or loopless imperative algorithm is an imperative algorithm that generates successive combinatorial objects, such as partitions, permutations, and combinations, in constant time and the first object in linear time."}}
,{"_index":"throwtable","_type":"algorithm","_id":"chaitin's-algorithm","_score":0,"_source":{"description":"Chaitin's algorithm is a bottom-up, graph coloring register allocation algorithm that uses cost/degree as its spill metric. It is named after its designer, Gregory Chaitin. Chaitin's algorithm was the first register allocation algorithm that made use of coloring of the interference graph for both register allocations and spilling.\nChaitin's algorithm was presented on the 1982 SIGPLAN Symposium on Compiler Construction, and published in the symposium proceedings. It was extension of an earlier 1981 paper on the use of graph coloring for register allocation. Chaitin's algorithm formed the basis of a large section of research into register allocators.","name":"Chaitin's algorithm","categories":["Graph algorithms"],"tag_line":"Chaitin's algorithm is a bottom-up, graph coloring register allocation algorithm that uses cost/degree as its spill metric."}}
,{"_index":"throwtable","_type":"algorithm","_id":"iso-7064","_score":0,"_source":{"description":"ISO 7064 define algorithms for calculating check digit characters.","name":"ISO 7064","categories":["All stub articles","Checksum algorithms","Computing stubs","Error detection and correction","ISO standards"],"tag_line":"ISO 7064 define algorithms for calculating check digit characters."}}
,{"_index":"throwtable","_type":"algorithm","_id":"d*","_score":0,"_source":{"description":"D* (pronounced \"D star\") is any one of the following three related incremental search algorithms:\nThe original D*, by Anthony Stentz, is an informed incremental search algorithm.\nFocused D* is an informed incremental heuristic search algorithm by Anthony Stentz that combines ideas of A* and the original D*. Focused D* resulted from a further development of the original D*.\nD* Lite is an incremental heuristic search algorithm by Sven Koenig and Maxim Likhachev that builds on LPA*, an incremental heuristic search algorithm that combines ideas of A* and Dynamic SWSF-FP.\nAll three search algorithms solve the same assumption-based path planning problems, including planning with the freespace assumption, where a robot has to navigate to given goal coordinates in unknown terrain. It makes assumptions about the unknown part of the terrain (for example: that it contains no obstacles) and finds a shortest path from its current coordinates to the goal coordinates under these assumptions. The robot then follows the path. When it observes new map information (such as previously unknown obstacles), it adds the information to its map and, if necessary, replans a new shortest path from its current coordinates to the given goal coordinates. It repeats the process until it reaches the goal coordinates or determines that the goal coordinates cannot be reached. When traversing unknown terrain, new obstacles may be discovered frequently, so this replanning needs to be fast. Incremental (heuristic) search algorithms speed up searches for sequences of similar search problems by using experience with the previous problems to speed up the search for the current one. Assuming the goal coordinates do not change, all three search algorithms are more efficient than repeated A* searches.\nD* and its variants have been widely used for mobile robot and autonomous vehicle navigation. Current systems are typically based on D* Lite rather than the original D* or Focused D*. In fact, even Stentz's lab uses D* Lite rather than D* in some implementations. Such navigation systems include a prototype system tested on the Mars rovers Opportunity and Spirit and the navigation system of the winning entry in the DARPA Urban Challenge, both developed at Carnegie Mellon University.\nThe original D* was introduced by Anthony Stentz in 1994. The name D* comes from the term \"Dynamic A*\", because the algorithm behaves like A* except that the arc costs can change as the algorithm runs.","name":"D*","categories":["Graph algorithms","Robot control","Search algorithms"],"tag_line":"D* (pronounced \"D star\") is any one of the following three related incremental search algorithms:\nThe original D*, by Anthony Stentz, is an informed incremental search algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"push–relabel-maximum-flow-algorithm","_score":0,"_source":{"description":"In mathematical optimization, the push–relabel algorithm (alternatively, preflow–push algorithm) is an algorithm for computing maximum flows. The name \"push–relabel\" comes from the two basic operations used in the algorithm. Throughout its execution, the algorithm maintains a \"preflow\" and gradually converts it into a maximum flow by moving flow locally between neighboring vertices using push operations under the guidance of an admissible network maintained by relabel operations. In comparison, the Ford–Fulkerson algorithm performs global augmentations that send flow following paths from the source all the way to the sink.\nThe push–relabel algorithm is considered one of the most efficient maximum flow algorithms. The generic algorithm has a strongly polynomial O(V2E) time complexity, which is asymptotically more efficient than the O(VE2) Edmonds–Karp algorithm. Specific variants of the algorithms achieve even lower time complexities. The variant based on the highest label vertex selection rule has O(V2√E) time complexity and is generally regarded as the benchmark for maximum flow algorithms. Subcubic O(VE log (V2/E)) time complexity can be achieved using dynamic trees, although in practice it is less efficient.\nThe push–relabel algorithm has been extended to compute minimum cost flows. The idea of distance labels has led to a more efficient augmenting path algorithm, which in turn can be incorporated back into the push–relabel algorithm to create a variant with even higher empirical performance.","name":"Push–relabel maximum flow algorithm","categories":["Graph algorithms","Network flow"],"tag_line":"In mathematical optimization, the push–relabel algorithm (alternatively, preflow–push algorithm) is an algorithm for computing maximum flows."}}
,{"_index":"throwtable","_type":"algorithm","_id":"iterative-deepening-a*","_score":0,"_source":{"description":"Iterative deepening A* (IDA*) is a graph traversal and path search algorithm that can find the shortest path between a designated start node and any member of a set of goal nodes in a weighted graph. It is a variant of iterative deepening depth-first search that borrows the idea to use a heuristic function to evaluate the remaining cost to get to the goal from the A* search algorithm. Since it is a depth-first search algorithm, its memory usage is lower than in A*, but unlike ordinary iterative deepening search, it concentrates on exploring the most promising nodes and thus doesn't go to the same depth everywhere in the search tree. Unlike A*, IDA* doesn't utilize dynamic programming and therefore often ends up exploring the same nodes many times.\nWhile the standard iterative deepening depth-first search uses search depth as the cutoff for each iteration, the IDA* uses the more informative  where  is the cost to travel from the root to node  and  is a problem-specific heuristic estimate of the cost to travel from  to the solution. As in A*, the heuristic has to have particular properties to guarantee optimality (shortest paths); see Properties, below.\nApplications of IDA* are found in such problems as planning. The algorithm was first described by Richard Korf in 1985.","name":"Iterative deepening A*","categories":["All articles needing expert attention","All articles that are too technical","Articles needing expert attention from November 2009","Articles with example pseudocode","Game artificial intelligence","Graph algorithms","Routing algorithms","Search algorithms","Wikipedia articles that are too technical from November 2009"],"tag_line":"Iterative deepening A* (IDA*) is a graph traversal and path search algorithm that can find the shortest path between a designated start node and any member of a set of goal nodes in a weighted graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"tarjan's-off-line-lowest-common-ancestors-algorithm","_score":0,"_source":{"description":"In computer science, Tarjan's off-line lowest common ancestors algorithm is an algorithm for computing lowest common ancestors for pairs of nodes in a tree, based on the union-find data structure. The lowest common ancestor of two nodes d and e in a rooted tree T is the node g that is an ancestor of both d and e and that has the greatest depth in T. It is named after Robert Tarjan, who discovered the technique in 1979. Tarjan's algorithm is an offline algorithm; that is, unlike other lowest common ancestor algorithms, it requires that all pairs of nodes for which the lowest common ancestor is desired must be specified in advance. The simplest version of the algorithm uses the union-find data structure, which unlike other lowest common ancestor data structures can take more than constant time per operation when the number of pairs of nodes is similar in magnitude to the number of nodes. A later refinement by Gabow & Tarjan (1983) speeds the algorithm up to linear time.\n\n","name":"Tarjan's off-line lowest common ancestors algorithm","categories":["Graph algorithms"],"tag_line":"In computer science, Tarjan's off-line lowest common ancestors algorithm is an algorithm for computing lowest common ancestors for pairs of nodes in a tree, based on the union-find data structure."}}
,{"_index":"throwtable","_type":"algorithm","_id":"suurballe's-algorithm","_score":0,"_source":{"description":"In theoretical computer science and network routing, Suurballe's algorithm is an algorithm for finding two disjoint paths in a nonnegatively-weighted directed graph, so that both paths connect the same pair of vertices and have minimum total length. The algorithm was conceived by John W. Suurballe and published in 1974. The main idea of Suurballe's algorithm is to use Dijkstra's algorithm to find one path, to modify the weights of the graph edges, and then to run Dijkstra's algorithm a second time. The modification to the weights is similar to the weight modification in Johnson's algorithm, and preserves the non-negativity of the weights while allowing the second instance of Dijkstra's algorithm to find the correct second path.\nThe objective is strongly related to that of minimum cost flow algorithms, where in this case there are two units of \"flow\" and nodes have unit \"capacity\".","name":"Suurballe's algorithm","categories":["Graph algorithms","Routing algorithms"],"tag_line":"In theoretical computer science and network routing, Suurballe's algorithm is an algorithm for finding two disjoint paths in a nonnegatively-weighted directed graph, so that both paths connect the same pair of vertices and have minimum total length."}}
,{"_index":"throwtable","_type":"algorithm","_id":"force-directed-graph-drawing","_score":0,"_source":{"description":"Force-directed graph drawing algorithms are a class of algorithms for drawing graphs in an aesthetically pleasing way. Their purpose is to position the nodes of a graph in two-dimensional or three-dimensional space so that all the edges are of more or less equal length and there are as few crossing edges as possible, by assigning forces among the set of edges and the set of nodes, based on their relative positions, and then using these forces either to simulate the motion of the edges and nodes or to minimize their energy.\nWhile graph drawing can be a difficult problem, force-directed algorithms, being physical simulations, usually require no special knowledge about graph theory such as planarity.","name":"Force-directed graph drawing","categories":["Articles with example pseudocode","Graph algorithms","Graph drawing","Pages containing cite templates with deprecated parameters"],"tag_line":"Force-directed graph drawing algorithms are a class of algorithms for drawing graphs in an aesthetically pleasing way."}}
,{"_index":"throwtable","_type":"algorithm","_id":"havel–hakimi-algorithm","_score":0,"_source":{"description":"The Havel–Hakimi algorithm is an algorithm in graph theory solving the graph realization problem, i.e. the question if there exists for a finite list of nonnegative integers a simple graph such that its degree sequence is exactly this list. For a positive answer the list of integers is called graphic. The algorithm constructs a special solution if one exists or proves that one cannot find a positive answer. This construction is based on a recursive algorithm. The algorithm was published by Havel (1955), and later by Hakimi (1962).","name":"Havel–Hakimi algorithm","categories":["CS1 Czech-language sources (cs)","Graph algorithms"],"tag_line":"The Havel–Hakimi algorithm is an algorithm in graph theory solving the graph realization problem, i.e."}}
,{"_index":"throwtable","_type":"algorithm","_id":"junction-tree-algorithm","_score":0,"_source":{"description":"The junction tree algorithm (also known as 'Clique Tree') is a method used in machine learning to extract marginalization in general graphs. In essence, it entails performing belief propagation on a modified graph called a junction tree. The basic premise is to eliminate cycles by clustering them into single nodes.","name":"Junction tree algorithm","categories":["All articles to be expanded","All articles with empty sections","All stub articles","Articles to be expanded from November 2010","Articles with empty sections from November 2010","Artificial intelligence stubs","Bayesian networks","Graph algorithms"],"tag_line":"The junction tree algorithm (also known as 'Clique Tree') is a method used in machine learning to extract marginalization in general graphs."}}
,{"_index":"throwtable","_type":"algorithm","_id":"courcelle's-theorem","_score":0,"_source":{"description":"In the study of graph algorithms, Courcelle's theorem is the statement that every graph property definable in the monadic second-order logic of graphs can be decided in linear time on graphs of bounded treewidth. The result was first proved by Bruno Courcelle in 1990 and independently rediscovered by Borie, Parker & Tovey (1992). It is considered the archetype of algorithmic meta-theorems.","name":"Courcelle's theorem","categories":["Graph algorithms","Graph minor theory","Metatheorems"],"tag_line":"In the study of graph algorithms, Courcelle's theorem is the statement that every graph property definable in the monadic second-order logic of graphs can be decided in linear time on graphs of bounded treewidth."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quicksort","_score":0,"_source":{"description":"Quicksort (sometimes called partition-exchange sort) is an efficient sorting algorithm, serving as a systematic method for placing the elements of an array in order. Developed by Tony Hoare in 1959, with his work published in 1961, it is still a commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than its main competitors, merge sort and heapsort.\nQuicksort is a comparison sort, meaning that it can sort items of any type for which a \"less-than\" relation (formally, a total order) is defined. In efficient implementations it is not a stable sort, meaning that the relative order of equal sort items is not preserved. Quicksort can operate in-place on an array, requiring small additional amounts of memory to perform the sorting.\nMathematical analysis of quicksort shows that, on average, the algorithm takes O(n log n) comparisons to sort n items. In the worst case, it makes O(n2) comparisons, though this behavior is rare.","name":"Quicksort","categories":["1961 in science","Accuracy disputes from August 2015","Accuracy disputes from July 2015","Articles with example pseudocode","Comparison sorts","Pages with DOIs inactive since 2015","Sorting algorithms","Use dmy dates from January 2012"],"tag_line":"Quicksort (sometimes called partition-exchange sort) is an efficient sorting algorithm, serving as a systematic method for placing the elements of an array in order."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pigeonhole-sort","_score":0,"_source":{"description":"Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) and the number of possible key values (N) are approximately the same. It requires O(n + N) time. It is similar to counting sort), but \"Pigeonhole sort moves items twice: once to the bucket array and again to the final destination [whereas] counting sort builds an auxiliary array then uses the array to compute each item's final destination and move the item there.\"\nThe pigeonhole algorithm works as follows:\nGiven an array of values to be sorted, set up an auxiliary array of initially empty \"pigeonholes,\" one pigeonhole for each key through the range of the original array.\nGoing over the original array, put each value into the pigeonhole corresponding to its key, such that each pigeonhole eventually contains a list of all values with that key.\nIterate over the pigeonhole array in order, and put elements from non-empty pigeonholes back into the original array.","name":"Pigeonhole sort","categories":["Sorting algorithms","Stable sorts"],"tag_line":"Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) and the number of possible key values (N) are approximately the same."}}
,{"_index":"throwtable","_type":"algorithm","_id":"7z","_score":0,"_source":{"description":"7z is a compressed archive file format that supports several different data compression, encryption and pre-processing algorithms. The 7z format initially appeared as implemented by the 7-Zip archiver. The 7-Zip program is publicly available under the terms of the GNU Lesser General Public License. The LZMA SDK 4.62 was placed in the public domain in December 2008. The latest stable version of 7-Zip and LZMA SDK is version 15.12.\nThe official 7z file format specification is distributed with 7-Zip's source code. The specification can be found in plain text format in the 'doc' sub-directory of the source code distribution.","name":"7z","categories":["1999 introductions","All articles with unsourced statements","Archive formats","Articles with unsourced statements from June 2014","Lossless compression algorithms","Russian inventions","Wikipedia articles needing clarification from October 2015"],"tag_line":"7z is a compressed archive file format that supports several different data compression, encryption and pre-processing algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"elevator-algorithm","_score":0,"_source":{"description":"The elevator algorithm (also SCAN) is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\nThis algorithm is named after the behavior of a building elevator, where the elevator continues to travel in its current direction (up or down) until empty, stopping only to let individuals off or to pick up new individuals heading in the same direction.\nFrom an implementation perspective, the drive maintains a buffer of pending read/write requests, along with the associated cylinder number of the request. Lower cylinder numbers indicate that the cylinder is closest to the spindle, and higher numbers indicate the cylinder is farther away.","name":"Elevator algorithm","categories":["All articles needing additional references","Articles needing additional references from November 2007","Disk scheduling algorithms","Sorting algorithms"],"tag_line":"The elevator algorithm (also SCAN) is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests."}}
,{"_index":"throwtable","_type":"algorithm","_id":"algorithm-bstw","_score":0,"_source":{"description":"The Algorithm BSTW is a data compression algorithm, named after its designers, Bentley, Sleator, Tarjan and Wei in 1986. BSTW is a dictionary-based algorithm that uses a move-to-front transform to keep recently seen dictionary entries at the front of the dictionary. Dictionary references are then encoded using any of a number of encoding methods, usually Elias delta coding or Elias gamma coding.","name":"Algorithm BSTW","categories":["Algorithms and data structures stubs","All articles needing additional references","All stub articles","Articles needing additional references from May 2008","Computer science stubs","Lossless compression algorithms"],"tag_line":"The Algorithm BSTW is a data compression algorithm, named after its designers, Bentley, Sleator, Tarjan and Wei in 1986."}}
,{"_index":"throwtable","_type":"algorithm","_id":"smoothsort","_score":0,"_source":{"description":"Smoothsort is a comparison-based sorting algorithm. It is a variation of heapsort developed by Edsger Dijkstra in 1981. Like heapsort, smoothsort is an in-place algorithm with an upper bound of O(n log n), but it is not a stable sort. The advantage of smoothsort is that it comes closer to O(n) time if the input is already sorted to some degree, whereas heapsort averages O(n log n) regardless of the initial sorted state.","name":"Smoothsort","categories":["Articles with example Java code","Comparison sorts","Dutch inventions","Heaps (data structures)","Sorting algorithms"],"tag_line":"Smoothsort is a comparison-based sorting algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"tournament-sort","_score":0,"_source":{"description":"Tournament sort is a sorting algorithm. It improves upon the naive selection sort by using a priority queue to find the next element in the sort. In the naive selection sort, it takes O(n) operations to select the next element of n elements; in a tournament sort, it takes O(log n) operations (after building the initial tournament in O(n)). Tournament sort is a variation of heapsort.","name":"Tournament sort","categories":["All articles needing additional references","Articles needing additional references from July 2012","Sorting algorithms"],"tag_line":"Tournament sort is a sorting algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lz77-and-lz78","_score":0,"_source":{"description":"LZ77 and LZ78 are the two lossless data compression algorithms published in papers by Abraham Lempel and Jacob Ziv in 1977 and 1978. They are also known as LZ1 and LZ2 respectively. These two algorithms form the basis for many variations including LZW, LZSS, LZMA and others. Besides their academic influence, these algorithms formed the basis of several ubiquitous compression schemes, including GIF and the DEFLATE algorithm used in PNG.\nThey are both theoretically dictionary coders. LZ77 maintains a sliding window during compression. This was later shown to be equivalent to the explicit dictionary constructed by LZ78—however, they are only equivalent when the entire data is intended to be decompressed. LZ78 decompression allows random access to the input as long as the entire dictionary is available, while LZ77 decompression must always start at the beginning of the input.\nThe algorithms were named an IEEE Milestone in 2004.","name":"LZ77 and LZ78","categories":["All accuracy disputes","All articles containing potentially dated statements","Articles containing potentially dated statements from 2008","Articles with disputed statements from November 2010","Lossless compression algorithms","Use dmy dates from August 2012"],"tag_line":"LZ77 and LZ78 are the two lossless data compression algorithms published in papers by Abraham Lempel and Jacob Ziv in 1977 and 1978."}}
,{"_index":"throwtable","_type":"algorithm","_id":"incremental-encoding","_score":0,"_source":{"description":"Incremental encoding, also known as front compression, back compression, or front coding, is a type of delta encoding compression algorithm whereby common prefixes or suffixes and their lengths are recorded so that they need not be duplicated. This algorithm is particularly well-suited for compressing sorted data, e.g., a list of words from a dictionary.\nFor example:\nThe encoding used to store the common prefix length itself varies from application to application. Typical techniques are storing the value as a single byte; delta encoding, which stores only the change in the common prefix length; and various universal codes. It may be combined with other general lossless data compression techniques such as entropy encoding and dictionary coders to compress the remaining suffixes.","name":"Incremental encoding","categories":["All stub articles","Database index techniques","Lossless compression algorithms","Storage software stubs"],"tag_line":"Incremental encoding, also known as front compression, back compression, or front coding, is a type of delta encoding compression algorithm whereby common prefixes or suffixes and their lengths are recorded so that they need not be duplicated."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dynamic-markov-compression","_score":0,"_source":{"description":"Dynamic Markov compression (DMC) is a lossless data compression algorithm developed by Gordon Cormack and Nigel Horspool. It uses predictive arithmetic coding similar to prediction by partial matching (PPM), except that the input is predicted one bit at a time (rather than one byte at a time). DMC has a good compression ratio and moderate speed, similar to PPM, but requires somewhat more memory and is not widely implemented. Some recent implementations include the experimental compression programs hook by Nania Francesco Antonio, ocamyd by Frank Schwellinger, and as a submodel in paq8l by Matt Mahoney. These are based on the 1993 implementation in C by Gordon Cormack.","name":"Dynamic Markov compression","categories":["Lossless compression algorithms","Markov models"],"tag_line":"Dynamic Markov compression (DMC) is a lossless data compression algorithm developed by Gordon Cormack and Nigel Horspool."}}
,{"_index":"throwtable","_type":"algorithm","_id":"burrows–wheeler-transform","_score":0,"_source":{"description":"The Burrows–Wheeler transform (BWT, also called block-sorting compression) rearranges a character string into runs of similar characters. This is useful for compression, since it tends to be easy to compress a string that has runs of repeated characters by techniques such as move-to-front transform and run-length encoding. More importantly, the transformation is reversible, without needing to store any additional data. The BWT is thus a \"free\" method of improving the efficiency of text compression algorithms, costing only some extra computation.","name":"Burrows–Wheeler transform","categories":["Articles with example Python code","Articles with example pseudocode","Lossless compression algorithms","Transforms"],"tag_line":"The Burrows–Wheeler transform (BWT, also called block-sorting compression) rearranges a character string into runs of similar characters."}}
,{"_index":"throwtable","_type":"algorithm","_id":"zopfli","_score":0,"_source":{"description":"Zopfli is a data compression algorithm that encodes data into DEFLATE, gzip and zlib formats. Zopfli is regarded as the most size-efficient DEFLATE encoder available. In February 2013, a reference implementation of the Zopfli algorithm was released by Google as a free software programming library under the Apache License, Version 2.0. The name Zöpfli is the Swiss German diminutive of “Zopf”, a special type of Hefezopf.","name":"Zopfli","categories":["All articles needing additional references","Articles needing additional references from September 2015","Free computer libraries","Lossless compression algorithms"],"tag_line":"Zopfli is a data compression algorithm that encodes data into DEFLATE, gzip and zlib formats."}}
,{"_index":"throwtable","_type":"algorithm","_id":"liblzg","_score":0,"_source":{"description":"liblzg is a compression library for performing lossless data compression. It implements an algorithm that is a variation of the LZ77 algorithm, called the LZG algorithm, with the primary focus of providing a very simple and fast decoding method. One of the key features of the algorithm is that it requires no memory during decompression. The software library is free software, distributed under the zlib license.","name":"Liblzg","categories":["All articles lacking reliable references","All articles with topics of unclear notability","Articles lacking reliable references from March 2015","Articles with topics of unclear notability from March 2015","Free data compression software","Lossless compression algorithms","Software using the zlib license"],"tag_line":"liblzg is a compression library for performing lossless data compression."}}
,{"_index":"throwtable","_type":"algorithm","_id":"felics","_score":0,"_source":{"description":"FELICS, which stands for Fast Efficient & Lossless Image Compression System, is a lossless image compression algorithm that performs 5-times faster than the original lossless JPEG codec and achieves a similar compression ratio.","name":"FELICS","categories":["All articles needing additional references","Articles needing additional references from August 2011","Lossless compression algorithms","Lossy compression algorithms","Use dmy dates from July 2013"],"tag_line":"FELICS, which stands for Fast Efficient & Lossless Image Compression System, is a lossless image compression algorithm that performs 5-times faster than the original lossless JPEG codec and achieves a similar compression ratio."}}
,{"_index":"throwtable","_type":"algorithm","_id":"3dc","_score":0,"_source":{"description":"3Dc (FourCC : ATI2), also known as DXN, BC5, or Block Compression 5 is a lossy data compression algorithm for normal maps invented and first implemented by ATI. It builds upon the earlier DXT5 algorithm and is an open standard. 3Dc is now implemented by both ATI and Nvidia.","name":"3Dc","categories":["3D graphics software","Lossy compression algorithms","Open formats","Texture compression"],"tag_line":"3Dc (FourCC : ATI2), also known as DXN, BC5, or Block Compression 5 is a lossy data compression algorithm for normal maps invented and first implemented by ATI."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lzx-(algorithm)","_score":0,"_source":{"description":"LZX is the name of an LZ77 family compression algorithm. It is also the name of a file archiver with the same name. Both were invented by Jonathan Forbes and Tomi Poutanen.","name":"LZX (algorithm)","categories":["Amiga","Lossless compression algorithms"],"tag_line":"LZX is the name of an LZ77 family compression algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"schönhage–strassen-algorithm","_score":0,"_source":{"description":"The Schönhage–Strassen algorithm is an asymptotically fast multiplication algorithm for large integers. It was developed by Arnold Schönhage and Volker Strassen in 1971. The run-time bit complexity is, in Big O notation, O(n log n log log n) for two n-digit numbers. The algorithm uses recursive Fast Fourier transforms in rings with 22n + 1 elements, a specific type of number theoretic transform.\nThe Schönhage–Strassen algorithm was the asymptotically fastest multiplication method known from 1971 until 2007, when a new method, Fürer's algorithm, was announced with lower asymptotic complexity; however, Fürer's algorithm currently only achieves an advantage for astronomically large values and is not used in practice.\nIn practice the Schönhage–Strassen algorithm starts to outperform older methods such as Karatsuba and Toom–Cook multiplication for numbers beyond 2215 to 2217 (10,000 to 40,000 decimal digits). The GNU Multi-Precision Library uses it for values of at least 1728 to 7808 64-bit words (33,000 to 150,000 decimal digits), depending on architecture. There is a Java implementation of Schönhage–Strassen which uses it above 74,000 decimal digits.\nApplications of the Schönhage–Strassen algorithm include mathematical empiricism, such as the Great Internet Mersenne Prime Search and computing approximations of π, as well as practical applications such as Kronecker substitution, in which multiplication of polynomials with integer coefficients can be efficiently reduced to large integer multiplication; this is used in practice by GMP-ECM for Lenstra elliptic curve factorization.","name":"Schönhage–Strassen algorithm","categories":["Computer arithmetic algorithms","Multiplication"],"tag_line":"The Schönhage–Strassen algorithm is an asymptotically fast multiplication algorithm for large integers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"s3-texture-compression","_score":0,"_source":{"description":"S3 Texture Compression (S3TC) (sometimes also called DXTn or DXTC) is a group of related lossy texture compression algorithms originally developed by Iourcha et al. of S3 Graphics, Ltd. for use in their Savage 3D computer graphics accelerator. The method of compression is strikingly similar to the previously published Color Cell Compression, which is in turn an adaptation of Block Truncation Coding published in the late 1970s. Unlike some image compression algorithms (e.g. JPEG), S3TC's fixed-rate data compression coupled with the single memory access (cf. Color Cell Compression and some VQ-based schemes) made it well-suited for use in compressing textures in hardware-accelerated 3D computer graphics. Its subsequent inclusion in Microsoft's DirectX 6.0 and OpenGL 1.3 (via the GL_EXT_texture_compression_s3tc extension) led to widespread adoption of the technology among hardware and software makers. While S3 Graphics is no longer a competitor in the graphics accelerator market, license fees are still levied and collected for the use of S3TC technology, for example in game consoles and graphics cards. The wide use of S3TC has led to a de facto requirement for OpenGL drivers to support it, but the patent-encumbered status of S3TC presents a major obstacle to open source implementations.","name":"S3 Texture Compression","categories":["3D computer graphics","Lossy compression algorithms","Texture compression"],"tag_line":"S3 Texture Compression (S3TC) (sometimes also called DXTn or DXTC) is a group of related lossy texture compression algorithms originally developed by Iourcha et al."}}
,{"_index":"throwtable","_type":"algorithm","_id":"block-truncation-coding","_score":0,"_source":{"description":"Block Truncation Coding, or BTC, is a type of lossy image compression technique for greyscale images. It divides the original images into blocks and then uses a quantiser to reduce the number of grey levels in each block whilst maintaining the same mean and standard deviation. It is an early predecessor of the popular hardware DXTC technique, although BTC compression method was first adapted to colour long before DXTC using a very similar approach called Color Cell Compression. BTC has also been adapted to video compression \nBTC was first proposed by E.J Delp and O.R. Mitchell  at Purdue University. Another variation of BTC is Absolute Moment Block Truncation Coding or AMBTC, in which instead of using the standard deviation the first absolute moment is preserved along with the mean. AMBTC is computationally simpler than BTC and also typically results in a lower Mean Squared Error (MSE). AMBTC was proposed by Maximo Lema and Robert Mitchell.\nUsing sub-blocks of 4x4 pixels gives a compression ratio of 4:1 assuming 8-bit integer values are used during transmission or storage. Larger blocks allow greater compression (\"a\" and \"b\" values spread over more pixels) however quality also reduces with the increase in block size due to the nature of the algorithm.\nThe BTC algorithm was used for compressing Mars Pathfinder's rover images.","name":"Block Truncation Coding","categories":["Image compression","Lossy compression algorithms"],"tag_line":"Block Truncation Coding, or BTC, is a type of lossy image compression technique for greyscale images."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bailey–borwein–plouffe-formula","_score":0,"_source":{"description":"The Bailey–Borwein–Plouffe formula (BBP formula) is a spigot algorithm for computing the nth binary digit of pi (symbol: π) using base 16 math. The formula can directly calculate the value of any given digit of π without calculating the preceding digits. The BBP is a summation-style formula that was discovered in 1995 by Simon Plouffe and was named after the authors of the paper in which the formula was published, David H. Bailey, Peter Borwein, and Simon Plouffe. Before that paper, it had been published by Plouffe on his own site. The formula is\n.\nThe discovery of this formula came as a surprise. For centuries it had been assumed that there was no way to compute the nth digit of π without calculating all of the preceding n − 1 digits.\nSince this discovery, many formulas for other irrational constants have been discovered of the general form\n\nwhere α is the constant and p and q are polynomials in integer coefficients and b ≥ 2 is an integer base.\nFormulas in this form are known as BBP-type formulas. Certain combinations of specific p, q and b result in well-known constants, but there is no systematic algorithm for finding the appropriate combinations; known formulas are discovered through experimental mathematics.","name":"Bailey–Borwein–Plouffe formula","categories":["All articles needing additional references","Articles needing additional references from March 2014","Pi algorithms"],"tag_line":"The Bailey–Borwein–Plouffe formula (BBP formula) is a spigot algorithm for computing the nth binary digit of pi (symbol: π) using base 16 math."}}
,{"_index":"throwtable","_type":"algorithm","_id":"wavelet-scalar-quantization","_score":0,"_source":{"description":"The Wavelet Scalar Quantization algorithm (WSQ) is a compression algorithm used for gray-scale fingerprint images. It is based on wavelet theory and has become a standard for the exchange and storage of fingerprint images. WSQ was developed by the FBI, the Los Alamos National Laboratory, and the National Institute of Standards and Technology (NIST).\nThis compression method is preferred over standard compression algorithms like JPEG because at the same compression ratios WSQ doesn't present the \"blocking artifacts\" and loss of fine-scale features that are not acceptable for identification in financial environments and criminal justice. \nMost American law enforcement agencies use Wavelet Scalar Quantization (WSQ) - for efficient storage of compressed fingerprint images at 500 pixels per inch (ppi). For fingerprints recorded at 1000 ppi spatial resolution, law enforcement (including the FBI) uses JPEG 2000 instead of WSQ.","name":"Wavelet scalar quantization","categories":["Fingerprints","Graphics file formats","Lossy compression algorithms"],"tag_line":"The Wavelet Scalar Quantization algorithm (WSQ) is a compression algorithm used for gray-scale fingerprint images."}}
,{"_index":"throwtable","_type":"algorithm","_id":"exponentiation-by-squaring","_score":0,"_source":{"description":"In mathematics and computer programming, exponentiating by squaring is a general method for fast computation of large positive integer powers of a number, or more generally of an element of a semigroup, like a polynomial or a square matrix. Some variants are commonly referred to as square-and-multiply algorithms or binary exponentiation. These can be of quite general use, for example in modular arithmetic or powering of matrices. For semigroups for which additive notation is commonly used, like elliptic curves used in cryptography, this method is also referred to as double-and-add.","name":"Exponentiation by squaring","categories":["Computer arithmetic","Computer arithmetic algorithms","Exponentials"],"tag_line":"In mathematics and computer programming, exponentiating by squaring is a general method for fast computation of large positive integer powers of a number, or more generally of an element of a semigroup, like a polynomial or a square matrix."}}
,{"_index":"throwtable","_type":"algorithm","_id":"chudnovsky-algorithm","_score":0,"_source":{"description":"The Chudnovsky algorithm is a fast method for calculating the digits of π. It was published by the Chudnovsky brothers in 1989, and was used in the world record calculations of 2.7 trillion digits of π in December 2009, 5 trillion digits of π in August 2010, 10 trillion digits of π in October 2011, and 12.1 trillion digits in December 2013.\nThe algorithm is based on the negated Heegner number , the j-function , and on the following rapidly convergent generalized hypergeometric series:\n\nNote that 545140134 = 163 x 3344418 and,\n\nThis identity is similar to some of Ramanujan's formulas involving π, and is an example of a Ramanujan–Sato series.","name":"Chudnovsky algorithm","categories":["All stub articles","Number stubs","Pi algorithms"],"tag_line":"The Chudnovsky algorithm is a fast method for calculating the digits of π."}}
,{"_index":"throwtable","_type":"algorithm","_id":"apple-video","_score":0,"_source":{"description":"Apple Video is a lossy video compression and decompression algorithm (codec) developed by Apple Inc. and first released as part of QuickTime 1.0 in 1991. The codec is also known as QuickTime Video, by its FourCC RPZA and the name Road Pizza. When used in the AVI container, the FourCC AZPR is also used. The bit-stream format of Apple Video has been reverse-engineered and a decoder has been implemented in the projects XAnim and libavcodec.","name":"Apple Video","categories":["Apple Inc. software","Lossy compression algorithms","Video codecs"],"tag_line":"Apple Video is a lossy video compression and decompression algorithm (codec) developed by Apple Inc. and first released as part of QuickTime 1.0 in 1991."}}
,{"_index":"throwtable","_type":"algorithm","_id":"agm-method","_score":0,"_source":{"description":"In mathematics, the AGM method (for arithmetic–geometric mean) makes it possible to construct fast algorithms for calculation of exponential and trigonometric functions, and some mathematical constants and in particular, to quickly compute .\n\n","name":"AGM method","categories":["All articles to be merged","Articles to be merged from September 2012","Computer arithmetic algorithms"],"tag_line":"In mathematics, the AGM method (for arithmetic–geometric mean) makes it possible to construct fast algorithms for calculation of exponential and trigonometric functions, and some mathematical constants and in particular, to quickly compute .\n\n"}}
,{"_index":"throwtable","_type":"algorithm","_id":"logical-clock","_score":0,"_source":{"description":"A logical clock is a mechanism for capturing chronological and causal relationships in a distributed system. Distributed system may have no physically synchronous global clock, so a logical clock allows global ordering on events from different processes in such systems. The first implementation, the Lamport timestamps, was proposed by Leslie Lamport in 1978 (Turing Award in 2013).\nIn logical clock systems each process has two data structures: logical local time and logical global time. Logical local time is used by the process to mark its own events, and logical global time is the local information about global time. A special protocol is used to update logical local time after each local event, and logical global time when processes exchange data.\nLogical clocks are useful in computation analysis, distributed algorithm design, individual event tracking, and exploring computational progress.\nSome noteworthy logical clock algorithms are:\nLamport timestamps, which are monotonically increasing software counters.\nVector clocks, that allow for partial ordering of events in a distributed system.\nVersion vectors, order replicas, according to updates, in an optimistic replicated system.\nMatrix clocks, an extension of vector clocks that also contains information about other processes' views of the system.","name":"Logical clock","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Distributed algorithms","Wikipedia articles with GND identifiers"],"tag_line":"A logical clock is a mechanism for capturing chronological and causal relationships in a distributed system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"distributed-algorithm","_score":0,"_source":{"description":"A distributed algorithm is an algorithm designed to run on computer hardware constructed from interconnected processors. Distributed algorithms are used in many varied application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation.\nDistributed algorithms are a sub-type of parallel algorithm, typically executed concurrently, with separate parts of the algorithm being run simultaneously on independent processors, and having limited information about what the other parts of the algorithm are doing. One of the major challenges in developing and implementing distributed algorithms is successfully coordinating the behavior of the independent parts of the algorithm in the face of processor failures and unreliable communications links. The choice of an appropriate distributed algorithm to solve a given problem depends on both the characteristics of the problem, and characteristics of the system the algorithm will run on such as the type and probability of processor or link failures, the kind of inter-process communication that can be performed, and the level of timing synchronization between separate processes.","name":"Distributed algorithm","categories":["Distributed algorithms"],"tag_line":"A distributed algorithm is an algorithm designed to run on computer hardware constructed from interconnected processors."}}
,{"_index":"throwtable","_type":"algorithm","_id":"suzuki-kasami-algorithm","_score":0,"_source":{"description":"The Suzuki-Kasami algorithm is a token-based algorithm for achieving mutual exclusion in distributed systems. The process holding the token is the only process able to enter its critical section.\nThis is a modification to Ricart–Agrawala algorithm in which a REQUEST and REPLY message are used for attaining the critical section. but in this algorithm they introduced a method in which a seniority vise and also by handing over the critical section to other node by sending a single PRIVILEGE message to other node. So, the node which has the privilege it can use the critical section and if it does not have one it cannot. If a process wants to enter its critical section and it does not have the token, it broadcasts a request message to all other processes in the system. The process that has the token, if it is not currently in a critical section, will then send the token to the requesting process. The algorithm makes use of increasing Request Numbers to allow messages to arrive out-of-order.","name":"Suzuki-Kasami algorithm","categories":["All articles needing additional references","All articles needing cleanup","All articles needing expert attention","All articles with topics of unclear notability","Articles needing additional references from September 2014","Articles needing cleanup from May 2009","Articles needing expert attention from May 2009","Articles needing expert attention with no reason or talk parameter","Articles with topics of unclear notability from May 2009","Cleanup tagged articles without a reason field from May 2009","Computer science articles needing expert attention","Distributed algorithms","Wikipedia pages needing cleanup from May 2009"],"tag_line":"The Suzuki-Kasami algorithm is a token-based algorithm for achieving mutual exclusion in distributed systems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"distributed-minimum-spanning-tree","_score":0,"_source":{"description":"The distributed minimum spanning tree (MST) problem involves the construction of a minimum spanning tree by a distributed algorithm, in a network where nodes communicate by message passing. It is radically different from the classical sequential problem, although the most basic approach resembles Borůvka's algorithm. One important application of this problem is to find a tree that can be used for broadcasting. In particular, if the cost for a message to pass through an edge in a graph is significant, a MST can minimize the total cost for a source process to communicate with all the other processes in the network.\nThe problem was first suggested and solved in  time in 1983 by Gallager et al., where  is the number of vertices in the graph. Later, the solution was improved to  and finally  where D is the network, or graph diameter. A lower bound on the time complexity of the solution has been eventually shown to be","name":"Distributed minimum spanning tree","categories":["Distributed algorithms","Spanning tree"],"tag_line":"The distributed minimum spanning tree (MST) problem involves the construction of a minimum spanning tree by a distributed algorithm, in a network where nodes communicate by message passing."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lamport's-bakery-algorithm","_score":0,"_source":{"description":"Lamport's bakery algorithm is a computer algorithm devised by computer scientist Leslie Lamport, which is intended to improve the safety in the usage of shared resources among multiple threads by means of mutual exclusion.\nIn computer science, it is common for multiple threads to simultaneously access the same resources. Data corruption can occur if two or more threads try to write into the same memory location, or if one thread reads a memory location before another has finished writing into it. Lamport's bakery algorithm is one of many mutual exclusion algorithms designed to prevent concurrent threads entering critical sections of code concurrently to eliminate the risk of data corruption.","name":"Lamport's bakery algorithm","categories":["All articles lacking in-text citations","All articles to be merged","Articles lacking in-text citations from December 2010","Articles to be merged from October 2013","Articles with example pseudocode","Concurrency control algorithms","Use dmy dates from December 2012"],"tag_line":"Lamport's bakery algorithm is a computer algorithm devised by computer scientist Leslie Lamport, which is intended to improve the safety in the usage of shared resources among multiple threads by means of mutual exclusion."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cristian's-algorithm","_score":0,"_source":{"description":"Cristian's Algorithm (introduced by Flaviu Cristian in 1989) is a method for clock synchronization which can be used in many fields of distributive computer science but is primarily used in low-latency intranets. Cristian observed that this simple algorithm is probabilistic, in that it only achieves synchronization if the round-trip time (RTT) of the request is short compared to required accuracy. It also suffers in implementations using a single server, making it unsuitable for many distributive applications where redundancy may be crucial.","name":"Cristian's algorithm","categories":["Distributed algorithms","Synchronization"],"tag_line":"Cristian's Algorithm (introduced by Flaviu Cristian in 1989) is a method for clock synchronization which can be used in many fields of distributive computer science but is primarily used in low-latency intranets."}}
,{"_index":"throwtable","_type":"algorithm","_id":"peterson's-algorithm","_score":0,"_source":{"description":"Peterson's algorithm (AKA Peterson's solution) is a concurrent programming algorithm for mutual exclusion that allows two processes to share a single-use resource without conflict, using only shared memory for communication. It was formulated by Gary L. Peterson in 1981. While Peterson's original formulation worked with only two processes, the algorithm can be generalized for more than two, as shown below.","name":"Peterson's algorithm","categories":["All articles with unsourced statements","Articles with example C code","Articles with unsourced statements from May 2015","Concurrency control algorithms"],"tag_line":"Peterson's algorithm (AKA Peterson's solution) is a concurrent programming algorithm for mutual exclusion that allows two processes to share a single-use resource without conflict, using only shared memory for communication."}}
,{"_index":"throwtable","_type":"algorithm","_id":"raymond's-algorithm","_score":0,"_source":{"description":"Raymond's Algorithm is a lock based algorithm for mutual exclusion on a distributed system. It imposes a logical structure (a K-ary tree) on distributed resources. As defined, each node has only a single parent, to which all requests to attain the token are made.","name":"Raymond's algorithm","categories":["Concurrency control algorithms"],"tag_line":"Raymond's Algorithm is a lock based algorithm for mutual exclusion on a distributed system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"local-algorithm","_score":0,"_source":{"description":"A local algorithm is a distributed algorithm that runs in constant time, independently of the size of the network.","name":"Local algorithm","categories":["All orphaned articles","All stub articles","Computing stubs","Distributed algorithms","Orphaned articles from September 2013"],"tag_line":"A local algorithm is a distributed algorithm that runs in constant time, independently of the size of the network."}}
,{"_index":"throwtable","_type":"algorithm","_id":"randomness-extractor","_score":0,"_source":{"description":"A randomness extractor, often simply called an \"extractor\", is a function, which being applied to output from a weakly random entropy source, together with a short, uniformly random seed, generates a highly random output that appears independent from the source and uniformly distributed. Examples of weakly random sources include radioactive decay or thermal noise; the only restriction on possible sources is that there is no way they can be fully controlled, calculated or predicted, and that a lower bound on their entropy rate can be established. For a given source, a randomness extractor can even be considered to be a true random number generator (TRNG); but there is no single extractor that has been proven to produce truly random output from any type of weakly random source.\nSometimes the term \"bias\" is used to denote a weakly random source's departure from uniformity, and in older literature, some extractors are called unbiasing algorithms, as they take the randomness from a so-called \"biased\" source and output a distribution that appears unbiased. The weakly random source will always be longer than the extractor's output, but an efficient extractor is one that lowers this ratio of lengths as much as possible, while simultaneously keeping the seed length low. Intuitively, this means that as much randomness as possible has been \"extracted\" from the source.\nNote that an extractor has some conceptual similarities with a pseudorandom generator (PRG), but the two concepts are not identical. Both are functions that take as input a small, uniformly random seed and produce a longer output that \"looks\" uniformly random. Some pseudorandom generators are, in fact, also extractors. (When a PRG is based on the existence of hard-core predicates, one can think of the weakly random source as a set of truth tables of such predicates and prove that the output is statistically close to uniform.) However, the general PRG definition does not specify that a weakly random source must be used, and while in the case of an extractor, the output should be statistically close to uniform, in a PRG it is only required to be computationally indistinguishable from uniform, a somewhat weaker concept.\nNIST Special Publication 800-90B (draft) recommends several extractors, including the SHA hash family and states that if the amount of entropy input is twice the number of bits output from them, that output can be considered essentially fully random.","name":"Randomness extractor","categories":["Computational complexity theory","Cryptographic algorithms","Random number generation","Randomness"],"tag_line":"A randomness extractor, often simply called an \"extractor\", is a function, which being applied to output from a weakly random entropy source, together with a short, uniformly random seed, generates a highly random output that appears independent from the source and uniformly distributed."}}
,{"_index":"throwtable","_type":"algorithm","_id":"discrete-logarithm-records","_score":0,"_source":{"description":"Discrete logarithm records are the best results achieved to date in solving the discrete logarithm problem, which is the problem of finding solutions x to the equation gx = h given elements g and h of a finite cyclic group G. The difficulty of this problem is the basis for the security of several cryptographic systems, including Diffie–Hellman key agreement, ElGamal encryption, the ElGamal signature scheme, the Digital Signature Algorithm, and the elliptic curve cryptography analogs of these. Common choices for G used in these algorithms include the multiplicative group of integers modulo p, the multiplicative group of a finite field, and the group of points on an elliptic curve over a finite field.","name":"Discrete logarithm records","categories":["All articles containing potentially dated statements","Articles containing potentially dated statements from 2010","Articles containing potentially dated statements from 2014","Articles containing potentially dated statements from January 2014","Asymmetric-key algorithms","Computational hardness assumptions","Logarithms","Modular arithmetic","World records"],"tag_line":"Discrete logarithm records are the best results achieved to date in solving the discrete logarithm problem, which is the problem of finding solutions x to the equation gx = h given elements g and h of a finite cyclic group G. The difficulty of this problem is the basis for the security of several cryptographic systems, including Diffie–Hellman key agreement, ElGamal encryption, the ElGamal signature scheme, the Digital Signature Algorithm, and the elliptic curve cryptography analogs of these."}}
,{"_index":"throwtable","_type":"algorithm","_id":"vector-clock","_score":0,"_source":{"description":"Vector clocks is an algorithm for generating a partial ordering of events in a distributed system and detecting causality violations. Just as in Lamport timestamps, interprocess messages contain the state of the sending process's logical clock. A vector clock of a system of N processes is an array/vector of N logical clocks, one clock per process; a local \"smallest possible values\" copy of the global clock-array is kept in each process, with the following rules for clock updates:\n\nInitially all clocks are zero.\nEach time a process experiences an internal event, it increments its own logical clock in the vector by one.\nEach time a process prepares to send a message, it sends its entire vector along with the message being sent.\nEach time a process receives a message, it increments its own logical clock in the vector by one and updates each element in its vector by taking the maximum of the value in its own vector clock and the value in the vector in the received message (for every element).\nThe vector clocks algorithm was independently developed by Colin Fidge and Friedemann Mattern in 1988.","name":"Vector clock","categories":["Distributed algorithms"],"tag_line":"Vector clocks is an algorithm for generating a partial ordering of events in a distributed system and detecting causality violations."}}
,{"_index":"throwtable","_type":"algorithm","_id":"symmetric-key-algorithm","_score":0,"_source":{"description":"Symmetric-key algorithms are algorithms for cryptography that use the same cryptographic keys for both encryption of plaintext and decryption of ciphertext. The keys may be identical or there may be a simple transformation to go between the two keys. The keys, in practice, represent a shared secret between two or more parties that can be used to maintain a private information link. This requirement that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption, in comparison to public-key encryption.","name":"Symmetric-key algorithm","categories":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from April 2012","Articles with unsourced statements from April 2012","Cryptographic algorithms","Wikipedia articles with GND identifiers"],"tag_line":"Symmetric-key algorithms are algorithms for cryptography that use the same cryptographic keys for both encryption of plaintext and decryption of ciphertext."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mosquito","_score":0,"_source":{"description":"In cryptography, MOSQUITO was a stream cypher algorithm designed by Joan Daemen and Paris Kitsos. It was submitted to the eSTREAM Project of the eCRYPT network. After the initial design was broken by Joux and Muller, a tweaked version named MOUSTIQUE was proposed which made it to Phase 3 of the eSTREAM evaluation process as the only self-synchronizing cipher remaining. However, MOUSTIQUE was subsequently broken by Käsper et al., leaving the design of a secure and efficient self-synchronising stream cipher as an open research problem.","name":"MOSQUITO","categories":["All articles to be expanded","All stub articles","Articles needing translation from Russian Wikipedia","Articles to be expanded from August 2013","Cryptographic algorithms","Cryptography stubs"],"tag_line":"In cryptography, MOSQUITO was a stream cypher algorithm designed by Joan Daemen and Paris Kitsos."}}
,{"_index":"throwtable","_type":"algorithm","_id":"schoof–elkies–atkin-algorithm","_score":0,"_source":{"description":"The Schoof–Elkies–Atkin algorithm (SEA) is an algorithm used for finding the order of or calculating the number of points on an elliptic curve over a finite field. Its primary application is in elliptic curve cryptography. The algorithm is an extension of Schoof's algorithm by Noam Elkies and A. O. L. Atkin to significantly improve its efficiency (under heuristic assumptions).\n\n","name":"Schoof–Elkies–Atkin algorithm","categories":["Asymmetric-key algorithms","Elliptic curve cryptography","Finite fields","Group theory","Number theory"],"tag_line":"The Schoof–Elkies–Atkin algorithm (SEA) is an algorithm used for finding the order of or calculating the number of points on an elliptic curve over a finite field."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cycles-per-byte","_score":0,"_source":{"description":"Cycles per byte (sometimes abbreviated cpb) is a unit of measurement which indicates the number of clock cycles a microprocessor will perform per byte (usually of octet size) of data processed in an algorithm. It is commonly used as a partial indicator of real-world performance in cryptographic functions.","name":"Cycles per byte","categories":["All stub articles","Computer benchmarks","Cryptographic algorithms","Cryptography stubs"],"tag_line":"Cycles per byte (sometimes abbreviated cpb) is a unit of measurement which indicates the number of clock cycles a microprocessor will perform per byte (usually of octet size) of data processed in an algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"euler's-factorization-method","_score":0,"_source":{"description":"Euler's factorization method is a technique for factoring a number by writing it as a sum of two squares in two different ways. For example the number  can be written as  or as  and Euler's method gives the factorization .\nThe idea that two distinct representations of an odd positive integer may lead to a factorization was apparently first proposed by Marin Mersenne. However, it was not put to use extensively until Euler one hundred years later. His most celebrated use of the method that now bears his name was to factor the number , which apparently was previously thought to be prime even though it is not a pseudoprime by any major primality test.\nEuler's factorization method is more effective than Fermat's for integers whose factors are not close together and potentially much more efficient than trial division if one can find representations of numbers as sums of two squares reasonably easily. Euler's development ultimately permitted much more efficient factoring of numbers and, by the 1910s, the development of large factor tables going up to about ten million. The methods used to find representations of numbers as sums of two squares are essentially the same as with finding differences of squares in Fermat's factorization method.\nThe great disadvantage of Euler's factorization method is that it cannot be applied to factoring an integer with any prime factor of the form 4k + 3 occurring to an odd power in its prime factorization, as such a number can never be the sum of two squares. Even odd composite numbers of the form 4k + 1 are often the product of two primes of the form 4k + 3 (e.g. 3053 = 43 × 71) and again cannot be factored by Euler's method.\nThis restricted applicability has made Euler's factorization method disfavoured for computer factoring algorithms, since any user attempting to factor a random integer is unlikely to know whether Euler's method can actually be applied to the integer in question. It is only relatively recently that there have been attempts to develop Euler's method into computer algorithms for use on specialised numbers where it is known Euler's method can be applied.","name":"Euler's factorization method","categories":["All articles with unsourced statements","Articles with unsourced statements from July 2013","Integer factorization algorithms"],"tag_line":"Euler's factorization method is a technique for factoring a number by writing it as a sum of two squares in two different ways."}}
,{"_index":"throwtable","_type":"algorithm","_id":"integer-factorization","_score":0,"_source":{"description":"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.\nWhen the numbers are very large, no efficient, non-quantum integer factorization algorithm is known; an effort by several researchers concluded in 2009, factoring a 232-digit number (RSA-768), utilizing hundreds of machines over a span of two years. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.\nNot all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.\nMany cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure.\n\n","name":"Integer factorization","categories":["Computational hardness assumptions","Integer factorization algorithms","Unsolved problems in computer science"],"tag_line":"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"algebraic-group-factorisation-algorithm","_score":0,"_source":{"description":"Algebraic-group factorisation algorithms are algorithms for factoring an integer N by working in an algebraic group defined modulo N whose group structure is the direct sum of the 'reduced groups' obtained by performing the equations defining the group arithmetic modulo the unknown prime factors p1, p2, ... By the Chinese remainder theorem, arithmetic modulo N corresponds to arithmetic in all the reduced groups simultaneously.\nThe aim is to find an element which is not the identity of the group modulo N, but is the identity modulo one of the factors, so a method for recognising such one-sided identities is required. In general, one finds them by performing operations that move elements around and leave the identities in the reduced groups unchanged. Once the algorithm finds a one-sided identity all future terms will also be one-sided identities, so checking periodically suffices.\nComputation proceeds by picking an arbitrary element x of the group modulo N and computing a large and smooth multiple Ax of it; if the order of at least one but not all of the reduced groups is a divisor of A, this yields a factorisation. It need not be a prime factorisation, as the element might be an identity in more than one of the reduced groups.\nGenerally, A is taken as a product of the primes below some limit K, and Ax is computed by successive multiplication of x by these primes; after each multiplication, or every few multiplications, the check is made for a one-sided identity.","name":"Algebraic-group factorisation algorithm","categories":["All articles lacking sources","Articles lacking sources from January 2015","Integer factorization algorithms"],"tag_line":"Algebraic-group factorisation algorithms are algorithms for factoring an integer N by working in an algebraic group defined modulo N whose group structure is the direct sum of the 'reduced groups' obtained by performing the equations defining the group arithmetic modulo the unknown prime factors p1, p2, ... By the Chinese remainder theorem, arithmetic modulo N corresponds to arithmetic in all the reduced groups simultaneously."}}
,{"_index":"throwtable","_type":"algorithm","_id":"wired-equivalent-privacy","_score":0,"_source":{"description":"Wired Equivalent Privacy (WEP) is a security algorithm for IEEE 802.11 wireless networks. Introduced as part of the original 802.11 standard ratified in 1997, its intention was to provide data confidentiality comparable to that of a traditional wired network. WEP, recognizable by the key of 10 or 26 hexadecimal digits, was at one time widely in use and was often the first security choice presented to users by router configuration tools.\nIn 2003 the Wi-Fi Alliance announced that WEP had been superseded by Wi-Fi Protected Access (WPA). In 2004, with the ratification of the full 802.11i standard (i.e. WPA2), the IEEE declared that both WEP-40 and WEP-104 have been deprecated.","name":"Wired Equivalent Privacy","categories":["All articles with unsourced statements","Articles with unsourced statements from December 2011","Broken cryptography algorithms","Computer network security","Cryptographic protocols","IEEE 802.11","Wireless networking"],"tag_line":"Wired Equivalent Privacy (WEP) is a security algorithm for IEEE 802.11 wireless networks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"continued-fraction-factorization","_score":0,"_source":{"description":"In number theory, the continued fraction factorization method (CFRAC) is an integer factorization algorithm. It is a general-purpose algorithm, meaning that it is suitable for factoring any integer n, not depending on special form or properties. It was described by D. H. Lehmer and R. E. Powers in 1931, and developed as a computer algorithm by Michael A. Morrison and John Brillhart in 1975.\nThe continued fraction method is based on Dixon's factorization method. It uses convergents in the regular continued fraction expansion of\n.\nSince this is a quadratic irrational, the continued fraction must be periodic (unless n is square, in which case the factorization is obvious).\nIt has a time complexity of , in the O and L notations.","name":"Continued fraction factorization","categories":["All stub articles","Integer factorization algorithms","Number theory stubs"],"tag_line":"In number theory, the continued fraction factorization method (CFRAC) is an integer factorization algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"universal-hashing","_score":0,"_source":{"description":"In mathematics and computing universal hashing (in a randomized algorithm or data structure) refers to selecting a hash function at random from a family of hash functions with a certain mathematical property (see definition below). This guarantees a low number of collisions in expectation, even if the data is chosen by an adversary. Many universal families are known (for hashing integers, vectors, strings), and their evaluation is often very efficient. Universal hashing has numerous uses in computer science, for example in implementations of hash tables, randomized algorithms, and cryptography.","name":"Universal hashing","categories":["Computational complexity theory","Cryptographic hash functions","Hashing","Search algorithms"],"tag_line":"In mathematics and computing universal hashing (in a randomized algorithm or data structure) refers to selecting a hash function at random from a family of hash functions with a certain mathematical property (see definition below)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shamir's-secret-sharing","_score":0,"_source":{"description":"Shamir's Secret Sharing is an algorithm in cryptography created by Adi Shamir. It is a form of secret sharing, where a secret is divided into parts, giving each participant its own unique part, where some of the parts or all of them are needed in order to reconstruct the secret.\nCounting on all participants to combine the secret might be impractical, and therefore sometimes the threshold scheme is used where any  of the parts are sufficient to reconstruct the original secret.","name":"Shamir's Secret Sharing","categories":["All articles needing expert attention","All articles that are too technical","Articles needing expert attention from March 2014","Information-theoretically secure algorithms","Secret sharing","Wikipedia articles that are too technical from March 2014"],"tag_line":"Shamir's Secret Sharing is an algorithm in cryptography created by Adi Shamir."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dixon's-factorization-method","_score":0,"_source":{"description":"In number theory, Dixon's factorization method (also Dixon's random squares method or Dixon's algorithm) is a general-purpose integer factorization algorithm; it is the prototypical factor base method. Unlike for other factor base methods, its run-time bound comes with a rigorous proof that does not rely on conjectures about the smoothness properties of the values taken by polynomial.\nThe algorithm was designed by John D. Dixon, a mathematician at Carleton University, and was published in 1981.\n\n","name":"Dixon's factorization method","categories":["Integer factorization algorithms"],"tag_line":"In number theory, Dixon's factorization method (also Dixon's random squares method or Dixon's algorithm) is a general-purpose integer factorization algorithm; it is the prototypical factor base method."}}
,{"_index":"throwtable","_type":"algorithm","_id":"basic-sequential-algorithmic-scheme","_score":0,"_source":{"description":"The basic sequential algorithmic scheme (BSAS) is a very basic clustering algorithm that is easy to understand. In the basic form vectors are presented only once and the number of clusters is not known at priori. What is needed is the dissimilarity measured as the distance d (x, C) between a vector point x and a cluster C, threshold of dissimilarity Θ and the number of maximum clusters allowed q. The idea is to assign every newly presented vector to an existing cluster or create a new cluster for this sample, depending on the distance to the already defined clusters. As pseudocode, the algorithm looks like the following:\n\n 1. m = 1; Cm = {x1}; // Init first cluster = first sample     \n 2. for every sample x from 2 to N      \n   a. find cluster Ck such that min d(x, Ck)     \n   b. if d(x, Ck) > Θ AND (m < q)      \n     i. m = m + 1; Cm = {x} // Create a new cluster      \n   c. else           i. Ck = Ck + {x} // Add sample to the nearest cluster      \n     ii. Update representative if needed      \n 3. end algorithm\n\nAs can be seen the algorithm is simple but still quite efficient. Different choices for the distance function lead to different results and unfortunately the order in which the samples are presented can also have a great effect to the final result. What’s also very important is a correct value for Θ. This value has a direct effect on the number of formed clusters. If Θ is too small unnecessary clusters are created and if too large a value is chosen less than required number of clusters are formed.\nOne detail is that if q is not defined the algorithm ‘decides’ the number of clusters on its own. This might be wanted under some circumstances but when dealing with limited resources a limited q is usually chosen. Also, BSAS can be used with a similarity function simply by replacing the min function with max.\nThere exists a modification to BSAS called modified BSAS (MBSAS), which runs twice through the samples. It overcomes the drawback that a final cluster for a single sample is decided before all the clusters have been created. The first phase of the algorithm creates the clusters (just like 2b in BSAS) and assigns only a single sample to each cluster. Then the second phase runs through the remaining samples and classifies them to the created clusters (step 2c in BSAS).","name":"Basic sequential algorithmic scheme","categories":["Algorithms and data structures stubs","All articles covered by WikiProject Wikify","All articles needing additional references","All articles needing cleanup","All articles with too few wikilinks","All stub articles","Articles covered by WikiProject Wikify from March 2014","Articles needing additional references from May 2014","Articles needing cleanup from February 2014","Articles with too few wikilinks from March 2014","Cleanup tagged articles with a reason field from February 2014","Computer science stubs","Data clustering algorithms","Wikipedia pages needing cleanup from February 2014"],"tag_line":"The basic sequential algorithmic scheme (BSAS) is a very basic clustering algorithm that is easy to understand."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shanks'-square-forms-factorization","_score":0,"_source":{"description":"Shanks's square forms factorization is a method for integer factorization devised by Daniel Shanks as an improvement on Fermat's factorization method.\nThe success of Fermat's method depends on finding integers  and  such that , where  is the integer to be factored. An improvement (noticed by Kraitchik) is to look for integers  and  such that . Finding a suitable pair  does not guarantee a factorization of , but it implies that  is a factor of , and there is a good chance that the prime divisors of  are distributed between these two factors, so that calculation of the greatest common divisor of  and  will give a non-trivial factor of .\nA practical algorithm for finding pairs  which satisfy  was developed by Shanks, who named it Square Forms Factorization or SQUFOF. The algorithm can be expressed in terms of continued fractions, or in terms of quadratic forms. Although there are now much more efficient factorization methods available, SQUFOF has the advantage that it is small enough to be implemented on a programmable calculator.","name":"Shanks' square forms factorization","categories":["All articles lacking in-text citations","All articles needing expert attention","Articles lacking in-text citations from March 2015","Articles needing expert attention from November 2008","Articles needing expert attention with no reason or talk parameter","Integer factorization algorithms","Mathematics articles needing expert attention"],"tag_line":"Shanks's square forms factorization is a method for integer factorization devised by Daniel Shanks as an improvement on Fermat's factorization method."}}
,{"_index":"throwtable","_type":"algorithm","_id":"saville","_score":0,"_source":{"description":"SAVILLE is a classified NSA Type 1 encryption algorithm. It is used broadly, often for voice encryption, and implemented in a large number of encryption devices.\nLittle is known publicly about the algorithm itself due to its classified nature and inclusion in the NSA's Suite A. Some documentation related to the KYK-13 fill device and statements made by military officials suggest that SAVILLE has a 128-bit key. On the AIM microchip, it runs at 4% of the clock rate (compare DES at 76% and BATON at 129%). The Cypris chip mentions 2 modes; specifications for Windster and Indictor specify that they provide Saville I.\nSome devices and protocols that implement SAVILLE:\nThe VINSON family (voice encryption)\nAPCO Project 25 (single-channel land mobile radios) (Saville has algorithm ID 04)\nVersatile encryption chips: AIM, Cypris, Sierra I/II, Windster, Indictor, Presidio, Railman","name":"SAVILLE","categories":["All articles lacking sources","All stub articles","Articles lacking sources from February 2008","Block ciphers","Cryptography stubs","Type 1 encryption algorithms"],"tag_line":"SAVILLE is a classified NSA Type 1 encryption algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dbscan","_score":0,"_source":{"description":"Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996. It is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.\nIn 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, KDD.","name":"DBSCAN","categories":["Data clustering algorithms"],"tag_line":"Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adleman–pomerance–rumely-primality-test","_score":0,"_source":{"description":"In computational number theory, the Adleman–Pomerance–Rumely primality test is an algorithm for determining whether a number is prime. Unlike other, more efficient algorithms for this purpose, it avoids the use of random numbers, so it is a deterministic primality test. It is named after its discoverers, Leonard Adleman, Carl Pomerance, and Robert Rumely. The test involves arithmetic in cyclotomic fields.\nIt was later improved by Henri Cohen and Hendrik Willem Lenstra, commonly referred to as APR-CL. It can test primality of an integer n in time:","name":"Adleman–Pomerance–Rumely primality test","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Number theory stubs","Primality tests"],"tag_line":"In computational number theory, the Adleman–Pomerance–Rumely primality test is an algorithm for determining whether a number is prime."}}
,{"_index":"throwtable","_type":"algorithm","_id":"skipjack-(cipher)","_score":0,"_source":{"description":"In cryptography, Skipjack is a block cipher—an algorithm for encryption—developed by the U.S. National Security Agency (NSA). Initially classified, it was originally intended for use in the controversial Clipper chip. Subsequently, the algorithm was declassified and now provides a unique insight into the cipher designs of a government intelligence agency.","name":"Skipjack (cipher)","categories":["All articles lacking in-text citations","All articles with specifically marked weasel-worded phrases","Articles lacking in-text citations from March 2009","Articles with specifically marked weasel-worded phrases from January 2014","Block ciphers","National Security Agency cryptography","Pages using duplicate arguments in template calls","Type 2 encryption algorithms"],"tag_line":"In cryptography, Skipjack is a block cipher—an algorithm for encryption—developed by the U.S. National Security Agency (NSA)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"special-number-field-sieve","_score":0,"_source":{"description":"In number theory, a branch of mathematics, the special number field sieve (SNFS) is a special-purpose integer factorization algorithm. The general number field sieve (GNFS) was derived from it.\nThe special number field sieve is efficient for integers of the form re ± s, where r and s are small (for instance Mersenne numbers).\nHeuristically, its complexity for factoring an integer  is of the form:\n\nin O and L-notations.\nThe SNFS has been used extensively by NFSNet (a volunteer distributed computing effort), NFS@Home and others to factorise numbers of the Cunningham project; for some time the records for integer factorisation have been numbers factored by SNFS.","name":"Special number field sieve","categories":["Integer factorization algorithms"],"tag_line":"In number theory, a branch of mathematics, the special number field sieve (SNFS) is a special-purpose integer factorization algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"affinity-propagation","_score":0,"_source":{"description":"In statistics and data mining, affinity propagation (AP) is a clustering algorithm based on the concept of \"message passing\" between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not require the number of clusters to be determined or estimated before running the algorithm. Like k-medoids, AP finds \"exemplars\", members of the input set that are representative of clusters.","name":"Affinity propagation","categories":["All stub articles","Computer science stubs","Data clustering algorithms"],"tag_line":"In statistics and data mining, affinity propagation (AP) is a clustering algorithm based on the concept of \"message passing\" between data points."}}
,{"_index":"throwtable","_type":"algorithm","_id":"statistical-classification","_score":0,"_source":{"description":"In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into \"spam\" or \"non-spam\" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.).\nIn the terminology of machine learning, classification is considered an instance of supervised learning, i.e. learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance.\nOften, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type), ordinal (e.g. \"large\", \"medium\" or \"small\"), integer-valued (e.g. the number of occurrences of a part word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.\nAn algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term \"classifier\" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.\nTerminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. There is also some argument over whether classification methods that do not involve a statistical model can be considered \"statistical\". Other fields may use different terminology: e.g. in community ecology, the term \"classification\" normally refers to cluster analysis, i.e. a type of unsupervised learning, rather than the supervised learning described in this article.\n\n","name":"Statistical classification","categories":["All articles lacking in-text citations","All articles with unsourced statements","All pages needing cleanup","Articles lacking in-text citations from January 2010","Articles needing cleanup from May 2012","Articles with sections that need to be turned into prose from May 2012","Articles with unsourced statements from August 2014","Classification algorithms","Machine learning","Statistical classification"],"tag_line":"In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adaptive-algorithm","_score":0,"_source":{"description":"An adaptive algorithm is an algorithm that changes its behavior based on information available at the time it is run. This might be information about computational resources available, or the history of data recently received.\nFor example, stable partition, using no additional memory is O(n lg n) but given O(n) memory, it can be O(n) in time. As implemented by the C++ Standard Library, stable_partition is adaptive and so it acquires as much memory as it can get (up to what it would need at most) and applies the algorithm using that available memory. Another example is adaptive sort, whose behaviour changes upon the presortedness of its input.\nAn example of an adaptive algorithm in radar systems is the constant false alarm rate (CFAR) detector.\nIn machine learning and optimization, many algorithms are adaptive or have adaptive variants, which usually means that the algorithm parameters are automatically adjusted according to statistics about the optimisation thus far (e.g. the rate of convergence). Examples include adaptive simulated annealing, adaptive coordinate descent, AdaBoost, and adaptive quadrature.\nIn data compression, adaptive coding algorithms such as Adaptive Huffman coding or Prediction by partial matching can take a stream of data as input, and adapt their compression technique based on the symbols that they have already encountered.\nIn signal processing, the Adaptive Transform Acoustic Coding (ATRAC) codec used in MiniDisc recorders is called \"adaptive\" because the window length (the size of an audio \"chunk\") can change according to the nature of the sound being compressed, to try and achieve the best-sounding compression strategy.","name":"Adaptive algorithm","categories":["Algorithms","All stub articles","Software engineering stubs"],"tag_line":"An adaptive algorithm is an algorithm that changes its behavior based on information available at the time it is run."}}
,{"_index":"throwtable","_type":"algorithm","_id":"driver-scheduling-problem","_score":0,"_source":{"description":"The driver scheduling problem (DSP) is type of problem in operations research and theoretical computer science.\nThe DSP consists of selecting a set of duties (assignments) for the drivers or pilots of vehicles (e.g., buses, trains, boats, or planes) involved in the transportation of passengers or goods.\nThis very complex problem involves several constraints related to labour and company rules and also different evaluation criteria and objectives. Being able to solve this problem efficiently can have a great impact on costs and quality of service for public transportation companies. There is a large number of different rules that a feasible duty might be required to satisfy, such as\nMinimum and maximum stretch duration\nMinimum and maximum break duration\nMinimum and maximum work duration\nMinimum and maximum total duration\nMaximum extra work duration\nMaximum number of vehicle changes\nMinimum driving duration of a particular vehicle\nOperations research has provided optimization models and algorithms that lead to efficient solutions for this problem. Among the most common models proposed to solve the DSP are the Set Covering and Set Partitioning Models (SPP/SCP). In the SPP model, each work piece (task) is covered by only one duty. In the SCP model, it is possible to have more than one duty covering a given work piece. In both models, the set of work pieces that needs to be covered is laid out in rows, and the set of previously defined feasible duties available for covering specific work pieces is arranged in columns. The DSP resolution, based on either of these models, is the selection of the set of feasible duties that guarantees that there is one (SPP) or more (SCP) duties covering each work piece while minimizing the total cost of the final schedule.","name":"Driver scheduling problem","categories":["Algorithms"],"tag_line":"The driver scheduling problem (DSP) is type of problem in operations research and theoretical computer science."}}
,{"_index":"throwtable","_type":"algorithm","_id":"jump-and-walk-algorithm","_score":0,"_source":{"description":"Jump-and-Walk is an algorithm for point location in triangulations (though most of the theoretical analysis were performed in 2D and 3D random Delaunay triangulations). Surprisingly, the algorithm does not need any preprocessing or complex data structures except some simple representation of the triangulation itself. The predecessor of Jump-and-Walk was due to Lawson (1977) and Green and Sibson (1978), which picks a random starting point S and then walks from S toward the query point Q one triangle at a time. But no theoretical analysis was known for these predecessors until after mid-1990s.\nJump-and-Walk picks a small group of sample points and starts the walk from the sample point which is the closest to Q until the simplex containing Q is found. The algorithm was a folklore in practice for some time, and the formal presentation of the algorithm and the analysis of its performance on 2D random Delaunay triangulation was done by Devroye, Mucke and Zhu in mid-1990s (the paper appeared in Algorithmica, 1998). The analysis on 3D random Delaunay triangulation was done by Mucke, Saias and Zhu (ACM Symposium of Computational Geometry, 1996). In both cases, a boundary condition was assumed, namely, Q must be slightly away from the boundary of the convex domain where the vertices of the random Delaunay triangulation are drawn. In 2004, Devroye, Lemaire and Moreau showed that in 2D the boundary condition can be withdrawn (the paper appeared in Computational Geometry: Theory and Applications, 2004).\nJump-and-Walk has been used in many famous software packages, e.g., QHULL, Triangle and CGAL.","name":"Jump-and-Walk algorithm","categories":["Algorithms","Triangulation (geometry)"],"tag_line":"Jump-and-Walk is an algorithm for point location in triangulations (though most of the theoretical analysis were performed in 2D and 3D random Delaunay triangulations)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hyphenation-algorithm","_score":0,"_source":{"description":"A hyphenation algorithm is a set of rules (especially one codified for implementation in a computer program) that decides at which points a word can be broken over two lines with a hyphen. For example, a hyphenation algorithm might decide that impeachment can be broken as impeach-ment or im-peachment, but not, say, as impe-achment.\nOne of the reasons for the complexity of the rules of word-breaking is that different \"dialects\" of English tend to differ on the rule: American English tends to work on sound, while British English tends to look to the origins of the word and then to sound. There are also a large number of exceptions, which further complicates matters.\nSome rules of thumb can be found in the reference \"On Hyphenation – Anarchy of Pedantry\". Among algorithmic approaches to hyphenation, the one implemented in the TeX typesetting system is widely used. It is thoroughly documented in the first two volumes of Computers and Typesetting and in Frank Liang's dissertation. Contrary to the belief that TeX relies on a large dictionary of exceptions, the point of Liang's work was to get the algorithm as accurate as he practically could and keep any exception dictionary small. In TeX's original hyphenation patterns for US English, the exception list contains fourteen words.","name":"Hyphenation algorithm","categories":["Algorithms","Articles needing more viewpoints from December 2013","Digital typography"],"tag_line":"A hyphenation algorithm is a set of rules (especially one codified for implementation in a computer program) that decides at which points a word can be broken over two lines with a hyphen."}}
,{"_index":"throwtable","_type":"algorithm","_id":"algorithm-design","_score":0,"_source":{"description":"Algorithm design is a specific method to create a mathematical process in solving problems. Applied algorithm design is algorithm engineering.\nAlgorithm design is identified and incorporated into many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are algorithm design patterns, such as template method pattern and decorator pattern, and uses of data structures, and name and sort lists. Some current day uses of algorithm design can be found in internet retrieval processes of web crawling, packet routing and caching.\nMainframe programming languages such as ALGOL (for Algorithmic language), FORTRAN, COBOL, PL/I, SAIL, and SNOBOL are computing tools to implement an \"algorithm design\"... but, an \"algorithm design\" (a/d) is not a language. An a/d can be a hand written process, e.g. set of equations, a series of mechanical processes done by hand, an analog piece of equipment, or a digital process and/or processor.\nOne of the most important aspects of algorithm design is creating an algorithm that has an efficient runtime, also known as its big Oh.\nSteps in development of Algorithms\nProblem definition\nDevelopment of a model\nSpecification of Algorithm\nDesigning an Algorithm\nChecking the correctness of Algorithm\nAnalysis of Algorithm\nImplementation of Algorithm\nProgram testing\nDocumentation Preparation","name":"Algorithm design","categories":["Algorithms","All stub articles","Mathematical analysis stubs","Operations research"],"tag_line":"Algorithm design is a specific method to create a mathematical process in solving problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hakmem","_score":0,"_source":{"description":"HAKMEM, alternatively known as AI Memo 239, is a February 1972 \"memo\" (technical report) of the MIT AI Lab containing a wide variety of hacks, including useful and clever algorithms for mathematical computation, some number theory and schematic diagrams for hardware — in Guy L. Steele's words, \"a bizarre and eclectic potpourri of technical trivia\". Contributors included about two dozen members and associates of the AI Lab. The title of the report is short for \"hacks memo\", abbreviated to six upper case characters that would fit in a single PDP-10 machine word (using a six-bit character set).\n^ a b Steele's foreword to Hank S. Warren (2012). Hacker's Delight (PDF). Addison–Wesley. p. xi.","name":"HAKMEM","categories":["1972 in Massachusetts","Algorithms","All articles lacking reliable references","All articles with unsourced statements","Articles lacking reliable references from July 2015","Articles with unsourced statements from August 2014","Computer science papers","Memoranda"],"tag_line":"HAKMEM, alternatively known as AI Memo 239, is a February 1972 \"memo\" (technical report) of the MIT AI Lab containing a wide variety of hacks, including useful and clever algorithms for mathematical computation, some number theory and schematic diagrams for hardware — in Guy L. Steele's words, \"a bizarre and eclectic potpourri of technical trivia\"."}}
,{"_index":"throwtable","_type":"algorithm","_id":"collaborative-diffusion","_score":0,"_source":{"description":"Collaborative Diffusion is a type of pathfinding algorithm which uses the concept of antiobjects, objects within a computer program that function opposite to what would be conventionally expected. Collaborative Diffusion is typically used in video games, when multiple agents must path towards a single target agent. For example, the ghosts in Pac-Man. In this case, the background tiles serve as antiobjects, carrying out the necessary calculations for creating a path and having the foreground objects react accordingly, whereas having foreground objects be responsible for their own pathing would be conventionally expected.\nCollaborative Diffusion is favored for its efficiency over other pathfinding algorithms, such as A*, when handling multiple agents. Also, this method allows elements of competition and teamwork to easily be incorporated between tracking agents. Notably, the time taken to calculate paths remains constant as the number of agents increases.","name":"Collaborative diffusion","categories":["Algorithms"],"tag_line":"Collaborative Diffusion is a type of pathfinding algorithm which uses the concept of antiobjects, objects within a computer program that function opposite to what would be conventionally expected."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cascade-learning-based-on-adaboost","_score":0,"_source":{"description":"The Boosting Algorithms for Detector Cascade Learning is proposed by Mohammad Saberian and Nuno Vasconcelos in 2014, it is based on Viola–Jones object detection framework.\n^ P. Bartlett and M. Traskin. Adaboost is consistent. Journal of Machine Learning Research,8:2347–2368, December 2007.\n^ S. Brubaker, M. Mullin, and J. Rehg. On the design of cascades of boosted ensembles for face detection. International Journal of Computer Vision, 77:65–86, 2008.\n^ Boosting Algorithms for Detector Cascade Learning\n^ Rapid object detection using a boosted cascade of simple features","name":"Cascade Learning Based on Adaboost","categories":["Algorithms","Algorithms and data structures","All articles covered by WikiProject Wikify","All articles needing expert attention","All articles that are too technical","All articles with too few wikilinks","All orphaned articles","Articles covered by WikiProject Wikify from August 2015","Articles needing expert attention from August 2015","Articles with too few wikilinks from August 2015","Face recognition","Object recognition and categorization","Orphaned articles from September 2015","Wikipedia articles that are too technical from August 2015"],"tag_line":"The Boosting Algorithms for Detector Cascade Learning is proposed by Mohammad Saberian and Nuno Vasconcelos in 2014, it is based on Viola–Jones object detection framework."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lancichinetti-fortunato-radicchi-benchmark","_score":0,"_source":{"description":"Lancichinetti-Fortunato-Radicchi (LFR) benchmark is an algorithm that generates benchmark networks (artificial networks that resemble real-world networks). They have a priori known communities and are used to compare different community detection methods. The advantage of LFR over other methods is that it accounts for the heterogeneity in the distributions of node degrees and of community sizes.","name":"Lancichinetti-Fortunato-Radicchi Benchmark","categories":["Algorithms","All articles covered by WikiProject Wikify","All articles needing expert attention","All articles that are too technical","All articles with too few wikilinks","All orphaned articles","Articles covered by WikiProject Wikify from June 2015","Articles needing expert attention from June 2015","Articles with too few wikilinks from June 2015","Computer benchmarks","Orphaned articles from June 2015","Statistical models","Wikipedia articles that are too technical from June 2015"],"tag_line":"Lancichinetti-Fortunato-Radicchi (LFR) benchmark is an algorithm that generates benchmark networks (artificial networks that resemble real-world networks)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"apx","_score":0,"_source":{"description":"In complexity theory the class APX (an abbreviation of \"approximable\") is the set of NP optimization problems that allow polynomial-time approximation algorithms with approximation ratio bounded by a constant (or constant-factor approximation algorithms for short). In simple terms, problems in this class have efficient algorithms that can find an answer within some fixed multiplicative factor of the optimal answer. The class APX is also sometimes known as MaxSNP because the basis of its definition is formed by SNP.\nAn approximation algorithm is called a -approximation algorithm for input size  if it can be proven that the solution that the algorithm finds is at most a multiplicative factor of  times worse than the optimal solution. Here,  is called the approximation ratio. Problems in APX are those with algorithms for which the approximation ratio  is a constant . The approximation ratio is conventionally stated greater than 1. In the case of minimization problems,  is the found solution's score divided by the optimum solution's score, while for maximization problems the reverse is the case. For maximization problems, where an inferior solution has a smaller score,  is sometimes stated as less than 1; in such cases, the reciprocal of  is the ratio of the score of the found solution to the score of the optimum solution.\nIf there is a polynomial-time algorithm to solve a problem to within every multiplicative factor of the optimum other than 1, then the problem is said to have a polynomial-time approximation scheme (PTAS). Unless P=NP there exist problems that are in APX but without a PTAS, so the class of problems with a PTAS is strictly contained in APX. One such problem is the bin packing problem.","name":"APX","categories":["Approximation algorithms","Complexity classes"],"tag_line":"In complexity theory the class APX (an abbreviation of \"approximable\") is the set of NP optimization problems that allow polynomial-time approximation algorithms with approximation ratio bounded by a constant (or constant-factor approximation algorithms for short)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sieve-of-eratosthenes","_score":0,"_source":{"description":"In mathematics, the sieve of Eratosthenes (Ancient Greek: κόσκινον Ἐρατοσθένους, kóskinon Eratosthénous), one of a number of prime number sieves, is a simple, ancient algorithm for finding all prime numbers up to any given limit. It does so by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting with the multiples of 2.\nThe multiples of a given prime are generated as a sequence of numbers starting from that prime, with constant difference between them that is equal to that prime. This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime.\nThe sieve of Eratosthenes is one of the most efficient ways to find all of the smaller primes. It is named after Eratosthenes of Cyrene, a Greek mathematician; although none of his works have survived, the sieve was described and attributed to Eratosthenes in the Introduction to Arithmetic by Nicomachus.\nThe sieve may be used to find primes in arithmetic progressions.","name":"Sieve of Eratosthenes","categories":["Algorithms","All articles needing additional references","Articles containing Ancient Greek-language text","Articles containing Greek-language text","Articles needing additional references from June 2015","Articles with example pseudocode","Pages using citations with accessdate and no URL","Primality tests","Sieve theory"],"tag_line":"In mathematics, the sieve of Eratosthenes (Ancient Greek: κόσκινον Ἐρατοσθένους, kóskinon Eratosthénous), one of a number of prime number sieves, is a simple, ancient algorithm for finding all prime numbers up to any given limit."}}
,{"_index":"throwtable","_type":"algorithm","_id":"alpha-max-plus-beta-min-algorithm","_score":0,"_source":{"description":"The alpha max plus beta min algorithm is a high-speed approximation of the square root of the sum of two squares. The square root of the sum of two squares, also known as Pythagorean addition, is a useful function, because it finds the hypotenuse of a right triangle given the two side lengths, the norm of a 2-D vector, or the magnitude of a complex number z=a+bi given the real and imaginary parts.\n\nThe algorithm avoids performing the square and square-root operations, instead using simple operations such as comparison, multiplication, and addition. Some choices of the α and β parameters of the algorithm allow the multiplication operation to be reduced to a simple shift of binary digits that is particularly well suited to implementation in high-speed digital circuitry.\nThe approximation is expressed as:\n\nWhere  is the maximum absolute value of a and b and  is the minimum absolute value of a and b.\nFor the closest approximation, the optimum values for  and  are  and , giving a maximum error of 3.96%.","name":"Alpha max plus beta min algorithm","categories":["All articles with links needing disambiguation","Approximation algorithms","Articles with links needing disambiguation from November 2014","Root-finding algorithms"],"tag_line":"The alpha max plus beta min algorithm is a high-speed approximation of the square root of the sum of two squares."}}
,{"_index":"throwtable","_type":"algorithm","_id":"one-pass-algorithm","_score":0,"_source":{"description":"In computing, a one-pass algorithm is one which reads its input exactly once, in order, without unbounded buffering. A one-pass algorithm generally requires O(n) (see 'big O' notation) time and less than O(n) storage (typically O(1)), where n is the size of the input.\nBasically one-pass algorithm operates as follows: (1) the object descriptions are processed serially; (2) the first object becomes the cluster representative of the first cluster; (3) each subsequent object is matched against all cluster representatives existing at its processing time; (4) a given object is assigned to one cluster (or more if overlap is allowed) according to some condition on the matching function; (5) when an object is assigned to a cluster the representative for that cluster is recomputed; (6) if an object fails a certain test it becomes the cluster representative of a new cluster","name":"One-pass algorithm","categories":["Algorithms","All articles lacking sources","Articles lacking sources from January 2007"],"tag_line":"In computing, a one-pass algorithm is one which reads its input exactly once, in order, without unbounded buffering."}}
,{"_index":"throwtable","_type":"algorithm","_id":"flowchart","_score":0,"_source":{"description":"A flowchart is a type of diagram that represents an algorithm, workflow or process, showing the steps as boxes of various kinds, and their order by connecting them with arrows. This diagrammatic representation illustrates a solution model to a given problem. Flowcharts are used in analyzing, designing, documenting or managing a process or program in various fields.","name":"Flowchart","categories":["Algorithm description languages","American inventions","Articles with example code","CS1 errors: external links","Computer programming","Diagrams","Quality control tools","Technical communication","Wikipedia articles with GND identifiers"],"tag_line":"A flowchart is a type of diagram that represents an algorithm, workflow or process, showing the steps as boxes of various kinds, and their order by connecting them with arrows."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pluscal","_score":0,"_source":{"description":"PlusCal (formerly called +CAL) is a formal specification language created by Leslie Lamport, which transpiles to TLA+. In contrast to TLA+'s action-oriented focus on distributed systems, PlusCal most resembles an imperative programming language and is better-suited to specifying sequential algorithms. PlusCal was designed to replace pseudocode, retaining its simplicity while providing a formally-defined and verifiable language. A one-bit clock is written in PlusCal as follows:\n\nPlusCal tools and documentation are found on the PlusCal Algorithm Language page.","name":"PlusCal","categories":["Algorithm description languages","All stub articles","Computer science stubs","Formal methods","Formal specification languages"],"tag_line":"PlusCal (formerly called +CAL) is a formal specification language created by Leslie Lamport, which transpiles to TLA+."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pollard's-kangaroo-algorithm","_score":0,"_source":{"description":"In computational number theory and computational algebra, Pollard's kangaroo algorithm (aka Pollard's lambda algorithm, see Naming below) is an algorithm for solving the discrete logarithm problem. The algorithm was introduced in 1978 by the number theorist J. M. Pollard, in the same paper  as his better-known ρ algorithm for solving the same problem. Although Pollard described the application of his algorithm to the discrete logarithm problem in the multiplicative group of units modulo a prime p, it is in fact a generic discrete logarithm algorithm—it will work in any finite cyclic group.","name":"Pollard's kangaroo algorithm","categories":["Computer algebra","Logarithms","Number theoretic algorithms"],"tag_line":"In computational number theory and computational algebra, Pollard's kangaroo algorithm (aka Pollard's lambda algorithm, see Naming below) is an algorithm for solving the discrete logarithm problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"xor-swap-algorithm","_score":0,"_source":{"description":"In computer programming, the XOR swap is an algorithm that uses the XOR bitwise operation to swap values of distinct variables having the same data type without using a temporary variable. \"Distinct\" means that the variables are stored at different memory addresses; the actual values of the variables do not have to be different.","name":"XOR swap algorithm","categories":["Algorithms","All articles needing additional references","Articles needing additional references from February 2012","Articles with example C code","Binary arithmetic"],"tag_line":"In computer programming, the XOR swap is an algorithm that uses the XOR bitwise operation to swap values of distinct variables having the same data type without using a temporary variable."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ping-pong-scheme","_score":0,"_source":{"description":"Algorithms said to employ a Ping-Pong scheme exist in different fields of Software Engineering. They are characterized by an alternation between two entities. In the examples described below, these entities are communication partners, network paths or file blocks.","name":"Ping-pong scheme","categories":["Algorithms","All articles needing additional references","Articles needing additional references from June 2010"],"tag_line":"Algorithms said to employ a Ping-Pong scheme exist in different fields of Software Engineering."}}
,{"_index":"throwtable","_type":"algorithm","_id":"unique-games-conjecture","_score":0,"_source":{"description":"In computational complexity theory, the Unique Games Conjecture is a conjecture made by Subhash Khot in 2002. The conjecture postulates that the problem of determining the approximate value of a certain type of game, known as a unique game, has NP-hard algorithmic complexity. It has broad applications in the theory of hardness of approximation. If it is true, then for many important problems it is not only impossible to get an exact solution in polynomial time (as postulated by the P versus NP problem), but also impossible to get a good polynomial-time approximation. The problems for which such an inapproximability result would hold include constraint satisfaction problems which crop up in a wide variety of disciplines.\nThe conjecture is unusual in that the academic world seems about evenly divided on whether it is true or not.\n\n\"Some very natural, intrinsically interesting statements about things like voting and foams just popped out of studying the UGC.... Even if the UGC turns out to be false, it has inspired a lot of interesting math research.","name":"Unique games conjecture","categories":["Approximation algorithms","Computational complexity theory","Computational hardness assumptions","Conjectures","Unsolved problems in computer science"],"tag_line":"In computational complexity theory, the Unique Games Conjecture is a conjecture made by Subhash Khot in 2002."}}
,{"_index":"throwtable","_type":"algorithm","_id":"genetic-algorithm","_score":0,"_source":{"description":"In the field of artificial intelligence, a genetic algorithm (GA) is a search heuristic that mimics the process of natural selection. This heuristic (also sometimes called a metaheuristic) is routinely used to generate useful solutions to optimization and search problems. Genetic algorithms belong to the larger class of evolutionary algorithms (EA), which generate solutions to optimization problems using techniques inspired by natural evolution, such as inheritance, mutation, selection, and crossover.","alt_names":["GATTO"],"name":"Genetic algorithm","categories":["All articles needing additional references","All articles with dead external links","All articles with unsourced statements","Articles needing additional references from May 2011","Articles with dead external links from September 2011","Articles with unsourced statements from August 2007","Articles with unsourced statements from December 2011","CS1 errors: dates","CS1 errors: external links","Cybernetics","Digital organisms","Genetic algorithms","Mathematical optimization","Optimization algorithms and methods","Pages using citations with accessdate and no URL","Search algorithms","Use dmy dates from July 2013"],"tag_line":"In the field of artificial intelligence, a genetic algorithm (GA) is a search heuristic that mimics the process of natural selection."}}
,{"_index":"throwtable","_type":"algorithm","_id":"genetic-fuzzy-systems","_score":0,"_source":{"description":"Genetic fuzzy systems are fuzzy systems constructed by using genetic algorithms or genetic programming, which mimic the process of natural evolution, to identify its structure and parameter.\nWhen it comes to automatically identifying and building a fuzzy system, given the high degree of nonlinearity of the output, traditional linear optimization tools have several limitations. Therefore, in the framework of soft computing, genetic algorithms (GAs) and genetic programming (GP) methods have been used successfully to identify structure and parameters of fuzzy systems.","name":"Genetic fuzzy systems","categories":["All Wikipedia articles needing context","All pages needing cleanup","Computational linguistics","Genetic algorithms","Wikipedia articles needing context from October 2009","Wikipedia introduction cleanup from October 2009"],"tag_line":"Genetic fuzzy systems are fuzzy systems constructed by using genetic algorithms or genetic programming, which mimic the process of natural evolution, to identify its structure and parameter."}}
,{"_index":"throwtable","_type":"algorithm","_id":"edge-recombination-operator","_score":0,"_source":{"description":"The edge recombination operator (ERO) is an operator that creates a path that is similar to a set of existing paths (parents) by looking at the edges rather than the vertices. The main application of this is for crossover in genetic algorithms when a genotype with non-repeating gene sequences is needed such as for the travelling salesman problem. It was described by Darrell Whitley and others in 1989.","name":"Edge recombination operator","categories":["All articles needing additional references","All articles needing expert attention","Articles needing additional references from June 2011","Articles needing expert attention from June 2011","Articles needing expert attention with no reason or talk parameter","Articles needing unspecified expert attention","Genetic algorithms"],"tag_line":"The edge recombination operator (ERO) is an operator that creates a path that is similar to a set of existing paths (parents) by looking at the edges rather than the vertices."}}
,{"_index":"throwtable","_type":"algorithm","_id":"l-reduction","_score":0,"_source":{"description":"In computer science, particularly the study of approximation algorithms, an L-reduction (\"linear reduction\") is a transformation of optimization problems which linearly preserves approximability features; it is one type of approximation-preserving reduction. L-reductions in studies of approximability of optimization problems play a similar role to that of polynomial reductions in the studies of computational complexity of decision problems.\nThe term L reduction is sometimes used to refer to log-space reductions, by analogy with the complexity class L, but this is a different concept.","name":"L-reduction","categories":["All stub articles","Approximation algorithms","Computational complexity theory","Theoretical computer science stubs"],"tag_line":"In computer science, particularly the study of approximation algorithms, an L-reduction (\"linear reduction\") is a transformation of optimization problems which linearly preserves approximability features; it is one type of approximation-preserving reduction."}}
,{"_index":"throwtable","_type":"algorithm","_id":"island-algorithm","_score":0,"_source":{"description":"The island algorithm is an algorithm for performing inference on hidden Markov models, or their generalization, dynamic Bayesian networks. It calculates the marginal distribution for each unobserved node, conditional on any observed nodes.\nThe island algorithm is a modification of belief propagation. It trades smaller memory usage for longer running time: while belief propagation takes O(n) time and O(n) memory, the island algorithm takes O(n log n) time and O(log n) memory. On a computer with an unlimited number of processors, this can be reduced to O(n) total time, while still taking only O(log n) memory.","name":"Island algorithm","categories":["Bioinformatics algorithms","Hidden Markov models"],"tag_line":"The island algorithm is an algorithm for performing inference on hidden Markov models, or their generalization, dynamic Bayesian networks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"crossover-(genetic-algorithm)","_score":0,"_source":{"description":"In genetic algorithms, crossover is a genetic operator used to vary the programming of a chromosome or chromosomes from one generation to the next. It is analogous to reproduction and biological crossover, upon which genetic algorithms are based. Cross over is a process of taking more than one parent solutions and producing a child solution from them. There are methods for selection of the chromosomes. Those are also given below.","name":"Crossover (genetic algorithm)","categories":["All accuracy disputes","All articles to be expanded","Articles to be expanded from June 2013","Articles with disputed statements from June 2014","Genetic algorithms"],"tag_line":"In genetic algorithms, crossover is a genetic operator used to vary the programming of a chromosome or chromosomes from one generation to the next."}}
,{"_index":"throwtable","_type":"algorithm","_id":"genetic-algorithms-in-economics","_score":0,"_source":{"description":"Genetic algorithms have increasingly been applied to economics since the pioneering work by John H. Miller in 1986. It has been used to characterize a variety of models including the cobweb model, the overlapping generations model, game theory, schedule optimization and asset pricing. Specifically, it has been used as a model to represent learning, rather than as a means for fitting a model.","name":"Genetic algorithms in economics","categories":["Computational economics","Econometrics","Genetic algorithms","Optimization algorithms and methods","Production economics"],"tag_line":"Genetic algorithms have increasingly been applied to economics since the pioneering work by John H. Miller in 1986."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hardness-of-approximation","_score":0,"_source":{"description":"In computer science, hardness of approximation is a field that studies the algorithmic complexity of finding near-optimal solutions to optimization problems.","name":"Hardness of approximation","categories":["All stub articles","Approximation algorithms","Computational complexity theory","Mathematical optimization","Theoretical computer science stubs"],"tag_line":"In computer science, hardness of approximation is a field that studies the algorithmic complexity of finding near-optimal solutions to optimization problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mutation-(genetic-algorithm)","_score":0,"_source":{"description":"Mutation is a genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next. It is analogous to biological mutation. Mutation alters one or more gene values in a chromosome from its initial state. In mutation, the solution may change entirely from the previous solution. Hence GA can come to better solution by using mutation. Mutation occurs during evolution according to a user-definable mutation probability. This probability should be set low. If it is set too high, the search will turn into a primitive random search.\nThe classic example of a mutation operator involves a probability that an arbitrary bit in a genetic sequence will be changed from its original state. A common method of implementing the mutation operator involves generating a random variable for each bit in a sequence. This random variable tells whether or not a particular bit will be modified. This mutation procedure, based on the biological point mutation, is called single point mutation. Other types are inversion and floating point mutation. When the gene encoding is restrictive as in permutation problems, mutations are swaps, inversions, and scrambles.\nThe purpose of mutation in GAs is preserving and introducing diversity. Mutation should allow the algorithm to avoid local minima by preventing the population of chromosomes from becoming too similar to each other, thus slowing or even stopping evolution. This reasoning also explains the fact that most GA systems avoid only taking the fittest of the population in generating the next but rather a random (or semi-random) selection with a weighting toward those that are fitter.\nFor different genome types, different mutation types are suitable:\nBit string mutation\n\nThe mutation of bit strings ensue through bit flips at random positions.\n\nExample:\n\nThe probability of a mutation of a bit is , where  is the length of the binary vector. Thus, a mutation rate of  per mutation and individual selected for mutation is reached.\n\nFlip Bit\nThis mutation operator takes the chosen genome and inverts the bits (i.e. if the genome bit is 1, it is changed to 0 and vice versa).\nBoundary\nThis mutation operator replaces the genome with either lower or upper bound randomly. This can be used for integer and float genes.\nNon-Uniform\nThe probability that amount of mutation will go to 0 with the next generation is increased by using non-uniform mutation operator. It keeps the population from stagnating in the early stages of the evolution. It tunes solution in later stages of evolution. This mutation operator can only be used for integer and float genes.\nUniform\nThis operator replaces the value of the chosen gene with a uniform random value selected between the user-specified upper and lower bounds for that gene. This mutation operator can only be used for integer and float genes.\nGaussian\nThis operator adds a unit Gaussian distributed random value to the chosen gene. If it falls outside of the user-specified lower or upper bounds for that gene, the new gene value is clipped. This mutation operator can only be used for integer and float genes.","name":"Mutation (genetic algorithm)","categories":["Genetic algorithms"],"tag_line":"Mutation is a genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next."}}
,{"_index":"throwtable","_type":"algorithm","_id":"doomsday-rule","_score":0,"_source":{"description":"The Doomsday rule or Doomsday algorithm is a way of calculating the day of the week of a given date. It provides a perpetual calendar because the Gregorian calendar moves in cycles of 400 years.\nThis algorithm for mental calculation was devised by John Conway after drawing inspiration from Lewis Carroll's work on a perpetual calendar algorithm. It takes advantage of each year having a certain day of the week (the doomsday) upon which certain easy-to-remember dates fall; for example, 4/4, 6/6, 8/8, 10/10, 12/12, and the last day of February all occur on the same day of the week in any given year. Applying the Doomsday algorithm involves three steps:\nDetermine the \"anchor day\" for the century.\nUse the anchor day for the century to calculate the doomsday for the year.\nChoose the closest date out of the ones that always fall on the doomsday (e.g. 4/4, 6/6, 8/8), and count the number of days (modulo 7) between that date and the date in question to arrive at the day of the week.\nThis technique applies to both the Gregorian calendar A.D. and the Julian calendar, although their doomsdays will usually be different days of the week.\nSince this algorithm involves treating days of the week like numbers modulo 7, John Conway suggests thinking of the days of the week as \"Noneday\" or \"Sansday\" (for Sunday), \"Oneday\", \"Twosday\", \"Treblesday\", \"Foursday\", \"Fiveday\", and \"Six-a-day\".\nThe algorithm is simple enough for anyone with basic arithmetic ability to do the calculations mentally. Conway can usually give the correct answer in under two seconds. To improve his speed, he practices his calendrical calculations on his computer, which is programmed to quiz him with random dates every time he logs on.\n\n","name":"Doomsday rule","categories":["1973 introductions","All articles with unsourced statements","Articles with unsourced statements from January 2008","Calendar algorithms","Gregorian calendar","Julian calendar"],"tag_line":"The Doomsday rule or Doomsday algorithm is a way of calculating the day of the week of a given date."}}
,{"_index":"throwtable","_type":"algorithm","_id":"search-based-software-engineering","_score":0,"_source":{"description":"Search-based software engineering (SBSE) applies metaheuristic search techniques such as genetic algorithms, simulated annealing and tabu search to software engineering problems. Many activities in software engineering can be stated as optimization problems. Optimization techniques of operations research such as linear programming or dynamic programming are mostly impractical for large scale software engineering problems because of their computational complexity. Researchers and practitioners use metaheuristic search techniques to find near-optimal or \"good-enough\" solutions.\nSBSE problems can be divided into two types:\nblack-box optimization problems, for example, assigning people to tasks (a typical combinatorial optimization problem).\nwhite-box problems where operations on source code need to be considered.","name":"Search-based software engineering","categories":["2001 introductions","All articles with unsourced statements","Articles with unsourced statements from October 2013","Genetic algorithms","Optimization algorithms and methods","Pages containing cite templates with deprecated parameters","Program analysis","Search algorithms","Software articles needing expert attention","Software engineering","Software quality","Software testing","Use dmy dates from November 2011"],"tag_line":"Search-based software engineering (SBSE) applies metaheuristic search techniques such as genetic algorithms, simulated annealing and tabu search to software engineering problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quality-control-and-genetic-algorithms","_score":0,"_source":{"description":"The combination of quality control and genetic algorithms led to novel solutions of complex quality control design and optimization problems. Quality control is a process by which entities review the quality of all factors involved in production. Quality is the degree to which a set of inherent characteristics fulfils a need or expectation that is stated, general implied or obligatory. Genetic algorithms are search algorithms, based on the mechanics of natural selection and natural genetics.","name":"Quality control and genetic algorithms","categories":["Genetic algorithms","Quality control"],"tag_line":"The combination of quality control and genetic algorithms led to novel solutions of complex quality control design and optimization problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"heap's-algorithm","_score":0,"_source":{"description":"Heap's algorithm generates all possible permutations of N objects. It was first proposed by B. R. Heap in 1963. The algorithm minimizes movement: it generates each permutation from the previous one by interchanging a single pair of elements; the other N−2 elements are not disturbed. In a 1977 review of permutation-generating algorithms, Robert Sedgewick concluded that it was at that time the most effective algorithm for generating permutations by computer.","name":"Heap's algorithm","categories":["Combinatorial algorithms","Permutations"],"tag_line":"Heap's algorithm generates all possible permutations of N objects."}}
,{"_index":"throwtable","_type":"algorithm","_id":"genetic-memory-(computer-science)","_score":0,"_source":{"description":"In computer science, genetic memory refers to an artificial neural network combination of genetic algorithm and the mathematical model of sparse distributed memory. It can be used to predict weather patterns. Genetic memory and genetic algorithms have also gained an interest in the creation of artificial life.","name":"Genetic memory (computer science)","categories":["All stub articles","Computer science stubs","Genetic algorithms"],"tag_line":"In computer science, genetic memory refers to an artificial neural network combination of genetic algorithm and the mathematical model of sparse distributed memory."}}
,{"_index":"throwtable","_type":"algorithm","_id":"damm-algorithm","_score":0,"_source":{"description":"In error detection, the Damm algorithm is a check digit algorithm that detects all single-digit errors and all adjacent transposition errors. It was presented by H. Michael Damm in 2004.","name":"Damm algorithm","categories":["Algebraic structures","CS1 German-language sources (de)","CS1 errors: external links","Checksum algorithms","Group theory","Latin squares"],"tag_line":"In error detection, the Damm algorithm is a check digit algorithm that detects all single-digit errors and all adjacent transposition errors."}}
,{"_index":"throwtable","_type":"algorithm","_id":"coreset","_score":0,"_source":{"description":"In computational geometry, a coreset is a small set of points that approximates the shape of a larger point set, in the sense that applying some geometric measure to the two sets (such as their minimum bounding box volume) results in approximately equal numbers. Many natural geometric optimization problems have coresets that approximate an optimal solution to within a factor of 1 + ε, that can be found quickly (in linear time or near-linear time), and that have size bounded by a function of 1/ε independent of the input size, where ε is an arbitrary positive number. When this is the case, one obtains a linear-time or near-linear time approximation scheme, based on the idea of finding a coreset and then applying an exact optimization algorithm to the coreset. Regardless of how slow the exact optimization algorithm is, for any fixed choice of ε, the running time of this approximation scheme will be O(1) plus the time to find the coreset.","name":"Coreset","categories":["Algorithms and data structures stubs","All stub articles","Computational geometry","Computer science stubs"],"tag_line":"In computational geometry, a coreset is a small set of points that approximates the shape of a larger point set, in the sense that applying some geometric measure to the two sets (such as their minimum bounding box volume) results in approximately equal numbers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"euler-tour-technique","_score":0,"_source":{"description":"The Euler tour technique (ETT), named after Leonhard Euler, is a method in graph theory for representing trees. The tree is viewed as a directed graph that contains two directed edges for each edge in the tree. The tree can then be represented as a Eulerian circuit of the directed graph, known as the Euler tour representation (ETR) of the tree. The ETT allows for efficient, parallel computation of solutions to common problems in algorithmic graph theory. It was introduced by Tarjan and Vishkin in 1984.","name":"Euler tour technique","categories":["Graph algorithms","Parallel computing"],"tag_line":"The Euler tour technique (ETT), named after Leonhard Euler, is a method in graph theory for representing trees."}}
,{"_index":"throwtable","_type":"algorithm","_id":"edmonds'-algorithm","_score":0,"_source":{"description":"In graph theory, a branch of mathematics, Edmonds' algorithm or Chu–Liu/Edmonds' algorithm is an algorithm for finding a spanning arborescence of minimum weight (sometimes called an optimum branching). It is the directed analog of the minimum spanning tree problem. The algorithm was proposed independently first by Yoeng-jin Chu and Tseng-hong Liu (1965) and then by Jack Edmonds (1967).","name":"Edmonds' algorithm","categories":["Graph algorithms"],"tag_line":"In graph theory, a branch of mathematics, Edmonds' algorithm or Chu–Liu/Edmonds' algorithm is an algorithm for finding a spanning arborescence of minimum weight (sometimes called an optimum branching)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"algorithmic-version-for-szemerédi-regularity-partition","_score":0,"_source":{"description":"A Simple Algorithm for Constructing Szemerédi's Regularity Partition is a paper by Alan M. Frieze and Ravi Kannan giving an algorithmic version of the Szemerédi regularity lemma to find an ε-regular partition of a given graph.","name":"Algorithmic version for Szemerédi regularity partition","categories":["Graph algorithms","Mathematics papers"],"tag_line":"A Simple Algorithm for Constructing Szemerédi's Regularity Partition is a paper by Alan M. Frieze and Ravi Kannan giving an algorithmic version of the Szemerédi regularity lemma to find an ε-regular partition of a given graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"kernighan–lin-algorithm","_score":0,"_source":{"description":"This article is about the heuristic algorithm for the graph partitioning problem. For a heuristic for the traveling salesperson problem, see Lin–Kernighan heuristic.\nKernighan–Lin is a O(n2 log(n)) heuristic algorithm for solving the graph partitioning problem. The algorithm has important applications in the layout of digital circuits and components in VLSI.","name":"Kernighan–Lin algorithm","categories":["Combinatorial algorithms","Combinatorial optimization","Heuristic algorithms"],"tag_line":"This article is about the heuristic algorithm for the graph partitioning problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"depth-limited-search","_score":0,"_source":{"description":"In computer science depth-limited search is an algorithm to explore the vertices of a graph. It is a modification of depth-first search and is used for example in the iterative deepening depth-first search algorithm.","name":"Depth-limited search","categories":["All articles lacking in-text citations","Articles lacking in-text citations from June 2013","Graph algorithms","Search algorithms"],"tag_line":"In computer science depth-limited search is an algorithm to explore the vertices of a graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"tompkins–paige-algorithm","_score":0,"_source":{"description":"The Tompkins–Paige algorithm is a computer algorithm for generating all permutations of a finite set of objects.","name":"Tompkins–Paige algorithm","categories":["All articles covered by WikiProject Wikify","All articles with too few wikilinks","Articles covered by WikiProject Wikify from March 2013","Articles with too few wikilinks from March 2013","Combinatorial algorithms","Permutations"],"tag_line":"The Tompkins–Paige algorithm is a computer algorithm for generating all permutations of a finite set of objects."}}
,{"_index":"throwtable","_type":"algorithm","_id":"prim's-algorithm","_score":0,"_source":{"description":"In computer science, Prim's algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. The algorithm operates by building this tree one vertex at a time, from an arbitrary starting vertex, at each step adding the cheapest possible connection from the tree to another vertex.\nThe algorithm was developed in 1930 by Czech mathematician Vojtěch Jarník and later rediscovered and republished by computer scientists Robert C. Prim in 1957 and Edsger W. Dijkstra in 1959. Therefore, it is also sometimes called the DJP algorithm, Jarník's algorithm, the Prim–Jarník algorithm, or the Prim–Dijkstra algorithm.\nOther well-known algorithms for this problem include Kruskal's algorithm and Borůvka's algorithm. These algorithms find the minimum spanning forest in a possibly disconnected graph; in contrast, the most basic form of Prim's algorithm only finds minimum spanning trees in connected graphs. However, running Prim's algorithm separately for each connected component of the graph, it can also be used to find the minimum spanning forest. In terms of their asymptotic time complexity, these three algorithms are equally fast for sparse graphs, but slower than other more sophisticated algorithms. However, for graphs that are sufficiently dense, Prim's algorithm can be made to run in linear time, meeting or improving the time bounds for other algorithms.","name":"Prim's algorithm","categories":["Articles containing proofs","Articles containing video clips","CS1 Czech-language sources (cs)","Graph algorithms","Spanning tree"],"tag_line":"In computer science, Prim's algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"jump-point-search","_score":0,"_source":{"description":"In computer science, Jump Point Search (JPS) is an optimization to the A* search algorithm pathfinding algorithm for uniform-cost grids. It reduces symmetries in the search procedure by means of graph pruning, eliminating certain nodes in the grid based on assumptions that can be made about the current node's neighbors, as long as certain conditions relating to the grid are satisfied. As a result, the algorithm can consider long \"jumps\" along straight (horizontal, vertical and diagonal) lines in the grid, rather than the small steps from one grid position to the next that ordinary A* considers.\nJump point search preserves A*'s optimality, while potentially reducing its running time by an order of magnitude.","name":"Jump point search","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Game artificial intelligence","Graph algorithms","Search algorithms"],"tag_line":"In computer science, Jump Point Search (JPS) is an optimization to the A* search algorithm pathfinding algorithm for uniform-cost grids."}}
,{"_index":"throwtable","_type":"algorithm","_id":"misra-&-gries-edge-coloring-algorithm","_score":0,"_source":{"description":"The Misra & Gries edge coloring algorithm is a polynomial time algorithm in graph theory that finds an edge coloring of any graph. The coloring produces uses at most  colors, where  is the maximum degree of the graph. This is optimal for some graphs, and by Vizing's theorem it uses at most one color more than the optimal for all others.\nIt was first published by Jayadev Misra and David Gries in 1992. It is a simplification of a prior algorithm by Béla Bollobás.\nThis algorithm is the fastest known almost-optimal algorithm for edge coloring, executing in  time. A faster time bound of  was claimed in a 1985 technical report by Gabow et al., but this has never been published.\nIn general, optimal edge coloring is NP-complete, so it is very unlikely that a polynomial time algorithm exists. There are however exponential time exact edge coloring algorithms that give an optimal solution.","name":"Misra & Gries edge coloring algorithm","categories":["Graph algorithms","Graph coloring"],"tag_line":"The Misra & Gries edge coloring algorithm is a polynomial time algorithm in graph theory that finds an edge coloring of any graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"path-based-strong-component-algorithm","_score":0,"_source":{"description":"In graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, one to keep track of the vertices in the current component and the second to keep track of the current search path. Versions of this algorithm have been proposed by Purdom (1970), Munro (1971), Dijkstra (1976), Cheriyan & Mehlhorn (1996), and Gabow (2000); of these, Dijkstra's version was the first to achieve linear time.","name":"Path-based strong component algorithm","categories":["Graph algorithms","Graph connectivity"],"tag_line":"In graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, one to keep track of the vertices in the current component and the second to keep track of the current search path."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rocha–thatte-cycle-detection-algorithm","_score":0,"_source":{"description":"Rocha–Thatte algorithm is a distributed algorithm in graph theory for detecting cycles on large-scale directed graphs based on the bulk synchronous message passing abstraction. This algorithm for detecting cycles by message passing is suitable to be implemented in distributed graph processing systems, and it is also suitable for implementations in systems for disk-based computations, such as the GraphChi, where the computation is mainly based on secondary memory. Disk-based computations are necessary when we have a single computer for processing large-scale graphs, and the computation exceeds the primary memory capacity.","name":"Rocha–Thatte cycle detection algorithm","categories":["Graph algorithms"],"tag_line":"Rocha–Thatte algorithm is a distributed algorithm in graph theory for detecting cycles on large-scale directed graphs based on the bulk synchronous message passing abstraction."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sma*","_score":0,"_source":{"description":"SMA* or Simplified Memory Bounded A* is a shortest path algorithm based on the A* algorithm. The main advantage of SMA* is that it uses a bounded memory, while the A* algorithm might need exponential memory. All other characteristics of SMA* are inherited from A*.\n\n","name":"SMA*","categories":["All articles needing additional references","All articles needing expert attention","All articles that are too technical","Articles needing additional references from March 2015","Articles needing expert attention from November 2009","Articles with example pseudocode","Game artificial intelligence","Graph algorithms","Routing algorithms","Search algorithms","Wikipedia articles that are too technical from November 2009"],"tag_line":"SMA* or Simplified Memory Bounded A* is a shortest path algorithm based on the A* algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"goal-node-(computer-science)","_score":0,"_source":{"description":"In computer science, a goal node is a node in a graph that meets defined criteria for success or termination.\nHeuristical artificial intelligence algorithms, like A* and B*, attempt to reach such nodes in optimal time by defining the distance to the goal node. When the goal node is reached, A* defines the distance to the goal node as 0 and all other nodes' distances as positive values.","name":"Goal node (computer science)","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Graph algorithms"],"tag_line":"In computer science, a goal node is a node in a graph that meets defined criteria for success or termination."}}
,{"_index":"throwtable","_type":"algorithm","_id":"tarjan's-strongly-connected-components-algorithm","_score":0,"_source":{"description":"Tarjan's Algorithm is an algorithm in graph theory for finding the strongly connected components of a graph. Although proposed earlier, it can be seen as an improved version of Kosaraju's algorithm, and is comparable in efficiency to the path-based strong component algorithm. Tarjan's Algorithm is named for its discoverer, Robert Tarjan.","name":"Tarjan's strongly connected components algorithm","categories":["Articles with example pseudocode","Graph algorithms","Graph connectivity"],"tag_line":"Tarjan's Algorithm is an algorithm in graph theory for finding the strongly connected components of a graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"flooding-algorithm","_score":0,"_source":{"description":"A flooding algorithm is an algorithm for distributing material to every part of a graph. The name derives from the concept of inundation by a flood.\nFlooding algorithms are used in computer networking and graphics. Flooding algorithms are also useful for solving many mathematical problems, including maze problems and many problems in graph theory.","name":"Flooding algorithm","categories":["All stub articles","Computer science stubs","Graph algorithms","Routing algorithms"],"tag_line":"A flooding algorithm is an algorithm for distributing material to every part of a graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"travelling-salesman-problem","_score":0,"_source":{"description":"The travelling salesman problem (TSP) asks the following question: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city? It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science.\n\nTSP is a special case of the travelling purchaser problem and the Vehicle routing problem.\nIn the theory of computational complexity, the decision version of the TSP (where, given a length L, the task is to decide whether the graph has any tour shorter than L) belongs to the class of NP-complete problems. Thus, it is possible that the worst-case running time for any algorithm for the TSP increases superpolynomially (perhaps, specifically, exponentially) with the number of cities.\nThe problem was first formulated in 1930 and is one of the most intensively studied problems in optimization. It is used as a benchmark for many optimization methods. Even though the problem is computationally difficult, a large number of heuristics and exact methods are known, so that some instances with tens of thousands of cities can be solved completely and even problems with millions of cities can be approximated within a small fraction of 1%.\nThe TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing. In these applications, the concept city represents, for example, customers, soldering points, or DNA fragments, and the concept distance represents travelling times or cost, or a similarity measure between DNA fragments. The TSP also appears in astronomy, as astronomers observing many sources will want to minimise the time spent slewing the telescope between the sources. In many applications, additional constraints such as limited resources or time windows may be imposed.","name":"Travelling salesman problem","categories":["Commons category with local link same as on Wikidata","Computational problems in graph theory","Graph algorithms","Hamiltonian paths and cycles","NP-complete problems","NP-hard problems","Operations research","Pages containing cite templates with deprecated parameters","Travelling salesman problem","Use dmy dates from July 2012"],"tag_line":"The travelling salesman problem (TSP) asks the following question: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city?"}}
,{"_index":"throwtable","_type":"algorithm","_id":"internal-sort","_score":0,"_source":{"description":"An internal sort is any data sorting process that takes place entirely within the main memory of a computer. This is possible whenever the data to be sorted is small enough to all be held in the main memory. For sorting larger datasets, it may be necessary to hold only a chunk of data in memory at a time, since it won’t all fit. The rest of the data is normally held on some larger, but slower medium, like a hard-disk. Any reading or writing of data to and from this slower media can slow the sortation process considerably. This issue has implications for different sort algorithms.\nSome common internal sorting algorithms include:\nBubble Sort\nInsertion Sort\nQuick Sort\nHeap Sort\nRadix Sort\nSelection sort\nConsider a Bubblesort, where adjacent records are swapped in order to get them into the right order, so that records appear to “bubble” up and down through the dataspace. If this has to be done in chunks, then when we have sorted all the records in chunk 1, we move on to chunk 2, but we find that some of the records in chunk 1 need to “bubble through” chunk 2, and vice versa (i.e., there are records in chunk 2 that belong in chunk 1, and records in chunk 1 that belong in chunk 2 or later chunks). This will cause the chunks to be read and written back to disk many times as records cross over the boundaries between them, resulting in a considerable degradation of performance. If the data can all be held in memory as one large chunk, then this performance hit is avoided.\nOn the other hand, some algorithms handle external sorting rather better. A Merge sort breaks the data up into chunks, sorts the chunks by some other algorithm (maybe bubblesort or Quick sort) and then recombines the chunks two by two so that each recombined chunk is in order. This approach minimises the number or reads and writes of data-chunks from disk, and is a popular external sort method.","name":"Internal sort","categories":["All articles to be merged","Articles to be merged from March 2012","Sorting algorithms"],"tag_line":"An internal sort is any data sorting process that takes place entirely within the main memory of a computer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gnome-sort","_score":0,"_source":{"description":"Gnome sort (or Stupid sort) is a sorting algorithm originally proposed by Dr. Hamid Sarbazi-Azad (Professor of Computer Engineering at Sharif University of Technology) in 2000 and called \"stupid sort\" (not to be confused with bogosort), and then later on described by Dick Grune and named \"gnome sort\" from the observation that it is \"how a gnome sorts a line of flower pots.\" It is a sorting algorithm which is similar to insertion sort, except that moving an element to its proper place is accomplished by a series of swaps, as in bubble sort. It is conceptually simple, requiring no nested loops. The average, or expected, running time is O(n2), but tends towards O(n) if the list is initially almost sorted.\nThe algorithm always finds the first place where two adjacent elements are in the wrong order, and swaps them. It takes advantage of the fact that performing a swap can introduce a new out-of-order adjacent pair only next to the two swapped elements. It does not assume that elements forward of the current position are sorted, so it only needs to check the position directly previous to the swapped elements.","name":"Gnome sort","categories":["All articles needing additional references","Articles needing additional references from August 2010","Articles needing additional references from November 2015","Comparison sorts","Sorting algorithms","Stable sorts"],"tag_line":"Gnome sort (or Stupid sort) is a sorting algorithm originally proposed by Dr. Hamid Sarbazi-Azad (Professor of Computer Engineering at Sharif University of Technology) in 2000 and called \"stupid sort\" (not to be confused with bogosort), and then later on described by Dick Grune and named \"gnome sort\" from the observation that it is \"how a gnome sorts a line of flower pots.\""}}
,{"_index":"throwtable","_type":"algorithm","_id":"widest-path-problem","_score":0,"_source":{"description":"In graph algorithms, the widest path problem, also known as the bottleneck shortest path problem or the maximum capacity path problem, is the problem of finding a path between two designated vertices in a weighted graph, maximizing the weight of the minimum-weight edge in the path.\nFor instance, if the graph represents connections between routers in the Internet, and the weight of an edge represents the bandwidth of a connection between two routers, the widest path problem is the problem of finding an end-to-end path between two Internet nodes that has the maximum possible bandwidth. The weight of the minimum-weight edge is known as the capacity or bandwidth of the path. As well as its applications in network routing, the widest path problem is also an important component of the Schulze method for deciding the winner of a multiway election, and has been applied to digital compositing, metabolic analysis, and the computation of maximum flows. It is possible to adapt most shortest path algorithms to compute widest paths, by modifying them to use the bottleneck distance instead of path length. However, in many cases even faster algorithms are possible.\nA closely related problem, the minimax path problem, asks for the path that minimizes the maximum weight of any of its edges. It has applications that include transportation planning. Any algorithm for the widest path problem can be transformed into an algorithm for the minimax path problem, or vice versa, by reversing the sense of all the weight comparisons performed by the algorithm, or equivalently by replacing every edge weight by its negation.","name":"Widest path problem","categories":["CS1 French-language sources (fr)","Computational problems in graph theory","Graph algorithms","Network theory","Polynomial-time problems"],"tag_line":"In graph algorithms, the widest path problem, also known as the bottleneck shortest path problem or the maximum capacity path problem, is the problem of finding a path between two designated vertices in a weighted graph, maximizing the weight of the minimum-weight edge in the path."}}
,{"_index":"throwtable","_type":"algorithm","_id":"integer-sorting","_score":0,"_source":{"description":"In computer science, integer sorting is the algorithmic problem of sorting a collection of data values by numeric keys, each of which is an integer. Algorithms designed for integer sorting may also often be applied to sorting problems in which the keys are floating point numbers or text strings. The ability to perform integer arithmetic on the keys allows integer sorting algorithms to be faster than comparison sorting algorithms in many cases, depending on the details of which operations are allowed in the model of computing and how large the integers to be sorted are.\nThe classical integer sorting algorithms of bucket sort, counting sort, and radix sort are widely used and practical. Much of the subsequent research on integer sorting algorithms has focused less on practicality and more on theoretical improvements in their worst case analysis, and the algorithms that come from this line of research are not believed to be practical for current 64-bit computer architectures, although experiments have shown that some of these methods may be an improvement on radix sorting for data with 128 or more bits per key. Additionally, for large data sets, the near-random memory access patterns of many integer sorting algorithms can handicap them compared to comparison sorting algorithms that have been designed with the memory hierarchy in mind.\nInteger sorting provides one of the six benchmarks in the DARPA High Productivity Computing Systems Discrete Mathematics benchmark suite, and one of eleven benchmarks in the NAS Parallel Benchmarks suite.","name":"Integer sorting","categories":["Sorting algorithms"],"tag_line":"In computer science, integer sorting is the algorithmic problem of sorting a collection of data values by numeric keys, each of which is an integer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"external-sorting","_score":0,"_source":{"description":"External sorting is a term for a class of sorting algorithms that can handle massive amounts of data. External sorting is required when the data being sorted do not fit into the main memory of a computing device (usually RAM) and instead they must reside in the slower external memory (usually a hard drive). External sorting typically uses a hybrid sort-merge strategy. In the sorting phase, chunks of data small enough to fit in main memory are read, sorted, and written out to a temporary file. In the merge phase, the sorted subfiles are combined into a single larger file.","name":"External sorting","categories":["All articles with unsourced statements","Articles with unsourced statements from October 2015","External memory algorithms","Sorting algorithms"],"tag_line":"External sorting is a term for a class of sorting algorithms that can handle massive amounts of data."}}
,{"_index":"throwtable","_type":"algorithm","_id":"binary-prioritization","_score":0,"_source":{"description":"Binary Prioritization is a sorting algorithm which prioritizes to-do tasks.\nUnlike other binary sort methods (e.g. binary search) this method assumes that the deferred work will be prioritized in a later process, but their order is not relevant in the first iteration. The faster processing of classified and important tasks is achieved by reducing the cost of sorting by not sorting the subset of the less important tasks. In each iteration, the cost is reduced by the sorted elements.","name":"Binary prioritization","categories":["All articles lacking sources","All orphaned articles","Articles lacking sources from October 2013","Data processing","Orphaned articles from October 2013","Sorting algorithms"],"tag_line":"Binary Prioritization is a sorting algorithm which prioritizes to-do tasks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adaptive-heap-sort","_score":0,"_source":{"description":"The adaptive heap sort is a sorting algorithm that is similar to heap sort, but uses a randomized binary search tree to structure the input according to any preexisting order. The randomized binary search tree is used to select candidates that are put into the heap, so the heap doesn't need to keep track of all elements. Adaptive heap sort is a part of the adaptive sorting family.\nThe first adaptive heapsort was Dijkstra's Smoothsort.","name":"Adaptive heap sort","categories":["Algorithms and data structures stubs","All stub articles","Comparison sorts","Computer science stubs","Heaps (data structures)","Sorting algorithms"],"tag_line":"The adaptive heap sort is a sorting algorithm that is similar to heap sort, but uses a randomized binary search tree to structure the input according to any preexisting order."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bitonic-sorter","_score":0,"_source":{"description":"Bitonic mergesort is a parallel algorithm for sorting. It is also used as a construction method for building a sorting network. The algorithm was devised by Ken Batcher. The resulting sorting networks consist of  comparators and have a delay of , where  is the number of items to be sorted.\nA sorted sequence is a monotonically non-decreasing (or non-increasing) sequence. A bitonic sequence is a sequence with  for some , or a circular shift of such a sequence.","name":"Bitonic sorter","categories":["Sorting algorithms"],"tag_line":"Bitonic mergesort is a parallel algorithm for sorting."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adaptive-coding","_score":0,"_source":{"description":"Adaptive coding refers to variants of entropy encoding methods of lossless data compression. They are particularly suited to streaming data, as they adapt to localized changes in the characteristics of the data, and don't require a first pass over the data to calculate a probability model. The cost paid for these advantages is that the encoder and decoder must be more complex to keep their states synchronized, and more computational power is needed to keep adapting the encoder/decoder state.\nAlmost all data compression methods involve the use of a model, a prediction of the composition of the data. When the data matches the prediction made by the model, the encoder can usually transmit the content of the data at a lower information cost, by making reference to the model. This general statement is a bit misleading as general data compression algorithm would include the popular LZW and LZ77 algorithms, which are hardly comparable to compression techniques typically called adaptive. Run length encoding and the typical JPEG compression with run length encoding and predefined Huffman codes do not transmit a model. A lot of other methods adapt their model to the current file and need to transmit it in addition to the encoded data, because both the encoder and the decoder need to use the model.\nIn adaptive coding, the encoder and decoder are instead equipped with a predefined meta-model about how they will alter their models in response to the actual content of the data, and otherwise start with a blank slate, meaning that no initial model needs to be transmitted. As the data is transmitted, both encoder and decoder adapt their models, so that unless the character of the data changes radically, the model becomes better-adapted to the data its handling and compresses it more efficiently approaching the efficiency of the static coding.\n\n","name":"Adaptive coding","categories":["All articles lacking sources","Articles lacking sources from June 2009","Lossless compression algorithms"],"tag_line":"Adaptive coding refers to variants of entropy encoding methods of lossless data compression."}}
,{"_index":"throwtable","_type":"algorithm","_id":"qsort","_score":0,"_source":{"description":"qsort is a C standard library function that implements a polymorphic sorting algorithm for arrays of arbitrary objects according to a user-provided comparison function. It is named after the \"quicker sort\" algorithm (a quicksort variant due to R. S. Scowen), which was originally used to implement it in the Unix C library, although the C standard does not require it to implement quicksort.\nImplementations of the qsort function achieve polymorphism, the ability to sort different kinds of data, by taking a function pointer to a three-way comparison function, as well as a parameter that specifies the size of its individual input objects. The C standard requires the comparison function to implement a total order on the items in the input array.\nA qsort function was in place in Version 3 Unix of 1973, but was then an assembler subroutine. A C version, with roughly the interface of the standard C version, was in-place in Version 6 Unix. It was rewritten in 1983 at Berkeley. The function was standardized in ANSI C (1989).","name":"Qsort","categories":["All stub articles","C standard library","Software engineering stubs","Sorting algorithms"],"tag_line":"qsort is a C standard library function that implements a polymorphic sorting algorithm for arrays of arbitrary objects according to a user-provided comparison function."}}
,{"_index":"throwtable","_type":"algorithm","_id":"chain-code","_score":0,"_source":{"description":"A chain code is a lossless compression algorithm for monochrome images. The basic principle of chain codes is to separately encode each connected component, or \"blob\", in the image.\nFor each such region, a point on the boundary is selected and its coordinates are transmitted. The encoder then moves along the boundary of the region and, at each step, transmits a symbol representing the direction of this movebment.\nThis continues until the encoder returns to the starting position, at which point the blob has been completely described, and encoding continues with the next blob in the image.\nThis encoding method is particularly effective for images consisting of a reasonably small number of large connected components.","name":"Chain code","categories":["Image compression","Lossless compression algorithms"],"tag_line":"A chain code is a lossless compression algorithm for monochrome images."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adam7-algorithm","_score":0,"_source":{"description":"Adam7 is an interlacing algorithm for raster images, best known as the interlacing scheme optionally used in PNG images. An Adam7 interlaced image is broken into seven subimages, which are defined by replicating this 8×8 pattern across the full image.\n\nThe subimages are then stored in the image file in numerical order.\nAdam7 uses seven passes and operates in both dimensions, compared to only four passes in the vertical dimension used by GIF. This means that an approximation of the entire image can be perceived much more quickly in the early passes, particularly if interpolation algorithms such as bicubic interpolation are used.","name":"Adam7 algorithm","categories":["Image compression","Lossless compression algorithms"],"tag_line":"Adam7 is an interlacing algorithm for raster images, best known as the interlacing scheme optionally used in PNG images."}}
,{"_index":"throwtable","_type":"algorithm","_id":"context-tree-weighting","_score":0,"_source":{"description":"The context tree weighting method (CTW) is a lossless compression and prediction algorithm by Willems, Shtarkov & Tjalkens 1995. The CTW algorithm is among the very few such algorithms that offer both theoretical guarantees and good practical performance (see, e.g. Begleiter, El-Yaniv & Yona 2004). The CTW algorithm is an “ensemble method,” mixing the predictions of many underlying variable order Markov models, where each such model is constructed using zero-order conditional probability estimators.","name":"Context tree weighting","categories":["All stub articles","Computer science stubs","Lossless compression algorithms"],"tag_line":"The context tree weighting method (CTW) is a lossless compression and prediction algorithm by Willems, Shtarkov & Tjalkens 1995."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quantum-sort","_score":0,"_source":{"description":"A quantum sort is any sorting algorithm that runs on a quantum computer. Any comparison-based quantum sorting algorithm would take at least  steps, which is already achievable by classical algorithms. Thus, for this task, quantum computers are no better than classical ones. However, in space-bounded sorts, quantum algorithms outperform their classical counterparts.","name":"Quantum sort","categories":["All stub articles","Quantum information science","Sorting algorithms","Theoretical computer science stubs"],"tag_line":"A quantum sort is any sorting algorithm that runs on a quantum computer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"splaysort","_score":0,"_source":{"description":"In computer science, splaysort is an adaptive comparison sorting algorithm based on the splay tree data structure.","name":"Splaysort","categories":["Sorting algorithms"],"tag_line":"In computer science, splaysort is an adaptive comparison sorting algorithm based on the splay tree data structure."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fee-method","_score":0,"_source":{"description":"In mathematics, the FEE method is the method of fast summation of series of a special form. It was constructed in 1990 by E. A. Karatsuba and was called FEE—Fast E-function Evaluation—because it makes it possible fast computations of the Siegel  -functions, and in particular, \nA class of functions, which are 'similar to the exponential function' was given the name 'E-functions' by Siegel. Among these functions are such special functions as the hypergeometric function, cylinder, spherical functions and so on.\nUsing the FEE, it is possible to prove the following theorem\nTheorem: Let  be an elementary Transcendental function, that is the exponential function, or a trigonometric function, or an elementary algebraic function, or their superposition, or their inverse, or a superposition of the inverses. Then\n\nHere  is the complexity of computation (bit) of the function  with accuracy up to  digits,  is the complexity of multiplication of two -digit integers.\nThe algorithms based on the method FEE include the algorithms for fast calculation of any elementary Transcendental function for any value of the argument, the classical constants e,  the Euler constant  the Catalan and the Apéry constants, such higher transcendental functions as the Euler gamma function and its derivatives, the hypergeometric, spherical, cylinder (including the Bessel) functions and some other functions for algebraic values of the argument and parameters, the Riemann zeta function for integer values of the argument and the Hurwitz zeta function for integer argument and algebraic values of the parameter, and also such special integrals as the integral of probability, the Fresnel integrals, the integral exponential function, the trigonometric integrals, and some other integrals for algebraic values of the argument with the complexity bound which is close to the optimal one, namely\n\nAt present, only the FEE makes it possible to calculate fast the values of the functions from the class of higher transcendental functions, certain special integrals of mathematical physics and such classical constants as Euler's, Catalan's and Apéry's constants. An additional advantage of the method FEE is the possibility of parallelizing the algorithms based on the FEE.","name":"FEE method","categories":["Computer arithmetic algorithms","Numerical analysis","Pi algorithms","Vague or ambiguous time from August 2011"],"tag_line":"In mathematics, the FEE method is the method of fast summation of series of a special form."}}
,{"_index":"throwtable","_type":"algorithm","_id":"vector-quantization","_score":0,"_source":{"description":"Vector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression. It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms.\nThe density matching property of vector quantization is powerful, especially for identifying the density of large and high-dimensioned data. Since data points are represented by the index of their closest centroid, commonly occurring data have low error, and rare data high error. This is why VQ is suitable for lossy data compression. It can also be used for lossy data correction and density estimation.\nVector quantization is based on the competitive learning paradigm, so it is closely related to the self-organizing map model and to sparse coding models used in deep learning algorithms such as autoencoder.","name":"Vector quantization","categories":["Articles to be expanded from February 2009","Incomplete lists from August 2008","Lossy compression algorithms"],"tag_line":"Vector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ut-video-codec-suite","_score":0,"_source":{"description":"UT Video Codec Suite is a fast, lossless video codec, developed by Takeshi Umezawa and released under the free GNU General Public License.\nThe algorithm of UT video builds on the Huffman code to.\nUT video was as an alternative to HuffYUV developed which allows for better compression and has appeared both as x86 and x64 than build. It can handle the color spaces YUV422 (ULY2), RGB (ULRG), RGBA (Ulra) and, most recently, YUV420 bypass (ULY0). Due to its support for multithreading this codec is also capable of HDTV material to encode in real time. However, this is a good support for the SSE2 -Befehlssatzes on the main processor required because Umezawa reblogged this instruction set used for optimization.\nUT video uses the following FOURCC codes: ULY0, ULY2, ULRA, ULRG.","name":"Ut Video Codec Suite","categories":["2008 software","All articles lacking sources","All orphaned articles","All stub articles","Articles lacking sources from April 2015","Free video codecs","Lossless compression algorithms","Orphaned articles from August 2015","Television technology stubs"],"tag_line":"UT Video Codec Suite is a fast, lossless video codec, developed by Takeshi Umezawa and released under the free GNU General Public License."}}
,{"_index":"throwtable","_type":"algorithm","_id":"methods-of-computing-square-roots","_score":0,"_source":{"description":"In numerical analysis, a branch of mathematics, there are several square root algorithms or methods of computing the principal square root of a nonnegative real number. For the square roots of a negative or complex number, see below.\nFinding  is the same as solving the equation . Therefore, any general numerical root-finding algorithm can be used. Newton's method, for example, reduces in this case to the so-called Babylonian method:\n\nGenerally, these methods yield approximate results. To get a higher precision for the root, a higher precision for the square is required and a larger number of steps must be calculated.","name":"Methods of computing square roots","categories":["All articles needing expert attention","All articles that are too technical","All articles that may contain original research","Articles needing expert attention from September 2012","Articles that may contain original research from January 2012","Computer arithmetic algorithms","Pages using web citations with no URL","Root-finding algorithms","Wikipedia articles that are too technical from September 2012"],"tag_line":"In numerical analysis, a branch of mathematics, there are several square root algorithms or methods of computing the principal square root of a nonnegative real number."}}
,{"_index":"throwtable","_type":"algorithm","_id":"microsoft-video-1","_score":0,"_source":{"description":"Microsoft Video 1 or MS-CRAM is an early lossy video compression and decompression algorithm (codec) that was released with version 1.0 of Microsoft's Video for Windows in November 1992. It is based on MotiVE, a vector quantization codec which Microsoft licensed from Media Vision. In 1993, Media Vision marketed the Pro Movie Spectrum, an ISA board that captured video in both raw and MSV1 formats (the MSV1 processing was done in hardware on the board).","name":"Microsoft Video 1","categories":["All articles with topics of unclear notability","All stub articles","Articles with topics of unclear notability from September 2011","Film and video technology","Lossy compression algorithms","Microsoft","Multimedia software stubs","Video codecs"],"tag_line":"Microsoft Video 1 or MS-CRAM is an early lossy video compression and decompression algorithm (codec) that was released with version 1.0 of Microsoft's Video for Windows in November 1992."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dictionary-coder","_score":0,"_source":{"description":"A dictionary coder, also sometimes known as a substitution coder, is a class of lossless data compression algorithms which operate by searching for matches between the text to be compressed and a set of strings contained in a data structure (called the 'dictionary') maintained by the encoder. When the encoder finds such a match, it substitutes a reference to the string's position in the data structure.","name":"Dictionary coder","categories":["All articles needing additional references","Articles needing additional references from September 2014","Lossless compression algorithms"],"tag_line":"A dictionary coder, also sometimes known as a substitution coder, is a class of lossless data compression algorithms which operate by searching for matches between the text to be compressed and a set of strings contained in a data structure (called the 'dictionary') maintained by the encoder."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shifting-nth-root-algorithm","_score":0,"_source":{"description":"The shifting nth root algorithm is an algorithm for extracting the nth root of a positive real number which proceeds iteratively by shifting in n digits of the radicand, starting with the most significant, and produces one digit of the root on each iteration, in a manner similar to long division.","name":"Shifting nth root algorithm","categories":["All articles lacking sources","Articles lacking sources from May 2010","Computer arithmetic algorithms","Root-finding algorithms"],"tag_line":"The shifting nth root algorithm is an algorithm for extracting the nth root of a positive real number which proceeds iteratively by shifting in n digits of the radicand, starting with the most significant, and produces one digit of the root on each iteration, in a manner similar to long division."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quicktime-graphics","_score":0,"_source":{"description":"QuickTime Graphics is a lossy video compression and decompression algorithm (codec) developed by Apple Inc. and first released as part of QuickTime 1.x in the early 1990s. The codec is also known by the name Apple Graphics and its FourCC SMC. The codec operates on 8-bit palettized RGB data. The bit-stream format of QuickTime Graphics has been reverse-engineered and a decoder has been implemented in the projects XAnim and libavcodec.","name":"QuickTime Graphics","categories":["Apple Inc. software","Lossy compression algorithms","Video codecs"],"tag_line":"QuickTime Graphics is a lossy video compression and decompression algorithm (codec) developed by Apple Inc. and first released as part of QuickTime 1.x in the early 1990s."}}
,{"_index":"throwtable","_type":"algorithm","_id":"package-merge-algorithm","_score":0,"_source":{"description":"The package-merge algorithm is an O(nL)-time algorithm for finding an optimal length-limited Huffman code for a given distribution on a given alphabet of size n, where no code word is longer than L. It is a greedy algorithm, and a generalization of Huffman's original algorithm. Package-merge works by reducing the code construction problem to the binary coin collector's problem.","name":"Package-merge algorithm","categories":["Coding theory","Lossless compression algorithms"],"tag_line":"The package-merge algorithm is an O(nL)-time algorithm for finding an optimal length-limited Huffman code for a given distribution on a given alphabet of size n, where no code word is longer than L. It is a greedy algorithm, and a generalization of Huffman's original algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"paxos-(computer-science)","_score":0,"_source":{"description":"Paxos is a family of protocols for solving consensus in a network of unreliable processors. Consensus is the process of agreeing on one result among a group of participants. This problem becomes difficult when the participants or their communication medium may experience failures.\nConsensus protocols are the basis for the state machine replication approach to distributed computing, as suggested by Leslie Lamport and surveyed by Fred B. Schneider. State machine replication is a technique for converting an algorithm into a fault-tolerant, distributed implementation. Ad-hoc techniques may leave important cases of failures unresolved. The principled approach proposed by Lamport et al. ensures all cases are handled safely.\nThe Paxos protocol was first published in 1989 and named after a fictional legislative consensus system used on the Paxos island in Greece. It was later published as a journal article in 1998.\nThe Paxos family of protocols includes a spectrum of trade-offs between the number of processors, number of message delays before learning the agreed value, the activity level of individual participants, number of messages sent, and types of failures. Although no deterministic fault-tolerant consensus protocol can guarantee progress in an asynchronous network (a result proven in a paper by Fischer, Lynch and Paterson), Paxos guarantees safety (consistency), and the conditions that could prevent it from making progress are difficult to provoke.\nPaxos is usually used where durability is required (for example, to replicate a file or a database), in which the amount of durable state could be large. The protocol attempts to make progress even during periods when some bounded number of replicas are unresponsive. There is also a mechanism to drop a permanently failed replica or to add a new replica.","name":"Paxos (computer science)","categories":["All articles with unsourced statements","Articles with unsourced statements from September 2009","Distributed algorithms","Fault-tolerant computer systems"],"tag_line":"Paxos is a family of protocols for solving consensus in a network of unreliable processors."}}
,{"_index":"throwtable","_type":"algorithm","_id":"prefix-sum","_score":0,"_source":{"description":"In computer science, the prefix sum, scan, or cumulative sum of a sequence of numbers x0, x1, x2, ... is a second sequence of numbers y0, y1, y2, ..., the sums of prefixes (running totals) of the input sequence:\ny0 = x0\ny1 = x0 + x1\ny2 = x0 + x1+ x2\n...\nFor instance, the prefix sums of the natural numbers are the triangular numbers:\n\nPrefix sums are trivial to compute in sequential models of computation, by using the formula yi = yi − 1 + xi to compute each output value in sequence order. However, despite their ease of computation, prefix sums are a useful primitive in certain algorithms such as counting sort, and they form the basis of the scan higher-order function in functional programming languages. When datasets are stored in Fenwick trees, prefix sums can be calculated in O(log) time. Prefix sums of large datasets can be computed in using Fenwick tree. Prefix sums have also been much studied in parallel algorithms, both as a test problem to be solved and as a useful primitive to be used as a subroutine in other parallel algorithms.\nAbstractly, a prefix sum requires only a binary associative operator ⊕, making it useful for many applications from calculating well-separated pair decompositions of points to string processing. \nMathematically, the operation of taking prefix sums can be generalized from finite to infinite sequences; in that context, a prefix sum is known as a partial sum of a series. Prefix summation or partial summation form linear operators on the vector spaces of finite or infinite sequences; their inverses are finite difference operators.","name":"Prefix sum","categories":["CS1 Russian-language sources (ru)","CS1 errors: external links","CS1 uses Russian-language script (ru)","Concurrent algorithms","Higher-order functions"],"tag_line":"In computer science, the prefix sum, scan, or cumulative sum of a sequence of numbers x0, x1, x2, ... is a second sequence of numbers y0, y1, y2, ..., the sums of prefixes (running totals) of the input sequence:\ny0 = x0\ny1 = x0 + x1\ny2 = x0 + x1+ x2\n...\nFor instance, the prefix sums of the natural numbers are the triangular numbers:\n\nPrefix sums are trivial to compute in sequential models of computation, by using the formula yi = yi − 1 + xi to compute each output value in sequence order."}}
,{"_index":"throwtable","_type":"algorithm","_id":"crypto++","_score":0,"_source":{"description":"Crypto++ (also known as CryptoPP, libcrypto++, and libcryptopp) is a free and open source C++ class library of cryptographic algorithms and schemes written by Wei Dai. Crypto++ has been widely used in academia, student projects, open source and non-commercial projects, as well as businesses. Released in 1995, the library fully supports 32-bit and 64-bit architectures for many major operating systems and platforms, including Android (using STLport), Apple (Mac OS X and iOS), BSD, Cygwin, IBM AIX and S/390, Linux, MinGW, Solaris, Windows, Windows Phone and Windows RT. The project also supports compilation under C++03 and C++11, a variety of compilers and IDEs, including Borland Turbo C++, Borland C++ Builder, Clang, CodeWarrior Pro, GCC (including Apple's GCC), Intel C++ Compiler (ICC), Microsoft Visual C/C++, and Sun Studio.","name":"Crypto++","categories":["All pages needing cleanup","Articles needing cleanup from November 2015","Articles with attributed pull quotes","Articles with sections that need to be turned into prose from November 2015","C++ libraries","Cryptographic algorithms","Cryptographic software","Free computer libraries","Pages using citations with accessdate and no URL","Pages using web citations with no URL"],"tag_line":"Crypto++ (also known as CryptoPP, libcrypto++, and libcryptopp) is a free and open source C++ class library of cryptographic algorithms and schemes written by Wei Dai."}}
,{"_index":"throwtable","_type":"algorithm","_id":"banker's-algorithm","_score":0,"_source":{"description":"The Banker's algorithm, sometimes referred to as the avoidance algorithm, is a resource allocation and deadlock avoidance algorithm developed by Edsger Dijkstra that tests for safety by simulating the allocation of predetermined maximum possible amounts of all resources, and then makes an \"s-state\" check to test for possible deadlock conditions for all other pending activities, before deciding whether allocation should be allowed to continue.\nThe algorithm was developed in the design process for the THE operating system and originally described (in Dutch) in EWD108. When a new process enters a system, it must declare the maximum number of instances of each resource type that it may ever claim; clearly, that number may not exceed the total number of resources in the system. Also, when a process gets all its requested resources it must return them in a finite amount of time.","name":"Banker's algorithm","categories":["All articles with unsourced statements","Articles with example pseudocode","Articles with unsourced statements from October 2015","Concurrency control algorithms","Dutch inventions"],"tag_line":"The Banker's algorithm, sometimes referred to as the avoidance algorithm, is a resource allocation and deadlock avoidance algorithm developed by Edsger Dijkstra that tests for safety by simulating the allocation of predetermined maximum possible amounts of all resources, and then makes an \"s-state\" check to test for possible deadlock conditions for all other pending activities, before deciding whether allocation should be allowed to continue."}}
,{"_index":"throwtable","_type":"algorithm","_id":"parallel-algorithm","_score":0,"_source":{"description":"In computer science, a parallel algorithm, as opposed to a traditional serial algorithm, is an algorithm which can be executed a piece at a time on many different processing devices, and then combined together again at the end to get the correct result.\nMany parallel algorithms are executed concurrently – though in general concurrent algorithms are a distinct concept – and thus these concepts are often conflated, with which aspect of an algorithm is parallel and which is concurrent not being clearly distinguished. Further, non-parallel, non-concurrent algorithms are often referred to as \"sequential algorithms\", by contrast with concurrent algorithms.","name":"Parallel algorithm","categories":["All articles needing additional references","All articles to be expanded","Articles needing additional references from November 2012","Articles to be expanded from February 2014","Concurrent algorithms","Distributed algorithms","Parallel computing"],"tag_line":"In computer science, a parallel algorithm, as opposed to a traditional serial algorithm, is an algorithm which can be executed a piece at a time on many different processing devices, and then combined together again at the end to get the correct result."}}
,{"_index":"throwtable","_type":"algorithm","_id":"edge-chasing","_score":0,"_source":{"description":"In computer science, edge-chasing is an algorithm for deadlock detection in distributed systems.\nWhenever a process A is blocked for some resource, a probe message is sent to all processes A may depend on. The probe message contains the process id of A along with the path that the message has followed through the distributed system. If a blocked process receives the probe it will update the path information and forward the probe to all the processes it depends on. Non-blocked processes may discard the probe.\nIf eventually the probe returns to process A, there is a circular waiting loop of blocked processes, and a deadlock is detected. Efficiently detecting such cycles in the “wait-for graph” of blocked processes is an important implementation problem.","name":"Edge chasing","categories":["All articles lacking sources","Articles lacking sources from December 2009","Distributed algorithms"],"tag_line":"In computer science, edge-chasing is an algorithm for deadlock detection in distributed systems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"generating-primes","_score":0,"_source":{"description":"In computational number theory, a variety of algorithms make it possible to generate prime numbers efficiently. These are used in various applications, for example hashing, public-key cryptography, and search of prime factors in large numbers.\nFor relatively small numbers, it is possible to just apply trial division to each successive odd number. Prime sieves are almost always faster.","name":"Generating primes","categories":["Cryptographic algorithms","Number theoretic algorithms","Prime numbers"],"tag_line":"In computational number theory, a variety of algorithms make it possible to generate prime numbers efficiently."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ticket-lock","_score":0,"_source":{"description":"In computer science, a ticket lock is a synchronization mechanism, or locking algorithm, that is a type of spinlock that uses \"tickets\" to control which thread of execution is allowed to enter a critical section.","name":"Ticket lock","categories":["All articles containing potentially dated statements","Articles containing potentially dated statements from July 2010","Concurrency control algorithms","Pages using citations with accessdate and no URL"],"tag_line":"In computer science, a ticket lock is a synchronization mechanism, or locking algorithm, that is a type of spinlock that uses \"tickets\" to control which thread of execution is allowed to enter a critical section."}}
,{"_index":"throwtable","_type":"algorithm","_id":"snapshot-algorithm","_score":0,"_source":{"description":"The snapshot algorithm is an algorithm used in distributed systems for recording a consistent global state of an asynchronous system. The algorithm discussed here is also known as the Chandy–Lamport algorithm, after Leslie Lamport and K. Mani Chandy.","name":"Snapshot algorithm","categories":["Distributed algorithms"],"tag_line":"The snapshot algorithm is an algorithm used in distributed systems for recording a consistent global state of an asynchronous system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"substitution-permutation-network","_score":0,"_source":{"description":"In cryptography, an SP-network, or substitution-permutation network (SPN), is a series of linked mathematical operations used in block cipher algorithms such as AES (Rijndael). Other ciphers that use SPNs are 3-Way, SAFER, SHARK, and Square.\nSuch a network takes a block of the plaintext and the key as inputs, and applies several alternating \"rounds\" or \"layers\" of substitution boxes (S-boxes) and permutation boxes (P-boxes) to produce the ciphertext block. The S-boxes and P-boxes transform (sub-)blocks of input bits into output bits. It is common for these transformations to be operations that are efficient to perform in hardware, such as exclusive or (XOR) and bitwise rotation. The key is introduced in each round, usually in the form of \"round keys\" derived from it. (In some designs, the S-boxes themselves depend on the key.)\nDecryption is done by simply reversing the process (using the inverses of the S-boxes and P-boxes and applying the round keys in reversed order).\nAn S-box substitutes a small block of bits (the input of the S-box) by another block of bits (the output of the S-box). This substitution should be one-to-one, to ensure invertibility (hence decryption). In particular, the length of the output should be the same as the length of the input (the picture on the right has S-boxes with 4 input and 4 output bits), which is different from S-boxes in general that could also change the length, as in DES (Data Encryption Standard), for example. An S-box is usually not simply a permutation of the bits. Rather, a good S-box will have the property that changing one input bit will change about half of the output bits (or an avalanche effect). It will also have the property that each output bit will depend on every input bit.\nA P-box is a permutation of all the bits: it takes the outputs of all the S-boxes of one round, permutes the bits, and feeds them into the S-boxes of the next round. A good P-box has the property that the output bits of any S-box are distributed to as many S-box inputs as possible.\nAt each round, the round key (obtained from the key with some simple operations, for instance, using S-boxes and P-boxes) is combined using some group operation, typically XOR.\nA single typical S-box or a single P-box alone does not have much cryptographic strength: an S-box could be thought of as a substitution cipher, while a P-box could be thought of as a transposition cipher. However, a well-designed SP network with several alternating rounds of S- and P-boxes already satisfies Shannon's confusion and diffusion properties:\nThe reason for diffusion is the following: If one changes one bit of the plaintext, then it is fed into an S-box, whose output will change at several bits, then all these changes are distributed by the P-box among several S-boxes, hence the outputs of all of these S-boxes are again changed at several bits, and so on. Doing several rounds, each bit changes several times back and forth, therefore, by the end, the ciphertext has changed completely, in a pseudorandom manner. In particular, for a randomly chosen input block, if one flips the i-th bit, then the probability that the j-th output bit will change is approximately a half, for any i and j, which is the Strict Avalanche Criterion. Vice versa, if one changes one bit of the ciphertext, then attempts to decrypt it, the result is a message completely different from the original plaintext -- SP ciphers are not easily malleable.\nThe reason for confusion is exactly the same as for diffusion: changing one bit of the key changes several of the round keys, and every change in every round key diffuses over all the bits, changing the ciphertext in a very complex manner.\nEven if an attacker somehow obtains one plaintext corresponding to one ciphertext -- a known-plaintext attack, or worse, a chosen plaintext or chosen-ciphertext attack -- the confusion and diffusion make it difficult for the attacker to recover the key.\nAlthough a Feistel network that uses S-boxes (such as DES) is quite similar to SP networks, there are some differences that make either this or that more applicable in certain situations. For a given amount of confusion and diffusion, an SP network has more \"inherent parallelism\" and so — given a CPU with a large number of execution units — can be computed faster than a Feistel network.  CPUs with few execution units — such as most smart cards — cannot take advantage of this inherent parallelism. Also SP ciphers require S-boxes to be invertible (to perform decryption); Feistel inner functions have no such restriction and can be constructed as one-way functions.","name":"Substitution-permutation network","categories":["Block ciphers","Cryptographic algorithms","Permutations"],"tag_line":"In cryptography, an SP-network, or substitution-permutation network (SPN), is a series of linked mathematical operations used in block cipher algorithms such as AES (Rijndael)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"time-based-one-time-password-algorithm","_score":0,"_source":{"description":"Time-based One-time Password Algorithm (TOTP) is an algorithm that computes a one-time password from a shared secret key and the current time. It has been adopted as Internet Engineering Task Force standard RFC 6238, is the cornerstone of Initiative For Open Authentication (OATH), and is used in a number of two factor authentication systems.\nTOTP is an example of a hash-based message authentication code (HMAC). It combines a secret key with the current timestamp using a cryptographic hash function to generate a one-time password. The timestamp typically increases in 30-second intervals, so passwords generated close together in time from the same secret key will be equal.\nIn a typical two-factor authentication application, user authentication proceeds as follows: a user will enter username and password into a website or other server, generate a one-time password for the server using TOTP running locally on a smartphone or other device, and type that password into the server as well. The server will then also run TOTP to verify the entered one-time password. For this to work, the clocks of the user's device and the server need to be roughly synchronized (the server will typically accept one-time passwords generated from timestamps that differ by ±1 from the client's timestamp). A single secret key, to be used for all subsequent authentication sessions, must have been shared between the server and the user's device over a secure channel ahead of time. If some more steps are carried out, the user can also authenticate the server using TOTP.","name":"Time-based One-time Password Algorithm","categories":["All pages needing cleanup","Articles needing cleanup from April 2014","Computer access control","Cryptographic algorithms","Internet protocols","Wikipedia list cleanup from April 2014"],"tag_line":"Time-based One-time Password Algorithm (TOTP) is an algorithm that computes a one-time password from a shared secret key and the current time."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pollard's-p-−-1-algorithm","_score":0,"_source":{"description":"Pollard's p − 1 algorithm is a number theoretic integer factorization algorithm, invented by John Pollard in 1974. It is a special-purpose algorithm, meaning that it is only suitable for integers with specific types of factors; it is the simplest example of an algebraic-group factorisation algorithm.\nThe factors it finds are ones for which the number preceding the factor, p − 1, is powersmooth; the essential observation is that, by working in the multiplicative group modulo a composite number N, we are also working in the multiplicative groups modulo all of N's factors.\nThe existence of this algorithm leads to the concept of safe primes, being primes for which p − 1 is two times a Sophie Germain prime q and thus minimally smooth. These primes are sometimes construed as \"safe for cryptographic purposes\", but they might be unsafe — in current recommendations for cryptographic strong primes (e.g. ANSI X9.31), it is necessary but not sufficient that p − 1 has at least one large prime factor. Most sufficiently large primes are strong; if a prime used for cryptographic purposes turns out to be non-strong, it is much more likely to be through malice than through an accident of random number generation. This terminology is considered obsolete by the cryptography industry. [1]","name":"Pollard's p − 1 algorithm","categories":["Integer factorization algorithms"],"tag_line":"Pollard's p − 1 algorithm is a number theoretic integer factorization algorithm, invented by John Pollard in 1974."}}
,{"_index":"throwtable","_type":"algorithm","_id":"modular-exponentiation","_score":0,"_source":{"description":"Modular exponentiation is a type of exponentiation performed over a modulus. It is useful in computer science, especially in the field of public-key cryptography.\nThe operation of modular exponentiation calculates the remainder when an integer b (the base) raised to the eth power (the exponent), be, is divided by a positive integer m (the modulus). In symbols, given base b, exponent e, and modulus m, the modular exponentiation c is: c ≡ be (mod m).\nFor example, given b = 5, e = 3 and m = 13, the solution c = 8 is the remainder of dividing 53 = 125 by 13.\nGiven integers b and e, and a positive integer m, a unique solution c exists with the property 0 ≤ c < m.\nModular exponentiation can be performed with a negative exponent e by finding the modular multiplicative inverse d of b modulo m using the extended Euclidean algorithm. That is:\n where e < 0 and \nModular exponentiation similar to the one described above are considered easy to compute, even when the numbers involved are enormous. On the other hand, computing the discrete logarithm – that is, the task of finding the exponent e when given b, c, and m – is believed to be difficult. This one-way function behavior makes modular exponentiation a candidate for use in cryptographic algorithms.","name":"Modular exponentiation","categories":["Cryptographic algorithms","Modular arithmetic","Number theoretic algorithms"],"tag_line":"Modular exponentiation is a type of exponentiation performed over a modulus."}}
,{"_index":"throwtable","_type":"algorithm","_id":"supersingular-isogeny-key-exchange","_score":0,"_source":{"description":"The Supersingular Isogeny Diffie–Hellman Key Exchange (SIDH) is a post-quantum public key cryptographic algorithm used to establish a secret key between two parties over an otherwise insecure communications channel. It was designed to resist cryptanalytic attack by an adversary in possession of a quantum computer. Because the SIDH has key sizes, computations and forward security protection similar to that of the widely supported Elliptic Curve Diffie–Hellman key exchange it is a natural candidate to replace Diffie-Hellman and Elliptic Curve Diffie-Hellman in the face of a growing quantum computer threat.","name":"Supersingular Isogeny Key Exchange","categories":["Cryptographic algorithms"],"tag_line":"The Supersingular Isogeny Diffie–Hellman Key Exchange (SIDH) is a post-quantum public key cryptographic algorithm used to establish a secret key between two parties over an otherwise insecure communications channel."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quadratic-sieve","_score":0,"_source":{"description":"The quadratic sieve algorithm (QS) is an integer factorization algorithm and, in practice, the second fastest method known (after the general number field sieve). It is still the fastest for integers under 100 decimal digits or so, and is considerably simpler than the number field sieve. It is a general-purpose factorization algorithm, meaning that its running time depends solely on the size of the integer to be factored, and not on special structure or properties. It was invented by Carl Pomerance in 1981 as an improvement to Schroeppel's linear sieve.","name":"Quadratic sieve","categories":["Integer factorization algorithms"],"tag_line":"The quadratic sieve algorithm (QS) is an integer factorization algorithm and, in practice, the second fastest method known (after the general number field sieve)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"nsa-product-types","_score":0,"_source":{"description":"Product types refers to nomenclature that has been used by the U.S. National Security Agency (NSA) to rank cryptographic products or algorithms by level of certification. The terminology is defined in the National Information Assurance Glossary (CNSSI No. 4009) which defines Type 1, Type 2, products and Type 3, and Type 4 algorithms.","name":"NSA product types","categories":["All stub articles","Cryptographic algorithms","Cryptography stubs","National Security Agency encryption devices","Type 1 encryption algorithms"],"tag_line":"Product types refers to nomenclature that has been used by the U.S. National Security Agency (NSA) to rank cryptographic products or algorithms by level of certification."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rational-sieve","_score":0,"_source":{"description":"In mathematics, the rational sieve is a general algorithm for factoring integers into prime factors. It is essentially a special case of the general number field sieve, and while it is far less efficient than the general algorithm, it is conceptually far simpler. So while it is rather useless as a practical factoring algorithm, it is a helpful first step for those trying to understand how the general number field sieve works.","name":"Rational sieve","categories":["Integer factorization algorithms"],"tag_line":"In mathematics, the rational sieve is a general algorithm for factoring integers into prime factors."}}
,{"_index":"throwtable","_type":"algorithm","_id":"securelog","_score":0,"_source":{"description":"In cryptology, SecureLog is an algorithm used to convert digital data into trusted data that can be verified if the authencity is questioned. SecureLog is used in IT solutions that generates data to support compliance regulations like SOX.","name":"SecureLog","categories":["All orphaned articles","Cryptographic algorithms","Orphaned articles from October 2013"],"tag_line":"In cryptology, SecureLog is an algorithm used to convert digital data into trusted data that can be verified if the authencity is questioned."}}
,{"_index":"throwtable","_type":"algorithm","_id":"canopy-clustering-algorithm","_score":0,"_source":{"description":"The canopy clustering algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and Lyle Ungar in 2000. It is often used as preprocessing step for the K-means algorithm or the Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, where using another algorithm directly may be impractical due to the size of the data set.\nThe algorithm proceeds as follows, using two thresholds  (the loose distance) and  (the tight distance), where  .\nBegin with the set of data points to be clustered.\nRemove a point from the set, beginning a new 'canopy'.\nFor each point left in the set, assign it to the new canopy if the distance less than the loose distance .\nIf the distance of the point is additionally less than the tight distance , remove it from the original set.\nRepeat from step 2 until there are no more data points in the set to cluster.\nThese relatively cheaply clustered canopies can be sub-clustered using a more expensive but accurate algorithm.\nAn important note is that individual data points may be part of several canopies. As an additional speed-up, an approximate and fast distance metric can be used for 3, where a more accurate and slow distance metric can be used for step 4.\nSince the algorithm uses distance functions and requires the specification of distance thresholds, its applicability for high-dimensional data is limited by the curse of dimensionality. Only when a cheap and approximative – low-dimensional – distance function is available, the produced canopies will preserve the clusters produced by K-means.\n\n","name":"Canopy clustering algorithm","categories":["Algorithms and data structures stubs","All articles with dead external links","All stub articles","Articles with dead external links from September 2015","Computer science stubs","Data clustering algorithms","Statistical algorithms"],"tag_line":"The canopy clustering algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and Lyle Ungar in 2000."}}
,{"_index":"throwtable","_type":"algorithm","_id":"boosting-(machine-learning)","_score":0,"_source":{"description":"Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms which convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): Can a set of weak learners create a single strong learner? A weak learner is defined to be a classifier which is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\nRobert Schapire's affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.\nWhen first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. \"Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm […] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner].\" Algorithms that achieve hypothesis boosting quickly became simply known as \"boosting\". Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.","name":"Boosting (machine learning)","categories":["All articles to be expanded","All articles to be merged","Articles to be expanded from December 2009","Articles to be merged from December 2012","Classification algorithms","Ensemble learning"],"tag_line":"Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms which convert weak learners to strong ones."}}
,{"_index":"throwtable","_type":"algorithm","_id":"information-bottleneck-method","_score":0,"_source":{"description":"The information bottleneck method is a technique in information theory introduced by Naftali Tishby et al. for finding the best tradeoff between accuracy and complexity (compression) when summarizing (e.g. clustering) a random variable X, given a joint probability distribution between X and an observed relevant variable Y. Other applications include distributional clustering, and dimension reduction. In a well defined sense it generalized the classical notion of minimal sufficient statistics from parametric statistics to arbitrary distributions, not necessarily of exponential form. It does so by relaxing the sufficiency condition to capture some fraction of the mutual information with the relevant variable Y.\nThe compressed variable is  and the algorithm minimizes the following quantity\n\nwhere  are the mutual information between  and  respectively, and  is a Lagrange multiplier.","name":"Information bottleneck method","categories":["Data clustering algorithms","Multivariate statistics"],"tag_line":"The information bottleneck method is a technique in information theory introduced by Naftali Tishby et al."}}
,{"_index":"throwtable","_type":"algorithm","_id":"birch","_score":0,"_source":{"description":"BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets. An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.\nIts inventors claim BIRCH to be the \"first clustering algorithm proposed in the database area to handle 'noise' (data points that are not part of the underlying pattern) effectively\", beating DBSCAN by two months. The algorithm received the SIGMOD 10 year test of time award in 2006.","name":"BIRCH","categories":["Data clustering algorithms","Wikipedia articles needing clarification from December 2014"],"tag_line":"BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets."}}
,{"_index":"throwtable","_type":"algorithm","_id":"trial-division","_score":0,"_source":{"description":"Trial division is the most laborious but easiest to understand of the integer factorization algorithms. The essential idea behind trial division tests to see if an integer n, the integer to be factored, can be divided by each number in turn that is less than n. For example, for the integer n = 12, the only numbers that divide it are 1,2,3,4,6,12. Selecting only the largest powers of primes in this list gives that 12 = 3 × 4.","name":"Trial division","categories":["All articles lacking in-text citations","Articles lacking in-text citations from March 2014","Articles with example Python code","Division (mathematics)","Integer factorization algorithms"],"tag_line":"Trial division is the most laborious but easiest to understand of the integer factorization algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"subclu","_score":0,"_source":{"description":"SUBCLU is an algorithm for clustering high-dimensional data by Karin Kailing, Hans-Peter Kriegel and Peer Kröger. It is a subspace clustering algorithm that builds on the density-based clustering algorithm DBSCAN. SUBCLU can find clusters in axis-parallel subspaces, and uses a bottom-up, greedy strategy to remain efficient.","name":"SUBCLU","categories":["All articles needing additional references","All articles needing expert attention","Articles needing additional references from February 2010","Articles needing expert attention from February 2010","Articles needing expert attention with no reason or talk parameter","Data clustering algorithms","Statistics articles needing expert attention"],"tag_line":"SUBCLU is an algorithm for clustering high-dimensional data by Karin Kailing, Hans-Peter Kriegel and Peer Kröger."}}
,{"_index":"throwtable","_type":"algorithm","_id":"williams'-p-+-1-algorithm","_score":0,"_source":{"description":"In computational number theory, Williams' p + 1 algorithm is an integer factorization algorithm, one of the family of algebraic-group factorisation algorithms. It was invented by Hugh C. Williams in 1982.\nIt works well if the number N to be factored contains one or more prime factors p such that p + 1 is smooth, i.e. p + 1 contains only small factors. It uses Lucas sequences to perform exponentiation in a quadratic field.\nIt is analogous to Pollard's p − 1 algorithm.","name":"Williams' p + 1 algorithm","categories":["Integer factorization algorithms"],"tag_line":"In computational number theory, Williams' p + 1 algorithm is an integer factorization algorithm, one of the family of algebraic-group factorisation algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"algorithm-characterizations","_score":0,"_source":{"description":"Algorithm characterizations are attempts to formalize the word algorithm. Algorithm does not have a generally accepted formal definition. Researchers are actively working on this problem. This article will present some of the \"characterizations\" of the notion of \"algorithm\" in more detail.\nThis article is a supplement to the article Algorithm.\n^ cf [164] Andreas Blass and Yuri Gurevich \"Algorithms: A Quest for Absolute Definitions\" Bulletin of the European Association for Theoretical Computer Science Number 81 (October 2003), pages 195–225. Reprinted in Chapter on Logic in Computer Science Current Trends in Theoretical Computer Science World Scientific, 2004, pages 283–311 Reprinted in Church's Thesis After 70 Years Ontos Verlag, 2006, 24–57}, or http://math.ucsd.edu/~sbuss/ResearchWeb/FutureOfLogic/paper.pdf (cited in a 2007 Dershowitz–Gurevich paper): Samual R. Buss, Alexander S. Kechris, Anand Pillay, and Richard A. Shore, “The Prospects for Mathematical Logic in the Twenty-first Century”.","name":"Algorithm characterizations","categories":["Algorithms","All articles to be expanded","All articles with empty sections","All articles with specifically marked weasel-worded phrases","Articles to be expanded from January 2014","Articles to be expanded from June 2008","Articles with empty sections from January 2014","Articles with specifically marked weasel-worded phrases from February 2013","Computability theory","Formal methods","Models of computation","Pages using duplicate arguments in template calls"],"tag_line":"Algorithm characterizations are attempts to formalize the word algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"run-time-algorithm-specialisation","_score":0,"_source":{"description":"In computer science, run-time algorithm specialization is a methodology for creating efficient algorithms for costly computation tasks of certain kinds. The methodology originates in the field of automated theorem proving and, more specifically, in the Vampire theorem prover project.\nThe idea is inspired by the use of partial evaluation in optimising program translation. Many core operations in theorem provers exhibit the following pattern. Suppose that we need to execute some algorithm  in a situation where a value of  is fixed for potentially many different values of . In order to do this efficiently, we can try to find a specialization of  for every fixed , i.e., such an algorithm , that executing  is equivalent to executing .\nThe specialized algorithm may be more efficient than the generic one, since it can exploit some particular properties of the fixed value . Typically,  can avoid some operations that  would have to perform, if they are known to be redundant for this particular parameter . In particular, we can often identify some tests that are true or false for , unroll loops and recursion, etc.","name":"Run-time algorithm specialisation","categories":["Algorithms","Software optimization"],"tag_line":"In computer science, run-time algorithm specialization is a methodology for creating efficient algorithms for costly computation tasks of certain kinds."}}
,{"_index":"throwtable","_type":"algorithm","_id":"metis","_score":0,"_source":{"description":"METIS is a software package for graph partitioning that implements various multilevel algorithms.\nMETIS' multilevel approach has three phases and comes with several algorithms for each phase:\nCoarsen the graph by generating a sequence of graphs G0, G1, ..., GN, where G0 is the original graph and for each 0 ≤ i ≤ j ≤ N, the number of vertices in Gi is greater than the number of vertices in Gj.\nCompute a partition of GN\nProject the partition back through the sequence in the order of GN, ..., G0, refining it with respect to each graph.\nThe final partition computed during the third phase (the refined partition projected onto G0) is a partition of the original graph.","name":"METIS","categories":["Algorithms","Algorithms and data structures stubs","All articles covered by WikiProject Wikify","All articles with too few wikilinks","All stub articles","Articles covered by WikiProject Wikify from December 2013","Articles with too few wikilinks from December 2013","Computational problems in graph theory","Computer science stubs"],"tag_line":"METIS is a software package for graph partitioning that implements various multilevel algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lossy-count-algorithm","_score":0,"_source":{"description":"The lossy count algorithm is an algorithm to identify elements in a data stream whose frequency count exceed a user-given threshold. The frequency computed by this algorithm is not always accurate, but has an error threshold that can be specified by the user. The run time space required by the algorithm is inversely proportional to the specified error threshold, hence larger the error, the smaller the footprint. It was created by eminent computer scientists Rajeev Motwani and Gurmeet Singh Manku. This algorithm finds huge application in computations where data takes the form of a continuous data stream instead of a finite data set, for e.g. network traffic measurements, web server logs, clickstreams.","name":"Lossy Count Algorithm","categories":["Algorithms"],"tag_line":"The lossy count algorithm is an algorithm to identify elements in a data stream whose frequency count exceed a user-given threshold."}}
,{"_index":"throwtable","_type":"algorithm","_id":"generalized-distributive-law","_score":0,"_source":{"description":"The generalized distributive law (GDL) is a general message passing algorithm devised by Srinivas M. Aji and Robert J. McEliece. It is a synthesis of the work of many authors in the information theory, digital communications, signal processing, statistics, and artificial intelligence communities. This article is based on a semi-tutorial by Srinivas M. Aji and Robert J. McEliece with the same title.","name":"Generalized distributive law","categories":["Algorithms","All articles lacking in-text citations","All articles needing additional references","Articles lacking in-text citations from June 2012","Articles needing additional references from June 2012","Artificial intelligence","Digital signal processing","Graphical models","Information theory","Wikipedia articles needing clarification from June 2015"],"tag_line":"The generalized distributive law (GDL) is a general message passing algorithm devised by Srinivas M. Aji and Robert J. McEliece."}}
,{"_index":"throwtable","_type":"algorithm","_id":"reservoir-sampling","_score":0,"_source":{"description":"Reservoir sampling is a family of randomized algorithms for randomly choosing a sample of k items from a list S containing n items, where n is either a very large or unknown number. Typically n is large enough that the list doesn't fit into main memory.","name":"Reservoir sampling","categories":["Algorithms","All Wikipedia articles needing clarification","All articles needing expert attention","Analysis of algorithms","Articles needing expert attention from February 2010","Articles needing expert attention with no reason or talk parameter","Computing articles needing expert attention","Probabilistic complexity theory","Wikipedia articles needing clarification from December 2009"],"tag_line":"Reservoir sampling is a family of randomized algorithms for randomly choosing a sample of k items from a list S containing n items, where n is either a very large or unknown number."}}
,{"_index":"throwtable","_type":"algorithm","_id":"manhattan-address-algorithm","_score":0,"_source":{"description":"The Manhattan address algorithm is used to estimate the number of the closest cross street for a given building number on the island of Manhattan. The algorithm is given in any print telephone directory  as well as on numerous web pages and in New York City guide books.\n\n","name":"Manhattan address algorithm","categories":["Algorithms","Streets in Manhattan"],"tag_line":"The Manhattan address algorithm is used to estimate the number of the closest cross street for a given building number on the island of Manhattan."}}
,{"_index":"throwtable","_type":"algorithm","_id":"domain-reduction-algorithm","_score":0,"_source":{"description":"Domain reduction algorithms are algorithms used to reduce constraints and degrees of freedom in order to provide solutions for partial differential equations.","name":"Domain reduction algorithm","categories":["Algorithms","All stub articles","Computer programming stubs"],"tag_line":"Domain reduction algorithms are algorithms used to reduce constraints and degrees of freedom in order to provide solutions for partial differential equations."}}
,{"_index":"throwtable","_type":"algorithm","_id":"chandy-misra-haas-algorithm:resource-model","_score":0,"_source":{"description":"K. Mani Chandy, Jayadev Misra and Laura M Haas devise Chandy Misra Haas algorithm for Resource model. It checks whether there is any deadlock in a distributed system or not.","name":"Chandy-Misra-Haas Algorithm:Resource Model","categories":["Algorithms"],"tag_line":"K. Mani Chandy, Jayadev Misra and Laura M Haas devise Chandy Misra Haas algorithm for Resource model."}}
,{"_index":"throwtable","_type":"algorithm","_id":"karloff–zwick-algorithm","_score":0,"_source":{"description":"The Karloff–Zwick algorithm, in computational complexity theory, is a randomised approximation algorithm taking an instance of MAX-3SAT Boolean satisfiability problem as input. If the instance is satisfiable, then the expected weight of the assignment found is at least 7/8 of optimal. It provides strong evidence (but not a mathematical proof) that the algorithm performs equally well on arbitrary MAX-3SAT instances. Howard Karloff and Uri Zwick presented the algorithm in 1997.\nFor the related MAX-E3SAT problem, in which all clauses in the input 3SAT formula are guaranteed to have exactly three literals, the simple randomized approximation algorithm which assigns a truth value to each variable independently and uniformly at random satisfies 7/8 of all clauses in expectation, irrespective of whether the original formula is satisfiable. Further, this simple algorithm can also be easily derandomized using the method of conditional expectations. The Karloff–Zwick algorithm, however, does not require the restriction that the input formula should have three literals in every clause.\nBuilding upon previous work on the PCP theorem, Johan Håstad showed that, assuming P ≠ NP, no polynomial-time algorithm for MAX 3SAT can achieve a performance ratio exceeding 7/8, even when restricted to satisfiable instances of the problem in which each clause contains exactly three literals. Both the Karloff–Zwick algorithm and the above simple algorithm are therefore optimal in this sense.","name":"Karloff–Zwick algorithm","categories":["Algorithms and data structures stubs","All stub articles","Approximation algorithms","CS1 errors: chapter ignored","Computer science stubs","Probabilistic complexity theory"],"tag_line":"The Karloff–Zwick algorithm, in computational complexity theory, is a randomised approximation algorithm taking an instance of MAX-3SAT Boolean satisfiability problem as input."}}
,{"_index":"throwtable","_type":"algorithm","_id":"blast","_score":0,"_source":{"description":"In bioinformatics, BLAST for Basic Local Alignment Search Tool is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. A BLAST search enables a researcher to compare a query sequence with a library or database of sequences, and identify library sequences that resemble the query sequence above a certain threshold.\nDifferent types of BLASTs are available according to the query sequences. For example, following the discovery of a previously unknown gene in the mouse, a scientist will typically perform a BLAST search of the human genome to see if humans carry a similar gene; BLAST will identify sequences in the human genome that resemble the mouse gene based on similarity of sequence. The BLAST algorithm and program were designed by Stephen Altschul, Warren Gish, Webb Miller, Eugene Myers, and David J. Lipman at the National Institutes of Health and was published in the Journal of Molecular Biology in 1990 and cited over 50,000 times.","name":"BLAST","categories":["All articles with unsourced statements","Articles with unsourced statements from August 2012","Bioinformatics algorithms","Bioinformatics software","Computational phylogenetics","Laboratory software","Public domain software"],"tag_line":"In bioinformatics, BLAST for Basic Local Alignment Search Tool is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ukkonen's-algorithm","_score":0,"_source":{"description":"In computer science, Ukkonen's algorithm is a linear-time, online algorithm for constructing suffix trees, proposed by Esko Ukkonen in 1995.\nThe algorithm begins with an implicit suffix tree containing the first character of the string. Then it steps through the string adding successive characters until the tree is complete. This order addition of characters gives Ukkonen's algorithm its \"on-line\" property. The original algorithm presented by Peter Weiner proceeded backward from the last character to the first one from the shortest to the longest suffix. A simpler algorithm was found by Edward M. McCreight, going from the longest to the shortest suffix.\nThe naive implementation for generating a suffix tree going forward requires O(n2) or even O(n3) time complexity in big O notation, where n is the length of the string. By exploiting a number of algorithmic techniques, Ukkonen reduced this to O(n) (linear) time, for constant-size alphabets, and O(n log n) in general, matching the runtime performance of the earlier two algorithms.","name":"Ukkonen's algorithm","categories":["Algorithms on strings","All stub articles","Bioinformatics algorithms","Computer science stubs","Substring indices"],"tag_line":"In computer science, Ukkonen's algorithm is a linear-time, online algorithm for constructing suffix trees, proposed by Esko Ukkonen in 1995."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sequential-algorithm","_score":0,"_source":{"description":"In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially – once through, from start to finish, without other processing executing – as opposed to concurrently or in parallel. The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap – many distributed algorithms are both concurrent and parallel – and thus \"sequential\" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.\n\"Sequential algorithm\" may also refer specifically to an algorithm for decoding a convolutional code.","alt_names":["Serial algorithm"],"name":"Sequential algorithm","categories":["Algorithms","Algorithms and data structures stubs","All stub articles","Computer science stubs"],"tag_line":"In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially – once through, from start to finish, without other processing executing – as opposed to concurrently or in parallel."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sequential-pattern-mining","_score":0,"_source":{"description":"Sequential Pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence. It is usually presumed that the values are discrete, and thus time series mining is closely related, but usually considered a different activity. Sequential pattern mining is a special case of structured data mining.\nThere are several key traditional computational problems addressed within this field. These include building efficient databases and indexes for sequence information, extracting the frequently occurring patterns, comparing sequences for similarity, and recovering missing sequence members. In general, sequence mining problems can be classified as string mining which is typically based on string processing algorithms and itemset mining which is typically based on association rule learning.\n\n","name":"Sequential pattern mining","categories":["Bioinformatics","Bioinformatics algorithms","Data mining"],"tag_line":"Sequential Pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gap-reduction","_score":0,"_source":{"description":"In computational complexity theory, a gap reduction is a reduction to a particular type of decision problem, known as a c-gap problem. Such reductions provide information about the hardness of approximating solutions to optimization problems. In short, a gap problem refers to one wherein the objective is to distinguish between cases where the best solution is above one threshold from cases where the best solution is below another threshold, such that the two thresholds have a gap in between. Gap reductions can be used to demonstrate inapproximability results, as if a problem may be approximated to a better factor than the size of gap, then the approximation algorithm can be used to solve the corresponding gap problem.","name":"Gap reduction","categories":["All articles needing expert attention","All articles that are too technical","Approximation algorithms","Articles needing expert attention from December 2014","Computational problems","Pages using web citations with no URL","Wikipedia articles that are too technical from December 2014"],"tag_line":"In computational complexity theory, a gap reduction is a reduction to a particular type of decision problem, known as a c-gap problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"simulation-algorithms-for-coupled-devs","_score":0,"_source":{"description":"Given a coupled DEVS model, simulation algorithms are methods to generate the model's legal behaviors, which are a set of trajectories not to reach illegal states. (see behavior of a Coupled DEVS model.) [Zeigler84] originally introduced the algorithms that handle time variables related to lifespan  and elapsed time  by introducing two other time variables, last event time, , and next event time  with the following relations:\n\nand\n\nwhere  denotes the current time. And the remaining time,\n\nis equivalently computed as\n\n, apparently .\nBased on these relationships, the algorithms to simulate the behavior of a given Coupled DEVS are written as follows.","name":"Simulation algorithms for coupled DEVS","categories":["Algorithms"],"tag_line":"Given a coupled DEVS model, simulation algorithms are methods to generate the model's legal behaviors, which are a set of trajectories not to reach illegal states."}}
,{"_index":"throwtable","_type":"algorithm","_id":"neighbor-joining","_score":0,"_source":{"description":"In bioinformatics, neighbor joining is a bottom-up (agglomerative) clustering method for the creation of phylogenetic trees, created by Naruya Saitou and Masatoshi Nei in 1987. Usually used for trees based on DNA or protein sequence data, the algorithm requires knowledge of the distance between each pair of taxa (e.g., species or sequences) to form the tree.","name":"Neighbor joining","categories":["All articles with unsourced statements","Articles containing Japanese-language text","Articles with unsourced statements from November 2012","Bioinformatics algorithms","Computational phylogenetics","Data clustering algorithms","Phylogenetics"],"tag_line":"In bioinformatics, neighbor joining is a bottom-up (agglomerative) clustering method for the creation of phylogenetic trees, created by Naruya Saitou and Masatoshi Nei in 1987."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pairwise-algorithm","_score":0,"_source":{"description":"A Pairwise Algorithm  is an algorithmic technique with its origins in Dynamic programming. Pairwise algorithms have several uses including comparing a protein profile (a residue scoring matrix for one or more aligned sequences) against the three translation frames of a DNA strand, allowing frameshifting. The most remarkable feature of PairWise as compared to other Protein-DNA alignment tools is that PairWise allows frameshifting during alignment.","name":"Pairwise Algorithm","categories":["Bioinformatics algorithms"],"tag_line":"A Pairwise Algorithm  is an algorithmic technique with its origins in Dynamic programming."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fitness-function","_score":0,"_source":{"description":"A fitness function is a particular type of objective function that is used to summarise, as a single figure of merit, how close a given design solution is to achieving the set aims.\nIn particular, in the fields of genetic programming and genetic algorithms, each design solution is commonly represented as a string of numbers (referred to as a chromosome). After each round of testing, or simulation, the idea is to delete the 'n' worst design solutions, and to breed 'n' new ones from the best design solutions. Each design solution, therefore, needs to be awarded a figure of merit, to indicate how close it came to meeting the overall specification, and this is generated by applying the fitness function to the test, or simulation, results obtained from that solution.\nThe reason that genetic algorithms cannot be considered to be a lazy way of performing design work is precisely because of the effort involved in designing a workable fitness function. Even though it is no longer the human designer, but the computer, that comes up with the final design, it is the human designer who has to design the fitness function. If this is designed badly, the algorithm will either converge on an inappropriate solution, or will have difficulty converging at all.\nMoreover, the fitness function must not only correlate closely with the designer's goal, it must also be computed quickly. Speed of execution is very important, as a typical genetic algorithm must be iterated many times in order to produce a usable result for a non-trivial problem.\nFitness approximation may be appropriate, especially in the following cases:\nFitness computation time of a single solution is extremely high\nPrecise model for fitness computation is missing\nThe fitness function is uncertain or noisy.\nTwo main classes of fitness functions exist: one where the fitness function does not change, as in optimizing a fixed function or testing with a fixed set of test cases; and one where the fitness function is mutable, as in niche differentiation or co-evolving the set of test cases.\nAnother way of looking at fitness functions is in terms of a fitness landscape, which shows the fitness for each possible chromosome.\nDefinition of the fitness function is not straightforward in many cases and often is performed iteratively if the fittest solutions produced by GA are not what is desired. In some cases, it is very hard or impossible to come up even with a guess of what fitness function definition might be. Interactive genetic algorithms address this difficulty by outsourcing evaluation to external agents (normally humans).","name":"Fitness function","categories":["All articles lacking in-text citations","Articles lacking in-text citations from May 2015","Genetic algorithms"],"tag_line":"A fitness function is a particular type of objective function that is used to summarise, as a single figure of merit, how close a given design solution is to achieving the set aims."}}
,{"_index":"throwtable","_type":"algorithm","_id":"upgma","_score":0,"_source":{"description":"UPGMA (Unweighted Pair Group Method with Arithmetic Mean) is a simple agglomerative (bottom-up) hierarchical clustering method. It is one of the most popular methods in ecology for the classification of sampling units (such as vegetation plots) on the basis of their pairwise similarities in relevant descriptor variables (such as species composition). In bioinformatics, UPGMA is used for the creation of phenetic trees (phenograms). In a phylogenetic context, UPGMA assumes a constant rate of evolution (molecular clock hypothesis), and is not a well-regarded method for inferring relationships unless this assumption has been tested and justified for the data set being used. UPGMA was initially designed for use in protein electrophoresis studies, but is currently most often used to produce guide trees for more sophisticated phylogenetic reconstruction algorithms.\nThe UPGMA algorithm constructs a rooted tree (dendrogram) that reflects the structure present in a pairwise similarity matrix (or a dissimilarity matrix).\nAt each step, the nearest two clusters are combined into a higher-level cluster. The distance between any two clusters A and B is taken to be the average of all distances between pairs of objects \"x\" in A and \"y\" in B, that is, the mean distance between elements of each cluster:\n\nThe method is generally attributed to Sokal and Michener. Fionn Murtagh found a time optimal  time algorithm to construct the UPGMA tree.","name":"UPGMA","categories":["All stub articles","Bioinformatics","Bioinformatics algorithms","Bioinformatics stubs","Computational phylogenetics","Data clustering algorithms","Phylogenetics"],"tag_line":"UPGMA (Unweighted Pair Group Method with Arithmetic Mean) is a simple agglomerative (bottom-up) hierarchical clustering method."}}
,{"_index":"throwtable","_type":"algorithm","_id":"inheritance-(genetic-algorithm)","_score":0,"_source":{"description":"In genetic algorithms, inheritance is the ability of modeled objects to mate, mutate (similar to biological mutation), and propagate their problem solving genes to the next generation, in order to produce an evolved solution to a particular problem. The selection of objects that will be inherited from in each successive generation is determined by a fitness function, which varies depending upon the problem being addressed.\nThe traits of these objects are passed on through chromosomes by a means similar to biological reproduction. These chromosomes are generally represented by a series of genes, which in turn are usually represented using binary numbers. This propagation of traits between generations is similar to the inheritance of traits between generations of biological organisms. This process can also be viewed as a form of reinforcement learning, because the evolution of the objects is driven by the passing of traits from successful objects which can be viewed as a reward for their success, thereby promoting beneficial traits.\n^ a b Russell, Stuart J.; Norvig, Peter (1995). Artificial Intelligence: A Modern Approach. Englewood Heights, NJ: Prentice-Hall.","name":"Inheritance (genetic algorithm)","categories":["All orphaned articles","Genetic algorithms","Orphaned articles from April 2013"],"tag_line":"In genetic algorithms, inheritance is the ability of modeled objects to mate, mutate (similar to biological mutation), and propagate their problem solving genes to the next generation, in order to produce an evolved solution to a particular problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"parallel-metaheuristic","_score":0,"_source":{"description":"Parallel metaheuristic is a class of techniques that are capable of reducing both the numerical effort and the run time of a metaheuristic. To this end, concepts and technologies from the field of parallelism in computer science are used to enhance and even completely modify the behavior of existing metaheuristics. Just as it exists a long list of metaheuristics like evolutionary algorithms, particle swarm, ant colony optimization, simulated annealing, etc. it also exists a large set of different techniques strongly or loosely based in these ones, whose behavior encompasses the multiple parallel execution of algorithm components that cooperate in some way to solve a problem on a given parallel hardware platform.","name":"Parallel metaheuristic","categories":["All accuracy disputes","All articles lacking in-text citations","All articles needing additional references","All articles with specifically marked weasel-worded phrases","Articles lacking in-text citations from June 2015","Articles needing additional references from June 2015","Articles with disputed statements from June 2015","Articles with specifically marked weasel-worded phrases from April 2012","Genetic algorithms","Mathematical optimization","Optimization algorithms and methods","Search algorithms","Wikipedia articles needing clarification from June 2015","Wikipedia articles with possible conflicts of interest from June 2015"],"tag_line":"Parallel metaheuristic is a class of techniques that are capable of reducing both the numerical effort and the run time of a metaheuristic."}}
,{"_index":"throwtable","_type":"algorithm","_id":"clonal-selection-algorithm","_score":0,"_source":{"description":"In artificial immune systems, Clonal selection algorithms are a class of algorithms inspired by the clonal selection theory of acquired immunity that explains how B and T lymphocytes improve their response to antigens over time called affinity maturation. These algorithms focus on the Darwinian attributes of the theory where selection is inspired by the affinity of antigen-antibody interactions, reproduction is inspired by cell division, and variation is inspired by somatic hypermutation. Clonal selection algorithms are most commonly applied to optimization and pattern recognition domains, some of which resemble parallel hill climbing and the genetic algorithm without the recombination operator.","name":"Clonal Selection Algorithm","categories":["Genetic algorithms"],"tag_line":"In artificial immune systems, Clonal selection algorithms are a class of algorithms inspired by the clonal selection theory of acquired immunity that explains how B and T lymphocytes improve their response to antigens over time called affinity maturation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hyperneat","_score":0,"_source":{"description":"Hypercube-based NEAT, or HyperNEAT, is a generative encoding that evolves artificial neural networks (ANNs) with the principles of the widely used NeuroEvolution of Augmented Topologies (NEAT) algorithm. It is a novel technique for evolving large-scale neural networks utilizing the geometric regularities of the task domain. It uses Compositional Pattern Producing Networks  (CPPNs), which are used to generate the images for Picbreeder.org and shapes for EndlessForms.com. HyperNEAT has recently been extended to also evolve plastic ANNs  and to evolve the location of every neuron in the network.","name":"HyperNEAT","categories":["Artificial neural networks","Evolutionary algorithms","Evolutionary computation","Genetic algorithms"],"tag_line":"Hypercube-based NEAT, or HyperNEAT, is a generative encoding that evolves artificial neural networks (ANNs) with the principles of the widely used NeuroEvolution of Augmented Topologies (NEAT) algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"velvet-assembler","_score":0,"_source":{"description":"Velvet is an algorithm package that has been designed to deal with de novo genome assembly and short read sequencing alignments. This is achieved through the manipulation of de Bruijn graphs for genomic sequence assembly via the removal of errors and the simplification of repeated regions. Velvet has also been implemented inside of commercial packages, such as Geneious, MacVector and BioNumerics.","name":"Velvet assembler","categories":["Bioinformatics algorithms","Bioinformatics software","DNA sequencing","Metagenomics software"],"tag_line":"Velvet is an algorithm package that has been designed to deal with de novo genome assembly and short read sequencing alignments."}}
,{"_index":"throwtable","_type":"algorithm","_id":"evolver-(software)","_score":0,"_source":{"description":"Evolver is a software package that allows users to solve a wide variety of optimization problems using a genetic algorithm. Launched in 1990, it was the first commercially available genetic algorithm package for personal computers. The program was originally developed by Axcelis, Inc. and is now owned by Palisade Corporation.","name":"Evolver (software)","categories":["All stub articles","Genetic algorithms","Software engineering stubs"],"tag_line":"Evolver is a software package that allows users to solve a wide variety of optimization problems using a genetic algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cultural-algorithm","_score":0,"_source":{"description":"Cultural algorithms (CA) are a branch of evolutionary computation where there is a knowledge component that is called the belief space in addition to the population component. In this sense, cultural algorithms can be seen as an extension to a conventional genetic algorithm. Cultural algorithms were introduced by Reynolds (see references).","name":"Cultural algorithm","categories":["Evolutionary algorithms","Genetic algorithms"],"tag_line":"Cultural algorithms (CA) are a branch of evolutionary computation where there is a knowledge component that is called the belief space in addition to the population component."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sha-1","_score":0,"_source":{"description":"In cryptography, SHA-1 (Secure Hash Algorithm 1) is a cryptographic hash function designed by the United States National Security Agency and is a U.S. Federal Information Processing Standard published by the United States NIST. SHA-1 is considered insecure against well-funded opponents, and it is recommended to use SHA-2 or SHA-3 instead.\nSHA-1 produces a 160-bit (20-byte) hash value known as a message digest. A SHA-1 hash value is typically rendered as a hexadecimal number, 40 digits long.\nSHA-1 is a member of the Secure Hash Algorithm family. The four SHA algorithms are structured differently and are named SHA-0, SHA-1, SHA-2, and SHA-3. SHA-0 is the original version of the 160-bit hash function published in 1993 under the name SHA: it was not adopted by many applications. Published in 1995, SHA-1 is very similar to SHA-0, but alters the original SHA hash specification to correct weaknesses that were unknown to the public at that time. SHA-2, published in 2001, is significantly different from the SHA-1 hash function.\nIn 2005, cryptanalysts found attacks on SHA-1 suggesting that the algorithm might not be secure enough for ongoing use. NIST required many applications in federal agencies to move to SHA-2 after 2010 because of the weakness. Although no successful attacks have yet been reported on SHA-2, it is algorithmically similar to SHA-1. In 2012, following a long-running competition, NIST selected an additional algorithm, Keccak, for standardization under SHA-3.\nMicrosoft, Google and Mozilla have all announced that their respective browsers will stop accepting SHA-1 SSL certificates by 2017. Windows XP SP2 and earlier, and Android 2.2 and earlier, do not support SHA2 certificates.","name":"SHA-1","categories":["All articles containing potentially dated statements","All articles needing additional references","All articles with specifically marked weasel-worded phrases","All articles with unsourced statements","Articles containing potentially dated statements from 2013","Articles containing potentially dated statements from October 2015","Articles needing additional references from May 2013","Articles with Chinese-language external links","Articles with DMOZ links","Articles with example pseudocode","Articles with specifically marked weasel-worded phrases from September 2015","Articles with unsourced statements from August 2012","Articles with unsourced statements from June 2015","Broken hash functions","CS1 errors: dates","Checksum algorithms","Cryptographic hash functions","National Security Agency cryptography"],"tag_line":"In cryptography, SHA-1 (Secure Hash Algorithm 1) is a cryptographic hash function designed by the United States National Security Agency and is a U.S. Federal Information Processing Standard published by the United States NIST."}}
,{"_index":"throwtable","_type":"algorithm","_id":"stochastic-universal-sampling","_score":0,"_source":{"description":"Stochastic universal sampling (SUS) is a technique used in genetic algorithms for selecting potentially useful solutions for recombination. It was introduced by James Baker.\nSUS is a development of fitness proportionate selection (FPS) which exhibits no bias and minimal spread. Where FPS chooses several solutions from the population by repeated random sampling, SUS uses a single random value to sample all of the solutions by choosing them at evenly spaced intervals. This gives weaker members of the population (according to their fitness) a chance to be chosen and thus reduces the unfair nature of fitness-proportional selection methods.\nOther methods like roulette wheel can have bad performance when a member of the population has a really large fitness in comparison with other members. Using a comb-like ruler, SUS starts from a small random number, and chooses the next candidates from the rest of population remaining, not allowing the fittest members to saturate the candidate space.\nDescribed as an algorithm, pseudocode for SUS looks like:\n\nSUS(Population, N)\n    F := total fitness of Population\n    N := number of offspring to keep\n    P := distance between the pointers (F/N)\n    Start := random number between 0 and P\n    Pointers := [Start + i*P | i in [0..(N-1)]]\n    return RWS(Population,Pointers)\n\nRWS(Population, Points)\n    Keep = []\n    i := 0\n    for P in Points\n        while fitness sum of Population[0..i] < P\n            i++\n        add Population[i] to Keep\n    return Keep\n\nWhere Population[0..i] is the set of individuals with array-index 0 to (and including) i.\nHere RWS() describes the bulk of fitness proportionate selection (also known as \"roulette wheel selection\") - in true fitness proportional selection the parameter Points is always a (sorted) list of random numbers from 0 to F. The algorithm above is intended to be illustrative rather than canonical.","name":"Stochastic universal sampling","categories":["Genetic algorithms","Pages using citations with accessdate and no URL","Stochastic algorithms"],"tag_line":"Stochastic universal sampling (SUS) is a technique used in genetic algorithms for selecting potentially useful solutions for recombination."}}
,{"_index":"throwtable","_type":"algorithm","_id":"promoter-based-genetic-algorithm","_score":0,"_source":{"description":"The promoter based genetic algorithm (PBGA) is a genetic algorithm for neuroevolution developed by F. Bellas and R.J. Duro in the Integrated Group for Engineering Research (GII) at the University of Coruña, in Spain. It evolves variable size feedforward artificial neural networks (ANN) that are encoded into sequences of genes for constructing a basic ANN unit. Each of these blocks is preceded by a gene promoter acting as an on/off switch that determines if that particular unit will be expressed or not.","name":"Promoter based genetic algorithm","categories":["Artificial neural networks","Evolutionary algorithms","Evolutionary computation","Genetic algorithms"],"tag_line":"The promoter based genetic algorithm (PBGA) is a genetic algorithm for neuroevolution developed by F. Bellas and R.J. Duro in the Integrated Group for Engineering Research (GII) at the University of Coruña, in Spain."}}
,{"_index":"throwtable","_type":"algorithm","_id":"astronomical-algorithm","_score":0,"_source":{"description":"Astronomical algorithms are the algorithms used to calculate ephemerides, calendars, and positions (as in celestial navigation or satellite navigation). Examples of large and complex astronomical algorithms are those used to calculate the position of the Moon. A simple example is the calculation of the Julian day.\nNumerical model of solar system discusses a generalized approach to local astronomical modeling. The variations séculaires des orbites planétaires describes an often used model.","name":"Astronomical algorithm","categories":["Algorithms and data structures stubs","All articles lacking sources","All stub articles","Articles lacking sources from April 2010","Astrodynamics","Astronomy stubs","Calendar algorithms","Computational physics","Computer science stubs"],"tag_line":"Astronomical algorithms are the algorithms used to calculate ephemerides, calendars, and positions (as in celestial navigation or satellite navigation)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"truncation-selection","_score":0,"_source":{"description":"Truncation selection is a selection method used in genetic algorithms to select potential candidate solutions for recombination.\nIn truncation selection the candidate solutions are ordered by fitness, and some proportion, p, (e.g. p = 1/2, 1/3, etc.), of the fittest individuals are selected and reproduced 1/p times. Truncation selection is less sophisticated than many other selection methods, and is not often used in practice. It is used in Muhlenbein's Breeder Genetic Algorithm.","name":"Truncation selection","categories":["All stub articles","Artificial intelligence stubs","Bioinformatics stubs","Computer science stubs","Genetic algorithms"],"tag_line":"Truncation selection is a selection method used in genetic algorithms to select potential candidate solutions for recombination."}}
,{"_index":"throwtable","_type":"algorithm","_id":"luhn-mod-n-algorithm","_score":0,"_source":{"description":"The Luhn mod N algorithm is an extension to the Luhn algorithm (also known as mod 10 algorithm) that allows it to work with sequences of non-numeric characters. This can be useful when a check digit is required to validate an identification string composed of letters, a combination of letters and digits or even any arbitrary set of characters.","name":"Luhn mod N algorithm","categories":["All articles lacking sources","Articles lacking sources from May 2010","Articles with example code","Checksum algorithms","Modular arithmetic"],"tag_line":"The Luhn mod N algorithm is an extension to the Luhn algorithm (also known as mod 10 algorithm) that allows it to work with sequences of non-numeric characters."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sysv-checksum","_score":0,"_source":{"description":"The SYSV checksum algorithm is commonly used, legacy checksum algorithms. It has been implemented in UNIX System V and is also available through the GNU sum command line utility.","name":"SYSV checksum","categories":["Checksum algorithms"],"tag_line":"The SYSV checksum algorithm is commonly used, legacy checksum algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"branch-and-bound","_score":0,"_source":{"description":"Branch and bound (BB or B&B) is an algorithm design paradigm for discrete and combinatorial optimization problems, as well as general real valued problems. A branch-and-bound algorithm consists of a systematic enumeration of candidate solutions by means of state space search: the set of candidate solutions is thought of as forming a rooted tree with the full set at the root. The algorithm explores branches of this tree, which represent subsets of the solution set. Before enumerating the candidate solutions of a branch, the branch is checked against upper and lower estimated bounds on the optimal solution, and is discarded if it cannot produce a better solution than the best one found so far by the algorithm.\nThe algorithm depends on the efficient estimation of the lower and upper bounds of a region/branch of the search space and approaches exhaustive enumeration as the size (n-dimensional volume) of the region tends to zero.\nThe method was first proposed by A. H. Land and A. G. Doig in 1960 for discrete programming, and has become the most commonly used tool for solving NP-hard optimization problems. The name \"branch and bound\" first occurred in the work of Little et al. on the traveling salesman problem.","name":"Branch and bound","categories":["All articles with unsourced statements","Articles with unsourced statements from July 2015","Articles with unsourced statements from September 2015","Combinatorial optimization","Optimization algorithms and methods","Wikipedia articles needing clarification from July 2015"],"tag_line":"Branch and bound (BB or B&B) is an algorithm design paradigm for discrete and combinatorial optimization problems, as well as general real valued problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"a*-search-algorithm","_score":0,"_source":{"description":"In computer science, A* (pronounced as \"A star\" ( listen)) is a computer algorithm that is widely used in pathfinding and graph traversal, the process of plotting an efficiently traversable path between multiple points, called nodes. Noted for its performance and accuracy, it enjoys widespread use. However, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, although other work has found A* to be superior to other approaches.\nPeter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first described the algorithm in 1968. It is an extension of Edsger Dijkstra's 1959 algorithm. A* achieves better performance by using heuristics to guide its search.\n\n","name":"A* search algorithm","categories":["All accuracy disputes","Articles with disputed statements from February 2014","Articles with example pseudocode","Combinatorial optimization","Game artificial intelligence","Graph algorithms","Routing algorithms","Search algorithms"],"tag_line":"In computer science, A* (pronounced as \"A star\" ( listen)) is a computer algorithm that is widely used in pathfinding and graph traversal, the process of plotting an efficiently traversable path between multiple points, called nodes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"greedy-algorithm","_score":0,"_source":{"description":"A greedy algorithm is an algorithm that follows the problem solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. In many problems, a greedy strategy does not in general produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a global optimal solution in a reasonable time.\nFor example, a greedy strategy for the traveling salesman problem (which is of a high computational complexity) is the following heuristic: \"At each stage visit an unvisited city nearest to the current city\". This heuristic need not find a best solution, but terminates in a reasonable number of steps; finding an optimal solution typically requires unreasonably many steps. In mathematical optimization, greedy algorithms solve combinatorial problems having the properties of matroids.","name":"Greedy algorithm","categories":["Combinatorial algorithms","Commons category without a link on Wikidata","Exchange algorithms","Matroid theory","Optimization algorithms and methods"],"tag_line":"A greedy algorithm is an algorithm that follows the problem solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lexicographic-breadth-first-search","_score":0,"_source":{"description":"In computer science, lexicographic breadth-first search or Lex-BFS is a linear time algorithm for ordering the vertices of a graph. The algorithm is different from breadth first search, but it produces an ordering that is consistent with breadth-first search.\nThe lexicographic breadth-first search algorithm is based on the idea of partition refinement and was first developed by Donald J. Rose, Robert E. Tarjan, and George S. Lueker (1976). A more detailed survey of the topic is presented by Corneil (2004). It has been used as a subroutine in other graph algorithms including the recognition of chordal graphs, and optimal coloring of distance-hereditary graphs.","name":"Lexicographic breadth-first search","categories":["Graph algorithms","Search algorithms"],"tag_line":"In computer science, lexicographic breadth-first search or Lex-BFS is a linear time algorithm for ordering the vertices of a graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"contraction-hierarchies","_score":0,"_source":{"description":"In applied mathematics, the method of contraction hierarchies is a technique to speed up shortest-path routing by first creating precomputed \"contracted\" versions of the connection graph. It can be regarded as a special case of \"highway-node routing\".\nContraction hierarchies can be used to generate shortest-path routes much more efficiently than Dijkstra's algorithm or previous highway-node routing approaches, and is used in many advanced routing techniques. It is publicly available in open source software to calculate routes from one place to another.","name":"Contraction hierarchies","categories":["Graph algorithms","Routing algorithms"],"tag_line":"In applied mathematics, the method of contraction hierarchies is a technique to speed up shortest-path routing by first creating precomputed \"contracted\" versions of the connection graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"color-coding","_score":0,"_source":{"description":"In computer science and graph theory, the method of color-coding efficiently finds k-vertex simple paths, k-vertex cycles, and other small subgraphs within a given graph using probabilistic algorithms, which can then be derandomized and turned into deterministic algorithms. This method shows that many subcases of the subgraph isomorphism problem (an NP-complete problem) can in fact be solved in polynomial time.\nThe theory and analysis of the color-coding method was proposed in 1994 by Noga Alon, Raphael Yuster, and Uri Zwick.","name":"Color-coding","categories":["Graph algorithms"],"tag_line":"In computer science and graph theory, the method of color-coding efficiently finds k-vertex simple paths, k-vertex cycles, and other small subgraphs within a given graph using probabilistic algorithms, which can then be derandomized and turned into deterministic algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"clique-percolation-method","_score":0,"_source":{"description":"The clique percolation method is a popular approach for analyzing the overlapping community structure of networks. The term network community (also called a module, cluster or cohesive group) has no widely accepted unique definition and it is usually defined as a group of nodes that are more densely connected to each other than to other nodes in the network. There are numerous alternative methods for detecting communities in networks, for example, the Girvan–Newman algorithm, hierarchical clustering and modularity maximization.","name":"Clique percolation method","categories":["Clustering algorithms","Graph algorithms","Network analysis","Networks"],"tag_line":"The clique percolation method is a popular approach for analyzing the overlapping community structure of networks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dinic's-algorithm","_score":0,"_source":{"description":"Dinic's algorithm or Dinitz's algorithm is a strongly polynomial algorithm for computing the maximum flow in a flow network, conceived in 1970 by Israeli (formerly Soviet) computer scientist Yefim (Chaim) A. Dinitz. The algorithm runs in  time and is similar to the Edmonds–Karp algorithm, which runs in  time, in that it uses shortest augmenting paths. The introduction of the concepts of the level graph and blocking flow enable Dinic's algorithm to achieve its performance.\n\n","name":"Dinic's algorithm","categories":["Graph algorithms","Network flow"],"tag_line":"Dinic's algorithm or Dinitz's algorithm is a strongly polynomial algorithm for computing the maximum flow in a flow network, conceived in 1970 by Israeli (formerly Soviet) computer scientist Yefim (Chaim) A. Dinitz."}}
,{"_index":"throwtable","_type":"algorithm","_id":"verhoeff-algorithm","_score":0,"_source":{"description":"The Verhoeff algorithm is a checksum formula for error detection developed by the Dutch mathematician Jacobus Verhoeff and was first published in 1969. It was the first decimal check digit algorithm which detects all single-digit errors, and all transposition errors involving two adjacent digits, which was at the time thought impossible with such a code.","name":"Verhoeff algorithm","categories":["Checksum algorithms","Error detection and correction","Modular arithmetic","Wikipedia articles needing clarification from April 2014"],"tag_line":"The Verhoeff algorithm is a checksum formula for error detection developed by the Dutch mathematician Jacobus Verhoeff and was first published in 1969."}}
,{"_index":"throwtable","_type":"algorithm","_id":"funnelsort","_score":0,"_source":{"description":"Funnelsort is a comparison-based sorting algorithm. It was introduced by Frigo, Leiserson, Prokop, and Ramachandran in 1999 in the context of the cache oblivious model.\nIn the external memory model, the number of memory transfers it needs to perform a sort of  items on a machine with cache of size  and cache lines of length  is , under the tall cache assumption that . This number of memory transfers has been shown to be asymptotically optimal for comparison sorts. Funnelsort also achieves the asymptotically optimal runtime complexity of .\n^ M. Frigo, C.E. Leiserson, H. Prokop, and S. Ramachandran. Cache-oblivious algorithms. In Proceedings of the 40th IEEE Symposium on Foundations of Computer Science (FOCS 99), pp. 285-297. 1999. Extended abstract at IEEE, at Citeseer.\n^ Harald Prokop. Cache-Oblivious Algorithms. Masters thesis, MIT. 1999.","name":"Funnelsort","categories":["All articles lacking in-text citations","All articles lacking reliable references","All articles needing expert attention","All articles that are too technical","Analysis of algorithms","Articles lacking in-text citations from May 2014","Articles lacking reliable references from May 2014","Articles needing expert attention from May 2014","CS1 errors: chapter ignored","Cache (computing)","Comparison sorts","External memory algorithms","Models of computation","Sorting algorithms","Wikipedia articles that are too technical from May 2014"],"tag_line":"Funnelsort is a comparison-based sorting algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"reverse-delete-algorithm","_score":0,"_source":{"description":"The reverse-delete algorithm is an algorithm in graph theory used to obtain a minimum spanning tree from a given connected, edge-weighted graph. It first appeared in Kruskal (1956), but it should not be confused with Kruskal's algorithm which appears in the same paper. If the graph is disconnected, this algorithm will find a minimum spanning tree for each disconnected part of the graph. The set of these minimum spanning trees is called a minimum spanning forest, which contains every vertex in the graph.\nThis algorithm is a greedy algorithm, choosing the best choice given any situation. It is the reverse of Kruskal's algorithm, which is another greedy algorithm to find a minimum spanning tree. Kruskal’s algorithm starts with an empty graph and adds edges while the Reverse-Delete algorithm starts with the original graph and deletes edges from it. The algorithm works as follows:\nStart with graph G, which contains a list of edges E.\nGo through E in decreasing order of edge weights.\nFor each edge, check if deleting the edge will further disconnect the graph.\nPerform any deletion that does not lead to additional disconnection.","name":"Reverse-delete algorithm","categories":["Graph algorithms","Spanning tree"],"tag_line":"The reverse-delete algorithm is an algorithm in graph theory used to obtain a minimum spanning tree from a given connected, edge-weighted graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bogosort","_score":0,"_source":{"description":"In computer science, bogosort (also stupid sort, slowsort, random sort, shotgun sort or monkey sort) is a particularly ineffective sorting algorithm based on the generate and test paradigm. It is not useful for sorting, but may be used for educational purposes, to contrast it with other more realistic algorithms; it has also been used as an example in logic programming. If bogosort were used to sort a deck of cards, it would consist of checking if the deck were in order, and if it were not, throwing the deck into the air, picking the cards up at random, and repeating the process until the deck is sorted. Its name comes from the word bogus.","name":"Bogosort","categories":["Accuracy disputes from November 2015","All accuracy disputes","Articles to be expanded from November 2015","Comparison sorts","Computer humor","Sorting algorithms","Use dmy dates from June 2011"],"tag_line":"In computer science, bogosort (also stupid sort, slowsort, random sort, shotgun sort or monkey sort) is a particularly ineffective sorting algorithm based on the generate and test paradigm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"topological-sorting","_score":0,"_source":{"description":"In the field of computer science, a topological sort (sometimes abbreviated toposort) or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks. A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time.","name":"Topological sorting","categories":["Articles with example pseudocode","Directed graphs","Graph algorithms","Sorting algorithms"],"tag_line":"In the field of computer science, a topological sort (sometimes abbreviated toposort) or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cache-oblivious-distribution-sort","_score":0,"_source":{"description":"The cache-oblivious distribution sort is a comparison-based sorting algorithm. It was introduced in 1999 in the context of the cache oblivious model. In the external memory model, the number of memory transfers it needs to perform a sort of  items on a machine with cache of size  and cache lines of length  is , under the tall cache assumption that . This number of memory transfers has been shown to be asymptotically optimal for comparison sorts. This distribution sort also achieves the asymptotically optimal runtime complexity of .","name":"Cache-oblivious distribution sort","categories":["All articles lacking reliable references","All articles needing expert attention","All articles that are too technical","All orphaned articles","Analysis of algorithms","Articles lacking reliable references from May 2014","Articles needing expert attention from May 2014","Cache (computing)","Comparison sorts","External memory algorithms","Models of computation","Orphaned articles from May 2014","Sorting algorithms","Wikipedia articles that are too technical from May 2014"],"tag_line":"The cache-oblivious distribution sort is a comparison-based sorting algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shortest-path-faster-algorithm","_score":0,"_source":{"description":"The Shortest Path Faster Algorithm (SPFA) is an improvement of the Bellman–Ford algorithm which computes single-source shortest paths in a weighted directed graph. The algorithm is believed to work well on random sparse graphs and is particularly suitable for graphs that contain negative-weight edges. However, the worst-case complexity of SPFA is the same as that of Bellman–Ford, so for graphs with nonnegative edge weights Dijkstra's algorithm is preferred. The SPFA algorithm was published in 1994 by Fanding Duan.","name":"Shortest Path Faster Algorithm","categories":["Graph algorithms"],"tag_line":"The Shortest Path Faster Algorithm (SPFA) is an improvement of the Bellman–Ford algorithm which computes single-source shortest paths in a weighted directed graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"yen's-algorithm","_score":0,"_source":{"description":"Yen's algorithm computes single-source K-shortest loopless paths for a graph with non-negative edge cost. The algorithm was published by Jin Y. Yen in 1971 and employs any shortest path algorithm to find the best path, then proceeds to find K − 1 deviations of the best path.","name":"Yen's algorithm","categories":["Articles with example pseudocode","Graph algorithms","Polynomial-time problems"],"tag_line":"Yen's algorithm computes single-source K-shortest loopless paths for a graph with non-negative edge cost."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adaptive-sort","_score":0,"_source":{"description":"A sorting algorithm falls into the adaptive sort family if it takes advantage of existing order in its input. It benefits from the presortedness in the input sequence – or a limited amount of disorder for various definitions of measures of disorder – and sorts faster. Adaptive sorting is usually performed by modifying existing sorting algorithms.","name":"Adaptive sort","categories":["Sorting algorithms"],"tag_line":"A sorting algorithm falls into the adaptive sort family if it takes advantage of existing order in its input."}}
,{"_index":"throwtable","_type":"algorithm","_id":"spectral-layout","_score":0,"_source":{"description":"Spectral layout is a class of algorithm for drawing graphs. The layout uses the eigenvectors of a matrix, such as the Laplace matrix of the graph, as Cartesian coordinates of the graph's vertices.","name":"Spectral layout","categories":["All stub articles","Applied mathematics stubs","Graph algorithms","Graph drawing"],"tag_line":"Spectral layout is a class of algorithm for drawing graphs."}}
,{"_index":"throwtable","_type":"algorithm","_id":"spreadsort","_score":0,"_source":{"description":"Spreadsort is a sorting algorithm invented by Steven J. Ross in 2002. It combines concepts from distribution-based sorts, such as radix sort and bucket sort, with partitioning concepts from comparison sorts such as quicksort and mergesort. In experimental results it was shown to be highly efficient, often outperforming traditional algorithms such as quicksort, particularly on distributions exhibiting structure.\nQuicksort identifies a pivot element in the list and then partitions the list into two sublists, those elements less than the pivot and those greater than the pivot. Spreadsort generalizes this idea by partitioning the list into n/c partitions at each step, where n is the total number of elements in the list and c is a small constant (in practice usually between 4 and 8 when comparisons are slow, or much larger in situations where they are fast). It uses distribution-based techniques to accomplish this, first locating the minimum and maximum value in the list, and then dividing the region between them into n/c equal-sized bins. Where caching is an issue, it can help to have a maximum number of bins in each recursive division step, causing this division process to take multiple steps. Though this causes more iterations, it reduces cache misses and can make the algorithm run faster overall.\nIn the case where the number of bins is at least the number of elements, spreadsort degenerates to bucket sort and the sort completes. Otherwise, each bin is sorted recursively. The algorithm uses heuristic tests to determine whether each bin would be more efficiently sorted by spreadsort or some other classical sort algorithm, then recursively sorts the bin.\nLike other distribution-based sorts, spreadsort has the weakness that the programmer is required to provide a means of converting each element into a numeric key, for the purpose of identifying which bin it falls in. Although it is possible to do this for arbitrary-length elements such as strings by considering each element to be followed by an infinite number of minimum values, and indeed for any datatype possessing a total order, this can be more difficult to implement correctly than a simple comparison function, especially on complex structures. Poor implementation of this value function can result in clustering that harms the algorithm's relative performance.\n^ Steven J. Ross. The Spreadsort High-performance General-case Sorting Algorithm. Parallel and Distributed Processing Techniques and Applications, Volume 3, pp.1100–1106. Las Vegas Nevada. 2002.","name":"Spreadsort","categories":["Sorting algorithms"],"tag_line":"Spreadsort is a sorting algorithm invented by Steven J. Ross in 2002."}}
,{"_index":"throwtable","_type":"algorithm","_id":"library-sort","_score":0,"_source":{"description":"Library sort, or gapped insertion sort is a sorting algorithm that uses an insertion sort, but with gaps in the array to accelerate subsequent insertions. The name comes from an analogy:\n\nSuppose a librarian were to store his books alphabetically on a long shelf, starting with the A's at the left end, and continuing to the right along the shelf with no spaces between the books until the end of the Z's. If the librarian acquired a new book that belongs to the B section, once he finds the correct space in the B section, he will have to move every book over, from the middle of the B's all the way down to the Z's in order to make room for the new book. This is an insertion sort. However, if he were to leave a space after every letter, as long as there was still space after B, he would only have to move a few books to make room for the new one. This is the basic principle of the Library Sort.\n\nThe algorithm was proposed by Michael A. Bender, Martín Farach-Colton, and Miguel Mosteiro in 2004 and was published in 2006.\nLike the insertion sort it is based on, library sort is a stable comparison sort and can be run as an online algorithm; however, it was shown to have a high probability of running in O(n log n) time (comparable to quicksort), rather than an insertion sort's O(n2). The mechanism used for this improvement is very similar to that of a skip list. There is no full implementation given in the paper, nor the exact algorithms of important parts, such as insertion and rebalancing. Further information would be needed to discuss how the efficiency of library sort compares to that of other sorting methods in reality.\nCompared to basic insertion sort, the drawback of library sort is that it requires extra space for the gaps. The amount and distribution of that space would be implementation dependent. In the paper the size of the needed array is (1 + ε)n, but with no further recommendations on how to choose ε.\nOne weakness of insertion sort is that it may require a high number of swap operations and be costly if memory write is expensive. Library sort may improve that somewhat in the insertion step, as fewer elements need to move to make room, but is also adding an extra cost in the rebalancing step. In addition, locality of reference will be poor compared to mergesort as each insertion from a random data set may access memory that is no longer in cache, especially with large data sets.","name":"Library sort","categories":["Comparison sorts","Online sorts","Sorting algorithms","Stable sorts"],"tag_line":"Library sort, or gapped insertion sort is a sorting algorithm that uses an insertion sort, but with gaps in the array to accelerate subsequent insertions."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sorting-network","_score":0,"_source":{"description":"In computer science, comparator networks are abstract devices built up of a fixed number of \"wires\", carrying values, and comparator modules that connect pairs of wires, swapping the values on the wires if they are not in a desired order. Such networks are typically designed to perform sorting on fixed numbers of values, in which case they are called sorting networks.\nSorting networks differ from general comparison sorts in that they are not capable of handling arbitrarily large inputs, and in that their sequence of comparisons is set in advance, regardless of the outcome of previous comparisons. This independence of comparison sequences is useful for parallel execution and for implementation in hardware. Despite the simplicity of sorting nets, their theory is surprisingly deep and complex. Sorting networks were first studied circa 1954 by Armstrong, Nelson and O'Connor, who subsequently patented the idea.\nSorting networks can be implemented either in hardware or in software. Donald Knuth describes how the comparators for binary integers can be implemented as simple, three-state electronic devices. Batcher, in 1968, suggested using them to construct switching networks for computer hardware, replacing both buses and the faster, but more expensive, crossbar switches. Since the 2000s, sorting nets (especially bitonic mergesort) are used by the GPGPU community for constructing sorting algorithms to run on graphics processing units.","name":"Sorting network","categories":["Computer engineering","Pages using duplicate arguments in template calls","Sorting algorithms"],"tag_line":"In computer science, comparator networks are abstract devices built up of a fixed number of \"wires\", carrying values, and comparator modules that connect pairs of wires, swapping the values on the wires if they are not in a desired order."}}
,{"_index":"throwtable","_type":"algorithm","_id":"insertion-sort","_score":0,"_source":{"description":"Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages:\nSimple implementation: Bentley shows a three-line C version, and a five-line optimized version\nEfficient for (quite) small data sets, much like other quadratic sorting algorithms\nMore efficient in practice than most other simple quadratic (i.e., O(n2)) algorithms such as selection sort or bubble sort\nAdaptive, i.e., efficient for data sets that are already substantially sorted: the time complexity is O(nk) when each element in the input is no more than k places away from its sorted position\nStable; i.e., does not change the relative order of elements with equal keys\nIn-place; i.e., only requires a constant amount O(1) of additional memory space\nOnline; i.e., can sort a list as it receives it\nWhen people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.","name":"Insertion sort","categories":["All articles with unsourced statements","Articles with example pseudocode","Articles with unsourced statements from September 2011","Articles with unsourced statements from September 2014","Commons category with local link same as on Wikidata","Comparison sorts","Online sorts","Sorting algorithms","Stable sorts"],"tag_line":"Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time."}}
,{"_index":"throwtable","_type":"algorithm","_id":"stooge-sort","_score":0,"_source":{"description":"Stooge sort is a recursive sorting algorithm with a time complexity of O(nlog 3 / log 1.5 ) = O(n2.7095...). The running time of the algorithm is thus slower compared to efficient sorting algorithms, such as Merge sort, and is even slower than Bubble sort, a canonical example of a fairly inefficient and simple sort.\nThe algorithm is defined as follows:\nIf the value at the end is smaller than the value at the start, swap them.\nIf there are 3 or more elements in the list, then:\nStooge sort the initial 2/3 of the list\nStooge sort the final 2/3 of the list\nStooge sort the initial 2/3 of the list again\n\nelse: exit the procedure\nIt is important to get the integer sort size used in the recursive calls by rounding the 2/3 upwards, e.g. rounding 2/3 of 5 should give 4 rather than 3, as otherwise the sort can fail on certain data. However, if the code is written to end on a base case of size 1, rather than terminating on either size 1 or size 2, rounding the 2/3 of 2 upwards gives an infinite number of calls.\nThe algorithm gets its name from slapstick routines of The Three Stooges, in which each stooge hits the other two.","name":"Stooge sort","categories":["All articles with unsourced statements","All stub articles","Articles with example pseudocode","Articles with unsourced statements from March 2010","Comparison sorts","Computer science stubs","Sorting algorithms","Use dmy dates from October 2010"],"tag_line":"Stooge sort is a recursive sorting algorithm with a time complexity of O(nlog 3 / log 1.5 ) = O(n2.7095...)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"selection-sort","_score":0,"_source":{"description":"In computer science, selection sort is a sorting algorithm, specifically an in-place comparison sort. It has O(n2) time complexity, making it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity, and it has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.\nThe algorithm divides the input list into two parts: the sublist of items already sorted, which is built up from left to right at the front (left) of the list, and the sublist of items remaining to be sorted that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right.","name":"Selection sort","categories":["Comparison sorts","Sorting algorithms"],"tag_line":"In computer science, selection sort is a sorting algorithm, specifically an in-place comparison sort."}}
,{"_index":"throwtable","_type":"algorithm","_id":"merge-algorithm","_score":0,"_source":{"description":"Merge algorithms are a family of algorithms that run sequentially over multiple sorted lists, typically producing more sorted lists as output. This is well-suited for machines with tape drives.\nThe general merge algorithm has a set of pointers p0..n that point to positions in a set of lists L0..n. Initially they point to the first item in each list. The algorithm is as follows:\nWhile any of p0..n still point to data inside of L0..n instead of past the end:\ndo something with the data items p0..n point to in their respective lists\nfind out which of those pointers points to the item with the lowest key; advance one of those pointers to the next item in its list","name":"Merge algorithm","categories":["All articles needing additional references","All articles needing expert attention","Articles needing additional references from August 2008","Articles needing expert attention from August 2009","Articles needing expert attention with no reason or talk parameter","Articles with example pseudocode","Computer science articles needing expert attention","Sorting algorithms"],"tag_line":"Merge algorithms are a family of algorithms that run sequentially over multiple sorted lists, typically producing more sorted lists as output."}}
,{"_index":"throwtable","_type":"algorithm","_id":"spaghetti-sort","_score":0,"_source":{"description":"Spaghetti sort is a linear-time, analog algorithm for sorting a sequence of items, by Alexander Dewdney in his column, Scientific American. This algorithm sorts a sequence of items requiring O(n) stack space in a stable manner. It requires a parallel processor.","name":"Spaghetti sort","categories":["Accuracy disputes from July 2013","All Wikipedia articles needing clarification","All accuracy disputes","All articles with unsourced statements","Articles with unsourced statements from April 2015","Sorting algorithms","Wikipedia articles needing clarification from July 2013"],"tag_line":"Spaghetti sort is a linear-time, analog algorithm for sorting a sequence of items, by Alexander Dewdney in his column, Scientific American."}}
,{"_index":"throwtable","_type":"algorithm","_id":"polyphase-merge-sort","_score":0,"_source":{"description":"A polyphase merge sort is an algorithm which decreases the number of runs at every iteration of the main loop by merging runs into larger runs. It is used for external sorting.","name":"Polyphase merge sort","categories":["Comparison sorts","Online sorts","Sorting algorithms"],"tag_line":"A polyphase merge sort is an algorithm which decreases the number of runs at every iteration of the main loop by merging runs into larger runs."}}
,{"_index":"throwtable","_type":"algorithm","_id":"embedded-zerotrees-of-wavelet-transforms","_score":0,"_source":{"description":"Embedded Zerotrees of Wavelet transforms (EZW) is a lossy image compression algorithm. At low bit rates, i.e. high compression ratios, most of the coefficients produced by a subband transform (such as the wavelet transform) will be zero, or very close to zero. This occurs because \"real world\" images tend to contain mostly low frequency information (highly correlated). However where high frequency information does occur (such as edges in the image) this is particularly important in terms of human perception of the image quality, and thus must be represented accurately in any high quality coding scheme.\nBy considering the transformed coefficients as a tree (or trees) with the lowest frequency coefficients at the root node and with the children of each tree node being the spatially related coefficients in the next higher frequency subband, there is a high probability that one or more subtrees will consist entirely of coefficients which are zero or nearly zero, such subtrees are called zerotrees. Due to this, we use the terms node and coefficient interchangeably, and when we refer to the children of a coefficient, we mean the child coefficients of the node in the tree where that coefficient is located. We use children to refer to directly connected nodes lower in the tree and descendants to refer to all nodes which are below a particular node in the tree, even if not directly connected.\nIn zerotree based image compression scheme such as EZW and SPIHT, the intent is to use the statistical properties of the trees in order to efficiently code the locations of the significant coefficients. Since most of the coefficients will be zero or close to zero, the spatial locations of the significant coefficients make up a large portion of the total size of a typical compressed image. A coefficient (likewise a tree) is considered significant if its magnitude (or magnitudes of a node and all its descendants in the case of a tree) is above a particular threshold. By starting with a threshold which is close to the maximum coefficient magnitudes and iteratively decreasing the threshold, it is possible to create a compressed representation of an image which progressively adds finer detail. Due to the structure of the trees, it is very likely that if a coefficient in a particular frequency band is insignificant, then all its descendants (the spatially related higher frequency band coefficients) will also be insignificant.\nEZW uses four symbols to represent (a) a zerotree root, (b) an isolated zero (a coefficient which is insignificant, but which has significant descendants), (c) a significant positive coefficient and (d) a significant negative coefficient. The symbols may be thus represented by two binary bits. The compression algorithm consists of a number of iterations through a dominant pass and a subordinate pass, the threshold is updated (reduced by a factor of two) after each iteration. The dominant pass encodes the significance of the coefficients which have not yet been found significant in earlier iterations, by scanning the trees and emitting one of the four symbols. The children of a coefficient are only scanned if the coefficient was found to be significant, or if the coefficient was an isolated zero. The subordinate pass emits one bit (the most significant bit of each coefficient not so far emitted) for each coefficient which has been found significant in the previous significance passes. The subordinate pass is therefore similar to bit-plane coding.\nThere are several important features to note. Firstly, it is possible to stop the compression algorithm at any time and obtain an approximation of the original image, the greater the number of bits received, the better the image. Secondly, due to the way in which the compression algorithm is structured as a series of decisions, the same algorithm can be run at the decoder to reconstruct the coefficients, but with the decisions being taken according to the incoming bit stream. In practical implementations, it would be usual to use an entropy code such as arithmetic code to further improve the performance of the dominant pass. Bits from the subordinate pass are usually random enough that entropy coding provides no further coding gain.\nThe coding performance of EZW has since been exceeded by SPIHT and its many derivatives.","name":"Embedded Zerotrees of Wavelet transforms","categories":["Commons category with local link same as on Wikidata","Image compression","Lossless compression algorithms","Trees (data structures)","Wavelets"],"tag_line":"Embedded Zerotrees of Wavelet transforms (EZW) is a lossy image compression algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"prefix-code","_score":0,"_source":{"description":"A prefix code is a type of code system (typically a variable-length code) distinguished by its possession of the \"prefix property\", which requires that there is no code word in the system that is a prefix (initial segment) of any other code word in the system. For example, a code with code words {9, 55} has the prefix property; a code consisting of {9, 5, 59, 55} does not, because \"5\" is a prefix of \"59\" and also of \"55\". A prefix code is a uniquely decodable code: a receiver can identify each word without requiring a special marker between words.\nPrefix codes are also known as prefix-free codes, prefix condition codes and instantaneous codes. Although Huffman coding is just one of many algorithms for deriving prefix codes, prefix codes are also widely referred to as \"Huffman codes\", even when the code was not produced by a Huffman algorithm. The term comma-free code is sometimes also applied as a synonym for prefix-free codes but in most mathematical books and articles (e.g.) a comma-free code is used to mean a self-synchronizing code, a subclass of prefix codes.\nUsing prefix codes, a message can be transmitted as a sequence of concatenated code words, without any out-of-band markers or (alternatively) special markers between words to frame the words in the message. The recipient can decode the message unambiguously, by repeatedly finding and removing sequences that form valid code words. This is not generally possible with codes that lack the prefix property, for example {0, 1, 10, 11}: a receiver reading a \"1\" at the start of a code word would not know whether that was the complete code word \"1\", or merely the prefix of the code word \"10\" or \"11\"; so the string \"10\" could be interpreted either as a single codeword or as the concatenation of the words \"1\" then \"0\".\nThe variable-length Huffman codes, country calling codes, the country and publisher parts of ISBNs, the Secondary Synchronization Codes used in the UMTS W-CDMA 3G Wireless Standard, and the instruction sets (machine language) of most computer microarchitectures are prefix codes.\nPrefix codes are not error-correcting codes. In practice, a message might first be compressed with a prefix code, and then encoded again with channel coding (including error correction) before transmission.\nKraft's inequality characterizes the sets of code word lengths that are possible in a uniquely decodable code.","name":"Prefix code","categories":["Coding theory","Data compression","Lossless compression algorithms","Prefixes","Wikipedia articles incorporating text from the Federal Standard 1037C"],"tag_line":"A prefix code is a type of code system (typically a variable-length code) distinguished by its possession of the \"prefix property\", which requires that there is no code word in the system that is a prefix (initial segment) of any other code word in the system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"microsoft-point-to-point-compression","_score":0,"_source":{"description":"Microsoft Point-to-Point Compression (MPPC; described in RFC 2118) is a streaming data compression algorithm based on an implementation of Lempel–Ziv using a sliding window buffer. According to Hifn's IP statement, MPPC is patent-encumbered.\nWhere V.44 or V.42bis operate at layer 1 on the OSI model, MPPC operates on layer 2, giving it a significant advantage in terms of computing resources available to it. The dialup modem's in-built compression (V.44 or V.42bis) can only occur after the data has been serially transmitted to the modem, typically at a maximum rate of 115,200 bit/s. MPPC, as it is controlled by the operating system, can receive as much data as it wishes to compress, before forwarding it on to the modem.\nThe modem's hardware must not delay data too much, while waiting for more to compress in one packet, otherwise an unacceptable latency level will result. It also cannot afford to, as this would require both sizable computing resources (on the scale of a modem) as well as significant buffer RAM. Software compression such as MPPC is free to use the host computer's resources which will typically include a CPU of several hundred Megahertz and several hundred Megabytes of RAM - greater computing power than that of the modem by several orders of magnitude. This allows it to keep a much larger buffer to work on at any one time, and it processes through a given amount of data much faster.\nThe end result is that where V.44 may achieve a maximum of 4:1 compression (230 kbit/s) but is usually limited to 115.2 kbit/s, MPPC is capable of a maximum of 8:1 compression (460 kbit/s). MPPC also, given the far greater computing power at its disposal, is more effective on data than V.44 and achieves higher compression ratios when 8:1 isn't achievable.\n^ http://www.ietf.org/ietf/IPR/hifn-ipr-draft-friend-tls-lzs-compression.txt","name":"Microsoft Point-to-Point Compression","categories":["All articles lacking sources","All articles needing expert attention","Articles lacking sources from August 2009","Articles needing expert attention from February 2009","Articles needing expert attention with no reason or talk parameter","Computing articles needing expert attention","Lossless compression algorithms","Microsoft initiatives","Modems"],"tag_line":"Microsoft Point-to-Point Compression (MPPC; described in RFC 2118) is a streaming data compression algorithm based on an implementation of Lempel–Ziv using a sliding window buffer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lossless-compression","_score":0,"_source":{"description":"Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast, lossy compression permits reconstruction only of an approximation of the original data, though this usually improves compression rates (and therefore reduces file sizes).\nLossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by the LAME MP3 encoder and other lossy audio encoders).\nLossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data could be deleterious. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods. Lossless audio formats are most often used for archiving or production purposes, while smaller lossy audio files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary.","alt_names":["Lossless compression benchmarks"],"name":"Lossless compression","categories":["All articles with unsourced statements","Articles with unsourced statements from August 2011","Articles with unsourced statements from December 2007","Articles with unsourced statements from November 2012","Articles with unsourced statements from November 2015","Data compression","Lossless compression algorithms"],"tag_line":"Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data."}}
,{"_index":"throwtable","_type":"algorithm","_id":"x-+-y-sorting","_score":0,"_source":{"description":"In computer science, X + Y sorting is the problem of sorting pairs of numbers by their sum. Given two finite sets X and Y, the problem is to order all pairs (x, y) in the Cartesian product X × Y by the key x + y. The problem is attributed to Elwyn Berlekamp.\nThis problem can be solved using a straightforward comparison sort on the Cartesian product, taking time O(nm log(nm)) for sets of sizes n and m. When it is assumed that m = n, the complexity is O(n2 log n2) = O(n2 log n), which is also the best known bound on the problem, but whether X + Y sorting can be done strictly faster than sorting n⋅m arbitrary numbers is an open problem. The number of required comparisons is certainly lower than for ordinary comparison sorting: Fredman showed, in 1976, that X + Y sorting can be done using only O(n2) comparisons, though he did not show an algorithm. The first actual algorithm that achieves this number of comparisons and O(n2 log n) total complexity was only published sixteen years later.\nOn a RAM machine with word size w and integer inputs 0 ≤ {x, y} < n = 2w, the problem can be solved in O(n log n) operations by means of the fast Fourier transform.\nSkiena recounts a practical application in transit fare minimisation, an instance of the shortest path problem: given fares x and y for trips from departure A to some intermediate destination B and from B to final destination C, determine the least expensive combined trip from A to C.\n\n","name":"X + Y sorting","categories":["Sorting algorithms","Unsolved problems in computer science"],"tag_line":"In computer science, X + Y sorting is the problem of sorting pairs of numbers by their sum."}}
,{"_index":"throwtable","_type":"algorithm","_id":"power-quality-compression-algorithm","_score":0,"_source":{"description":"A power quality compression algorithm is an algorithm used in power quality analysis. To provide high quality electric power service, it is essential to monitor the quality of the electric signals also termed as power quality (PQ) at different locations along an electrical power network. Electrical utilities carefully monitor waveforms and currents at various network locations constantly, to understand what lead up to any unforeseen events such as a power outage and blackouts. This is particularly critical at sites where the environment and public safety are at risk (institutions such as hospitals, sewage treatment plants, mines, etc.).","name":"Power quality compression algorithm","categories":["All articles needing cleanup","All articles to be merged","All articles with topics of unclear notability","Articles needing cleanup from November 2010","Articles to be merged from March 2013","Articles with topics of unclear notability from December 2012","Cleanup tagged articles without a reason field from November 2010","Compression algorithms","Power engineering","Wikipedia pages needing cleanup from November 2010"],"tag_line":"A power quality compression algorithm is an algorithm used in power quality analysis."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bzip2","_score":0,"_source":{"description":"bzip2 is a free and open-source file compression program that uses the Burrows–Wheeler algorithm. It only compresses single files and is not a file archiver. It is developed and maintained by Julian Seward. Seward made the first public release of bzip2, version 0.15, in July 1996. The compressor's stability and popularity grew over the next several years, and Seward released version 1.0 in late 2000.","name":"Bzip2","categories":["1996 software","All articles containing potentially dated statements","All articles with specifically marked weasel-worded phrases","All articles with unsourced statements","Archive formats","Articles containing potentially dated statements from May 2010","Articles with specifically marked weasel-worded phrases from February 2014","Articles with unsourced statements from February 2014","Cross-platform software","Free data compression software","Lossless compression algorithms","Unix archivers and compression-related utilities","Use dmy dates from August 2012"],"tag_line":"bzip2 is a free and open-source file compression program that uses the Burrows–Wheeler algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lempel–ziv–stac","_score":0,"_source":{"description":"Lempel–Ziv–Stac (LZS, or Stac compression) is a lossless data compression algorithm that uses a combination of the LZ77 sliding-window compression algorithm and fixed Huffman coding. It was originally developed by Stac Electronics for tape compression, and subsequently adapted for hard disk compression and sold as the Stacker disk compression software. It was later specified as a compression algorithm for various network protocols. LZS is specified in the Cisco IOS stack.","name":"Lempel–Ziv–Stac","categories":["Lossless compression algorithms"],"tag_line":"Lempel–Ziv–Stac (LZS, or Stac compression) is a lossless data compression algorithm that uses a combination of the LZ77 sliding-window compression algorithm and fixed Huffman coding."}}
,{"_index":"throwtable","_type":"algorithm","_id":"toom–cook-multiplication","_score":0,"_source":{"description":"Toom–Cook, sometimes known as Toom-3, named after Andrei Toom, who introduced the new algorithm with its low complexity, and Stephen Cook, who cleaned the description of it, is a multiplication algorithm, a method of multiplying two large integers.\nGiven two large integers, a and b, Toom–Cook splits up a and b into k smaller parts each of length l, and performs operations on the parts. As k grows, one may combine many of the multiplication sub-operations, thus reducing the overall complexity of the algorithm. The multiplication sub-operations can then be computed recursively using Toom–Cook multiplication again, and so on. Although the terms \"Toom-3\" and \"Toom–Cook\" are sometimes incorrectly used interchangeably, Toom-3 is only a single instance of the Toom–Cook algorithm, where k = 3.\nToom-3 reduces 9 multiplications to 5, and runs in Θ(nlog(5)/log(3)), about Θ(n1.465). In general, Toom-k runs in Θ(c(k) ne), where e = log(2k − 1) / log(k), ne is the time spent on sub-multiplications, and c is the time spent on additions and multiplication by small constants. The Karatsuba algorithm is a special case of Toom–Cook, where the number is split into two smaller ones. It reduces 4 multiplications to 3 and so operates at Θ(nlog(3)/log(2)), which is about Θ(n1.585). Ordinary long multiplication is equivalent to Toom-1, with complexity Θ(n2).\nAlthough the exponent e can be set arbitrarily close to 1 by increasing k, the function c unfortunately grows very rapidly. The growth rate for mixed-level Toom-Cook schemes was still an open research problem in 2005. An implementation described by Donald Knuth achieves the time complexity Θ(n 2√(2 log n) log n).\nDue to its overhead, Toom–Cook is slower than long multiplication with small numbers, and it is therefore typically used for intermediate-size multiplications, before the asymptotically faster Schönhage–Strassen algorithm (with complexity Θ(n log n log log n)) becomes practical.\nToom first described this algorithm in 1963, and Cook published an improved (asymptotically equivalent) algorithm in his PhD thesis in 1966.","name":"Toom–Cook multiplication","categories":["Computer arithmetic algorithms","Multiplication"],"tag_line":"Toom–Cook, sometimes known as Toom-3, named after Andrei Toom, who introduced the new algorithm with its low complexity, and Stephen Cook, who cleaned the description of it, is a multiplication algorithm, a method of multiplying two large integers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"color-cell-compression","_score":0,"_source":{"description":"Color Cell Compression is an early lossy image compression algorithm first described by Campbell et al. in 1986. It is a variant of Block Truncation Coding. The encoding process works on small blocks of pixels. For each block, it first partitions the pixels in that block into two sets based on their luminance values, then generates representative colour values for each of these sets, and a bitmap that specifies which pixels belong to which set. The two colour values and the bitmap for each block are then output directly without any further quantization or entropy coding.\nThe decoding process is simple; each pixel of an output block is generated by choosing one of the two representative colours for that block, based on that block's bitmap.\nIn spite of its very simple mechanism, the algorithm yields surprisingly good results on photographic images, and it has the advantage of being very fast to decode with limited hardware. Although far surpassed in compression ratio by later block-transform coding methods such as JPEG, it had the advantage of very simple decompression and fast random access into the compressed image, and it can be regarded as a forerunner of modern texture compression algorithms.","name":"Color Cell Compression","categories":["All stub articles","Computer graphics stubs","Lossy compression algorithms"],"tag_line":"Color Cell Compression is an early lossy image compression algorithm first described by Campbell et al."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gauss–legendre-algorithm","_score":0,"_source":{"description":"The Gauss–Legendre algorithm is an algorithm to compute the digits of π. It is notable for being rapidly convergent, with only 25 iterations producing 45 million correct digits of π. However, the drawback is that it is memory intensive and it is therefore sometimes not used over Machin-like formulas.\nThe method is based on the individual work of Carl Friedrich Gauss (1777–1855) and Adrien-Marie Legendre (1752–1833) combined with modern algorithms for multiplication and square roots. It repeatedly replaces two numbers by their arithmetic and geometric mean, in order to approximate their arithmetic-geometric mean.\nThe version presented below is also known as the Gauss–Euler, Brent–Salamin (or Salamin–Brent) algorithm; it was independently discovered in 1975 by Richard Brent and Eugene Salamin. It was used to compute the first 206,158,430,000 decimal digits of π on September 18 to 20, 1999, and the results were checked with Borwein's algorithm.\n\n","name":"Gauss–Legendre algorithm","categories":["Pi algorithms"],"tag_line":"The Gauss–Legendre algorithm is an algorithm to compute the digits of π."}}
,{"_index":"throwtable","_type":"algorithm","_id":"concurrent-algorithm","_score":0,"_source":{"description":"In computer science, a concurrent algorithm is one that can be executed concurrently. Most standard computer algorithms are sequential algorithms, and assume that the algorithm is run from start to finish without any other processes executing. These often do not behave correctly when run concurrently, as demonstrated at right, and are often nondeterministic, as the actual sequence of computations is determined by the external scheduler. Concurrency often adds significant complexity to an algorithm, requiring concurrency control such as mutual exclusion to avoid problems such as race conditions.\nMany parallel algorithms are run concurrently, particularly distributed algorithms, though these are distinct concepts in general.","name":"Concurrent algorithm","categories":["Algorithms and data structures stubs","All articles lacking sources","All stub articles","Articles lacking sources from February 2014","Computer science stubs","Concurrent algorithms"],"tag_line":"In computer science, a concurrent algorithm is one that can be executed concurrently."}}
,{"_index":"throwtable","_type":"algorithm","_id":"computational-complexity-of-mathematical-operations","_score":0,"_source":{"description":"The following tables list the running time of various algorithms for common mathematical operations.\nHere, complexity refers to the time complexity of performing computations on a multitape Turing machine. See big O notation for an explanation of the notation used.\nNote: Due to the variety of multiplication algorithms, M(n) below stands in for the complexity of the chosen multiplication algorithm.","name":"Computational complexity of mathematical operations","categories":["All articles needing additional references","Articles needing additional references from April 2015","Computational complexity theory","Computer arithmetic algorithms","Mathematics-related lists","Number theoretic algorithms","Unsolved problems in computer science"],"tag_line":"The following tables list the running time of various algorithms for common mathematical operations."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gwic","_score":0,"_source":{"description":"GWIC (GNU Wavelet Image Codec) is a lossy image compression algorithm.","name":"GWIC","categories":["All articles lacking sources","All articles with dead external links","All articles with topics of unclear notability","All orphaned articles","All stub articles","Articles lacking sources from February 2014","Articles with dead external links from February 2014","Articles with topics of unclear notability from November 2015","Graphics software stubs","Lossy compression algorithms","Orphaned articles from February 2009"],"tag_line":"GWIC (GNU Wavelet Image Codec) is a lossy image compression algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"parallel-tebd","_score":0,"_source":{"description":"The parallel-TEBD is a version of the TEBD algorithm adapted to run on multiple hosts. The task of parallelizing TEBD could be achieved in various ways.\nAs a first option, one could use the OpenMP API (this would probably be the simplest way to do it), using preprocessor directives to decide which portion of the code should be parallelized. The drawback of this is that one is confined to Symmetric multiprocessing (SMP) architectures and the user has no control on how the code is parallelized. An Intel extension of OpenMP, called Cluster OpenMP [2], is a socket-based implementation of OpenMP which can make use of a whole cluster of SMP machines; this spares the user of explicitly writing messaging code while giving access to multiple hosts via a distributed shared-memory system. The OpenMP paradigm (hence its extension Cluster OpenMP as well) allows the user a straightforward parallelization of serial code by embedding a set of directives in it.\nThe second option is using the Message Passing Interface (MPI) API. MPI can treat each core of the multi-core machines as separate execution host, so a cluster of, let's say, 10 compute nodes with dual-core processors will appear as 20 compute nodes, on which the MPI application can be distributed. MPI offers the user more control over the way the program is parallelized. The drawback of MPI is that is not very easy to implement and the programmer has to have a certain understanding of parallel simulation systems.\nFor the determined programmer the third option would probably be the most appropriate: to write ones own routines, using a combination of threads and TCP/IP sockets to complete the task. The threads are necessary in order to make the socket-based communication between the programs non-blocking (the communication between programs has to take place in threads, so that the main thread doesn't have to wait for the communication to end and can execute other parts of the code). This option offers the programmer complete control over the code and eliminates any overhead which might come from the use of the Cluster OpenMP or MPI libraries.\nThis article introduces the conceptual basis of the implementation, using MPI-based pseudo-code for exemplification, while not restricting itself to MPI - the same basic schema could be implemented with the use of home-grown messaging routines.","name":"Parallel-TEBD","categories":["Computational physics","Distributed algorithms"],"tag_line":"The parallel-TEBD is a version of the TEBD algorithm adapted to run on multiple hosts."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lamport-timestamps","_score":0,"_source":{"description":"The algorithm of Lamport timestamps is a simple algorithm used to determine the order of events in a distributed computer system. As different nodes or processes will typically not be perfectly synchronized, this algorithm is used to provide a partial ordering of events with minimal overhead, and conceptually provide a starting point for the more advanced vector clock method. They are named after their creator, Leslie Lamport.\nDistributed algorithms such as resource synchronization often depend on some method of ordering events to function. For example, consider a system with two processes and a disk. The processes send messages to each other, and also send messages to the disk requesting access. The disk grants access in the order the messages were sent. Now, imagine process 1 sends a message to the disk asking for access to write, and then sends a message to process 2 asking it to read. Process 2 receives the message, and as a result sends its own message to the disk. Now, due to some timing delay, the disk receives both messages at the same time: how does it determine which message happened-before the other? ( happens-before  if one can get from  to  by a sequence of moves of two types: moving forward while remaining in the same process, and following a message from its sending to its reception.) A logical clock algorithm provides a mechanism to determine facts about the order of such events.\nLamport invented a simple mechanism by which the happened-before ordering can be captured numerically. A Lamport logical clock is an incrementing software counter maintained in each process.\nIt follows some simple rules:\nA process increments its counter before each event in that process;\nWhen a process sends a message, it includes its counter value with the message;\nOn receiving a message, the receiver process sets its counter to be the maximum of the message counter and its own counter (already incremented due to rule 1), before it considers the message received.\nConceptually, this logical clock can be thought of as a clock that only has meaning in relation to messages moving between processes. When a process receives a message, it resynchronizes its logical clock with that sender.\n\n","name":"Lamport timestamps","categories":["Distributed algorithms"],"tag_line":"The algorithm of Lamport timestamps is a simple algorithm used to determine the order of events in a distributed computer system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"verification-based-message-passing-algorithms-in-compressed-sensing","_score":0,"_source":{"description":"Verification-based message-passing algorithms (VB-MPAs) in compressed sensing (CS), a branch of digital signal processing that deals with measuring sparse signals, are some methods to efficiently solve the recovery problem in compressed sensing. One of the main goal in compressed sensing is the recovery process. Generally speaking, recovery process in compressed sensing is a method by which the original signal is estimated using the knowledge of the compressed signal and the measurement matrix. Mathematically, the recovery process in Compressed Sensing is finding the sparsest possible solution of an under-determined system of linear equations. Based on the nature of the measurement matrix one can employ different reconstruction methods. If the measurement matrix is also sparse, one efficient way is to use Message Passing Algorithms for signal recovery. Although there are message passing approaches that deals with dense matrices, the nature of those algorithms are to some extent different from the algorithms working on sparse matrices.","name":"Verification-based message-passing algorithms in compressed sensing","categories":["All articles covered by WikiProject Wikify","All articles lacking in-text citations","All articles needing references cleanup","All orphaned articles","Articles covered by WikiProject Wikify from February 2015","Articles lacking in-text citations from February 2015","Digital signal processing","Distributed algorithms","Inter-process communication","Orphaned articles from January 2015","Pages using web citations with no URL","Wikipedia references cleanup from February 2015"],"tag_line":"Verification-based message-passing algorithms (VB-MPAs) in compressed sensing (CS), a branch of digital signal processing that deals with measuring sparse signals, are some methods to efficiently solve the recovery problem in compressed sensing."}}
,{"_index":"throwtable","_type":"algorithm","_id":"timestamp-based-concurrency-control","_score":0,"_source":{"description":"In computer science, a timestamp-based concurrency control algorithm is a non-lock concurrency control method. It is used in some databases to safely handle transactions, using timestamps.","name":"Timestamp-based concurrency control","categories":["Accuracy disputes from April 2012","All accuracy disputes","All articles lacking sources","All articles needing cleanup","Articles lacking sources from June 2007","Articles needing cleanup from April 2012","Cleanup tagged articles without a reason field from April 2012","Concurrency control","Concurrency control algorithms","Transaction processing","Wikipedia pages needing cleanup from April 2012"],"tag_line":"In computer science, a timestamp-based concurrency control algorithm is a non-lock concurrency control method."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ostrich-algorithm","_score":0,"_source":{"description":"In computer science, the ostrich algorithm is a strategy of ignoring potential problems on the basis that they may be exceedingly rare. It is named for the ostrich effect which is defined as \"to stick one's head in the sand and pretend there is no problem.\" It is used when it is more cost-effective to allow the problem to occur than to attempt its prevention.","name":"Ostrich algorithm","categories":["Concurrent algorithms"],"tag_line":"In computer science, the ostrich algorithm is a strategy of ignoring potential problems on the basis that they may be exceedingly rare."}}
,{"_index":"throwtable","_type":"algorithm","_id":"berkeley-algorithm","_score":0,"_source":{"description":"The Berkeley algorithm is a method of clock synchronisation in distributed computing which assumes no machine has an accurate time source. It was developed by Gusella and Zatti at the University of California, Berkeley in 1989  and like Cristian's algorithm is intended for use within intranets.","name":"Berkeley algorithm","categories":["Distributed algorithms"],"tag_line":"The Berkeley algorithm is a method of clock synchronisation in distributed computing which assumes no machine has an accurate time source."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ricart–agrawala-algorithm","_score":0,"_source":{"description":"The Ricart-Agrawala Algorithm is an algorithm for mutual exclusion on a distributed system. This algorithm is an extension and optimization of Lamport's Distributed Mutual Exclusion Algorithm, by removing the need for  messages. It was developed by Glenn Ricart and Ashok Agrawala.","name":"Ricart–Agrawala algorithm","categories":["All articles lacking sources","Articles lacking sources from December 2009","Distributed algorithms"],"tag_line":"The Ricart-Agrawala Algorithm is an algorithm for mutual exclusion on a distributed system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"maekawa's-algorithm","_score":0,"_source":{"description":"Maekawa's algorithm is an algorithm for mutual exclusion on a distributed system. The basis of this algorithm is a quorum like approach where any one site needs only to seek permissions from a subset of other sites.","name":"Maekawa's algorithm","categories":["All articles lacking sources","Articles lacking sources from December 2009","Concurrency control algorithms"],"tag_line":"Maekawa's algorithm is an algorithm for mutual exclusion on a distributed system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"chang-and-roberts-algorithm","_score":0,"_source":{"description":"The Chang and Roberts algorithm is a ring-based coordinator election algorithm, employed in distributed computing.","name":"Chang and Roberts algorithm","categories":["Distributed algorithms"],"tag_line":"The Chang and Roberts algorithm is a ring-based coordinator election algorithm, employed in distributed computing."}}
,{"_index":"throwtable","_type":"algorithm","_id":"montgomery-modular-multiplication","_score":0,"_source":{"description":"In modular arithmetic computation, Montgomery modular multiplication, more commonly referred to as Montgomery multiplication, is a method for performing fast modular multiplication, introduced in 1985 by the American mathematician Peter L. Montgomery.  \nGiven two integers a and b, the classical modular multiplication algorithm computes ab mod N. Montgomery multiplication works by transforming a and b into a special representation known as Montgomery form. For a modulus N, the Montgomery form of a is defined to be aR mod N for some constant R depending only on N and the underlying computer architecture. If aR mod N and bR mod N are the Montgomery forms of a and b, then their Montgomery product is abR mod N. Montgomery multiplication is a fast algorithm to compute the Montgomery product. Transforming the result out of Montgomery form yields the classical modular product ab mod N.\nBecause of the overhead involved in converting a and b into Montgomery form, computing a single product by Montgomery multiplication is slower than computing the product in the integers and performing a modular reduction by division or Barrett reduction. However, when many products are required, as in modular exponentiation, the conversion to Montgomery form becomes a negligible fraction of the time of the computation, and performing the computation by Montgomery multiplication is faster than the available alternatives. Many important cryptosystems such as RSA and Diffie–Hellman key exchange are based on arithmetic operations modulo a large number, and for these cryptosystems, the increased speed afforded by Montgomery multiplication can be important in practice.","name":"Montgomery modular multiplication","categories":["Computer arithmetic","Cryptographic algorithms","Modular arithmetic"],"tag_line":"In modular arithmetic computation, Montgomery modular multiplication, more commonly referred to as Montgomery multiplication, is a method for performing fast modular multiplication, introduced in 1985 by the American mathematician Peter L. Montgomery."}}
,{"_index":"throwtable","_type":"algorithm","_id":"primality-test","_score":0,"_source":{"description":"A primality test is an algorithm for determining whether an input number is prime. Amongst other fields of mathematics, it is used for cryptography. Unlike integer factorization, primality tests do not generally give prime factors, only stating whether the input number is prime or not. Factorization is thought to be a computationally difficult problem, whereas primality testing is comparatively easy (its running time is polynomial in the size of the input). Some primality tests prove that a number is prime, while others like Miller–Rabin prove that a number is composite. Therefore, the latter might be called compositeness tests instead of primality tests.","name":"Primality test","categories":["All articles needing additional references","All articles with specifically marked weasel-worded phrases","Articles needing additional references from August 2013","Articles with specifically marked weasel-worded phrases from April 2010","Asymmetric-key algorithms","Pages containing cite templates with deprecated parameters","Primality tests"],"tag_line":"A primality test is an algorithm for determining whether an input number is prime."}}
,{"_index":"throwtable","_type":"algorithm","_id":"coppersmith-method","_score":0,"_source":{"description":"The Coppersmith method, proposed by Don Coppersmith, is a method to find small integer zeroes of univariate or bivariate polynomials modulo a given integer.\nThe method uses the Lenstra–Lenstra–Lovász lattice basis reduction algorithm (LLL) to find a polynomial that has the same zeroes as the target polynomial but smaller coefficients.\nIn cryptography, the Coppersmith method is mainly used in attacks on RSA when parts of the secret key are known and forms a base for Coppersmith's Attack.","name":"Coppersmith method","categories":["All articles needing additional references","All articles needing cleanup","All articles needing expert attention","Articles needing additional references from January 2010","Articles needing cleanup from January 2010","Articles needing expert attention from January 2010","Articles needing expert attention with no reason or talk parameter","Asymmetric-key algorithms","Cleanup tagged articles without a reason field from January 2010","Mathematics articles needing expert attention","Wikipedia pages needing cleanup from January 2010"],"tag_line":"The Coppersmith method, proposed by Don Coppersmith, is a method to find small integer zeroes of univariate or bivariate polynomials modulo a given integer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"master-password","_score":0,"_source":{"description":"Master Password is an algorithm designed by Maarten Billemont for creating unique passwords in a reproducible manner. It differs from traditional password managers in that the passwords are not stored on disk or in the cloud, but are recreated every time by using information entered by the user; most importantly, their full name, a master password, and a unique name for the service the password is intended for.\nBy not storing the passwords anywhere, this approach tries to make it harder for attackers to steal or intercept them. It also removes the need for synchronization between devices, and backups of potential password databases.","name":"Master Password","categories":["Cryptographic algorithms","Free security software","Official website not in Wikidata"],"tag_line":"Master Password is an algorithm designed by Maarten Billemont for creating unique passwords in a reproducible manner."}}
,{"_index":"throwtable","_type":"algorithm","_id":"key-wrap","_score":0,"_source":{"description":"Key Wrap constructions are a class of symmetric encryption algorithms designed to encapsulate (encrypt) cryptographic key material. The Key Wrap algorithms are intended for applications such as (a) protecting keys while in untrusted storage, or (b) transmitting keys over untrusted communications networks. The constructions are typically built from standard primitives such as block ciphers and cryptographic hash functions.\nKey Wrap may be considered as a form of key encapsulation algorithm, although it should not be confused with the more commonly known asymmetric (public-key) key encapsulation algorithms (e.g., PSEC-KEM). Key Wrap algorithms can be used in a similar application: to securely transport a session key by encrypting it under a long-term encryption key.","name":"Key Wrap","categories":["Cryptographic algorithms"],"tag_line":"Key Wrap constructions are a class of symmetric encryption algorithms designed to encapsulate (encrypt) cryptographic key material."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hmac-based-one-time-password-algorithm","_score":0,"_source":{"description":"HOTP is an HMAC-based one-time password (OTP) algorithm. It is a cornerstone of Initiative For Open Authentication (OATH).\nHOTP was published as an informational IETF RFC 4226 in December 2005, documenting the algorithm along with a Java implementation. Since then, the algorithm has been adopted by many companies worldwide (see below). The HOTP algorithm is a freely available open standard.","name":"HMAC-based One-time Password Algorithm","categories":["All accuracy disputes","All articles with unsourced statements","Articles with disputed statements from August 2015","Articles with unsourced statements from August 2015","Computer access control protocols","Cryptographic algorithms","Internet protocols"],"tag_line":"HOTP is an HMAC-based one-time password (OTP) algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rc-algorithm","_score":0,"_source":{"description":"The RC algorithms are a set of symmetric-key encryption algorithms invented by Ron Rivest. The \"RC\" may stand for either Rivest's cipher or, more informally, Ron's code. Despite the similarity in their names, the algorithms are for the most part unrelated. There have been six RC algorithms so far:\nRC1 was never published.\nRC2 was a 64-bit block cipher developed in 1987.\nRC3 was broken before ever being used.\nRC4 is the world's most widely used stream cipher.\nRC5 is a 32/64/128-bit block cipher developed in 1994.\nRC6, a 128-bit block cipher based heavily on RC5, was an AES finalist developed in 1997.","name":"RC algorithm","categories":["Cryptographic algorithms"],"tag_line":"The RC algorithms are a set of symmetric-key encryption algorithms invented by Ron Rivest."}}
,{"_index":"throwtable","_type":"algorithm","_id":"barrett-reduction","_score":0,"_source":{"description":"In modular arithmetic, Barrett reduction is a reduction algorithm introduced in 1986 by P.D. Barrett. A naive way of computing\n\nwould be to use a fast division algorithm. Barrett reduction is an algorithm designed to optimize this operation assuming  is constant, and , replacing divisions by multiplications.","name":"Barrett reduction","categories":["Computer arithmetic","Cryptographic algorithms","Modular arithmetic","Wikipedia articles needing clarification from January 2014"],"tag_line":"In modular arithmetic, Barrett reduction is a reduction algorithm introduced in 1986 by P.D."}}
,{"_index":"throwtable","_type":"algorithm","_id":"three-pass-protocol","_score":0,"_source":{"description":"In cryptography, the three-pass protocol for sending messages is a framework which allows one party to securely send a message to a second party without the need to exchange or distribute encryption keys. This message protocol should not be confused with various other algorithms which use 3 passes for authentication.\nIt is called the three-pass protocol because the sender and the receiver exchange three encrypted messages. The first three-pass protocol was developed by Adi Shamir circa 1980, and is described in more detail in a later section. The basic concept of the Three-Pass Protocol is that each party has a private encryption key and a private decryption key. The two parties use their keys independently, first to encrypt the message, and then to decrypt the message.\nThe protocol uses an encryption function E and a decryption function D. The encryption function uses an encryption key e to change a plaintext message m into an encrypted message, or ciphertext, E(e,m). Corresponding to each encryption key e there is a decryption key d which allows the message to be recovered using the decryption function, D(d,E(e,m))=m. Sometimes the encryption function and decryption function are the same.\nIn order for the encryption function and decryption function to be suitable for the Three-Pass Protocol they must have the property that for any message m, any encryption key e with corresponding decryption key d and any independent encryption key k,  D(d,E(k,E(e,m))) = E(k,m). In other words, it must be possible to remove the first encryption with the key e even though a second encryption with the key k has been performed. This will always be possible with a commutative encryption. A commutative encryption is an encryption that is order-independent, i.e. it satisfies E(a,E(b,m))=E(b,E(a,m)) for all encryption keys a and b and all messages m. Commutative encryptions satisfy D(d,E(k,E(e,m))) = D(d,E(e,E(k,m))) = E(k,m).\nThe Three-Pass Protocol works as follows:\nThe sender chooses a private encryption key s and a corresponding decryption key t. The sender encrypts the message m with the key s and sends the encrypted message E(s,m) to the receiver.\nThe receiver chooses a private encryption key r and a corresponding decryption key q and super-encrypts the first message E(s,m) with the key r and sends the doubly encrypted message E(r,E(s,m)) back to the sender.\nThe sender decrypts the second message with the key t. Because of the commutativity property described above D(t,E(r,E(s,m)))=E(r,m) which is the message encrypted with only the receiver's private key. The sender sends this to the receiver.\nThe receiver can now decrypt the message using the key q, namely D(q,E(r,m))=m the original message.\nNotice that all of the operations involving the sender's private keys s and t are performed by the sender, and all of the operations involving the receiver's private keys r and q are performed by the receiver, so that neither party needs to know the other party's keys.","name":"Three-pass protocol","categories":["Asymmetric-key algorithms","Cryptographic protocols"],"tag_line":"In cryptography, the three-pass protocol for sending messages is a framework which allows one party to securely send a message to a second party without the need to exchange or distribute encryption keys."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quantum-algorithm-for-linear-systems-of-equations","_score":0,"_source":{"description":"The quantum algorithm for linear systems of equations, designed by Aram Harrow, Avinatan Hassidim, and Seth Lloyd, is a quantum algorithm for solving linear systems formulated in 2009. The algorithm estimates the result of a scalar measurement on the solution vector to a given linear system of equations.\nThe algorithm is one of the main fundamental algorithms expected to provide an exponential speedup over their classical counterparts, along with Shor's factoring algorithm, Grover's search algorithm and quantum simulation. Provided the linear system is a sparse and has a low condition number , and that the user is interested in the result of a scalar measurement on the solution vector, instead of the values of the solution vector itself, then the algorithm has a runtime of , where  is the number of variables in the linear system.. This offers an exponential speedup over the fastest classical algorithm, which runs in  (or  for positive semidefinite matrices).\nAn implementation of the quantum algorithm for linear systems of equations was first demonstrated in 2013 by Cai et al., Barz et al.and Pan et al. in parallel. The demonstrations consisted of simple linear equations on specially designed quantum devices. \nDue to the prevalence of linear systems in virtually all areas of science and engineering, the quantum algorithm for linear systems of equations has the potential for widespread applicability.","name":"Quantum algorithm for linear systems of equations","categories":["Articles containing proofs","Integer factorization algorithms","Quantum algorithms","Quantum information science"],"tag_line":"The quantum algorithm for linear systems of equations, designed by Aram Harrow, Avinatan Hassidim, and Seth Lloyd, is a quantum algorithm for solving linear systems formulated in 2009."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ms-chap","_score":0,"_source":{"description":"MS-CHAP is the Microsoft version of the Challenge-Handshake Authentication Protocol, CHAP. The protocol exists in two versions, MS-CHAPv1 (defined in RFC 2433) and MS-CHAPv2 (defined in RFC 2759). MS-CHAPv2 was introduced with Windows NT 4.0 SP4 and was added to Windows 98 in the \"Windows 98 Dial-Up Networking Security Upgrade Release\" and Windows 95 in the \"Dial Up Networking 1.3 Performance & Security Update for MS Windows 95\" upgrade. Windows Vista dropped support for MS-CHAPv1.\nMS-CHAP is used as one authentication option in Microsoft's implementation of the PPTP protocol for virtual private networks. It is also used as an authentication option with RADIUS servers which are used for WiFi security using the WPA-Enterprise protocol. It is further used as the main authentication option of the Protected Extensible Authentication Protocol (PEAP).\nCompared with CHAP, MS-CHAP:\nis enabled by negotiating CHAP Algorithm 0x80 (0x81 for MS-CHAPv2) in LCP option 3, Authentication Protocol\nprovides an authenticator-controlled password change mechanism\nprovides an authenticator-controlled authentication retry mechanism\ndefines failure codes returned in the Failure packet message field\nMS-CHAPv2 provides mutual authentication between peers by piggybacking a peer challenge on the Response packet and an authenticator response on the Success packet.","name":"MS-CHAP","categories":["All articles lacking in-text citations","Articles lacking in-text citations from July 2013","Broken cryptography algorithms","Computer access control protocols","Internet protocols","Microsoft Windows security technology"],"tag_line":"MS-CHAP is the Microsoft version of the Challenge-Handshake Authentication Protocol, CHAP."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lattice-sieving","_score":0,"_source":{"description":"Lattice sieving is a technique for finding smooth values of a bivariate polynomial  over a large region. It is almost exclusively used in conjunction with the number field sieve. The original idea of the lattice sieve came from John Pollard.\nThe algorithm implicitly involves the ideal structure of the number field of the polynomial; it takes advantage of the theorem that any prime ideal above some rational prime p can be written as . One then picks many prime numbers q of an appropriate size, usually just above the factor base limit, and proceeds by\n\nFor each q, list the prime ideals above q by factorising the polynomial f(a,b) over For each of these prime ideals, which are called 'special 's, construct a reduced basis  for the lattice L generated by ; set a two-dimensional array called the sieve region to zero.\nFor each prime ideal  in the factor base, construct a reduced basis  for the sublattice of L generated byFor each element of that sublattice lying within a sufficiently large sieve region, add  to that entry.\n\nRead out all the entries in the sieve region with a large enough value\n\nFor the number field sieve application, it is necessary for two polynomials both to have smooth values; this is handled by running the inner loop over both polynomials, whilst the special-q can be taken from either side.","name":"Lattice sieving","categories":["Integer factorization algorithms"],"tag_line":"Lattice sieving is a technique for finding smooth values of a bivariate polynomial  over a large region."}}
,{"_index":"throwtable","_type":"algorithm","_id":"xtr","_score":0,"_source":{"description":"In cryptography, XTR is an algorithm for public-key encryption. XTR stands for ‘ECSTR’, which is an abbreviation for Efficient and Compact Subgroup Trace Representation. It is a method to represent elements of a subgroup of a multiplicative group of a finite field. To do so, it uses the trace over  to represent elements of a subgroup of .\nFrom a security point of view, XTR relies on the difficulty of solving Discrete Logarithm related problems in the full multiplicative group of a finite field. Unlike many cryptographic protocols that are based on the generator of the full multiplicative group of a finite field, XTR uses the generator  of a relatively small subgroup of some prime order  of a subgroup of . With the right choice of , computing Discrete Logarithms in the group, generated by , is, in general, as hard as it is in  and thus cryptographic applications of XTR use  arithmetics while achieving full  security leading to substantial savings both in communication and computational overhead without compromising security. Some other advantages of XTR are its fast key generation, small key sizes and speed.","name":"XTR","categories":["Asymmetric-key algorithms","Finite fields"],"tag_line":"In cryptography, XTR is an algorithm for public-key encryption."}}
,{"_index":"throwtable","_type":"algorithm","_id":"schoof's-algorithm","_score":0,"_source":{"description":"Schoof's algorithm is an efficient algorithm to count points on elliptic curves over finite fields. The algorithm has applications in elliptic curve cryptography where it is important to know the number of points to judge the difficulty of solving the discrete logarithm problem in the group of points on an elliptic curve.\nThe algorithm was published by René Schoof in 1985 and it was a theoretical breakthrough, as it was the first deterministic polynomial time algorithm for counting points on elliptic curves. Before Schoof's algorithm, approaches to counting points on elliptic curves such as the naive and baby-step giant-step algorithms were, for the most part, tedious and had an exponential running time.\nThis article explains Schoof's approach, laying emphasis on the mathematical ideas underlying the structure of the algorithm.","name":"Schoof's algorithm","categories":["Asymmetric-key algorithms","Elliptic curve cryptography","Elliptic curves","Finite fields","Group theory","Number theory"],"tag_line":"Schoof's algorithm is an efficient algorithm to count points on elliptic curves over finite fields."}}
,{"_index":"throwtable","_type":"algorithm","_id":"reeds–sloane-algorithm","_score":0,"_source":{"description":"The Reeds–Sloane algorithm, named after J. A. Reeds and N. J. A. Sloane, is an extension of the Berlekamp–Massey algorithm, an algorithm for finding the shortest linear feedback shift register (LFSR) for a given output sequence, for use on sequences that take their values from the integers mod n.","name":"Reeds–Sloane algorithm","categories":["All stub articles","Cryptanalytic algorithms","Cryptography stubs"],"tag_line":"The Reeds–Sloane algorithm, named after J."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adaboost","_score":0,"_source":{"description":"AdaBoost, short for \"Adaptive Boosting\", is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire who won the Gödel Prize in 2003 for their work. It can be used in conjunction with many other types of learning algorithms to improve their performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers. In some problems, however, it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing (i.e., their error rate is smaller than 0.5 for binary classification), the final model can be proven to converge to a strong learner\nWhile every learning algorithm will tend to suit some problem types better than others, and will typically have many different parameters and configurations to be adjusted before achieving optimal performance on a dataset, AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier. When used with decision tree learning, information gathered at each stage of the AdaBoost algorithm about the relative 'hardness' of each training sample is fed into the tree growing algorithm such that later trees tend to focus on harder to classify examples.","name":"AdaBoost","categories":["Classification algorithms","Ensemble learning"],"tag_line":"AdaBoost, short for \"Adaptive Boosting\", is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire who won the Gödel Prize in 2003 for their work."}}
,{"_index":"throwtable","_type":"algorithm","_id":"in-place-algorithm","_score":0,"_source":{"description":"In computer science, an in-place algorithm is an algorithm which transforms input using a data structure with a small amount of extra storage space. The input is usually overwritten by the output as the algorithm executes. An algorithm which is not in-place is sometimes called not-in-place or out-of-place.\nIn-place can have slightly different meanings. In its strictest form, the algorithm can only have a constant amount of extra space, counting everything including function calls and pointers. However, this form is very limited as simply having an index to a length n array requires O(log n) bits. More broadly, in-place means that the algorithm does not use extra space for manipulating the input but may require a small though nonconstant extra space for its operation. Usually, this space is O(log n), though sometimes anything in o(n) is allowed. Note that space complexity also has varied choices in whether or not to count the index lengths as part of the space used. Often, the space complexity is given in terms of the number of indices or pointers needed, ignoring their length. In this article, we refer to total space complexity (DSPACE), counting pointer lengths. Therefore, the space requirements here have an extra log n factor compared to an analysis that ignores the length of indices and pointers.\nAn algorithm may or may not count the output as part of its space usage. Since in-place algorithms usually overwrite their input with output, no additional space is needed. When writing the output to write-only memory or a stream, it may make be more appropriate to only consider the working space of the algorithm. In theory applications such as log-space reductions, it is more typical to always ignore output space (in these cases it is more essential that the output is write-only).","name":"In-place algorithm","categories":["Algorithms","All articles needing additional references","Articles needing additional references from January 2015"],"tag_line":"In computer science, an in-place algorithm is an algorithm which transforms input using a data structure with a small amount of extra storage space."}}
,{"_index":"throwtable","_type":"algorithm","_id":"divide-and-conquer-algorithms","_score":0,"_source":{"description":"In computer science, divide and conquer (D&C) is an algorithm design paradigm based on multi-branched recursion. A divide and conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same (or related) type (divide), until these become simple enough to be solved directly (conquer). The solutions to the sub-problems are then combined to give a solution to the original problem.\nThis divide and conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. Karatsuba), syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFTs).\nUnderstanding and designing D&C algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization. These D&C complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion.\nThe correctness of a divide and conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.","name":"Divide and conquer algorithms","categories":["Algorithms","Operations research","Optimization algorithms and methods","Pages with citations lacking titles"],"tag_line":"In computer science, divide and conquer (D&C) is an algorithm design paradigm based on multi-branched recursion."}}
,{"_index":"throwtable","_type":"algorithm","_id":"algorithmic-logic","_score":0,"_source":{"description":"Algorithmic logic is a calculus of programs which allows the expression of semantical properties of programs by appropriate logical formulas. It provides a framework that enables proving the formulas from the axioms of program constructs such as assignment, iteration and composition instructions and from the axioms of the data structures in question see Mirkowska & Salwicki (1987), Banachowski et al. (1977).\nThe following diagram helps to locate algorithmic logic among other logics. \nThe formalized language of algorithmic logic (and of algorithmic theories of various data structures) contains three types of well formed expressions: terms - i.e. expressions denoting operations on elements of data structures, formulas - i.e. expressions denoting the relations among elements of data structures, programs - i.e. algorithms - these expressions describe the computations. For semantics of terms and formulas consult pages on first order logic and Tarski's semantic. The meaning of a program  is the set of possible computations of the program.\n\nAlgorithmic logic is one of many logics of programs. Another logic of programs is dynamic logic, see dynamic logic, Harel, Kozen & Tiuryn (2000).","name":"Algorithmic logic","categories":["Algorithms","All articles covered by WikiProject Wikify","All articles with too few wikilinks","All orphaned articles","All stub articles","Articles covered by WikiProject Wikify from June 2015","Articles with too few wikilinks from June 2015","Mathematical logic stubs","Orphaned articles from June 2015","Theoretical computer science"],"tag_line":"Algorithmic logic is a calculus of programs which allows the expression of semantical properties of programs by appropriate logical formulas."}}
,{"_index":"throwtable","_type":"algorithm","_id":"the-art-of-computer-programming","_score":0,"_source":{"description":"The Art of Computer Programming (sometimes known by its initials TAOCP) is a comprehensive monograph written by Donald Knuth that covers many kinds of programming algorithms and their analysis.\nKnuth began the project, originally conceived as a single book with twelve chapters, in 1962. The first three of what was then expected to be a seven-volume set were published in 1968, 1969, and 1973. The first installment of Volume 4 (a paperback fascicle) was published in 2005. The hardback volume 4A was published in 2011. Additional fascicle installments are planned for release approximately biannually.","name":"The Art of Computer Programming","categories":["1968 books","1969 books","1973 books","1981 books","2011 books","Addison-Wesley books","Algorithms","All articles containing potentially dated statements","All articles with unsourced statements","Analysis of algorithms","Articles containing potentially dated statements from 2011","Articles with inconsistent citation formats","Articles with unsourced statements from June 2012","Books by Donald Knuth","Computer programming books","Computer science books","Monographs"],"tag_line":"The Art of Computer Programming (sometimes known by its initials TAOCP) is a comprehensive monograph written by Donald Knuth that covers many kinds of programming algorithms and their analysis."}}
,{"_index":"throwtable","_type":"algorithm","_id":"avt-statistical-filtering-algorithm","_score":0,"_source":{"description":"AVT Statistical filtering algorithm is an approach to improving quality of raw data collected from various sources. One way to improve signal/noise ratio is to implement filtering to separate useful signal from noise. In ideal situation the useful signal has different frequency then noise and noise is separated/filtered out by frequency discrimination using various filters. Frequency discrimination filtering is done using Low Pass, High Pass and Band Pass filtering which refers to relative frequency filtering criteria target for such configuration. Filters are created using passive and active components and sometimes are implemented using software algorithms based on FFT.\nSometimes signal frequency coincides with noise frequency in real life. In this situations frequency discrimination filtering does not work since the noise and useful signal are indistinguishable. To achieve filtering in such conditions there are several algorithms available which is described below in more detail.","name":"AVT Statistical filtering algorithm","categories":["Algorithms"],"tag_line":"AVT Statistical filtering algorithm is an approach to improving quality of raw data collected from various sources."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dakota","_score":0,"_source":{"description":"The Design Analysis Kit for Optimization and Terascale Applications (DAKOTA) is a software toolkit developed by engineers at Sandia National Laboratories to provide a flexible, extensible interface between analysis codes and iterative systems analysis methods. DAKOTA contains optimization algorithms using gradient and nongradient-based methods, parameter estimation with nonlinear least squares methods, uncertainty quantification with sampling, reliability, and stochastic finite element methods, and sensitivity/variance analysis with design of experiments and parameter study capabilities.","name":"DAKOTA","categories":["Algorithms","All articles lacking sources","All articles with topics of unclear notability","All orphaned articles","All stub articles","Articles lacking sources from February 2015","Articles with topics of unclear notability from February 2015","Computer programming tool stubs","Orphaned articles from February 2009","Software","United States government stubs"],"tag_line":"The Design Analysis Kit for Optimization and Terascale Applications (DAKOTA) is a software toolkit developed by engineers at Sandia National Laboratories to provide a flexible, extensible interface between analysis codes and iterative systems analysis methods."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hcs-clustering-algorithm","_score":0,"_source":{"description":"The HCS (Highly Connected Subgraphs) clustering algorithm (also known as the HCS algorithm , and other names such as Highly Connected Clusters/Components/Kernels) is an algorithm based on graph connectivity for Cluster analysis, by first representing the similarity data in a similarity graph, and afterwards finding all the highly connected subgraphs as clusters. The algorithm does not make any prior assumptions on the number of the clusters. This algorithm was published by Erez Hartuv (erez dot hartuv at gmail dot com) and Ron Shamir in 1998.\nThe HCS algorithm gives clustering solution, which is inherently meaningful in the application domain, since each solution cluster must have diameter 2 while a union of two solution clusters will have diameter 3.","name":"HCS clustering algorithm","categories":["Algorithms"],"tag_line":"The HCS (Highly Connected Subgraphs) clustering algorithm (also known as the HCS algorithm , and other names such as Highly Connected Clusters/Components/Kernels) is an algorithm based on graph connectivity for Cluster analysis, by first representing the similarity data in a similarity graph, and afterwards finding all the highly connected subgraphs as clusters."}}
,{"_index":"throwtable","_type":"algorithm","_id":"list-of-algorithm-general-topics","_score":0,"_source":{"description":"This is a list of algorithm general topics.\nAnalysis of algorithms\nAnt colony algorithm\nApproximation algorithm\nBest and worst cases\nBig O notation\nCombinatorial search\nCompetitive analysis\nComputability theory\nComputational complexity theory\nEmbarrassingly parallel problem\nEmergent algorithm\nEvolutionary algorithm\nFast Fourier transform\nGenetic algorithm\nGraph exploration algorithm\nHeuristic\nHill climbing\nImplementation\nLas Vegas algorithm\nLock-free and wait-free algorithms\nMonte Carlo algorithm\nNumerical analysis\nOnline algorithm\nPolynomial time approximation scheme\nProblem size\nPseudorandom number generator\nQuantum algorithm\nRandom-restart hill climbing\nRandomized algorithm\nRunning time\nSorting algorithm\nSearch algorithm\nStable algorithm (disambiguation)\nSuper-recursive algorithm\nTree search algorithm","name":"List of algorithm general topics","categories":["Algorithms","Mathematics-related lists"],"tag_line":"This is a list of algorithm general topics."}}
,{"_index":"throwtable","_type":"algorithm","_id":"devex-algorithm","_score":0,"_source":{"description":"In applied mathematics, the devex algorithm is a pivot rule for the simplex method developed by Harris. It identifies the steepest-edge approximately in its search for the optimal solution.","name":"Devex algorithm","categories":["Algorithms","Algorithms and data structures stubs","All articles needing additional references","All stub articles","Articles needing additional references from August 2013","Computer science stubs"],"tag_line":"In applied mathematics, the devex algorithm is a pivot rule for the simplex method developed by Harris."}}
,{"_index":"throwtable","_type":"algorithm","_id":"list-of-algorithms","_score":0,"_source":{"description":"The following is a list of algorithms along with one-line descriptions for each.\n\n","name":"List of algorithms","categories":["Algorithms","All articles needing additional references","Articles contradicting other articles","Articles needing additional references from April 2014","Mathematics-related lists"],"tag_line":"The following is a list of algorithms along with one-line descriptions for each.\n\n"}}
,{"_index":"throwtable","_type":"algorithm","_id":"super-recursive-algorithm","_score":0,"_source":{"description":"In computability theory, super-recursive algorithms are a generalization of ordinary algorithms that are more powerful, that is, compute more than Turing machines. The term was introduced by Mark Burgin, whose book \"Super-recursive algorithms\" develops their theory and presents several mathematical models. Turing machines and other mathematical models of conventional algorithms allow researchers to find properties of recursive algorithms and their computations. In a similar way, mathematical models of super-recursive algorithms, such as inductive Turing machines, allow researchers to find properties of super-recursive algorithms and their computations.\nBurgin, as well as other researchers (including Selim Akl, Eugene Eberbach, Peter Kugel, Jan van Leeuwen, Hava Siegelmann, Peter Wegner, and Jiří Wiedermann) who studied different kinds of super-recursive algorithms and contributed to the theory of super-recursive algorithms, have argued that super-recursive algorithms can be used to disprove the Church-Turing thesis, but this point of view has been criticized within the mathematical community and is not widely accepted.","name":"Super-recursive algorithm","categories":["Algorithms","Hypercomputation","Pages using duplicate arguments in template calls","Theory of computation"],"tag_line":"In computability theory, super-recursive algorithms are a generalization of ordinary algorithms that are more powerful, that is, compute more than Turing machines."}}
,{"_index":"throwtable","_type":"algorithm","_id":"berlekamp–zassenhaus-algorithm","_score":0,"_source":{"description":"In mathematics, in particular in computational algebra, the Berlekamp–Zassenhaus algorithm is an algorithm for factoring polynomials over the integers, named after Elwyn Berlekamp and Hans Zassenhaus. As a consequence of Gauss's lemma, this amounts to solving the problem also over the rationals.\nThe algorithm starts by finding factorizations over suitable finite fields using Hensel's lemma to lift the solution from modulo a prime p to a convenient power of p. After this the right factors are found as a subset of these. The worst case of this algorithm is exponential in the number of factors.\nVan Hoeij (2002) improved this algorithm by using the LLL algorithm, substantially reducing the time needed to choose the right subsets of mod p factors.","name":"Berlekamp–Zassenhaus algorithm","categories":["Algebra stubs","Algorithms and data structures stubs","All stub articles","Computer algebra","Computer science stubs"],"tag_line":"In mathematics, in particular in computational algebra, the Berlekamp–Zassenhaus algorithm is an algorithm for factoring polynomials over the integers, named after Elwyn Berlekamp and Hans Zassenhaus."}}
,{"_index":"throwtable","_type":"algorithm","_id":"simulation-algorithms-for-atomic-devs","_score":0,"_source":{"description":"Given an atomic DEVS model, simulation algorithms are methods to generate the model's legal behaviors which are trajectories not to reach to illegal states. (see Behavior of DEVS). [Zeigler84] originally introduced the algorithms that handle time variables related to lifespan  and elapsed time  by introducing two other time variables, last event time, , and next event time  with the following relations:\n\nand\n\nwhere  denotes the current time. And the remaining time,\n\nis equivalently computed as\n\n, apparently .\n\nSince the behavior of a given atomic DEVS model can be defined in two different views depending on the total state and the external transition function (refer to Behavior of DEVS), the simulation algorithms are also introduced in two different views as below.","name":"Simulation algorithms for atomic DEVS","categories":["Algorithms"],"tag_line":"Given an atomic DEVS model, simulation algorithms are methods to generate the model's legal behaviors which are trajectories not to reach to illegal states."}}
,{"_index":"throwtable","_type":"algorithm","_id":"streaming-algorithm","_score":0,"_source":{"description":"In computer science, streaming algorithms are algorithms for processing data streams in which the input is presented as a sequence of items and can be examined in only a few passes (typically just one). These algorithms have limited memory available to them (much less than the input size) and also limited processing time per item.\nThese constraints may mean that an algorithm produces an approximate answer based on a summary or \"sketch\" of the data stream in memory.","name":"Streaming algorithm","categories":["Algorithms","All articles with unsourced statements","Articles with unsourced statements from March 2013","Pages using duplicate arguments in template calls"],"tag_line":"In computer science, streaming algorithms are algorithms for processing data streams in which the input is presented as a sequence of items and can be examined in only a few passes (typically just one)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rendezvous-hashing","_score":0,"_source":{"description":"Rendezvous or Highest Random Weight (HRW) hashing is an algorithm that allows clients to achieve distributed agreement on a set of k options out of a possible set of n options. A typical application is when clients need to agree on which sites (or proxies) objects are to assigned to. When k is 1, it accomplishes goals similar to consistent hashing, using an entirely different method.","name":"Rendezvous hashing","categories":["Algorithms","Hashing"],"tag_line":"Rendezvous or Highest Random Weight (HRW) hashing is an algorithm that allows clients to achieve distributed agreement on a set of k options out of a possible set of n options."}}
,{"_index":"throwtable","_type":"algorithm","_id":"kisao","_score":0,"_source":{"description":"The Kinetic Simulation Algorithm Ontology (KiSAO) supplies information about existing algorithms available for the simulation of systems biology models, their characterization and interrelationships. KiSAO is part of the BioModels.net project and of the COMBINE initiative.","name":"KiSAO","categories":["Algorithms","Bioinformatics","Biological databases","Free science software","Systems biology"],"tag_line":"The Kinetic Simulation Algorithm Ontology (KiSAO) supplies information about existing algorithms available for the simulation of systems biology models, their characterization and interrelationships."}}
,{"_index":"throwtable","_type":"algorithm","_id":"timeline-of-algorithms","_score":0,"_source":{"description":"The following timeline outlines the development of algorithms (mainly \"mathematical recipes\") since their inception.","name":"Timeline of algorithms","categories":["Algorithms","Computing timelines","Mathematics timelines"],"tag_line":"The following timeline outlines the development of algorithms (mainly \"mathematical recipes\") since their inception."}}
,{"_index":"throwtable","_type":"algorithm","_id":"approximation-algorithm","_score":0,"_source":{"description":"In computer science and operations research, approximation algorithms are algorithms used to find approximate solutions to optimization problems. Approximation algorithms are often associated with NP-hard problems; since it is unlikely that there can ever be efficient polynomial-time exact algorithms solving NP-hard problems, one settles for polynomial-time sub-optimal solutions. Unlike heuristics, which usually only find reasonably good solutions reasonably fast, one wants provable solution quality and provable run-time bounds. Ideally, the approximation is optimal up to a small constant factor (for instance within 5% of the optimal solution). Approximation algorithms are increasingly being used for problems where exact polynomial-time algorithms are known but are too expensive due to the input size. A typical example for an approximation algorithm is the one for vertex cover in graphs: find an uncovered edge and add both endpoints to the vertex cover, until none remain. It is clear that the resulting cover is at most twice as large as the optimal one. This is a constant factor approximation algorithm with a factor of 2.\nNP-hard problems vary greatly in their approximability; some, such as the bin packing problem, can be approximated within any factor greater than 1 (such a family of approximation algorithms is often called a polynomial time approximation scheme or PTAS). Others are impossible to approximate within any constant, or even polynomial factor unless P = NP, such as the maximum clique problem.\nNP-hard problems can often be expressed as integer programs (IP) and solved exactly in exponential time. Many approximation algorithms emerge from the linear programming relaxation of the integer program.\nNot all approximation algorithms are suitable for all practical applications. They often use IP/LP/Semidefinite solvers, complex data structures or sophisticated algorithmic techniques which lead to difficult implementation problems. Also, some approximation algorithms have impractical running times even though they are polynomial time, for example O(n2156) . Yet the study of even very expensive algorithms is not a completely theoretical pursuit as they can yield valuable insights. A classic example is the initial PTAS for Euclidean TSP due to Sanjeev Arora which had prohibitive running time, yet within a year, Arora refined the ideas into a linear time algorithm. Such algorithms are also worthwhile in some applications where the running times and cost can be justified e.g. computational biology, financial engineering, transportation planning, and inventory management. In such scenarios, they must compete with the corresponding direct IP formulations.\nAnother limitation of the approach is that it applies only to optimization problems and not to \"pure\" decision problems like satisfiability, although it is often possible to conceive optimization versions of such problems, such as the maximum satisfiability problem (Max SAT).\nInapproximability has been a fruitful area of research in computational complexity theory since the 1990 result of Feige, Goldwasser, Lovász, Safra and Szegedy on the inapproximability of Independent Set. After Arora et al. proved the PCP theorem a year later, it has now been shown that Johnson's 1974 approximation algorithms for Max SAT, Set Cover, Independent Set and Coloring all achieve the optimal approximation ratio, assuming P != NP.","name":"Approximation algorithm","categories":["All articles lacking in-text citations","Approximation algorithms","Articles lacking in-text citations from April 2009","Computational complexity theory","Wikipedia articles with GND identifiers"],"tag_line":"In computer science and operations research, approximation algorithms are algorithms used to find approximate solutions to optimization problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"domination-analysis","_score":0,"_source":{"description":"Domination analysis of an approximation algorithm is a way to estimate its performance, introduced by Glover and Punnen in 1997. Unlike the classical approximation ratio analysis, which compares the numerical quality of a calculated solution with that of an optimal solution, domination analysis involves examining the rank of the calculated solution in the sorted order of all possible solutions. In this style of analysis, an algorithm is said to have dominance number or domination number K, if there exists a subset of K different solutions to the problem among which the algorithm's output is the best. Domination analysis can also be expressed using a domination ratio, which is the fraction of the solution space that is no better than the given solution; this number always lies within the interval [0,1], with larger numbers indicating better solutions. Domination analysis is most commonly applied to problems for which the total number of possible solutions is known and for which exact solution is difficult.\nFor instance, in the Traveling salesman problem, there are (n-1)! possible solutions for a problem instance with n cities. If an algorithm can be shown to have dominance number close to (n-1)!, or equivalently to have domination ratio close to 1, then it can be taken as preferable to an algorithm with lower dominance number.\nIf it is possible to efficiently find random samples of a problem's solution space, as it is in the Traveling salesman problem, then it is straightforward for a randomized algorithm to find a solution that with high probability has high domination ratio: simply construct a set of samples and select the best solution from among them. (See, e.g., Orlin and Sharma.)\nThe dominance number described here should not be confused with the domination number of a graph, which refers to the number of vertices in the smallest dominating set of the graph.\nRecently, a growing number of articles in which domination analysis has been applied to assess the performance of heuristics has appeared. This kind of analysis may be seen as competing with the classical approximation ratio analysis tradition. The two measures may also be viewed as complementary.","name":"Domination analysis","categories":["Approximation algorithms"],"tag_line":"Domination analysis of an approximation algorithm is a way to estimate its performance, introduced by Glover and Punnen in 1997."}}
,{"_index":"throwtable","_type":"algorithm","_id":"faugère's-f4-and-f5-algorithms","_score":0,"_source":{"description":"In computer algebra, the Faugère F4 algorithm, by Jean-Charles Faugère, computes the Gröbner basis of an ideal of a multivariate polynomial ring. The algorithm uses the same mathematical principles as the Buchberger algorithm, but computes many normal forms in one go by forming a generally sparse matrix and using fast linear algebra to do the reductions in parallel.\nThe Faugère F5 algorithm first calculates the Gröbner basis of a pair of generator polynomials of the ideal. Then it uses this basis to reduce the size of the initial matrices of generators for the next larger basis:\n\nIf Gprev is an already computed Gröbner basis (f2, …, fm) and we want to compute a Gröbner basis of (f1) + Gprev then we will construct matrices whose rows are m f1 such that m is a monomial not divisible by the leading term of an element of Gprev.\n\nThis strategy allows the algorithm to apply two new criteria based on what Faugère calls signatures of polynomials. Thanks to these criteria, the algorithm can compute Gröbner bases for a large class of interesting polynomial systems, called regular sequences, without ever simplifying a single polynomial to zero—the most time-consuming operation in algorithms that compute Gröbner bases. It is also very effective for a large number of non-regular sequences.","name":"Faugère's F4 and F5 algorithms","categories":["Algorithms and data structures stubs","All articles with unsourced statements","All stub articles","Articles with unsourced statements from February 2013","Computer algebra","Computer science stubs"],"tag_line":"In computer algebra, the Faugère F4 algorithm, by Jean-Charles Faugère, computes the Gröbner basis of an ideal of a multivariate polynomial ring."}}
,{"_index":"throwtable","_type":"algorithm","_id":"method-of-conditional-probabilities","_score":0,"_source":{"description":"In mathematics and computer science, the probabilistic method is used to prove the existence of mathematical objects with desired combinatorial properties. The proofs are probabilistic — they work by showing that a random object, chosen from some probability distribution, has the desired properties with positive probability. Consequently, they are nonconstructive — they don't explicitly describe an efficient method for computing the desired objects.\nThe method of conditional probabilities (Erdős & Selfridge 1973), (Spencer 1987), (Raghavan 1988) converts such a proof, in a \"very precise sense\", into an efficient deterministic algorithm, one that is guaranteed to compute an object with the desired properties. That is, the method derandomizes the proof. The basic idea is to replace each random choice in a random experiment by a deterministic choice, so as to keep the conditional probability of failure, given the choices so far, below 1.\nThe method is particularly relevant in the context of randomized rounding (which uses the probabilistic method to design approximation algorithms).\nWhen applying the method of conditional probabilities, the technical term pessimistic estimator refers to a quantity used in place of the true conditional probability (or conditional expectation) underlying the proof.","name":"Method of conditional probabilities","categories":["All articles lacking in-text citations","Approximation algorithms","Articles lacking in-text citations from June 2012","Probabilistic arguments"],"tag_line":"In mathematics and computer science, the probabilistic method is used to prove the existence of mathematical objects with desired combinatorial properties."}}
,{"_index":"throwtable","_type":"algorithm","_id":"polynomial-time-algorithm-for-approximating-the-volume-of-convex-bodies","_score":0,"_source":{"description":"The paper is a joint work by Martin Dyer, Alan M. Frieze and Ravindran Kannan.\nThe main result of the paper is a randomized algorithm for finding an  approximation to the volume of a convex body  in -dimensional Euclidean space by assuming the existence of a membership oracle. The algorithm takes time bounded by a polynomial in , the dimension of  and .\nThe algorithm is a sophisticated usage of the so-called Markov chain Monte Carlo (MCMC) method. The basic scheme of the algorithm is a nearly uniform sampling from within  by placing a grid consisting -dimensional cubes and doing a random walk over these cubes. By using the theory of rapidly mixing Markov chains, they show that it takes a polynomial time for the random walk to settle down to being a nearly uniform distribution.","name":"Polynomial-time algorithm for approximating the volume of convex bodies","categories":["Approximation algorithms","Computational geometry"],"tag_line":"The paper is a joint work by Martin Dyer, Alan M. Frieze and Ravindran Kannan."}}
,{"_index":"throwtable","_type":"algorithm","_id":"k-approximation-of-k-hitting-set","_score":0,"_source":{"description":"In computer science, k-approximation of k-hitting set is an approximation algorithm for weighted hitting set. The input is a collection S of subsets of some universe T and a mapping W from T to non-negative numbers called the weights of the elements of T. In k-hitting set the size of the sets in S cannot be larger than k. That is, . The problem is now to pick some subset T' of T such that every set in S contains some element of T', and such that the total weight of all elements in T' is as small as possible.","name":"K-approximation of k-hitting set","categories":["Approximation algorithms"],"tag_line":"In computer science, k-approximation of k-hitting set is an approximation algorithm for weighted hitting set."}}
,{"_index":"throwtable","_type":"algorithm","_id":"flower-pollination-algorithm","_score":0,"_source":{"description":"Flower pollination algorithm (FPA) is a metaheuristic algorithm that was developed by Xin-She Yang, based on the pollination process of flowering plants. FPA has been applied to solve practical problems in engineering, solar PV parameter estimation, and fuzzy selection for dynamic economic dispatch.","name":"Flower pollination algorithm","categories":["All articles needing expert attention","Articles needing expert attention from July 2015","Bioinformatics algorithms","Computer science articles needing expert attention","Pollination"],"tag_line":"Flower pollination algorithm (FPA) is a metaheuristic algorithm that was developed by Xin-She Yang, based on the pollination process of flowering plants."}}
,{"_index":"throwtable","_type":"algorithm","_id":"selection-(genetic-algorithm)","_score":0,"_source":{"description":"Selection is the stage of a genetic algorithm in which individual genomes are chosen from a population for later breeding (using the crossover operator).\nA generic selection procedure may be implemented as follows:\nThe fitness function is evaluated for each individual, providing fitness values, which are then normalized. Normalization means dividing the fitness value of each individual by the sum of all fitness values, so that the sum of all resulting fitness values equals 1.\nThe population is sorted by descending fitness values.\nAccumulated normalized fitness values are computed (the accumulated fitness value of an individual is the sum of its own fitness value plus the fitness values of all the previous individuals). The accumulated fitness of the last individual should be 1 (otherwise something went wrong in the normalization step).\nA random number R between 0 and 1 is chosen.\nThe selected individual is the first one whose accumulated normalized value is greater than R.\nIf this procedure is repeated until there are enough selected individuals, this selection method is called fitness proportionate selection or roulette-wheel selection. If instead of a single pointer spun multiple times, there are multiple, equally spaced pointers on a wheel that is spun once, it is called stochastic universal sampling. Repeatedly selecting the best individual of a randomly chosen subset is tournament selection. Taking the best half, third or another proportion of the individuals is truncation selection.\nThere are other selection algorithms that do not consider all individuals for selection, but only those with a fitness value that is higher than a given (arbitrary) constant. Other algorithms select from a restricted pool where only a certain percentage of the individuals are allowed, based on fitness value.\nRetaining the best individuals in a generation unchanged in the next generation, is called elitism or elitist selection. It is a successful (slight) variant of the general process of constructing a new population.","name":"Selection (genetic algorithm)","categories":["Genetic algorithms"],"tag_line":"Selection is the stage of a genetic algorithm in which individual genomes are chosen from a population for later breeding (using the crossover operator)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"tournament-selection","_score":0,"_source":{"description":"Tournament selection is a method of selecting an individual from a population of individuals in a genetic algorithm. Tournament selection involves running several \"tournaments\" among a few individuals (or 'chromosomes') chosen at random from the population. The winner of each tournament (the one with the best fitness) is selected for crossover. Selection pressure is easily adjusted by changing the tournament size. If the tournament size is larger, weak individuals have a smaller chance to be selected.\nThe tournament selection method may be described in pseudo code:\n\nchoose k (the tournament size) individuals from the population at random\nchoose the best individual from pool/tournament with probability p\nchoose the second best individual with probability p*(1-p)\nchoose the third best individual with probability p*((1-p)^2)\nand so on...\n\nDeterministic tournament selection selects the best individual (when p = 1) in any tournament. A 1-way tournament (k = 1) selection is equivalent to random selection. The chosen individual can be removed from the population that the selection is made from if desired, otherwise individuals can be selected more than once for the next generation. In comparison with the (stochastic) fitness proportionate selection method, tournament selection is often implemented in practice due to its lack of stochastic noise.\nTournament selection has several benefits over alternative selection methods for genetic algorithms (for example, fitness proportionate selection and reward-based selection): it is efficient to code, works on parallel architectures and allows the selection pressure to be easily adjusted. Tournament selection has also been shown to be independent of the scaling of the genetic algorithm fitness function (or 'objective function') in some classifier systems.","name":"Tournament selection","categories":["Genetic algorithms","Pages using citations with accessdate and no URL"],"tag_line":"Tournament selection is a method of selecting an individual from a population of individuals in a genetic algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"uclust","_score":0,"_source":{"description":"UCLUST is an algorithm designed to cluster nucleotide or amino-acid sequences into clusters based on sequence similarity. The algorithm was published in 2010 and implemented in a program also named UCLUST. The algorithm is described by the author as following two simple clustering criteria, in regard to the requested similarity threshold T. The first criterion states that any given cluster's centroid sequence will have a similarity smaller than T to any other clusters' centroid sequence. The second criterion states that each member sequence in a given cluster will have similarity to the cluster's centroid sequence that is equal or greater than T.\nUCLUST algorithm is a greedy one. As a result, the order of the sequences in the input file will have an impact on the resulting clusters and their quality. For this reason, it's advised that the sequences will be sorted before entering clustering stage. The program UCLUST is equipped with some options to sort the input sequences prior to clustering them.\nUCLUST program is widely utilized among the bioinformatic research community, where it used for multiple applications including OTU assignment (e.g. 16s), creating non-redundant gene catalogs, taxonomic assignment and phylogenetic analysis.","name":"UCLUST","categories":["2010 software","All articles covered by WikiProject Wikify","All articles with too few wikilinks","Articles covered by WikiProject Wikify from March 2015","Articles with too few wikilinks from March 2015","Bioinformatics algorithms","Metagenomics"],"tag_line":"UCLUST is an algorithm designed to cluster nucleotide or amino-acid sequences into clusters based on sequence similarity."}}
,{"_index":"throwtable","_type":"algorithm","_id":"genetic-operator","_score":0,"_source":{"description":"A genetic operator is an operator used in genetic algorithms to guide the algorithm towards a solution to a given problem. There are three main types of operators (mutation, crossover and selection), which must work in conjunction with one another in order for the algorithm to be successful. Genetic operators are used to create and maintain genetic diversity (mutation operator), combine existing solutions (also known as chromosomes) into new solutions (crossover) and select between solutions (selection). In his book discussing the use of genetic programming for the optimization of complex problems, computer scientist John Koza has also identified an 'inversion' or 'permutation' operator; however, the effectiveness of this operator has never been conclusively demonstrated and this operator is rarely discussed.\nMutation (or mutation-like) operators are said to be unary operators, as they only operate on one chromosome at a time. In contrast, crossover operators are said to be binary operators, as they operate on two chromosomes at a time, combining two existing chromosomes into one new chromosome.","name":"Genetic operator","categories":["Genetic algorithms"],"tag_line":"A genetic operator is an operator used in genetic algorithms to guide the algorithm towards a solution to a given problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gene-expression-programming","_score":0,"_source":{"description":"In computer programming, gene expression programming (GEP) is an evolutionary algorithm that creates computer programs or models. These computer programs are complex tree structures that learn and adapt by changing their sizes, shapes, and composition, much like a living organism. And like living organisms, the computer programs of GEP are also encoded in simple linear chromosomes of fixed length. Thus, GEP is a genotype-phenotype system, benefiting from a simple genome to keep and transmit the genetic information and a complex phenotype to explore the environment and adapt to it.","name":"Gene expression programming","categories":["Evolutionary algorithms","Evolutionary computation","Gene expression programming","Genetic algorithms","Genetic programming","Wikipedia articles with possible conflicts of interest from November 2012"],"tag_line":"In computer programming, gene expression programming (GEP) is an evolutionary algorithm that creates computer programs or models."}}
,{"_index":"throwtable","_type":"algorithm","_id":"determination-of-the-day-of-the-week","_score":0,"_source":{"description":"There are various methods to calculate the day of the week for any particular date in the past or future. These methods ultimately rely on algorithms to determine the day of the week for any given date, including those based solely on tables as found in perpetual calendars that require no calculations to be performed by the user. A typical application is to calculate the day of the week on which someone was born or any other specific event occurred.","name":"Determination of the day of the week","categories":["CS1 German-language sources (de)","Calendar algorithms","Days of the week","Gregorian calendar","Julian calendar"],"tag_line":"There are various methods to calculate the day of the week for any particular date in the past or future."}}
,{"_index":"throwtable","_type":"algorithm","_id":"santa-fe-trail-problem","_score":0,"_source":{"description":"The Santa Fe Trail problem is a Genetic programming exercise in which Artificial ants search for food pellets according to a programmed set of instructions. The layout of food pellets in the Santa Fe Trail problem has become a standard for comparing different genetic programming algorithms and solutions.\nOne method for programming and testing algorithms on the Santa Fe Trail problem is by using the NetLogo application. There is at least one case of a student creating a Lego robotic ant to solve the problem.","name":"Santa Fe Trail problem","categories":["Genetic algorithms","Genetic programming"],"tag_line":"The Santa Fe Trail problem is a Genetic programming exercise in which Artificial ants search for food pellets according to a programmed set of instructions."}}
,{"_index":"throwtable","_type":"algorithm","_id":"schema-(genetic-algorithms)","_score":0,"_source":{"description":"A schema is a template in computer science used in the field of genetic algorithms that identifies a subset of strings with similarities at certain string positions. Schemata are a special case of cylinder sets; and so form a topological space.","name":"Schema (genetic algorithms)","categories":["All articles to be merged","All stub articles","Articles to be merged from December 2011","Artificial intelligence stubs","Bioinformatics stubs","Computer science stubs","Genetic algorithms","Genetic programming"],"tag_line":"A schema is a template in computer science used in the field of genetic algorithms that identifies a subset of strings with similarities at certain string positions."}}
,{"_index":"throwtable","_type":"algorithm","_id":"harmony-search","_score":0,"_source":{"description":"In computer science and operations research, harmony search (HS) is a phenomenon-mimicking algorithm (also known as metaheuristic algorithm, soft computing algorithm or evolutionary algorithm) inspired by the improvisation process of musicians proposed by Zong Woo Geem in 2001. In the HS algorithm, each musician (= decision variable) plays (= generates) a note (= a value) for finding a best harmony (= global optimum) all together. Proponents claim the following merits:\nHS does not require differential gradients, thus it can consider discontinuous functions as well as continuous functions.\nHS can handle discrete variables as well as continuous variables.\nHS does not require initial value setting for the variables.\nHS is free from divergence.\nHS may escape local optima.\nHS may overcome the drawback of GA's building block theory which works well only if the relationship among variables in a chromosome is carefully considered. If neighbor variables in a chromosome have weaker relationship than remote variables, building block theory may not work well because of crossover operation. However, HS explicitly considers the relationship using ensemble operation.\nHS has a novel stochastic derivative applied to discrete variables, which uses musician's experiences as a searching direction.\nCertain HS variants do not require algorithm parameters such as HMCR and PAR, thus novice users can easily use the algorithm.\n\n^ \"A new meta-heuristic algorithm for continuous engineering optimization: harmony search theory and practice\". Computer Methods in Applied Mechanics and Engineering 194: 3902–3933. doi:10.1016/j.cma.2004.09.007. \n^ \"Improved Harmony Search from Ensemble of Music Players\". Lecture Notes in Computer Science: 86–93. doi:10.1007/11892960_11. \n^ \"Novel derivative of harmony search algorithm for discrete design variables\". Applied Mathematics and Computation 199: 223–230. doi:10.1016/j.amc.2007.09.049.","name":"Harmony search","categories":["All NPOV disputes","All articles lacking in-text citations","All articles with unsourced statements","Articles lacking in-text citations from April 2013","Articles with unsourced statements from April 2013","Combinatorial optimization","Evolutionary algorithms","NPOV disputes from April 2013","Optimization algorithms and methods","Wikipedia articles with possible conflicts of interest from April 2013"],"tag_line":"In computer science and operations research, harmony search (HS) is a phenomenon-mimicking algorithm (also known as metaheuristic algorithm, soft computing algorithm or evolutionary algorithm) inspired by the improvisation process of musicians proposed by Zong Woo Geem in 2001."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dijkstra's-algorithm","_score":0,"_source":{"description":"Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.\nThe algorithm exists in many variants; Dijkstra's original variant found the shortest path between two nodes, but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest path tree.\nFor a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road, Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. As a result, the shortest path algorithm is widely used in network routing protocols, most notably IS-IS and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson's.\nDijkstra's original algorithm does not use a min-priority queue and runs in time  (where  is the number of nodes). The idea of this algorithm is also given in (Leyzorek et al. 1957). The implementation based on a min-priority queue implemented by a Fibonacci heap and running in  (where  is the number of edges) is due to (Fredman & Tarjan 1984). This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights. However, specialized cases (such as bounded/integer weights, directed acyclic graphs etc) can indeed be improved further as detailed in § Specialized variants.\nIn some fields, artificial intelligence in particular, Dijkstra's algorithm or a variant of it is known as uniform-cost search and formulated as an instance of the more general idea of best-first search.","name":"Dijkstra's algorithm","categories":["1959 in computer science","Articles with example pseudocode","Combinatorial optimization","Commons category with local link same as on Wikidata","Dutch inventions","Graph algorithms","Routing algorithms","Search algorithms","Use dmy dates from February 2011"],"tag_line":"Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"md5","_score":0,"_source":{"description":"The MD5 message-digest algorithm is a widely used cryptographic hash function producing a 128-bit (16-byte) hash value, typically expressed in text format as a 32 digit hexadecimal number. MD5 has been utilized in a wide variety of cryptographic applications, and is also commonly used to verify data integrity.\nMD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function, MD4. The source code in RFC 1321 contains a \"by attribution\" RSA license.\nIn 1996 a flaw was found in the design of MD5. While it was not deemed a fatal weakness at the time, cryptographers began recommending the use of other algorithms, such as SHA-1—which has since been found to be vulnerable as well. In 2004 it was shown that MD5 is not collision resistant. As such, MD5 is not suitable for applications like SSL certificates or digital signatures that rely on this property for digital security. Also in 2004 more serious flaws were discovered in MD5, making further use of the algorithm for security purposes questionable; specifically, a group of researchers described how to create a pair of files that share the same MD5 checksum. Further advances were made in breaking MD5 in 2005, 2006, and 2007. In December 2008, a group of researchers used this technique to fake SSL certificate validity. As of 2010, the CMU Software Engineering Institute considers MD5 \"cryptographically broken and unsuitable for further use\", and most U.S. government applications now require the SHA-2 family of hash functions. In 2012, the Flame malware exploited the weaknesses in MD5 to fake a Microsoft digital signature.","name":"MD5","categories":["All articles with unsourced statements","Articles with example pseudocode","Articles with unsourced statements from August 2014","Broken hash functions","Checksum algorithms","Cryptographic hash functions","Use dmy dates from April 2014"],"tag_line":"The MD5 message-digest algorithm is a widely used cryptographic hash function producing a 128-bit (16-byte) hash value, typically expressed in text format as a 32 digit hexadecimal number."}}
,{"_index":"throwtable","_type":"algorithm","_id":"robinson–schensted-correspondence","_score":0,"_source":{"description":"In mathematics, the Robinson–Schensted correspondence is a bijective correspondence between permutations and pairs of standard Young tableaux of the same shape. It has various descriptions, all of which are of algorithmic nature, it has many remarkable properties, and it has applications in combinatorics and other areas such as representation theory. The correspondence has been generalized in numerous ways, notably by Knuth to what is known as the Robinson–Schensted–Knuth correspondence, and a further generalization to pictures by Zelevinsky.\nThe simplest description of the correspondence is using the Schensted algorithm (Schensted 1961), a procedure that constructs one tableau by successively inserting the values of the permutation according to a specific rule, while the other tableau records the evolution of the shape during construction. The correspondence had been described, in a rather different form, much earlier by Robinson (Robinson 1938), in an attempt to prove the Littlewood–Richardson rule. The correspondence is often referred to as the Robinson–Schensted algorithm, although the procedure used by Robinson is radically different from the Schensted–algorithm, and almost entirely forgotten. Other methods of defining the correspondence include a nondeterministic algorithm in terms of jeu de taquin.\nThe bijective nature of the correspondence relates it to the enumerative identity:\n\nwhere  denotes the set of partitions of n (or of Young diagrams with n squares), and tλ denotes the number of standard Young tableaux of shape λ.","name":"Robinson–Schensted correspondence","categories":["Algebraic combinatorics","Combinatorial algorithms","Permutations","Representation theory of finite groups"],"tag_line":"In mathematics, the Robinson–Schensted correspondence is a bijective correspondence between permutations and pairs of standard Young tableaux of the same shape."}}
,{"_index":"throwtable","_type":"algorithm","_id":"combinatorial-search","_score":0,"_source":{"description":"In computer science and artificial intelligence, combinatorial search studies search algorithms for solving instances of problems that are believed to be hard in general, by efficiently exploring the usually large solution space of these instances. Combinatorial search algorithms achieve this efficiency by reducing the effective size of the search space or employing heuristics. Some algorithms are guaranteed to find the optimal solution, while others may only return the best solution found in the part of the state space that was explored.\nClassic combinatorial search problems include solving the eight queens puzzle or evaluating moves in games with a large game tree, such as reversi or chess.\nA study of computational complexity theory helps to motivate combinatorial search. Combinatorial search algorithms are typically concerned with problems that are NP-hard. Such problems are not believed to be efficiently solvable in general. However, the various approximations of complexity theory suggest that some instances (e.g. \"small\" instances) of these problems could be efficiently solved. This is indeed the case, and such instances often have important practical ramifications.\n\n","name":"Combinatorial search","categories":["All articles lacking in-text citations","Analysis of algorithms","Articles lacking in-text citations from January 2013","Combinatorial optimization","Computational complexity theory","Game artificial intelligence","Search algorithms"],"tag_line":"In computer science and artificial intelligence, combinatorial search studies search algorithms for solving instances of problems that are believed to be hard in general, by efficiently exploring the usually large solution space of these instances."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bottleneck-traveling-salesman-problem","_score":0,"_source":{"description":"The Bottleneck traveling salesman problem (bottleneck TSP) is a problem in discrete or combinatorial optimization. It is stated as follows: Find the Hamiltonian cycle in a weighted graph which minimizes the weight of the most weighty edge of the cycle.\nThe problem is known to be NP-hard. The decision problem version of this, \"for a given length x, is there a Hamiltonian cycle in a graph g with no edge longer than x?\", is NP-complete.\nIn an asymmetric bottleneck TSP, there are cases where the weight from node A to B is different from the weight from B to A (e. g. travel time between two cities with a traffic jam in one direction).\nEuclidean bottleneck TSP, or planar bottleneck TSP, is the bottleneck TSP with the distance being the ordinary Euclidean distance. The problem still remains NP-hard, however many heuristics work better.\nIf the graph is a metric space then there is an efficient approximation algorithm that finds a Hamiltonian cycle with maximum edge weight being no more than twice the optimum.","name":"Bottleneck traveling salesman problem","categories":["Combinatorial optimization","Graph algorithms","Hamiltonian paths and cycles"],"tag_line":"The Bottleneck traveling salesman problem (bottleneck TSP) is a problem in discrete or combinatorial optimization."}}
,{"_index":"throwtable","_type":"algorithm","_id":"branch-and-cut","_score":0,"_source":{"description":"Branch and cut is a method of combinatorial optimization for solving integer linear programs (ILPs), that is, linear programming (LP) problems where some or all the unknowns are restricted to integer values. Branch and cut involves running a branch and bound algorithm and using cutting planes to tighten the linear programming relaxations. Note that if cuts are only used to tighten the initial LP relaxation, the algorithm is called cut and branch.","name":"Branch and cut","categories":["Combinatorial optimization","Optimization algorithms and methods"],"tag_line":"Branch and cut is a method of combinatorial optimization for solving integer linear programs (ILPs), that is, linear programming (LP) problems where some or all the unknowns are restricted to integer values."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bsd-checksum","_score":0,"_source":{"description":"The BSD checksum algorithm is a commonly used, legacy checksum algorithm. It has been implemented in BSD and is also available through the GNU sum command line utility.","name":"BSD checksum","categories":["Checksum algorithms"],"tag_line":"The BSD checksum algorithm is a commonly used, legacy checksum algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"euclidean-shortest-path","_score":0,"_source":{"description":"The Euclidean shortest path problem is a problem in computational geometry: given a set of polyhedral obstacles in a Euclidean space, and two points, find the shortest path between the points that does not intersect any of the obstacles.\nIn two dimensions, the problem can be solved in polynomial time in a model of computation allowing addition and comparisons of real numbers, despite theoretical difficulties involving the numerical precision needed to perform such calculations. These algorithms are based on two different principles, either performing a shortest path algorithm such as Dijkstra's algorithm on a visibility graph derived from the obstacles or (in an approach called the continuous Dijkstra method) propagating a wavefront from one of the points until it meets the other.\nIn three (and higher) dimensions the problem is NP-hard in the general case , but there exist efficient approximation algorithms that run in polynomial time based on the idea of finding a suitable sample of points on the obstacle edges and performing a visibility graph calculation using these sample points.\nThere are many results on computing shortest paths which stays on a polyhedral surface. Given two points s and t, say on the surface of a convex polyhedron, the problem is to compute a shortest path that never leaves the surface and connects s with t. This is a generalization of the problem from 2-dimension but it is much easier than the 3-dimensional problem.\nAlso, there are variations of this problem, where the obstacles are weighted, i.e., one can go through an obstacle, but it incurs an extra cost to go through an obstacle. The standard problem is the special case where the obstacles have infinite weight. This is termed as the weighted region problem in the literature.","name":"Euclidean shortest path","categories":["All stub articles","Combinatorics stubs","Computational geometry","Geometric algorithms","Geometry stubs"],"tag_line":"The Euclidean shortest path problem is a problem in computational geometry: given a set of polyhedral obstacles in a Euclidean space, and two points, find the shortest path between the points that does not intersect any of the obstacles."}}
,{"_index":"throwtable","_type":"algorithm","_id":"blossom-algorithm","_score":0,"_source":{"description":"The blossom algorithm is an algorithm in graph theory for constructing maximum matchings on graphs. The algorithm was developed by Jack Edmonds in 1961, and published in 1965. Given a general graph G = (V, E), the algorithm finds a matching M such that each vertex in V is incident with at most one edge in M and |M| is maximized. The matching is constructed by iteratively improving an initial empty matching along augmenting paths in the graph. Unlike bipartite matching, the key new idea is that an odd-length cycle in the graph (blossom) is contracted to a single vertex, with the search continuing iteratively in the contracted graph.\nA major reason that the blossom algorithm is important is that it gave the first proof that a maximum-size matching could be found using a polynomial amount of computation time. Another reason is that it led to a linear programming polyhedral description of the matching polytope, yielding an algorithm for min-weight matching. As elaborated by Alexander Schrijver, further significance of the result comes from the fact that this was the first polytope whose proof of integrality \"does not simply follow just from total unimodularity, and its description was a breakthrough in polyhedral combinatorics.\"","name":"Blossom algorithm","categories":["Graph algorithms","Matching"],"tag_line":"The blossom algorithm is an algorithm in graph theory for constructing maximum matchings on graphs."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bron–kerbosch-algorithm","_score":0,"_source":{"description":"In computer science, the Bron–Kerbosch algorithm is an algorithm for finding maximal cliques in an undirected graph. That is, it lists all subsets of vertices with the two properties that each pair of vertices in one of the listed subsets is connected by an edge, and no listed subset can have any additional vertices added to it while preserving its complete connectivity. The Bron–Kerbosch algorithm was designed by Dutch scientists Joep Kerbosch and Coenraad Bron, who published its description in 1973. Although other algorithms for solving the clique problem have running times that are, in theory, better on inputs that have few maximal independent sets, the Bron–Kerbosch algorithm and subsequent improvements to it are frequently reported as being more efficient in practice than the alternatives. It is well-known and widely used in application areas of graph algorithms such as computational chemistry.\nA contemporaneous algorithm of Akkoyunlu (1973), although presented in different terms, can be viewed as being the same as the Bron–Kerbosch algorithm, as it generates the same recursive search tree.","name":"Bron–Kerbosch algorithm","categories":["Articles with example pseudocode","Graph algorithms"],"tag_line":"In computer science, the Bron–Kerbosch algorithm is an algorithm for finding maximal cliques in an undirected graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bidirectional-search","_score":0,"_source":{"description":"Bidirectional search is a graph search algorithm that finds a shortest path from an initial vertex to a goal vertex in a directed graph. It runs two simultaneous searches: one forward from the initial state, and one backward from the goal, stopping when the two meet in the middle. The reason for this approach is that in many cases it is faster: for instance, in a simplified model of search problem complexity in which both searches expand a tree with branching factor b, and the distance from start to goal is d, each of the two searches has complexity O(bd/2) (in Big O notation), and the sum of these two search times is much less than the O(bd) complexity that would result from a single search from the beginning to the goal.\nAs in A* search, bi-directional search can be guided by a heuristic estimate of the remaining distance to the goal (in the forward tree) or from the start (in the backward tree).\nIra Pohl (1971) was the first one to design and implement a bi-directional heuristic search algorithm. Andrew Goldberg and others explained the correct termination conditions for the bidirectional version of Dijkstra’s Algorithm.","name":"Bidirectional search","categories":["Graph algorithms","Search algorithms"],"tag_line":"Bidirectional search is a graph search algorithm that finds a shortest path from an initial vertex to a goal vertex in a directed graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"borůvka's-algorithm","_score":0,"_source":{"description":"Borůvka's algorithm is an algorithm for finding a minimum spanning tree in a graph for which all edge weights are distinct.\nIt was first published in 1926 by Otakar Borůvka as a method of constructing an efficient electricity network for Moravia. The algorithm was rediscovered by Choquet in 1938; again by Florek, Łukasiewicz, Perkal, Steinhaus, and Zubrzycki in 1951; and again by Sollin  in 1965. Because Sollin was the only computer scientist in this list living in an English speaking country, this algorithm is frequently called Sollin's algorithm, especially in the parallel computing literature.\nThe algorithm begins by first examining each vertex and adding the cheapest edge from that vertex to another in the graph, without regard to already added edges, and continues joining these groupings in a like manner until a tree spanning all vertices is completed.","name":"Borůvka's algorithm","categories":["CS1 Czech-language sources (cs)","CS1 French-language sources (fr)","CS1 maint: Unrecognized language","Graph algorithms","Spanning tree"],"tag_line":"Borůvka's algorithm is an algorithm for finding a minimum spanning tree in a graph for which all edge weights are distinct."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dijkstra–scholten-algorithm","_score":0,"_source":{"description":"The Dijkstra–Scholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Dijkstra and Scholten in 1980.\nFirst, let us consider the case of a simple process graph which is a tree. A distributed computation which is tree-structured is not uncommon. Such a process graph may arise when the computation is strictly a divide-and-conquer type. A node starts the computation and divides the problem in two (or more, usually a multiple of 2) roughly equal parts and distribute those parts to other processors. This process continues recursively until the problems are of sufficiently small size to solve in a single processor.","name":"Dijkstra–Scholten algorithm","categories":["Graph algorithms","Termination algorithms"],"tag_line":"The Dijkstra–Scholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"depth-first-search","_score":0,"_source":{"description":"Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. One starts at the root (selecting some arbitrary node as the root in the case of a graph) and explores as far as possible along each branch before backtracking.\nA version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Trémaux as a strategy for solving mazes.","name":"Depth-first search","categories":["All articles needing additional references","Articles containing video clips","Articles needing additional references from July 2010","Articles with example pseudocode","Commons category with local link same as on Wikidata","Graph algorithms","Search algorithms"],"tag_line":"Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures."}}
,{"_index":"throwtable","_type":"algorithm","_id":"edmonds–karp-algorithm","_score":0,"_source":{"description":"In computer science, the Edmonds–Karp algorithm is an implementation of the Ford–Fulkerson method for computing the maximum flow in a flow network in O(V E2) time. The algorithm was first published by Yefim (Chaim) Dinic in 1970 and independently published by Jack Edmonds and Richard Karp in 1972. Dinic's algorithm includes additional techniques that reduce the running time to O(V2E).","name":"Edmonds–Karp algorithm","categories":["Graph algorithms","Network flow"],"tag_line":"In computer science, the Edmonds–Karp algorithm is an implementation of the Ford–Fulkerson method for computing the maximum flow in a flow network in O(V E2) time."}}
,{"_index":"throwtable","_type":"algorithm","_id":"knuth's-simpath-algorithm","_score":0,"_source":{"description":"Simpath is an algorithm introduced by Donald Knuth that constructs a zero-suppressed decision diagram (ZDD) representing all simple paths between two vertices in a given graph.","name":"Knuth's Simpath algorithm","categories":["Algorithms and data structures stubs","All stub articles","Commons category without a link on Wikidata","Computer science stubs","Graph algorithms","Mathematical logic","Theoretical computer science"],"tag_line":"Simpath is an algorithm introduced by Donald Knuth that constructs a zero-suppressed decision diagram (ZDD) representing all simple paths between two vertices in a given graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"iterative-compression","_score":0,"_source":{"description":"Iterative compression is an algorithmic technique invented by Reed, Smith and Vetta to show that the problem Odd Cycle Transversal was solvable in time O(3k kmn). Odd Cycle Transversal was a longstanding central open question in parameterized complexity. This technique later proved very useful in showing fixed-parameter tractability results. It is now considered to be one of the fundamental techniques in the area of parameterized algorithmics.\nIterative compression has been used successfully in many problems, for instance odd cycle transversal (see below) and edge bipartization, feedback vertex set, cluster vertex deletion and directed feedback vertex set. It has also been used successfully for exact exponential time algorithms for independent set.","name":"Iterative compression","categories":["Analysis of algorithms","Computational complexity theory","Graph algorithms","Network flow","Pages using citations with accessdate and no URL","Parameterized complexity"],"tag_line":"Iterative compression is an algorithmic technique invented by Reed, Smith and Vetta to show that the problem Odd Cycle Transversal was solvable in time O(3k kmn)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"karger's-algorithm","_score":0,"_source":{"description":"In computer science and graph theory, Karger's algorithm is a randomized algorithm to compute a minimum cut of a connected graph. It was invented by David Karger and first published in 1993.\nThe idea of the algorithm is based on the concept of contraction of an edge  in an undirected graph . Informally speaking, the contraction of an edge merges the nodes  and  into one, reducing the total number of nodes of the graph by one. All other edges connecting either  or  are \"reattached\" to the merged node, effectively producing a multigraph. Karger's basic algorithm iteratively contracts randomly chosen edges until only two nodes remain; those nodes represent a cut in the original graph. By iterating this basic algorithm a sufficient number of times, a minimum cut can be found with high probability.","name":"Karger's algorithm","categories":["Graph algorithms","Graph connectivity"],"tag_line":"In computer science and graph theory, Karger's algorithm is a randomized algorithm to compute a minimum cut of a connected graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cartesian-tree","_score":0,"_source":{"description":"In computer science, a Cartesian tree is a binary tree derived from a sequence of numbers; it can be uniquely defined from the properties that it is heap-ordered and that a symmetric (in-order) traversal of the tree returns the original sequence. Introduced by Vuillemin (1980) in the context of geometric range searching data structures, Cartesian trees have also been used in the definition of the treap and randomized binary search tree data structures for binary search problems. The Cartesian tree for a sequence may be constructed in linear time using a stack-based algorithm for finding all nearest smaller values in a sequence.","name":"Cartesian tree","categories":["Binary trees","CS1 French-language sources (fr)","Sorting algorithms"],"tag_line":"In computer science, a Cartesian tree is a binary tree derived from a sequence of numbers; it can be uniquely defined from the properties that it is heap-ordered and that a symmetric (in-order) traversal of the tree returns the original sequence."}}
,{"_index":"throwtable","_type":"algorithm","_id":"k-shortest-path-routing","_score":0,"_source":{"description":"The K shortest path routing algorithm is an extension algorithm of the shortest path routing algorithm in a given network.\nIt is sometimes crucial to have more than one path between two nodes in a given network. In the event there are additional constraints, other paths different from the shortest path can be computed. To find the shortest path one can use shortest path algorithms such as Dijkstra’s algorithm or Bellman Ford algorithm and extend them to find more than one path. The K Shortest path routing algorithm is a generalization of the shortest path problem. The algorithm not only finds the shortest path, but also K-1 other paths in order of increasing cost. K is the number of shortest paths to find. The problem can be restricted to have the K shortest path without loops (loopless K shortest path) or with loop.","name":"K shortest path routing","categories":["Computational problems in graph theory","Graph algorithms","Network theory","Polynomial-time problems"],"tag_line":"The K shortest path routing algorithm is an extension algorithm of the shortest path routing algorithm in a given network."}}
,{"_index":"throwtable","_type":"algorithm","_id":"tree-traversal","_score":0,"_source":{"description":"In computer science, tree traversal (also known as tree search) is a form of graph traversal and refers to the process of visiting (examining and/or updating) each node in a tree data structure, exactly once. Such traversals are classified by the order in which the nodes are visited. The following algorithms are described for a binary tree, but they may be generalized to other trees as well.","name":"Tree traversal","categories":["All articles needing additional references","Articles needing additional references from June 2013","Articles needing additional references from May 2009","Articles with example Haskell code","Articles with example Java code","Articles with example pseudocode","Graph algorithms","Iteration in programming","Recursion","Trees (data structures)"],"tag_line":"In computer science, tree traversal (also known as tree search) is a form of graph traversal and refers to the process of visiting (examining and/or updating) each node in a tree data structure, exactly once."}}
,{"_index":"throwtable","_type":"algorithm","_id":"johnson's-algorithm","_score":0,"_source":{"description":"Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in a sparse, edge weighted, directed graph. It allows some of the edge weights to be negative numbers, but no negative-weight cycles may exist. It works by using the Bellman–Ford algorithm to compute a transformation of the input graph that removes all negative weights, allowing Dijkstra's algorithm to be used on the transformed graph. It is named after Donald B. Johnson, who first published the technique in 1977.\nA similar reweighting technique is also used in Suurballe's algorithm for finding two disjoint paths of minimum total length between the same two vertices in a graph with non-negative edge weights.","name":"Johnson's algorithm","categories":["Graph algorithms"],"tag_line":"Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in a sparse, edge weighted, directed graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"kosaraju's-algorithm","_score":0,"_source":{"description":"In computer science, Kosaraju's algorithm (also known as the Kosaraju–Sharir algorithm) is a linear time algorithm to find the strongly connected components of a directed graph. Aho, Hopcroft and Ullman credit it to an unpublished paper from 1978 by S. Rao Kosaraju. The same algorithm was independently discovered by Micha Sharir and published by him in 1981. It makes use of the fact that the transpose graph (the same graph with the direction of every edge reversed) has exactly the same strongly connected components as the original graph.","name":"Kosaraju's algorithm","categories":["Graph algorithms","Graph connectivity"],"tag_line":"In computer science, Kosaraju's algorithm (also known as the Kosaraju–Sharir algorithm) is a linear time algorithm to find the strongly connected components of a directed graph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"kleitman–wang-algorithms","_score":0,"_source":{"description":"The Kleitman–Wang algorithms are two different algorithms in graph theory solving the digraph realization problem, i.e. the question if there exists for a finite list of nonnegative integer pairs a simple directed graph such that its degree sequence is exactly this list. For a positive answer the list of integer pairs is called digraphic. Both algorithms construct a special solution if one exists or prove that one cannot find a positive answer. These constructions are based on recursive algorithms. Kleitman and Wang  gave these algorithms in 1973.","name":"Kleitman–Wang algorithms","categories":["Graph algorithms"],"tag_line":"The Kleitman–Wang algorithms are two different algorithms in graph theory solving the digraph realization problem, i.e."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hierarchical-clustering-of-networks","_score":0,"_source":{"description":"Hierarchical clustering is one method for finding community structures in a network. The technique arranges the network into a hierarchy of groups according to a specified weight function. The data can then be represented in a tree structure known as a dendrogram. Hierarchical clustering can either be agglomerative or divisive depending on whether one proceeds through the algorithm by adding links to or removing links from the network, respectively. One divisive technique is the Girvan–Newman algorithm.","name":"Hierarchical clustering of networks","categories":["Graph algorithms","Network analysis"],"tag_line":"Hierarchical clustering is one method for finding community structures in a network."}}
,{"_index":"throwtable","_type":"algorithm","_id":"radix-sort","_score":0,"_source":{"description":"In computer science, radix sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position and value. A positional notation is required, but because integers can represent strings of characters (e.g., names or dates) and specially formatted floating point numbers, radix sort is not limited to integers. Radix sort dates back as far as 1887 to the work of Herman Hollerith on tabulating machines.\nMost digital computers internally represent all of their data as electronic representations of binary numbers, so processing the digits of integer representations by groups of binary digit representations is most convenient. Two classifications of radix sorts are least significant digit (LSD) radix sorts and most significant digit (MSD) radix sorts. LSD radix sorts process the integer representations starting from the least digit and move towards the most significant digit. MSD radix sorts work the other way around.\nLSD radix sorts typically use the following sorting order: short keys come before longer keys, and keys of the same length are sorted lexicographically. This coincides with the normal order of integer representations, such as the sequence 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11.\nMSD radix sorts use lexicographic order, which is suitable for sorting strings, such as words, or fixed-length integer representations. A sequence such as \"b, c, d, e, f, g, h, i, j, ba\" would be lexicographically sorted as \"b, ba, c, d, e, f, g, h, i, j\". If lexicographic ordering is used to sort variable-length integer representations, then the representations of the numbers from 1 to 10 would be output as 1, 10, 2, 3, 4, 5, 6, 7, 8, 9, as if the shorter keys were left-justified and padded on the right with blank characters to make the shorter keys as long as the longest key for the purpose of determining sorted order.","name":"Radix sort","categories":["Articles with example C code","Sorting algorithms","Stable sorts","String sorting algorithms","Use dmy dates from January 2012"],"tag_line":"In computer science, radix sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position and value."}}
,{"_index":"throwtable","_type":"algorithm","_id":"counting-sort","_score":0,"_source":{"description":"In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small integers; that is, it is an integer sorting algorithm. It operates by counting the number of objects that have each distinct key value, and using arithmetic on those counts to determine the positions of each key value in the output sequence. Its running time is linear in the number of items and the difference between the maximum and minimum key values, so it is only suitable for direct use in situations where the variation in keys is not significantly greater than the number of items. However, it is often used as a subroutine in another sorting algorithm, radix sort, that can handle larger keys more efficiently.\nBecause counting sort uses key values as indexes into an array, it is not a comparison sort, and the Ω(n log n) lower bound for comparison sorting does not apply to it. Bucket sort may be used for many of the same tasks as counting sort, with a similar time analysis; however, compared to counting sort, bucket sort requires linked lists, dynamic arrays or a large amount of preallocated memory to hold the sets of items within each bucket, whereas counting sort instead stores a single number (the count of items) per bucket.","name":"Counting sort","categories":["Sorting algorithms","Stable sorts"],"tag_line":"In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small integers; that is, it is an integer sorting algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"heapsort","_score":0,"_source":{"description":"In computer science, heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.\nAlthough somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(n log n) runtime. Heapsort is an in-place algorithm, but it is not a stable sort.\nHeapsort was invented by J. W. J. Williams in 1964. This was also the birth of the heap, presented already by Williams as a useful data structure in its own right. In the same year, R. W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.","name":"Heapsort","categories":["All articles with unsourced statements","Articles with example pseudocode","Articles with unsourced statements from June 2012","Articles with unsourced statements from September 2014","Comparison sorts","Heaps (data structures)","Sorting algorithms","Use dmy dates from July 2012"],"tag_line":"In computer science, heapsort is a comparison-based sorting algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"proxmap-sort","_score":0,"_source":{"description":"ProxmapSort, or Proxmap sort, is a sorting algorithm that works by partitioning an array of data items, or keys, into a number of \"subarrays\" (termed buckets, in similar sorts). The name is short for computing a \"proximity map,\" which indicates for each key K the beginning of a subarray where K will reside in the final sorted order. Keys are placed into each subarray using insertion sort. If keys are \"well distributed\" among the subarrays, sorting occurs in linear time. The computational complexity estimates involve the number of subarrays and the proximity mapping function, the \"map key,\" used. It is a form of bucket and radix sort.\nOnce a ProxmapSort is complete, ProxmapSearch can be used to find keys in the sorted array in  time if the keys were well distributed during the sort.\nBoth algorithms were invented in the late 1980s by Prof. Thomas A. Standish at the University of California, Irvine.","name":"Proxmap sort","categories":["All articles that may contain original research","All articles with unsourced statements","Articles that may contain original research from May 2015","Articles with example pseudocode","Articles with unsourced statements from May 2015","Sorting algorithms","Stable sorts"],"tag_line":"ProxmapSort, or Proxmap sort, is a sorting algorithm that works by partitioning an array of data items, or keys, into a number of \"subarrays\" (termed buckets, in similar sorts)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cycle-sort","_score":0,"_source":{"description":"Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm. It is based on the idea that the permutation to be sorted can be factored into cycles, which can individually be rotated to give a sorted result.\nUnlike nearly every other sort, items are never written elsewhere in the array simply to push them out of the way of the action. Each value is either written zero times, if it's already in its correct position, or written one time to its correct position. This matches the minimal number of overwrites required for a completed in-place sort.\nMinimizing the number of writes is useful when making writes to some huge data set is very expensive, such as with EEPROMs like Flash memory where each write reduces the lifespan of the memory.","name":"Cycle sort","categories":["Articles with example pseudocode","Comparison sorts","Online sorts","Sorting algorithms"],"tag_line":"Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"comparison-sort","_score":0,"_source":{"description":"A comparison sort is a type of sorting algorithm that only reads the list elements through a single abstract comparison operation (often a \"less than or equal to\" operator or a three-way comparison) that determines which of two elements should occur first in the final sorted list. The only requirement is that the operator obey two of the properties of a total order:\nif a ≤ b and b ≤ c then a ≤ c (transitivity)\nfor all a and b, either a ≤ b or b ≤ a (totalness or trichotomy).\nIt is possible that both a ≤ b and b ≤ a; in this case either may come first in the sorted list. In a stable sort, the input order determines the sorted order in this case.\nA metaphor for thinking about comparison sorts is that someone has a set of unlabelled weights and a balance scale. Their goal is to line up the weights in order by their weight without any information except that obtained by placing two weights on the scale and seeing which one is heavier (or if they weigh the same).","name":"Comparison sort","categories":["Sorting algorithms"],"tag_line":"A comparison sort is a type of sorting algorithm that only reads the list elements through a single abstract comparison operation (often a \"less than or equal to\" operator or a three-way comparison) that determines which of two elements should occur first in the final sorted list."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pancake-sorting","_score":0,"_source":{"description":"Pancake sorting is the colloquial term for the mathematical problem of sorting a disordered stack of pancakes in order of size when a spatula can be inserted at any point in the stack and used to flip all pancakes above it. A pancake number is the maximum number of flips required for a given number of pancakes. In this form, the problem was first discussed by American geometer Jacob E. Goodman. It is a variation of the sorting problem in which the only allowed operation is to reverse the elements of some prefix of the sequence. Unlike a traditional sorting algorithm, which attempts to sort with the fewest comparisons possible, the goal is to sort the sequence in as few reversals as possible. A variant of the problem is concerned with burnt pancakes, where each pancake has a burnt side and all pancakes must, in addition, end up with the burnt side on bottom.","name":"Pancake sorting","categories":["Articles with inconsistent citation formats","Sorting algorithms","Use mdy dates from October 2014"],"tag_line":"Pancake sorting is the colloquial term for the mathematical problem of sorting a disordered stack of pancakes in order of size when a spatula can be inserted at any point in the stack and used to flip all pancakes above it."}}
,{"_index":"throwtable","_type":"algorithm","_id":"odd–even-sort","_score":0,"_source":{"description":"In computing, an odd–even sort or odd–even transposition sort (also known as brick sort) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections. It is a comparison sort related to bubble sort, with which it shares many characteristics. It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, if a pair is in the wrong order (the first is larger than the second) the elements are switched. The next step repeats this for even/odd indexed pairs (of adjacent elements). Then it alternates between odd/even and even/odd steps until the list is sorted.","name":"Odd–even sort","categories":["Accuracy disputes from July 2014","All stub articles","Articles containing proofs","Articles with example pseudocode","Comparison sorts","Computer science stubs","Sorting algorithms","Stable sorts"],"tag_line":"In computing, an odd–even sort or odd–even transposition sort (also known as brick sort) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections."}}
,{"_index":"throwtable","_type":"algorithm","_id":"flashsort","_score":0,"_source":{"description":"Flashsort is a distribution sorting algorithm showing linear computational complexity  for uniformly distributed data sets and relatively little additional memory requirement. The original work was published in 1998 by Karl-Dietrich Neubert.\n\n","name":"Flashsort","categories":["Sorting algorithms"],"tag_line":"Flashsort is a distribution sorting algorithm showing linear computational complexity  for uniformly distributed data sets and relatively little additional memory requirement."}}
,{"_index":"throwtable","_type":"algorithm","_id":"huffman-coding","_score":0,"_source":{"description":"In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Ph.D. student at MIT, and published in the 1952 paper \"A Method for the Construction of Minimum-Redundancy Codes\".\nThe output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in linear time to the number of input weights if these weights are sorted. However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.","name":"Huffman coding","categories":["1952 in computer science","All articles lacking in-text citations","Articles lacking in-text citations from January 2011","Binary trees","Commons category with local link same as on Wikidata","Lossless compression algorithms","Wikipedia articles needing clarification from February 2012"],"tag_line":"In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sort-(c++)","_score":0,"_source":{"description":"sort is a function in C++ Standard Library that takes two random-access iterators, the start and the end, as arguments and performs a comparison sort on the range of elements between the two iterators, front-inclusive and end-exclusive: [start, end).\nThe specific sorting algorithm is not mandated and may vary across implementations. However, the worst-case complexity must be O(n log n). In previous versions of C++, such as C++03, only average complexity was required to be O(n log n). This was to allow the use of algorithms like (median-of-3) quicksort, which are fast in the average case, indeed significantly faster than other algorithms like heap sort with optimal worst-case complexity, and where the worst-case quadratic complexity rarely occurs. The introduction of hybrid algorithms such as introsort allowed both fast average performance and optimal worst-case performance, and thus the complexity requirements were tightened in later standards.\nDifferent implementations use different algorithms. The GNU Standard C++ library, for example, uses a 3-part hybrid sorting algorithm: introsort is performed first (introsort itself being a hybrid of quicksort and heap sort), to a maximum depth given by 2×log2 n, where n is the number of elements, followed by an insertion sort on the result.","name":"Sort (C++)","categories":["C++ Standard Library","Sorting algorithms","Use dmy dates from January 2012"],"tag_line":"sort is a function in C++ Standard Library that takes two random-access iterators, the start and the end, as arguments and performs a comparison sort on the range of elements between the two iterators, front-inclusive and end-exclusive: [start, end)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"samplesort","_score":0,"_source":{"description":"Samplesort is a sorting algorithm that is a divide and conquer algorithm often used in parallel processing systems. Conventional divide and conquer sorting algorithms partitions the array into sub-intervals or buckets. The buckets are then sorted individually and then concatenated together. However, if the array is non-uniformly distributed, the performance of these sorting algorithms can be significantly throttled. Samplesort addresses this issue by selecting a sample of size s from the n-element sequence, and determining the range of the buckets by sorting the sample and choosing m -1 elements from the result. These elements (called splitters) then divide the sample into m equal-sized buckets. Samplesort is described in the 1970 paper, \"Samplesort: A Sampling Approach to Minimal Storage Tree Sorting\", by W D Frazer and A C McKellar. In recent years, the algorithm has been adapted to implement randomized sorting on parallel computers.","name":"Samplesort","categories":["All articles needing expert attention","Articles needing expert attention from April 2009","Articles needing expert attention with no reason or talk parameter","Computer science articles needing expert attention","Sorting algorithms"],"tag_line":"Samplesort is a sorting algorithm that is a divide and conquer algorithm often used in parallel processing systems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"context-adaptive-binary-arithmetic-coding","_score":0,"_source":{"description":"Context-adaptive binary arithmetic coding (CABAC) is a form of entropy encoding used in the H.264/MPEG-4 AVC and High Efficiency Video Coding (HEVC) standards. It is a lossless compression technique, although the video coding standards in which it is used are typically for lossy compression applications. CABAC is notable for providing much better compression than most other entropy encoding algorithms used in video encoding, and it is one of the key elements that provides the H.264/AVC encoding scheme with better compression capability than its predecessors.\nIn H.264/MPEG-4 AVC, CABAC is only supported in the Main and higher profiles of the standard, as it requires a larger amount of processing to decode than the simpler scheme known as context-adaptive variable-length coding (CAVLC) that is used in the standard's Baseline profile. CABAC is also difficult to parallelize and vectorize, so other forms of parallelism (such as spatial region parallelism) may be coupled with its use. In HEVC, CABAC is used in all profiles of the standard.","name":"Context-adaptive binary arithmetic coding","categories":["Lossless compression algorithms","MPEG","Video compression"],"tag_line":"Context-adaptive binary arithmetic coding (CABAC) is a form of entropy encoding used in the H.264/MPEG-4 AVC and High Efficiency Video Coding (HEVC) standards."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lempel–ziv–storer–szymanski","_score":0,"_source":{"description":"Lempel–Ziv–Storer–Szymanski (LZSS) is a lossless data compression algorithm, a derivative of LZ77, that was created in 1982 by James Storer and Thomas Szymanski. LZSS was described in article \"Data compression via textual substitution\" published in Journal of the ACM (pp. 928–951).\nLZSS is a dictionary encoding technique. It attempts to replace a string of symbols with a reference to a dictionary location of the same string.\nThe main difference between LZ77 and LZSS is that in LZ77 the dictionary reference could actually be longer than the string it was replacing. In LZSS, such references are omitted if the length is less than the \"break even\" point. Furthermore, LZSS uses one-bit flags to indicate whether the next chunk of data is a literal (byte) or a reference to an offset/length pair.","name":"Lempel–Ziv–Storer–Szymanski","categories":["Lossless compression algorithms"],"tag_line":"Lempel–Ziv–Storer–Szymanski (LZSS) is a lossless data compression algorithm, a derivative of LZ77, that was created in 1982 by James Storer and Thomas Szymanski."}}
,{"_index":"throwtable","_type":"algorithm","_id":"huffyuv","_score":0,"_source":{"description":"Huffyuv (or HuffYUV) is a lossless video codec created by Ben Rudiak-Gould which is meant to replace uncompressed YCbCr as a video capture format. The codec can also compress in the RGB color space.\n\"Lossless\" means that the output from the decompressor is bit-for-bit identical with the original input to the compressor. Lossless only occurs when the compression color space matches the input and output color space. When the color spaces do not match, a low loss compression is performed.\nHuffyuv's algorithm is similar to that of lossless JPEG, in that it predicts each sample and then Huffman-encodes the error.","name":"Huffyuv","categories":["Free video codecs","Lossless compression algorithms"],"tag_line":"Huffyuv (or HuffYUV) is a lossless video codec created by Ben Rudiak-Gould which is meant to replace uncompressed YCbCr as a video capture format."}}
,{"_index":"throwtable","_type":"algorithm","_id":"geohash-36","_score":0,"_source":{"description":"The Geohash-36 geocode is an opensource compression algorithm for world coordinate data. It was developed as a variation of the OpenPostcode format developed as a candidate geolocation postcode for the Republic of Ireland. It is similar in function to the original public domain Geohash code. It is calculated differently and uses a more accurate base 36 (or rather radix 36) representation rather than the original base 32 representation.","name":"Geohash-36","categories":["All articles needing additional references","Articles needing additional references from June 2012","Compression algorithms","Geocodes"],"tag_line":"The Geohash-36 geocode is an opensource compression algorithm for world coordinate data."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lempel–ziv–oberhumer","_score":0,"_source":{"description":"Lempel–Ziv–Oberhumer (LZO) is a lossless data compression algorithm that is focused on decompression speed.\n\n","name":"Lempel–Ziv–Oberhumer","categories":["All articles lacking reliable references","All articles needing additional references","All stub articles","Articles lacking reliable references from March 2015","Articles needing additional references from July 2014","C libraries","Free data compression software","Lossless compression algorithms","Software stubs"],"tag_line":"Lempel–Ziv–Oberhumer (LZO) is a lossless data compression algorithm that is focused on decompression speed.\n\n"}}
,{"_index":"throwtable","_type":"algorithm","_id":"vocoder","_score":0,"_source":{"description":"A vocoder (/ˈvoʊkoʊdər/, short for voice encoder) is a category of voice codec that analyzes and synthesizes the human voice signal for audio data compression, multiplexing, voice encryption, voice transformation, etc.\nThe earliest type of vocoder, channel vocoder was originally developed as a speech coder for telecommunications applications in the 1930s, the idea being to code a speech for reducing bandwidth (i.e. audio data compression) for multiplexing transmission. On the channel vocoder algorithm, among the two components of analytic signal, only consider the amplitude component and simply ignore the phase component, and it tend to result in the unclear voice. For the improvement of this issue, see phase vocoder.\nIn the encoder, the input is passed through a multiband filter, each band is passed through an envelope follower, and the control signals from the envelope followers are transmitted to the decoder. The decoder applies these (amplitude) control signals to corresponding filters for re–synthesis. Since the control signals change only slowly compared to the original speech waveform, the bandwidth required to transmit speech can be reduced. This allows more speech channels to share the single communication channel such as a radio channel or a submarine cable (i.e. multiplexing).\nBy encrypting the control signals, voice transmission can be secured against interception. Its primary use in this fashion is for secure radio communication. The advantage of this method of encryption is that none of the original signal is sent, but rather envelopes of the bandpass filters. The receiving unit needs to be set up in the same filter configuration to re–synthesise a version of the original signal spectrum.\nThe vocoder has also been used extensively as an electronic musical instrument (see #Uses in music). The decoder portion of the vocoder, called a voder, can be used independently for speech synthesis (see #History).","name":"Vocoder","categories":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from May 2013","Articles with hAudio microformats","Articles with specifically marked weasel-worded phrases from July 2014","Articles with unsourced statements from July 2012","Articles with unsourced statements from July 2014","Articles with unsourced statements from June 2011","Articles with unsourced statements from March 2015","Articles with unsourced statements from October 2008","Audio effects","CS1 German-language sources (de)","Electronic musical instruments","Lossy compression algorithms","Music hardware","Robotics","Speech codecs"],"tag_line":"A vocoder (/ˈvoʊkoʊdər/, short for voice encoder) is a category of voice codec that analyzes and synthesizes the human voice signal for audio data compression, multiplexing, voice encryption, voice transformation, etc."}}
,{"_index":"throwtable","_type":"algorithm","_id":"opus-(audio-format)","_score":0,"_source":{"description":"Opus is a lossy audio coding format developed by the Internet Engineering Task Force (IETF) that is particularly suitable for interactive real-time applications over the Internet.\nOpus incorporates technology from two other audio coding formats: the speech-oriented SILK and the low-latency CELT. Opus can be adjusted seamlessly between high and low bitrates, and internally, it transitions between linear predictive coding at lower bitrates and transform coding at higher bitrates (as well as a hybrid for a short overlap). Opus has a very low algorithmic delay (26.5 ms by default), which is a necessity for use as part of a low audio latency communication link, which can permit natural conversation, networked music performances, or lip sync at live events. Opus permits trading-off quality or bitrate to achieve an even smaller algorithmic delay, down to 5 ms. Its delay is very low compared to well over 100 ms for popular music formats such as MP3, Ogg Vorbis and HE-AAC; yet Opus performs very competitively with these formats in terms of quality per bitrate. Unlike Ogg Vorbis, Opus does not require the definition of large codebooks for each individual file, making it preferable to Vorbis for short clips of audio.\nAs an open format standardized through RFC 6716, a reference implementation audio codec called opus-tools is available under the New BSD License. All known software patents which cover Opus are licensed under royalty-free terms.","name":"Opus (audio format)","categories":["All articles with unsourced statements","Articles with hAudio microformats","Articles with unsourced statements from March 2015","Articles with unsourced statements from May 2014","CS1 Russian-language sources (ru)","CS1 errors: external links","Free audio codecs","Lossy compression algorithms","Software using the BSD license","Speech codecs","Xiph.Org projects"],"tag_line":"Opus is a lossy audio coding format developed by the Internet Engineering Task Force (IETF) that is particularly suitable for interactive real-time applications over the Internet."}}
,{"_index":"throwtable","_type":"algorithm","_id":"zpaq","_score":0,"_source":{"description":"ZPAQ is an open source (GPL) command line archiver for Windows and Linux. It uses a journaling or append-only format which can be rolled back to an earlier state to retrieve older versions of files and directories. It supports fast incremental update by adding only files whose last-modified date has changed since the previous update. It compresses using deduplication and several algorithms (LZ77, BWT, and context mixing) depending on the data type and the selected compression level. To preserve forward and backward compatibility between versions as the compression algorithm is improved, it stores the decompression algorithm in the archive. The ZPAQ source code includes a public domain API, libzpaq, which provides compression and decompression services to C++ applications. The format is believed to be unencumbered by patents.","name":"ZPAQ","categories":["All articles lacking reliable references","All articles with topics of unclear notability","Archive formats","Articles lacking reliable references from October 2013","Articles with topics of unclear notability from October 2013","Free data compression software","Lossless compression algorithms","Open formats","Wikipedia articles with possible conflicts of interest from October 2013"],"tag_line":"ZPAQ is an open source (GPL) command line archiver for Windows and Linux."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lzrw","_score":0,"_source":{"description":"Lempel–Ziv Ross Williams (LZRW) refers to variants of the LZ77 lossless data compression algorithms with an emphasis on improving compression speed through the use of hash tables and other techniques. This family was explored by Ross Williams, who published a series of algorithms beginning with LZRW1 in 1991.\nThe variants are:\nLZRW1\nLZRW1-A\nLZRW2\nLZRW3\nLZRW3-A\nLZRW4\nLZRW5\nThe LZJB algorithm used in ZFS is derived from LZRW1.","name":"LZRW","categories":["All stub articles","Computer science stubs","Free data compression software","Lossless compression algorithms"],"tag_line":"Lempel–Ziv Ross Williams (LZRW) refers to variants of the LZ77 lossless data compression algorithms with an emphasis on improving compression speed through the use of hash tables and other techniques."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quad-(compressor)","_score":0,"_source":{"description":"QUAD is a high-performance data compressor based on the LZ algorithms (LZ77, LZ78, LZW). It's designed to produce small files but still decompress fast and with little memory. QUAD is licensed under the LGPL.","name":"QUAD (compressor)","categories":["All articles lacking reliable references","All articles with topics of unclear notability","All stub articles","Archive formats","Articles lacking reliable references from May 2013","Articles with topics of unclear notability from May 2013","Free data compression software","Lossless compression algorithms","Storage software stubs"],"tag_line":"QUAD is a high-performance data compressor based on the LZ algorithms (LZ77, LZ78, LZW)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lzjb","_score":0,"_source":{"description":"LZJB is a lossless data compression algorithm invented by Jeff Bonwick to compress crash dumps and data in ZFS. It includes a number of improvements to the LZRW1 algorithm, a member of the Lempel–Ziv family of compression algorithms. The name LZJB is derived from its parent algorithm and its creator—Lempel Ziv Jeff Bonwick. Bonwick is also one of two architects of ZFS, and the creator of the Slab Allocator.","name":"LZJB","categories":["All stub articles","Computer science stubs","Lossless compression algorithms","Sun Microsystems software"],"tag_line":"LZJB is a lossless data compression algorithm invented by Jeff Bonwick to compress crash dumps and data in ZFS."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lzwl","_score":0,"_source":{"description":"LZWL is a syllable-based variant of the character-based LZW compression algorithm that can work with syllables obtained by all algorithms of decomposition into syllables. The algorithm can be used for words too.","name":"LZWL","categories":["All articles needing additional references","All articles needing style editing","Articles needing additional references from January 2013","Lossless compression algorithms","Wikipedia articles needing style editing from August 2009"],"tag_line":"LZWL is a syllable-based variant of the character-based LZW compression algorithm that can work with syllables obtained by all algorithms of decomposition into syllables."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lz4-(compression-algorithm)","_score":0,"_source":{"description":"LZ4 is a lossless data compression algorithm that is focused on compression and decompression speed. It belongs to the LZ77 family of byte-oriented compression schemes.\n\n","name":"LZ4 (compression algorithm)","categories":["C libraries","Free data compression software","Lossless compression algorithms"],"tag_line":"LZ4 is a lossless data compression algorithm that is focused on compression and decompression speed."}}
,{"_index":"throwtable","_type":"algorithm","_id":"liu-hui's-π-algorithm","_score":0,"_source":{"description":"Liu Hui's π algorithm was invented by Liu Hui (fl. 3rd century), a mathematician of Wei Kingdom. Before his time, the ratio of the circumference of a circle to diameter was often taken experimentally as three in China, while Zhang Heng (78–139) rendered it as 3.1724 (from the proportion of the celestial circle to the diameter of the earth, 92/29) or as . Liu Hui was not satisfied with this value. He commented that it was too large and overshot the mark. Another mathematician Wan Fan (219–257) provided π ≈ 142/45 ≈ 3.156. All these empirical π values were accurate to two digits (i.e. one decimal place). Liu Hui was the first Chinese mathematician to provide a rigorous algorithm for calculation of π to any accuracy. Liu Hui's own calculation with a 96-gon provided an accuracy of five digits: π ≈ 3.1416.\nLiu Hui remarked in his commentary to the The Nine Chapters on the Mathematical Art, that the ratio of the circumference of an inscribed hexagon to the diameter of the circle was three, hence π must be greater than three. He went on to provide a detailed step-by-step description of an iterative algorithm to calculate π to any required accuracy based on bisecting polygons; he calculated π to between 3.141024 and 3.142708 with a 96-gon; he suggested that 3.14 was a good enough approximation, and expressed π as 157/50; he admitted that this number was a bit small. Later he invented an ingenious quick method to improve on it, and obtained π ≈ 3.1416 with only a 96-gon, with an accuracy comparable to that from a 1536-gon. His most important contribution in this area was his simple iterative π algorithm.","name":"Liu Hui's π algorithm","categories":["All articles that may contain original research","Articles that may contain original research from March 2009","Cao Wei","Chinese mathematics","Pi","Pi algorithms"],"tag_line":"Liu Hui's π algorithm was invented by Liu Hui (fl."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mpir-(mathematics-software)","_score":0,"_source":{"description":"Multiple Precision Integers and Rationals (MPIR) is an open-source software multiprecision integer library forked from the GNU Multiple Precision Arithmetic Library (GMP) project. It consists of much code from past GMP releases, and some original contributed code.\nAccording to the MPIR developers, some of the main goals of the MPIR project are:\nDeveloping parallel algorithms for multiprecision arithmetic including support for graphics processing units (GPU) and other multi-core processors.\nMaintaining compatibility with GMP - so that MPIR can be used as a replacement for GMP.\nProviding build support for Linux, Mac OS, Solaris and Windows systems.\nSupporting building MPIR using Microsoft based build tools for use in 32- and 64-bit versions of Windows.\nMPIR is optimised for many processors (CPUs). Assembly language code exists for these as of 2012: ARM, DEC Alpha 21064, 21164, and 21264, AMD K6, K6-2, Athlon, K8 and K10, Intel Pentium, Pentium Pro-II-III, Pentium 4, generic x86, Intel IA-64, Core 2, i7, Atom, Motorola-IBM PowerPC 32 and 64, MIPS R3000, R4000, SPARCv7, SuperSPARC, generic SPARCv8, UltraSPARC.","name":"MPIR (mathematics software)","categories":["All articles containing potentially dated statements","All articles with unsourced statements","Articles containing potentially dated statements from 2012","Articles with unsourced statements from April 2014","C libraries","Computer arithmetic","Computer arithmetic algorithms","Free software programmed in C","Numerical software"],"tag_line":"Multiple Precision Integers and Rationals (MPIR) is an open-source software multiprecision integer library forked from the GNU Multiple Precision Arithmetic Library (GMP) project."}}
,{"_index":"throwtable","_type":"algorithm","_id":"division-algorithm","_score":0,"_source":{"description":"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.\nDivision algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.\nDiscussion will refer to the form , where\nN = Numerator (dividend)\nD = Denominator (divisor)\nis the input, and\nQ = Quotient\nR = Remainder\nis the output.\n\n","name":"Division algorithm","categories":["All articles to be expanded","All articles with unsourced statements","All pages needing factual verification","Articles to be expanded from September 2012","Articles with example pseudocode","Articles with unsourced statements from February 2012","Articles with unsourced statements from February 2014","Binary arithmetic","Computer arithmetic","Computer arithmetic algorithms","Division (mathematics)","Wikipedia articles needing clarification from July 2015","Wikipedia articles needing factual verification from June 2015"],"tag_line":"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dekker's-algorithm","_score":0,"_source":{"description":"Dekker's algorithm is the first known correct solution to the mutual exclusion problem in concurrent programming. The solution is attributed to Dutch mathematician Th. J. Dekker by Edsger W. Dijkstra in an unpublished paper on sequential process descriptions and his manuscript on cooperating sequential processes. It allows two threads to share a single-use resource without conflict, using only shared memory for communication.\nIt avoids the strict alternation of a naïve turn-taking algorithm, and was one of the first mutual exclusion algorithms to be invented.\n^ Dijkstra, Edsger W. Over de sequentialiteit van procesbeschrijvingen (EWD-35). E.W. Dijkstra Archive. Center for American History, University of Texas at Austin.  (original; transcription) (undated, 1962 or 1963); English translation About the sequentiality of process descriptions\n^ Dijkstra, Edsger W. Cooperating sequential processes (EWD-123). E.W. Dijkstra Archive. Center for American History, University of Texas at Austin.  (original; transcription) (September 1965)","name":"Dekker's algorithm","categories":["All articles needing additional references","Articles needing additional references from May 2015","Concurrency control algorithms","Dutch inventions"],"tag_line":"Dekker's algorithm is the first known correct solution to the mutual exclusion problem in concurrent programming."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quote-notation","_score":0,"_source":{"description":"Quote notation is a number system for representing rational numbers which was designed to be attractive for use in computer architecture. In a typical computer architecture, the representation and manipulation of rational numbers is a complex topic. In quote notation, arithmetic operations take particularly simple, consistent forms, and can produce exact answers with no roundoff error.\nQuote notation’s arithmetic algorithms work with a typical right-to-left direction, in which the addition, subtraction, and multiplication algorithms have the same complexity for natural numbers, and division is easier than a typical division algorithm.\nThe notation was invented by Eric Hehner of the University of Toronto and Nigel Horspool, then at McGill University, and published in the SIAM Journal on Computing, v.8, n.2, May 1979, pp. 124–134. The construction of this system follows the approach of Kurt Hensel's p-adic numbers.","name":"Quote notation","categories":["CS1 errors: external links","Computer arithmetic algorithms"],"tag_line":"Quote notation is a number system for representing rational numbers which was designed to be attractive for use in computer architecture."}}
,{"_index":"throwtable","_type":"algorithm","_id":"szymański's-algorithm","_score":0,"_source":{"description":"Szymanski's Mutual Exclusion Algorithm is a mutual exclusion algorithm devised by computer scientist Dr. Boleslaw Szymanski, which has many favorable properties including linear wait, and which extension  solved the open problem posted by Leslie Lamport whether there is an algorithm with a constant number of communication bits per process that satisfies every reasonable fairness and failure-tolerance requirement that Lamport conceived of (Lamport's solution used n factorial communication variables vs. Szymanski's 5).","name":"Szymański's algorithm","categories":["All stub articles","Computer science stubs","Concurrency control algorithms"],"tag_line":"Szymanski's Mutual Exclusion Algorithm is a mutual exclusion algorithm devised by computer scientist Dr. Boleslaw Szymanski, which has many favorable properties including linear wait, and which extension  solved the open problem posted by Leslie Lamport whether there is an algorithm with a constant number of communication bits per process that satisfies every reasonable fairness and failure-tolerance requirement that Lamport conceived of (Lamport's solution used n factorial communication variables vs. Szymanski's 5)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"karatsuba-algorithm","_score":0,"_source":{"description":"The Karatsuba algorithm is a fast multiplication algorithm. It was discovered by Anatoly Karatsuba in 1960 and published in 1962. It reduces the multiplication of two n-digit numbers to at most  single-digit multiplications in general (and exactly  when n is a power of 2). It is therefore faster than the classical algorithm, which requires n2 single-digit products. For example, the Karatsuba algorithm requires 310 = 59,049 single-digit multiplications to multiply two 1024-digit numbers (n = 1024 = 210), whereas the classical algorithm requires (210)2 = 1,048,576.\nThe Karatsuba algorithm was the first multiplication algorithm asymptotically faster than the quadratic \"grade school\" algorithm. The Toom–Cook algorithm is a faster generalization of Karatsuba's method, and the Schönhage–Strassen algorithm is even faster, for sufficiently large n.","name":"Karatsuba algorithm","categories":["Computer arithmetic algorithms","Multiplication","Pages with syntax highlighting errors"],"tag_line":"The Karatsuba algorithm is a fast multiplication algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"borwein's-algorithm","_score":0,"_source":{"description":"In mathematics, Borwein's algorithm is an algorithm devised by Jonathan and Peter Borwein to calculate the value of 1/π. They devised several other algorithms. They published a book: Jonathon M. Borwein, Peter B. Borwein, Pi and the AGM - A Study in Analytic Number Theory and Computational Complexity, Wiley, New York, 1987. Many of their results are available in: Jorg Arndt, Christoph Haenel, Pi Unleashed, Springer, Berlin, 2001, ISBN 3-540-66572-2.","name":"Borwein's algorithm","categories":["All articles with unsourced statements","Articles with unsourced statements from June 2011","Pi algorithms"],"tag_line":"In mathematics, Borwein's algorithm is an algorithm devised by Jonathan and Peter Borwein to calculate the value of 1/π."}}
,{"_index":"throwtable","_type":"algorithm","_id":"s-box","_score":0,"_source":{"description":"In cryptography, an S-box (substitution-box) is a basic component of symmetric key algorithms which performs substitution. In block ciphers, they are typically used to obscure the relationship between the key and the ciphertext — Shannon's property of confusion.\nIn general, an S-box takes some number of input bits, m, and transforms them into some number of output bits, n, where n is not necessarily equal to m. An m×n S-box can be implemented as a lookup table with 2m words of n bits each. Fixed tables are normally used, as in the Data Encryption Standard (DES), but in some ciphers the tables are generated dynamically from the key (e.g. the Blowfish and the Twofish encryption algorithms).\nOne good example of a fixed table is the S-box from DES (S5), mapping 6-bit input into a 4-bit output:\nGiven a 6-bit input, the 4-bit output is found by selecting the row using the outer two bits (the first and last bits), and the column using the inner four bits. For example, an input \"011011\" has outer bits \"01\" and inner bits \"1101\"; the corresponding output would be \"1001\".\nThe 8 S-boxes of DES were the subject of intense study for many years out of a concern that a backdoor — a vulnerability known only to its designers — might have been planted in the cipher. The S-box design criteria were eventually published (in Coppersmith 1994) after the public rediscovery of differential cryptanalysis, showing that they had been carefully tuned to increase resistance against this specific attack. Biham and Shamir found that even small modifications to an S-box could significantly weaken DES.\nThere has been a great deal of research into the design of good S-boxes, and much more is understood about their use in block ciphers than when DES was released.\nAny S-box where each output bit is produced by a bent function of the input bits, and where any linear combination of the output bits is also a bent function of the input bits, is a perfect S-box.\n^ Chandrasekaran, J. et al. (2011). \"A Chaos Based Approach for Improving Non Linearity in the S-Box Design of Symmetric Key Cryptosystems\". In Meghanathan, N. et al. Advances in Networks and Communications: First International Conference on Computer Science and Information Technology, CCSIT 2011, Bangalore, India, January 2-4, 2011. Proceedings, Part 2. Springer. p. 516. ISBN 978-3-642-17877-1.  CS1 maint: Explicit use of et al. (link)\n^ Buchmann, Johannes A. (2001). \"5. DES\". Introduction to cryptography (Corr. 2. print. ed.). New York, NY [u.a.]: Springer. pp. 119–120. ISBN 0-387-95034-6. \n^ Gargiulo's \"S-Box Modifications and Their Effect in DES-like Encryption Systems\" p. 9.\n^ RFC 4086. Section 5.3 \"Using S-Boxes for Mixing\"","name":"S-box","categories":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from March 2009","Articles with unsourced statements from April 2012","CS1 maint: Explicit use of et al.","Cryptographic algorithms","S-box"],"tag_line":"In cryptography, an S-box (substitution-box) is a basic component of symmetric key algorithms which performs substitution."}}
,{"_index":"throwtable","_type":"algorithm","_id":"kochanski-multiplication","_score":0,"_source":{"description":"Kochanski multiplication is an algorithm that allows modular arithmetic (multiplication or operations based on it, such as exponentiation) to be performed efficiently when the modulus is large (typically several hundred bits). This has particular application in number theory and in cryptography: for example, in the RSA cryptosystem and Diffie-Hellman key exchange.\nThe most common way of implementing large-integer multiplication in hardware is to express the multiplier in binary and enumerate its bits, one bit at a time, starting with the most significant bit, perform the following operations on an accumulator:\nDouble the contents of the accumulator (if the accumulator stores numbers in binary, as is usually the case, this is a simple \"shift left\" that requires no actual computation).\nIf the current bit of the multiplier is 1, add the multiplicand into the accumulator; if it is 0, do nothing.\nFor an n-bit multiplier, this will take n clock cycles (where each cycle does either a shift or a shift-and-add).\nTo convert this into an algorithm for modular multiplication, with a modulus r, it is necessary to subtract r conditionally at each stage:\nDouble the contents of the accumulator.\nIf the result is greater than or equal to r, subtract r. (Equivalently, subtract r from the accumulator and store the result back into the accumulator if and only if it is non-negative).\nIf the current bit of the multiplier is 1, add the multiplicand into the accumulator; if it is 0, do nothing.\nIf the result of the addition is greater than or equal to r, subtract r. If no addition took place, do nothing.\nThis algorithm works. However, it is critically dependent on the speed of addition.\nAddition of long integers suffers from the problem that carries have to be propagated from right to left and the final result is not known until this process has been completed. Carry propagation can be speeded up with carry look-ahead logic, but this still makes addition very much slower than it needs to be (for 512-bit addition, addition with carry look-ahead is 32 times slower than addition without carries at all).\nNon-modular multiplication can make use of carry-save adders, which save time by storing the carries from each digit position and using them later: for example, by computing 111111111111+000000000010 as 111111111121 instead waiting for the carry to propagate through the whole number to yield the true binary value 1000000000001. That final propagation still has to be done to yield a binary result but this only needs to be done once at the very end of the multiplication.\nUnfortunately the modular multiplication method outlined above needs to know the magnitude of the accumulated value at every step, in order to decide whether to subtract r: for example, if it needs to know whether the value in the accumulator is greater than 1000000000000, the carry-save representation 111111111121 is useless and needs to be converted to its true binary value for the comparison to be made.\nIt therefore seems that one can have either the speed of carry-save or modular multiplication, but not both.","name":"Kochanski multiplication","categories":["Cryptographic algorithms","Modular arithmetic"],"tag_line":"Kochanski multiplication is an algorithm that allows modular arithmetic (multiplication or operations based on it, such as exponentiation) to be performed efficiently when the modulus is large (typically several hundred bits)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"crypt-(c)","_score":0,"_source":{"description":"crypt is the library function which is used to compute a password hash that can be used to store user account passwords while keeping them relatively secure (a passwd file). The output of the function is not simply the hash— it is a text string which also encodes the salt (usually the first two characters are the salt itself and the rest is the hashed result), and identifies the hash algorithm used (defaulting to the \"traditional\" one explained below). This output string is what is meant for putting in a password record which may be stored in a plain text file.\nMore formally, crypt provides cryptographic key derivation functions for password validation and storage on Unix systems.","name":"Crypt (C)","categories":["All articles with unsourced statements","Articles with unsourced statements from April 2010","Articles with unsourced statements from July 2011","Broken cryptography algorithms","Computer access control protocols","Cryptographic hash functions","Key derivation functions","Password authentication"],"tag_line":"crypt is the library function which is used to compute a password hash that can be used to store user account passwords while keeping them relatively secure (a passwd file)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hs-algorithm","_score":0,"_source":{"description":"The HS Algorithm is named after Dan Hirschberg and J. B. Sinclair. It is a distributed algorithm designed for the Leader Election problem in a Synchronous Ring.\nThe algorithm requires the use of unique IDs (UID) for each process. The algorithm works in phases and sends its UID out in both directions. The message goes out a distance of 2Phase Number hops and then the message heads back to the originating process. While the messages are heading \"out\" each receiving process will compare the incoming UID to its own. If the UID is greater than its own UID then it will continue the message on. Otherwise if the UID is less than its own UID, it will not pass the information on. At the end of a phase, a process can determine if it will send out messages in the next round by if it received both of its incoming messages. Phases continue until a process receives both of its out messages, from both of its neighbors. At this time the process knows it is the largest UID in the ring and declares itself the leader.","name":"HS algorithm","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Distributed algorithms"],"tag_line":"The HS Algorithm is named after Dan Hirschberg and J."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cannon's-algorithm","_score":0,"_source":{"description":"In computer science, Cannon's algorithm is a distributed algorithm for matrix multiplication for two-dimensional meshes first described in 1969 by Lynn Elliot Cannon.\nIt is especially suitable for computers laid out in an N × N mesh. While Cannon's algorithm works well in homogeneous 2D grids, extending it to heterogeneous 2D grids has been shown to be difficult.\nThe main advantage of the algorithm is that its storage requirements remain constant and are independent of the number of processors.\nThe Scalable Universal Matrix Multiplication Algorithm (SUMMA) is a more practical algorithm that requires less workspace and overcomes the need for a square 2D grid. It is used by the ScaLAPACK, PLAPACK, and Elemental libraries.","name":"Cannon's algorithm","categories":["All stub articles","Applied mathematics stubs","Distributed algorithms","Matrix multiplication algorithms"],"tag_line":"In computer science, Cannon's algorithm is a distributed algorithm for matrix multiplication for two-dimensional meshes first described in 1969 by Lynn Elliot Cannon."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cdmf","_score":0,"_source":{"description":"In cryptography, CDMF (Commercial Data Masking Facility) is an algorithm developed at IBM in 1992 to reduce the security strength of the 56-bit DES cipher to that of 40-bit encryption, at the time a requirement of U.S. restrictions on export of cryptography. Rather than a separate cipher from DES, CDMF constitutes a key generation algorithm, called key shortening. It is one of the cryptographic algorithms supported by S-HTTP.","name":"CDMF","categories":["All stub articles","Block ciphers","Cryptographic algorithms","Cryptography stubs","Data Encryption Standard","Key management"],"tag_line":"In cryptography, CDMF (Commercial Data Masking Facility) is an algorithm developed at IBM in 1992 to reduce the security strength of the 56-bit DES cipher to that of 40-bit encryption, at the time a requirement of U.S. restrictions on export of cryptography."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bach's-algorithm","_score":0,"_source":{"description":"Bach's algorithm is a probabilistic polynomial time algorithm for generating random numbers along with their factorization, named after its discoverer, Eric Bach. It is of interest because no algorithm is known that efficiently factors numbers, so the straightforward method, namely generating a random number and then factoring it, is impractical.\nThe algorithm performs, in expectation, O(log n) primality tests.\nA simpler, but less efficient algorithm (performing, in expectation, O(log2 n) primality tests), is known and is due to Adam Kalai","name":"Bach's algorithm","categories":["Cryptographic algorithms"],"tag_line":"Bach's algorithm is a probabilistic polynomial time algorithm for generating random numbers along with their factorization, named after its discoverer, Eric Bach."}}
,{"_index":"throwtable","_type":"algorithm","_id":"common-scrambling-algorithm","_score":0,"_source":{"description":"The Common Scrambling Algorithm (or CSA) is the encryption algorithm used in the DVB digital television broadcasting for encrypting video streams.\nCSA was specified by ETSI and adopted by the DVB consortium in May 1994. It is being succeeded by CSA3, based on a combination of 128-bit AES and a confidential block cipher, XRC. However, CSA3 is not yet in any significant use, so CSA continues to be the dominant cipher for protecting DVB broadcasts.","name":"Common Scrambling Algorithm","categories":["Cryptographic algorithms","Digital Video Broadcasting"],"tag_line":"The Common Scrambling Algorithm (or CSA) is the encryption algorithm used in the DVB digital television broadcasting for encrypting video streams."}}
,{"_index":"throwtable","_type":"algorithm","_id":"information-theoretic-security","_score":0,"_source":{"description":"A cryptosystem is information-theoretically secure if its security derives purely from information theory. That is, it cannot be broken even when the adversary has unlimited computing power. The adversary simply does not have enough information to break the encryption, so these cryptosystems are considered cryptanalytically unbreakable.\nAn encryption protocol that has information-theoretic security does not depend for its effectiveness on unproven assumptions about computational hardness, and such an algorithm is not vulnerable to future developments in computer power such as quantum computing. An example of an information-theoretically secure cryptosystem is the one-time pad. The concept of information-theoretically secure communication was introduced in 1949 by American mathematician Claude Shannon, the inventor of information theory, who used it to prove that the one-time pad system was secure. Information-theoretically secure cryptosystems have been used for the most sensitive governmental communications, such as diplomatic cables and high-level military communications, because of the great efforts enemy governments expend toward breaking them.\nAn interesting special case is perfect security: an encryption algorithm is perfectly secure if a ciphertext produced using it provides no information about the plaintext without knowledge of the key. If E is a perfectly secure encryption function, for any fixed message m there must exist for each ciphertext c at least one key k such that . It has been proved that any cipher with the perfect secrecy property must use keys with effectively the same requirements as one-time pad keys.\nIt is common for a cryptosystem to leak some information but nevertheless maintain its security properties even against an adversary that has unlimited computational resources. Such a cryptosystem would have information theoretic but not perfect security. The exact definition of security would depend on the cryptosystem in question.\nThere are a variety of cryptographic tasks for which information-theoretic security is a meaningful and useful requirement. A few of these are:\nSecret sharing schemes such as Shamir's are information-theoretically secure (and also perfectly secure) in that less than the requisite number of shares of the secret provide no information about the secret.\nMore generally, secure multiparty computation protocols often, but not always, have information-theoretic security.\nPrivate information retrieval with multiple databases can be achieved with information-theoretic privacy for the user's query.\nReductions between cryptographic primitives or tasks can often be achieved information-theoretically. Such reductions are important from a theoretical perspective, because they establish that primitive  can be realized if primitive  can be realized.\nSymmetric encryption can be constructed under an information-theoretic notion of security called entropic security, which assumes that the adversary knows almost nothing about the message being sent. The goal here is to hide all functions of the plaintext rather than all information about it.\nQuantum cryptography is largely part of information-theoretic cryptography.\nConventional secrecy entails encrypting messages. Beyond this, some scenarios require covert communication, a stronger type of secrecy which also hides the fact that communication is happening at all.","name":"Information-theoretic security","categories":["Information-theoretically secure algorithms","Theory of cryptography"],"tag_line":"A cryptosystem is information-theoretically secure if its security derives purely from information theory."}}
,{"_index":"throwtable","_type":"algorithm","_id":"algorithm","_score":0,"_source":{"description":"In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ AL-gə-ri-dhəm) is a self-contained step-by-step set of operations to be performed. Algorithms exist that perform calculation, data processing, and automated reasoning.\nAn algorithm is an effective method that can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\nThe concept of algorithm has existed for centuries, however a partial formalization of what would become the modern algorithm began with attempts to solve the Entscheidungsproblem (the \"decision problem\") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define \"effective calculability\" or \"effective method\"; those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's \"Formulation 1\" of 1936, and Alan Turing's Turing machines of 1936–7 and 1939. Giving a formal definition of algorithms, corresponding to the intuitive notion, remains a challenging problem.","name":"Algorithm","categories":["Algorithms","Articles containing Persian-language text","Articles including recorded pronunciations","Articles with DMOZ links","Articles with example pseudocode","Mathematical logic","Pages using duplicate arguments in template calls","Theoretical computer science","Use mdy dates from June 2013","Wikipedia articles with GND identifiers"],"tag_line":"In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ AL-gə-ri-dhəm) is a self-contained step-by-step set of operations to be performed."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adaptive-dimensional-search","_score":0,"_source":{"description":"Adaptive dimensional search algorithms differ from nature-inspired metaheuristic techniques in the sense that they do not use any metaphor as an underlying principle for implementation. Rather, they utilize a simple, performance-oriented methodology based on the update of the search dimensionality ratio (SDR) parameter at each iteration.\nMany robust metaheuristic techniques, such as simulated annealing, evolutionary algorithms, particle swarm optimization, and ant colony optimization, have been introduced by researchers in the last few decades through clearly identifying and formulating similarities between algorithms and the processes they are modeled on. However, over time this trend of developing new search methods has made researchers feel obligated to associate their innovative ideas with some natural event to provide a basis for justification of their thoughts and the originality of their algorithms. As a result, literature has abounded with metaheuristic algorithms that have weak or no similarities to the natural processes which they are purported to derive from.","name":"Adaptive dimensional search","categories":["Algorithms","All articles needing expert attention","All articles that are too technical","Articles needing expert attention from October 2015","Computer science","Mathematical optimization","Optimization algorithms and methods","Wikipedia articles that are too technical from October 2015"],"tag_line":"Adaptive dimensional search algorithms differ from nature-inspired metaheuristic techniques in the sense that they do not use any metaphor as an underlying principle for implementation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"british-museum-algorithm","_score":0,"_source":{"description":"The British Museum algorithm is a general approach to find a solution by checking all possibilities one by one, beginning with the smallest. The term refers to a conceptual, not a practical, technique where the number of possibilities is enormous.\nFor instance, one may, in theory, find the smallest program that solves a particular problem in the following way: Generate all possible source codes of length one character. Check each one to see if it solves the problem. (Note: the halting problem makes this check troublesome.) If not, generate and check all programs of two characters, three characters, etc. Conceptually, this finds the smallest program, but in practice it tends to take an unacceptable amount of time (more than the lifetime of the universe, in many instances).\nSimilar arguments can be made to show that optimizations, theorem proving, language recognition, etc. are possible or impossible.\nNewell, Shaw, and Simon called this procedure the British Museum algorithm\n\"... since it seemed to them as sensible as placing monkeys in front of typewriters in order to reproduce all the books in the British Museum.\"","name":"British Museum algorithm","categories":["Algorithms","All articles with unsourced statements","Articles with unsourced statements from August 2013","British Museum","Use British English from August 2015","Use dmy dates from August 2015"],"tag_line":"The British Museum algorithm is a general approach to find a solution by checking all possibilities one by one, beginning with the smallest."}}
,{"_index":"throwtable","_type":"algorithm","_id":"holographic-algorithm","_score":0,"_source":{"description":"In computer science, a holographic algorithm is an algorithm that uses a holographic reduction. A holographic reduction is a constant-time reduction that maps solution fragments many-to-many such that the sum of the solution fragments remains unchanged. These concepts were introduced by Leslie Valiant, who called them holographic because \"their effect can be viewed as that of producing interference patterns among the solution fragments\". The algorithms are unrelated to laser holography, except metaphorically. Their power comes from the mutual cancellation of many contributions to a sum, analogous to the interference patterns in a hologram.\nHolographic algorithms have been used to find polynomial-time solutions to problems without such previously known solutions for special cases of satisfiability, vertex cover, and other graph problems. They have received notable coverage due to speculation that they are relevant to the P versus NP problem and their impact on computational complexity theory. Although some of the general problems are #P-hard problems, the special cases solved are not themselves #P-hard, and thus do not prove FP = #P.\nHolographic algorithms have some similarities with quantum computation, but are completely classical.","name":"Holographic algorithm","categories":["Algorithms"],"tag_line":"In computer science, a holographic algorithm is an algorithm that uses a holographic reduction."}}
,{"_index":"throwtable","_type":"algorithm","_id":"label-propagation-algorithm","_score":0,"_source":{"description":"Within complex networks, real networks tend to have community structure. Label propagation is an algorithm  for finding communities. In comparison with other algorithms label propagation has advantage in its running time, amount of a priori information needed about the network structure (no parameter is required to be known beforehand). Disadvantage is that it produces no unique solution, but an aggregate of many solutions.\n\n","name":"Label Propagation Algorithm","categories":["Algorithms","All articles needing expert attention","All articles that are too technical","All orphaned articles","Articles needing expert attention from June 2015","Networks","Orphaned articles from June 2015","Wikipedia articles that are too technical from June 2015"],"tag_line":"Within complex networks, real networks tend to have community structure."}}
,{"_index":"throwtable","_type":"algorithm","_id":"algorithm-engineering","_score":0,"_source":{"description":"Algorithm Engineering focuses on the design, analysis, implementation, optimization, profiling and experimental evaluation of computer algorithms, bridging the gap between algorithm theory and practical applications of algorithms in software engineering. It is a general methodology for algorithmic research.","name":"Algorithm engineering","categories":["Algorithms","Theoretical computer science"],"tag_line":"Algorithm Engineering focuses on the design, analysis, implementation, optimization, profiling and experimental evaluation of computer algorithms, bridging the gap between algorithm theory and practical applications of algorithms in software engineering."}}
,{"_index":"throwtable","_type":"algorithm","_id":"kleene's-algorithm","_score":0,"_source":{"description":"In theoretical computer science, in particular in formal language theory, Kleene's algorithm transforms a given deterministic finite automaton (DFA) into a regular expression. Together with other conversion algorithms, it establishes the equivalence of several description formats for regular languages.","name":"Kleene's algorithm","categories":["Algorithms","Finite automata","Regular expressions"],"tag_line":"In theoretical computer science, in particular in formal language theory, Kleene's algorithm transforms a given deterministic finite automaton (DFA) into a regular expression."}}
,{"_index":"throwtable","_type":"algorithm","_id":"grey-wolf-optimizer","_score":0,"_source":{"description":"The Grey Wolf Optimizer (GWO) is a recently proposed swarm-based meta-heuristic. This algorithm mimics the social leadership and hunting behaviour of gray wolves in nature. The main phases of hunt in a pack of wolves have been mathematically modeled to solve optimization problems.","name":"Grey wolf optimizer","categories":["Algorithms"],"tag_line":"The Grey Wolf Optimizer (GWO) is a recently proposed swarm-based meta-heuristic."}}
,{"_index":"throwtable","_type":"algorithm","_id":"maze-generation-algorithm","_score":0,"_source":{"description":"Maze generation algorithms are automated methods for the creation of mazes.","name":"Maze generation algorithm","categories":["Algorithms","All articles needing additional references","Articles containing video clips","Articles needing additional references from August 2012","Articles with example Python code","Mazes","Random graphs","Wikipedia articles needing clarification from October 2014"],"tag_line":"Maze generation algorithms are automated methods for the creation of mazes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rna22","_score":0,"_source":{"description":"Rna22 is a pattern-based algorithm for the discovery of microRNA target sites and the corresponding heteroduplexes.\nThe algorithm is conceptually distinct from other methods for predicting microRNA:mRNA heteroduplexes in that it does not use experimentally validated heteroduplexes for training, instead relying only on the sequences of known mature miRNAs that are found in the public databases. The key idea of rna22 is that the reverse complement of any salient sequence features that one can identify in mature microRNA sequences (using pattern discovery techniques) should allow one to identify candidate microRNA target sites in a sequence of interest: rna22 makes use of the Teiresias algorithm to discover such salient features. Once a candidate microRNA target site has been located, the targeting microRNA can be identified with the help of any of several algorithms able to compute RNA:RNA heteroduplexes. A new version (v2.0) of the algorithm is now available: v2.0-beta adds probability estimates to each prediction, gives users the ability to choose the sensitivity/specificity settings on-the-fly, is significantly faster than the original, and can be accessed through http://cm.jefferson.edu/rna22/Interactive/.\nRna22 neither relies on nor imposes any cross-organism conservation constraints to filter out unlikely candidates; this gives it the ability to discover microRNA binding sites that may not be conserved in phylogenetically proximal organisms. Also, as mentioned above, rna22 can identify putative microRNA binding sites without needing to know the identity of the targeting microRNA. A notable property of rna22 is that it does not require the presence of the exact reverse complement of a microRNA's seed in a putative target permitting bulges and G:U wobbles in the seed region of the heteroduplex. Lastly, the algorithm has been shown to achieve high signal-to-noise ratio.\nUse of rna22 led to the discovery of \"non-canonical\" microRNA targets in the coding regions of the mouse Nanog, Oct4 and Sox2. Most of these targets are not conserved in the human orthologues of these three transcription factors even though they reside in the coding region of the corresponding mRNAs. Moreover, most of these targets contain G:U wobbles, one or more bulges, or both, in the seed region of the heteroduplex. In addition to coding regions, rna22 has helped discover non-canonical targets in 3'UTRs.\nA recent study examined the problem of non-canonical miRNA targets using molecular dynamics simulations of the crystal structure of the Argonaute-miRNA:mRNA ternary complex. The study found that several kinds of modifications, including combinations of multiple G:U wobbles and mismatches in the seed region, are admissible and result in only minor structural fluctuations that do not affect the stability of the ternary complex. The study also showed that the findings of the molecular dynamics simulation are supported by HITS-CLIP (CLIP-seq) data. These results suggest that bona fide miRNA targets transcend the canonical seed-model in turn making target prediction tools like rna22 an ideal choice for exploring the newly augmented spectrum of miRNA targets.","name":"RNA22","categories":["Algorithms","MicroRNA","Pattern matching","RNA"],"tag_line":"Rna22 is a pattern-based algorithm for the discovery of microRNA target sites and the corresponding heteroduplexes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"nearest-neighbour-algorithm","_score":0,"_source":{"description":"The nearest neighbour algorithm was one of the first algorithms used to determine a solution to the travelling salesman problem. In it, the salesman starts at a random city and repeatedly visits the nearest city until all have been visited. It quickly yields a short tour, but usually not the optimal one.\nBelow is the application of nearest neighbour algorithm on TSP\nThese are the steps of the algorithm:\nstart on an arbitrary vertex as current vertex.\nfind out the shortest edge connecting current vertex and an unvisited vertex V.\nset current vertex to V.\nmark V as visited.\nif all the vertices in domain are visited, then terminate.\nGo to step 2.\nThe sequence of the visited vertices is the output of the algorithm.\nThe nearest neighbour algorithm is easy to implement and executes quickly, but it can sometimes miss shorter routes which are easily noticed with human insight, due to its \"greedy\" nature. As a general guide, if the last few stages of the tour are comparable in length to the first stages, then the tour is reasonable; if they are much greater, then it is likely that there are much better tours. Another check is to use an algorithm such as the lower bound algorithm to estimate if this tour is good enough.\nIn the worst case, the algorithm results in a tour that is much longer than the optimal tour. To be precise, for every constant r there is an instance of the traveling salesman problem such that the length of the tour computed by the nearest neighbour algorithm is greater than r times the length of the optimal tour. Moreover, for each number of cities there is an assignment of distances between the cities for which the nearest neighbor heuristic produces the unique worst possible tour.\nThe nearest neighbour algorithm may not find a feasible tour at all, even when one exists.","name":"Nearest neighbour algorithm","categories":["Approximation algorithms","Graph algorithms","Heuristic algorithms","Travelling salesman problem"],"tag_line":"The nearest neighbour algorithm was one of the first algorithms used to determine a solution to the travelling salesman problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"submodular-set-function","_score":0,"_source":{"description":"In mathematics, a submodular set function (also known as a submodular function) is a set function whose value, informally, has the property that the difference in the incremental value of the function, that a single element makes when added to an input set , decreases as the size of the input set increases. Submodular functions have a natural diminishing returns property which makes them suitable for many applications, including approximation algorithms, game theory (as functions modeling user preferences) and electrical networks. Recently, submodular functions have also found immense utility in several real world problems in machine learning and artificial intelligence, including automatic summarization, multi-document summarization, feature selection, active learning, sensor placement, image collection summarization and many other domains.","name":"Submodular set function","categories":["All articles with unsourced statements","Approximation algorithms","Articles with unsourced statements from August 2014","Articles with unsourced statements from November 2013","Combinatorial optimization","Matroid theory"],"tag_line":"In mathematics, a submodular set function (also known as a submodular function) is a set function whose value, informally, has the property that the difference in the incremental value of the function, that a single element makes when added to an input set , decreases as the size of the input set increases."}}
,{"_index":"throwtable","_type":"algorithm","_id":"tomasulo-algorithm","_score":0,"_source":{"description":"Tomasulo’s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution, designed to efficiently utilize multiple execution units. It was developed by Robert Tomasulo at IBM in 1967, and first implemented in the IBM System/360 Model 91’s floating point unit.\nThe major innovations of Tomasulo’s algorithm include register renaming in hardware, reservation stations for all execution units, and a common data bus (CDB) on which computed values broadcast to all reservation stations that may need them. These developments allow for improved parallel execution of instructions that would otherwise stall under the use of scoreboarding or other earlier algorithms.\nRobert Tomasulo received the Eckert-Mauchly Award in 1997 for his work on the algorithm.","name":"Tomasulo algorithm","categories":["Algorithms","Instruction processing","Wikipedia articles needing clarification from March 2015"],"tag_line":"Tomasulo’s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution, designed to efficiently utilize multiple execution units."}}
,{"_index":"throwtable","_type":"algorithm","_id":"maze-solving-algorithm","_score":0,"_source":{"description":"There are a number of different maze solving algorithms, that is, automated methods for the solving of mazes. The random mouse, wall follower, Pledge, and Trémaux's algorithms are designed to be used inside the maze by a traveler with no prior knowledge of the maze, whereas the dead-end filling and shortest path algorithms are designed to be used by a person or computer program that can see the whole maze at once.\nMazes containing no loops are known as \"simply connected\", or \"perfect\" mazes, and are equivalent to a tree in graph theory. Thus many maze solving algorithms are closely related to graph theory. Intuitively, if one pulled and stretched out the paths in the maze in the proper way, the result could be made to resemble a tree.\n\n","name":"Maze solving algorithm","categories":["Algorithms","Mazes"],"tag_line":"There are a number of different maze solving algorithms, that is, automated methods for the solving of mazes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"xulvi-brunet---sokolov-algorithm","_score":0,"_source":{"description":"Xulvi-Brunet and Sokolov’s algorithm generates networks with chosen degree correlations. This method is based on link rewiring, in which the desired degree is governed by parameter ρ. By varying this single parameter it is possible to generate networks from random (when ρ = 0) to perfectly assortative or disassortative (when ρ = 1). This algorithm allows to keep network’s degree distribution unchanged when changing the value of ρ.","name":"Xulvi-Brunet - Sokolov algorithm","categories":["Algorithms","All orphaned articles","Network theory","Orphaned articles from June 2015"],"tag_line":"Xulvi-Brunet and Sokolov’s algorithm generates networks with chosen degree correlations."}}
,{"_index":"throwtable","_type":"algorithm","_id":"randomization-function","_score":0,"_source":{"description":"In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm.\nRandomizing functions are related to random number generators and hash functions, but have somewhat different requirements and uses, and often need specific algorithms.","name":"Randomization function","categories":["Algorithms","All articles lacking sources","All stub articles","Articles lacking sources from April 2009","Computer science stubs"],"tag_line":"In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pointer-jumping","_score":0,"_source":{"description":"Pointer jumping or path doubling is a design technique for parallel algorithms that operate on pointer structures, such as linked lists and directed graphs. It can be used to find the roots of a forest of rooted trees, and can also be applied to parallelize many other graph algorithms including connected components, minimum spanning trees, and biconnected components.","name":"Pointer jumping","categories":["Algorithms","Parallel computing"],"tag_line":"Pointer jumping or path doubling is a design technique for parallel algorithms that operate on pointer structures, such as linked lists and directed graphs."}}
,{"_index":"throwtable","_type":"algorithm","_id":"zassenhaus-algorithm","_score":0,"_source":{"description":"In mathematics, the Zassenhaus algorithm is a method to calculate a basis for the intersection and sum of two subspaces of a vector space. It is named after Hans Zassenhaus, but no publication of this algorithm by him is known. It is used in computer algebra systems.","name":"Zassenhaus algorithm","categories":["Algorithms","CS1 German-language sources (de)","Linear algebra"],"tag_line":"In mathematics, the Zassenhaus algorithm is a method to calculate a basis for the intersection and sum of two subspaces of a vector space."}}
,{"_index":"throwtable","_type":"algorithm","_id":"genetic-algorithm-scheduling","_score":0,"_source":{"description":"To be competitive, corporations must minimize inefficiencies and maximize productivity. In manufacturing, productivity is inherently linked to how well you can optimize the resources you have, reduce waste and increase efficiency. Finding the best way to maximize efficiency in a manufacturing process can be extremely complex. Even on simple projects, there are multiple inputs, multiple steps, many constraints and limited resources. In general a resource constrained scheduling problem consists of:\nA set of jobs that must be executed\nA finite set of resources that can be used to complete each job\nA set of constraints that must be satisfied\nTemporal Constraints–the time window to complete the task\nProcedural Constraints–the order each task must be completed\nResource Constraints - is the resource available\n\nA set of objectives to evaluate the scheduling performance\nA typical factory floor setting is a good example of this where scheduling which jobs need to be completed on which machines, by which employees in what order and at what time. In very complex problems such as scheduling there is no known way to get to a final answer, so we resort to searching for it trying to find a “good” answer. Scheduling problems most often use heuristic algorithms to search for the optimal solution. Heuristic search methods suffer as the inputs become more complex and varied. This type of problem is known in computer science as an NP-Hard problem. This means that there are no known algorithms for finding an optimal solution in polynomial time.\n\nGenetic algorithms are well suited to solving production scheduling problems, because unlike heuristic methods genetic algorithms operate on a population of solutions rather than a single solution. In production scheduling this population of solutions consists of many answers that may have different sometimes conflicting objectives. For example, in one solution we may be optimizing a production process to be completed in a minimal amount of time. In another solution we may be optimizing for a minimal amount of defects. By cranking up the speed at which we produce we may run into an increase in defects in our final product.\nAs we increase the number of objectives we are trying to achieve we also increase the number of constraints on the problem and similarly increase the complexity. Genetic algorithms are ideal for these types of problems where the search space is large and the number of feasible solutions is small.\n\nTo apply a genetic algorithm to a scheduling problem we must first represent it as a genome. One way to represent a scheduling genome is to define a sequence of tasks and the start times of those tasks relative to one another. Each task and its corresponding start time represents a gene.\nA specific sequence of tasks and start times (genes) represents one genome in our population. To make sure that our genome is a feasible solution we must take care that it obeys our precedence constraints. We generate an initial population using random start times within the precedence constraints. With genetic algorithms we then take this initial population and cross it, combining genomes along with a small amount of randomness (mutation). The offspring of this combination is selected based on a fitness function that includes one or many of our constraints, such as minimizing time and minimizing defects. We let this process continue either for a pre-allotted time or until we find a solution that fits our minimum criteria. Overall each successive generation will have a greater average fitness i.e. taking less time with higher quality than the preceding generations. In scheduling problems, as with other genetic algorithm solutions, we must make sure that we do not select offspring that are infeasible, such as offspring that violate our precedence constraint. We of course may have to add further fitness values such as minimizing costs however each constraint that we add greatly increases the search space and lowers the number of solutions that are good matches.","name":"Genetic algorithm scheduling","categories":["All articles covered by WikiProject Wikify","All pages needing cleanup","Articles covered by WikiProject Wikify from September 2009","Genetic algorithms","Operations research","Pages missing lead section","Production and manufacturing","Wikipedia introduction cleanup from September 2009"],"tag_line":"To be competitive, corporations must minimize inefficiencies and maximize productivity."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fitness-proportionate-selection","_score":0,"_source":{"description":"Fitness proportionate selection, also known as roulette wheel selection, is a genetic operator used in genetic algorithms for selecting potentially useful solutions for recombination.\nIn fitness proportionate selection, as in all selection methods, the fitness function assigns a fitness to possible solutions or chromosomes. This fitness level is used to associate a probability of selection with each individual chromosome. If  is the fitness of individual  in the population, its probability of being selected is , where  is the number of individuals in the population.\nThis could be imagined similar to a Roulette wheel in a casino. Usually a proportion of the wheel is assigned to each of the possible selections based on their fitness value. This could be achieved by dividing the fitness of a selection by the total fitness of all the selections, thereby normalizing them to 1. Then a random selection is made similar to how the roulette wheel is rotated.\nWhile candidate solutions with a higher fitness will be less likely to be eliminated, there is still a chance that they may be. Contrast this with a less sophisticated selection algorithm, such as truncation selection, which will eliminate a fixed percentage of the weakest candidates. With fitness proportionate selection there is a chance some weaker solutions may survive the selection process; this is an advantage, as though a solution may be weak, it may include some component which could prove useful following the recombination process.\nThe analogy to a roulette wheel can be envisaged by imagining a roulette wheel in which each candidate solution represents a pocket on the wheel; the size of the pockets are proportionate to the probability of selection of the solution. Selecting N chromosomes from the population is equivalent to playing N games on the roulette wheel, as each candidate is drawn independently.\nOther selection techniques, such as stochastic universal sampling or tournament selection, are often used in practice. This is because they have less stochastic noise, or are fast, easy to implement and have a constant selection pressure [Blickle, 1996].\nThe naive implementation is carried out by first generating the cumulative probability distribution (CDF) over the list of individuals using a probability proportional to the fitness of the individual. A uniform random number from the range [0,1) is chosen and the inverse of the CDF for that number gives an individual. This corresponds to the roulette ball falling in the bin of an individual with a probability proportional to its width. The \"bin\" corresponding to the inverse of the uniform random number can be found most quickly by using a binary search over the elements of the CDF. It takes in the O(log n) time to choose an individual. A faster alternative that generates individuals in O(1) time will be to use the alias method.\nRecently, a very simple O(1) algorithm was introduced that is based on \"stochastic acceptance\". The algorithm randomly selects an individual (say ) and accepts the selection with probability , where  is the maximum fitness in the population. Certain analysis indicates that the stochastic acceptance version has a considerably better performance than versions based on linear or binary search, especially in applications where fitness values might change during the run.","name":"Fitness proportionate selection","categories":["Genetic algorithms"],"tag_line":"Fitness proportionate selection, also known as roulette wheel selection, is a genetic operator used in genetic algorithms for selecting potentially useful solutions for recombination."}}
,{"_index":"throwtable","_type":"algorithm","_id":"needleman–wunsch-algorithm","_score":0,"_source":{"description":"The Needleman–Wunsch algorithm is an algorithm used in bioinformatics to align protein or nucleotide sequences. It was one of the first applications of dynamic programming to compare biological sequences. The algorithm was developed by Saul B. Needleman and Christian D. Wunsch and published in 1970. The algorithm essentially divides a large problem (e.g. the full sequence) into a series of smaller problems and uses the solutions to the smaller problems to reconstruct a solution to the larger problem. It is also sometimes referred to as the optimal matching algorithm and the global alignment technique. The Needleman–Wunsch algorithm is still widely used for optimal global alignment, particularly when the quality of the global alignment is of the utmost importance.","name":"Needleman–Wunsch algorithm","categories":["All articles needing expert attention","All articles that are too technical","Articles needing expert attention from September 2013","Articles with example pseudocode","Bioinformatics algorithms","Computational phylogenetics","Dynamic programming","Pages using citations with accessdate and no URL","Pages using web citations with no URL","Sequence alignment algorithms","Wikipedia articles that are too technical from September 2013"],"tag_line":"The Needleman–Wunsch algorithm is an algorithm used in bioinformatics to align protein or nucleotide sequences."}}
,{"_index":"throwtable","_type":"algorithm","_id":"property-testing","_score":0,"_source":{"description":"In computer science, a property testing algorithm for a decision problem is an algorithm whose query complexity to its input is much smaller than the instance size of the problem. Typically property testing algorithms are used to decide if some mathematical object (such as a graph or a boolean function) has a \"global\" property, or is \"far\" from having this property, using only a small number of \"local\" queries to the object.\nFor example, the following promise problem admits an algorithm whose query complexity is independent of the instance size (for an arbitrary constant ε > 0):\n\"Given a graph G on n vertices, decide if G is bipartite, or G cannot be made bipartite even after removing an arbitrary subset of at most  edges of G.\"\nProperty testing algorithms are important in the theory of probabilistically checkable proofs.","name":"Property testing","categories":["Approximation algorithms","Randomized algorithms","Theoretical computer science"],"tag_line":"In computer science, a property testing algorithm for a decision problem is an algorithm whose query complexity to its input is much smaller than the instance size of the problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"blast2go","_score":0,"_source":{"description":"Blast2GO, first published in 2005, is a bioinformatics software tool for the automatic, high-throughput functional annotation of novel sequence data (genes proteins). It makes use of the BLAST algorithm to identify similar sequences to then transfers existing functional annotation from yet characterised sequences to the novel one. The functional information is represented via the Gene Ontology (GO), a controlled vocabulary of functional attributes. The Gene Ontology, or GO, is a major bioinformatics initiative to unify the representation of gene and gene product attributes across all species.","name":"Blast2GO","categories":["All stub articles","Bioinformatics algorithms","Bioinformatics software","Bioinformatics stubs","Genomics","Laboratory software","Official website different in Wikidata and Wikipedia","Omics","Public domain software"],"tag_line":"Blast2GO, first published in 2005, is a bioinformatics software tool for the automatic, high-throughput functional annotation of novel sequence data (genes proteins)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hirschberg's-algorithm","_score":0,"_source":{"description":"In computer science, Hirschberg's algorithm, named after its inventor, Dan Hirschberg, is a dynamic programming algorithm that finds the optimal sequence alignment between two strings. Optimality is measured with the Levenshtein distance, defined to be the sum of the costs of insertions, replacements, deletions, and null actions needed to change one string into the other. Hirschberg's algorithm is simply described as a divide and conquer version of the Needleman–Wunsch algorithm. Hirschberg's algorithm is commonly used in computational biology to find maximal global alignments of DNA and protein sequences.","name":"Hirschberg's algorithm","categories":["Articles with example pseudocode","Bioinformatics algorithms","Dynamic programming","Sequence alignment algorithms"],"tag_line":"In computer science, Hirschberg's algorithm, named after its inventor, Dan Hirschberg, is a dynamic programming algorithm that finds the optimal sequence alignment between two strings."}}
,{"_index":"throwtable","_type":"algorithm","_id":"kabsch-algorithm","_score":0,"_source":{"description":"The Kabsch algorithm, named after Wolfgang Kabsch, is a method for calculating the optimal rotation matrix that minimizes the RMSD (root mean squared deviation) between two paired sets of points. It is useful in graphics, cheminformatics to compare molecular structures, and also bioinformatics for comparing protein structures (in particular, see root-mean-square deviation (bioinformatics)).\nThe algorithm only computes the rotation matrix, but it also requires the computation of a translation vector. When both the translation and rotation are actually performed, the algorithm is sometimes called partial Procrustes superimposition (see also orthogonal Procrustes problem).","name":"Kabsch algorithm","categories":["Bioinformatics algorithms"],"tag_line":"The Kabsch algorithm, named after Wolfgang Kabsch, is a method for calculating the optimal rotation matrix that minimizes the RMSD (root mean squared deviation) between two paired sets of points."}}
,{"_index":"throwtable","_type":"algorithm","_id":"spades-(software)","_score":0,"_source":{"description":"SPAdes (St. Petersburg genome assembler) is a genome assembly algorithm which was designed for single cell and multi-cells bacterial data sets. However, it might not be suitable for large genomes projects.\nSPAdes works with Ion Torrent, PacBio and Illumina paired-end, mate-pairs and single reads. Recently, SPAdes has been integrated into Galaxy pipelines by Guy Lionel and Philip Mabon.","name":"SPAdes (software)","categories":["Bioinformatics algorithms","Bioinformatics software","DNA sequencing","Metagenomics software","Pages with duplicate reference names","Pages with reference errors","Use mdy dates from October 2013"],"tag_line":"SPAdes (St. Petersburg genome assembler) is a genome assembly algorithm which was designed for single cell and multi-cells bacterial data sets."}}
,{"_index":"throwtable","_type":"algorithm","_id":"checksum","_score":0,"_source":{"description":"A checksum or hash sum is a small-size datsum from a block of digital data for the purpose of detecting errors which may have been introduced during its transmission or storage. It is usually applied to an installation file after it is received from the download server. By themselves checksums are often used to verify data integrity, but should not be relied upon to also verify data authenticity.\nThe actual procedure which yields the checksum, given a data input is called a checksum function or checksum algorithm. Depending on its design goals, a good checksum algorithm will usually output a significantly different value, even for small changes made to the input. This is especially true of cryptographic hash functions, which may be used to detect many data corruption errors and verify overall data integrity; if the computed checksum for the current data input matches the stored value of a previously computed checksum, there is a very high probability the data has not been accidentally altered or corrupted.\nChecksum functions are related to hash functions, fingerprints, randomization functions, and cryptographic hash functions. However, each of those concepts has different applications and therefore different design goals. Checksums are used as cryptographic primitives in larger authentication algorithms. For cryptographic systems with these two specific design goals, see HMAC.\nCheck digits and parity bits are special cases of checksums, appropriate for small blocks of data (such as Social Security numbers, bank account numbers, computer words, single bytes, etc.). Some error-correcting codes are based on special checksums which not only detect common errors but also allow the original data to be recovered in certain cases.","name":"Checksum","categories":["All articles needing additional references","Articles needing additional references from August 2012","Checksum algorithms"],"tag_line":"A checksum or hash sum is a small-size datsum from a block of digital data for the purpose of detecting errors which may have been introduced during its transmission or storage."}}
,{"_index":"throwtable","_type":"algorithm","_id":"luhn-algorithm","_score":0,"_source":{"description":"The Luhn algorithm or Luhn formula, also known as the \"modulus 10\" or \"mod 10\" algorithm, is a simple checksum formula used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the US, and Canadian Social Insurance Numbers. It was created by IBM scientist Hans Peter Luhn and described in U.S. Patent No. 2,950,048, filed on January 6, 1954, and granted on August 23, 1960.\nThe algorithm is in the public domain and is in wide use today. It is specified in ISO/IEC 7812-1. It is not intended to be a cryptographically secure hash function; it was designed to protect against accidental errors, not malicious attacks. Most credit cards and many government identification numbers use the algorithm as a simple method of distinguishing valid numbers from mistyped or otherwise incorrect numbers.","name":"Luhn algorithm","categories":["Articles with example Python code","Checksum algorithms","Error detection and correction","Modular arithmetic"],"tag_line":"The Luhn algorithm or Luhn formula, also known as the \"modulus 10\" or \"mod 10\" algorithm, is a simple checksum formula used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the US, and Canadian Social Insurance Numbers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"speciation-(genetic-algorithm)","_score":0,"_source":{"description":"Speciation is a process that occurs naturally in evolution and is modeled explicitly in some genetic algorithms.\nSpeciation in nature occurs when two similar reproducing beings evolve to become too dissimilar to share genetic information effectively or correctly. In the case of living organisms, they are incapable of mating to produce offspring. Interesting special cases of different species being able to breed exist, such as a horse and a donkey mating to produce a mule. However in this case the Mule is usually infertile, and so the genetic isolation of the two parent species is maintained.\nIn implementations of genetic search algorithms, the event of speciation is defined by some mathematical function that describes the similarity between two candidate solutions (usually described as individuals) in the population. If the result of the similarity is too low, the crossover operator is disallowed between those individuals.","name":"Speciation (genetic algorithm)","categories":["All articles lacking sources","Articles lacking sources from December 2007","Evolutionary algorithms","Genetic algorithms"],"tag_line":"Speciation is a process that occurs naturally in evolution and is modeled explicitly in some genetic algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bees-algorithm","_score":0,"_source":{"description":"In computer science and operations research, the Bees Algorithm is a population-based search algorithm which was developed in 2005. It mimics the food foraging behaviour of honey bee colonies. In its basic version the algorithm performs a kind of neighbourhood search combined with global search, and can be used for both combinatorial optimization and continuous optimization. The only condition for the application of the Bees Algorithm is that some measure of topological distance between the solutions is defined. The effectiveness and specific abilities of the Bees Algorithm have been proven in a number of studies.","name":"Bees algorithm","categories":["Artificial intelligence","Bees","Collective intelligence","Combinatorial algorithms","Optimization algorithms and methods"],"tag_line":"In computer science and operations research, the Bees Algorithm is a population-based search algorithm which was developed in 2005."}}
,{"_index":"throwtable","_type":"algorithm","_id":"top-nodes-algorithm","_score":0,"_source":{"description":"The top-nodes algorithm is an algorithm for managing a resource reservation calendar.\nIt is used when a resource is shared among lots of users (for example bandwidth in a telecommunication link, or disk capacity in a large data center).\nThe algorithm allows\nto check if an amount of resource is available during a specific period of time,\nto reserve an amount of resource for a specific period of time,\nto delete a previous reservation,\nto move the calendar forward (the calendar covers a defined duration, and it must be moved forward as time goes by).","name":"Top-nodes algorithm","categories":["All articles covered by WikiProject Wikify","All articles with too few wikilinks","Articles covered by WikiProject Wikify from December 2012","Articles with French-language external links","Articles with too few wikilinks from December 2012","Calendar algorithms","Scheduling algorithms"],"tag_line":"The top-nodes algorithm is an algorithm for managing a resource reservation calendar."}}
,{"_index":"throwtable","_type":"algorithm","_id":"reward-based-selection","_score":0,"_source":{"description":"Reward-based selection is a technique used in evolutionary algorithms for selecting potentially useful solutions for recombination. The probability of being selected for an individual is proportional to the cumulative reward, obtained by the individual. The cumulative reward can be computed as a sum of the individual reward and the reward, inherited from parents.","name":"Reward-based selection","categories":["Articles created via the Article Wizard","Evolutionary algorithms","Genetic algorithms"],"tag_line":"Reward-based selection is a technique used in evolutionary algorithms for selecting potentially useful solutions for recombination."}}
,{"_index":"throwtable","_type":"algorithm","_id":"zeller's-congruence","_score":0,"_source":{"description":"Zeller's congruence is an algorithm devised by Christian Zeller to calculate the day of the week for any Julian or Gregorian calendar date. It can be considered to be based on the conversion between Julian day and the calendar date.","name":"Zeller's congruence","categories":["CS1 German-language sources (de)","CS1 Latin-language sources (la)","Calendar algorithms","Gregorian calendar","Julian calendar","Modular arithmetic"],"tag_line":"Zeller's congruence is an algorithm devised by Christian Zeller to calculate the day of the week for any Julian or Gregorian calendar date."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adler-32","_score":0,"_source":{"description":"Adler-32 is a checksum algorithm which was invented by Mark Adler in 1995, and is a modification of the Fletcher checksum. Compared to a cyclic redundancy check of the same length, it trades reliability for speed (preferring the latter). Adler-32 is more reliable than Fletcher-16, and slightly less reliable than Fletcher-32.","name":"Adler-32","categories":["Checksum algorithms"],"tag_line":"Adler-32 is a checksum algorithm which was invented by Mark Adler in 1995, and is a modification of the Fletcher checksum."}}
,{"_index":"throwtable","_type":"algorithm","_id":"list-of-genetic-algorithm-applications","_score":0,"_source":{"description":"This is a list of Genetic Algorithm (GA) applications","name":"List of genetic algorithm applications","categories":["All articles with dead external links","All articles with unsourced statements","Articles with dead external links from December 2014","Articles with dead external links from October 2015","Articles with unsourced statements from November 2008","Genetic algorithms","Mathematics-related lists"],"tag_line":"This is a list of Genetic Algorithm (GA) applications"}}
,{"_index":"throwtable","_type":"algorithm","_id":"smawk-algorithm","_score":0,"_source":{"description":"The SMAWK algorithm is an algorithm for finding the minimum value in each row of an implicitly-defined totally monotone matrix. It is named after the initials of its five inventors, Peter Shor, Shlomo Moran, Alok Aggarwal, Robert Wilber, and Maria Klawe.\nFor the purposes of this algorithm, a matrix is defined to be monotone if each row's minimum value occurs in a column which is equal to or greater than the column of the previous row's minimum. It is totally monotone if the same property is true for every submatrix (defined by an arbitrary subset of the rows and columns of the given matrix). Equivalently, a matrix is totally monotone if there does not exist a 2×2 submatrix whose row minima are in the top right and bottom left corners. Every Monge array is totally monotone, but not necessarily vice versa.\nFor the SMAWK algorithm, the matrix to be searched should be defined as a function, and this function is given as input to the algorithm (together with the dimensions of the matrix). The algorithm then evaluates the function whenever it needs to know the value of a particular matrix cell. If this evaluation takes O(1), then, for a matrix with r rows and c columns, the running time and number of function evaluations are both O(c(1 + log(r/c))). This is much faster than the O(rc) time of a naive algorithm that evaluates all matrix cells.\nThe main applications of this method presented in the original paper by Aggarwal et al. were in computational geometry, in finding the farthest point from each point of a convex polygon, and in finding optimal enclosing polygons. Subsequent research found applications of the same algorithm in breaking paragraphs into lines, RNA secondary structure prediction, DNA and protein sequence alignment, the construction of prefix codes, and image thresholding, among others.","name":"SMAWK algorithm","categories":["Algorithms and data structures stubs","All stub articles","Combinatorial algorithms","Computer science stubs","Matrix theory"],"tag_line":"The SMAWK algorithm is an algorithm for finding the minimum value in each row of an implicitly-defined totally monotone matrix."}}
,{"_index":"throwtable","_type":"algorithm","_id":"belief-propagation","_score":0,"_source":{"description":"Belief propagation, also known as sum-product message passing, is a message passing algorithm for performing inference on graphical models, such as Bayesian networks and Markov random fields. It calculates the marginal distribution for each unobserved node, conditional on any observed nodes. Belief propagation is commonly used in artificial intelligence and information theory and has demonstrated empirical success in numerous applications including low-density parity-check codes, turbo codes, free energy approximation, and satisfiability.\nThe algorithm was first proposed by Judea Pearl in 1982, who formulated this algorithm on trees, and was later extended to polytrees. It has since been shown to be a useful approximate algorithm on general graphs.\nIf X={Xi} is a set of discrete random variables with a joint mass function p, the marginal distribution of a single Xi is simply the summation of p over all other variables:\n\nHowever, this quickly becomes computationally prohibitive: if there are 100 binary variables, then one needs to sum over 299 ≈ 6.338 × 1029 possible values. By exploiting the polytree structure, belief propagation allows the marginals to be computed much more efficiently.","name":"Belief propagation","categories":["All articles lacking in-text citations","Articles lacking in-text citations from April 2009","Coding theory","Graph algorithms","Graphical models","Probability theory","Use dmy dates from June 2013"],"tag_line":"Belief propagation, also known as sum-product message passing, is a message passing algorithm for performing inference on graphical models, such as Bayesian networks and Markov random fields."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bellman–ford-algorithm","_score":0,"_source":{"description":"The Bellman–Ford algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph. It is slower than Dijkstra's algorithm for the same problem, but more versatile, as it is capable of handling graphs in which some of the edge weights are negative numbers. The algorithm is named after two of its developers, Richard Bellman and Lester Ford, Jr., who published it in 1958 and 1956, respectively; however, Edward F. Moore also published the same algorithm in 1957, and for this reason it is also sometimes called the Bellman–Ford–Moore algorithm.\nNegative edge weights are found in various applications of graphs, hence the usefulness of this algorithm. If a graph contains a \"negative cycle\" (i.e. a cycle whose edges sum to a negative value) that is reachable from the source, then there is no cheapest path: any path can be made cheaper by one more walk around the negative cycle. In such a case, the Bellman–Ford algorithm can detect negative cycles and report their existence.","name":"Bellman–Ford algorithm","categories":["Articles with example C code","Articles with example pseudocode","Dynamic programming","Graph algorithms","Polynomial-time problems"],"tag_line":"The Bellman–Ford algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph."}}
,{"_index":"throwtable","_type":"algorithm","_id":"criss-cross-algorithm","_score":0,"_source":{"description":"In mathematical optimization, the criss-cross algorithm denotes a family of algorithms for linear programming. Variants of the criss-cross algorithm also solve more general problems with linear inequality constraints and nonlinear objective functions; there are criss-cross algorithms for linear-fractional programming problems, quadratic-programming problems, and linear complementarity problems.\nLike the simplex algorithm of George B. Dantzig, the criss-cross algorithm is not a polynomial-time algorithm for linear programming. Both algorithms visit all 2D corners of a (perturbed) cube in dimension D, the Klee–Minty cube (after Victor Klee and George J. Minty), in the worst case. However, when it is started at a random corner, the criss-cross algorithm on average visits only D additional corners. Thus, for the three-dimensional cube, the algorithm visits all 8 corners in the worst case and exactly 3 additional corners on average.","name":"Criss-cross algorithm","categories":["All articles to be expanded","Articles to be expanded from April 2011","Combinatorial algorithms","Combinatorial optimization","Exchange algorithms","Geometric algorithms","Linear programming","Optimization algorithms and methods","Oriented matroids","Use dmy dates from December 2013"],"tag_line":"In mathematical optimization, the criss-cross algorithm denotes a family of algorithms for linear programming."}}
,{"_index":"throwtable","_type":"algorithm","_id":"steinhaus–johnson–trotter-algorithm","_score":0,"_source":{"description":"The Steinhaus–Johnson–Trotter algorithm or Johnson–Trotter algorithm, also called plain changes, is an algorithm named after Hugo Steinhaus, Selmer M. Johnson and Hale F. Trotter that generates all of the permutations of n elements. Each permutation in the sequence that it generates differs from the previous permutation by swapping two adjacent elements of the sequence. Equivalently, this algorithm finds a Hamiltonian path in the permutohedron.\nThis method was known already to 17th-century English change ringers, and Sedgewick (1977) calls it \"perhaps the most prominent permutation enumeration algorithm\". As well as being simple and computationally efficient, it has the advantage that subsequent computations on the permutations that it generates may be sped up because these permutations are so similar to each other.","name":"Steinhaus–Johnson–Trotter algorithm","categories":["Combinatorial algorithms","Permutations"],"tag_line":"The Steinhaus–Johnson–Trotter algorithm or Johnson–Trotter algorithm, also called plain changes, is an algorithm named after Hugo Steinhaus, Selmer M. Johnson and Hale F. Trotter that generates all of the permutations of n elements."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cycle-detection","_score":0,"_source":{"description":"In computer science, cycle detection is the algorithmic problem of finding a cycle in a sequence of iterated function values.\nFor any function ƒ that maps a finite set S to itself, and any initial value x0 in S, the sequence of iterated function values\n\nmust eventually use the same value twice: there must be some i ≠ j such that xi = xj. Once this happens, the sequence must continue periodically, by repeating the same sequence of values from xi to xj−1. Cycle detection is the problem of finding i and j, given ƒ and x0.","name":"Cycle detection","categories":["Articles with example Python code","Combinatorial algorithms","Fixed points (mathematics)"],"tag_line":"In computer science, cycle detection is the algorithmic problem of finding a cycle in a sequence of iterated function values."}}
,{"_index":"throwtable","_type":"algorithm","_id":"barabási–albert-model","_score":0,"_source":{"description":"The Barabási–Albert (BA) model is an algorithm for generating random scale-free networks using a preferential attachment mechanism. Scale-free networks are widely observed in natural and human-made systems, including the Internet, the world wide web, citation networks, and some social networks. The algorithm is named for its inventors Albert-László Barabási and Réka Albert.","name":"Barabási–Albert model","categories":["Graph algorithms","Pages containing links to subscription-only content","Random graphs","Social networks"],"tag_line":"The Barabási–Albert (BA) model is an algorithm for generating random scale-free networks using a preferential attachment mechanism."}}
,{"_index":"throwtable","_type":"algorithm","_id":"b*","_score":0,"_source":{"description":"In computer science, B* (pronounced \"B star\") is a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals). First published by Hans Berliner in 1979, it is related to the A* search algorithm.","name":"B*","categories":["Combinatorial optimization","Game artificial intelligence","Graph algorithms","Routing algorithms","Search algorithms"],"tag_line":"In computer science, B* (pronounced \"B star\") is a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lemke–howson-algorithm","_score":0,"_source":{"description":"The Lemke–Howson algorithm  is an algorithm that computes a Nash equilibrium of a bimatrix game. It is said to be “the best known among the combinatorial algorithms for finding a Nash equilibrium”.","name":"Lemke–Howson algorithm","categories":["Combinatorial algorithms","Game theory","Non-cooperative games"],"tag_line":"The Lemke–Howson algorithm  is an algorithm that computes a Nash equilibrium of a bimatrix game."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fringe-search","_score":0,"_source":{"description":"In computer science, fringe search is a recent graph search algorithm that finds the least-cost path from a given initial node to one goal node.\nIn essence, fringe search is a middle ground between A* and the iterative deepening A* variant (IDA*).\nIf g(x) is the cost of the search path from the first node to the current, and h(x) is the heuristic estimate of the cost from the current node to the goal, then ƒ(x) = g(x) + h(x), and h* is the actual path cost to the goal. Consider IDA*, which does a recursive left-to-right depth-first search from the root node, stopping the recursion once the goal has been found or the nodes have reached a maximum value ƒ. If no goal is found in the first threshold ƒ, the threshold is then increased and the algorithm searches again. I.E. It iterates on the threshold.\nThere are three major inefficiencies with IDA*. First, IDA* will repeat states when there are multiple (sometimes non-optimal) paths to a goal node - this is often solved by keeping a cache of visited states. IDA* thus altered is denoted as memory-enhanced IDA* (ME-IDA*), since it uses some storage. Furthermore, IDA* repeats all previous operations in a search when it iterates in a new threshold, which is necessary to operate with no storage. By storing the leaf nodes of a previous iteration and using them as the starting position of the next, IDA*'s efficiency is significantly improved (otherwise, in the last iteration it would always have to visit every node in the tree).\nFringe search implements these improvements on IDA* by making use of a data structure that is more or less two lists to iterate over the frontier or fringe of the search tree. One list now, stores the current iteration, and the other list later stores the immediate next iteration. So from the root node of the search tree, now will be the root and later will be empty. Then the algorithm takes one of two actions: If ƒ(head) is greater than the current threshold, remove head from now and append it to the end of later; i.e. save head for the next iteration. Otherwise, if ƒ(head) is less than or equal to the threshold, expand head and discard head, consider its children, adding them to the beginning of now. At the end of an iteration, the threshold is increased, the later list becomes the now list, and later is emptied.\nAn important difference here between fringe and A* is that the contents of the lists in fringe do not necessarily have to be sorted - a significant gain over A*, which requires the often expensive maintenance of order in its open list. Unlike A*, however, fringe will have to visit the same nodes repeatedly, but the cost for each such visit is constant compared to the worst-case logarithmic time of sorting the list in A*.","name":"Fringe search","categories":["All articles lacking in-text citations","Articles lacking in-text citations from June 2013","Graph algorithms"],"tag_line":"In computer science, fringe search is a recent graph search algorithm that finds the least-cost path from a given initial node to one goal node."}}
,{"_index":"throwtable","_type":"algorithm","_id":"graph-kernel","_score":0,"_source":{"description":"In structure mining, a domain of learning on structured data objects in machine learning, a graph kernel is a kernel function that computes an inner product on graphs. Graph kernels can be intuitively understood as functions measuring the similarity of pairs of graphs. They allow kernelized learning algorithms such as support vector machines to work directly on graphs, without having to do feature extraction to transform them to fixed-length, real-valued feature vectors. They find applications in bioinformatics, in chemoinformatics (as a type of molecule kernels), and in social network analysis.\nGraph kernels were first described in 2002 by R. I. Kondor and John Lafferty as kernels on graphs, i.e. similarity functions between the nodes of a single graph, with the World Wide Web hyperlink graph as a suggested application. Vishwanathan et al. instead defined kernels between graphs.\nAn example of a kernel between graphs is the random walk kernel, which conceptually performs random walks on two graphs simultaneously, then counts the number of paths that were produced by both walks. This is equivalent to doing random walks on the direct product of the pair of graphs, and from this, a kernel can be derived that can be efficiently computed.","name":"Graph kernel","categories":["All stub articles","Computer science stubs","Graph algorithms","Kernel methods for machine learning"],"tag_line":"In structure mining, a domain of learning on structured data objects in machine learning, a graph kernel is a kernel function that computes an inner product on graphs."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hopcroft–karp-algorithm","_score":0,"_source":{"description":"In computer science, the Hopcroft–Karp algorithm is an algorithm that takes as input a bipartite graph and produces as output a maximum cardinality matching – a set of as many edges as possible with the property that no two edges share an endpoint. It runs in  time in the worst case, where  is set of edges in the graph, and  is set of vertices of the graph. In the case of dense graphs the time bound becomes , and for random graphs it runs in near-linear time.\nThe algorithm was found by John Hopcroft and Richard Karp (1973). As in previous methods for matching such as the Hungarian algorithm and the work of Edmonds (1965), the Hopcroft–Karp algorithm repeatedly increases the size of a partial matching by finding augmenting paths. However, instead of finding just a single augmenting path per iteration, the algorithm finds a maximal set of shortest augmenting paths. As a result, only  iterations are needed. The same principle has also been used to develop more complicated algorithms for non-bipartite matching with the same asymptotic running time as the Hopcroft–Karp algorithm.","name":"Hopcroft–Karp algorithm","categories":["Graph algorithms","Matching"],"tag_line":"In computer science, the Hopcroft–Karp algorithm is an algorithm that takes as input a bipartite graph and produces as output a maximum cardinality matching – a set of as many edges as possible with the property that no two edges share an endpoint."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cuthill–mckee-algorithm","_score":0,"_source":{"description":"In the mathematical subfield of matrix theory, the Cuthill–McKee algorithm (CM), named for Elizabeth Cuthill and J. McKee , is an algorithm to permute a sparse matrix that has a symmetric sparsity pattern into a band matrix form with a small bandwidth. The reverse Cuthill–McKee algorithm (RCM) due to Alan George is the same algorithm but with the resulting index numbers reversed. In practice this generally results in less fill-in than the CM ordering when Gaussian elimination is applied.\nThe Cuthill McKee algorithm is a variant of the standard breadth-first search algorithm used in graph algorithms. It starts with a peripheral node and then generates levels  for  until all nodes are exhausted. The set  is created from set  by listing all vertices adjacent to all nodes in . These nodes are listed in increasing degree. This last detail is the only difference with the breadth-first search algorithm.","name":"Cuthill–McKee algorithm","categories":["Graph algorithms","Matrix theory","Sparse matrices"],"tag_line":"In the mathematical subfield of matrix theory, the Cuthill–McKee algorithm (CM), named for Elizabeth Cuthill and J. McKee , is an algorithm to permute a sparse matrix that has a symmetric sparsity pattern into a band matrix form with a small bandwidth."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ford–fulkerson-algorithm","_score":0,"_source":{"description":"The Ford–Fulkerson method or Ford–Fulkerson algorithm (FFA) is an algorithm that computes the maximum flow in a flow network. It is called a \"method\" instead of an \"algorithm\" as the approach to finding augmenting paths in a residual graph is not fully specified or it is specified in several implementations with different running times. It was published in 1956 by L. R. Ford, Jr. and D. R. Fulkerson. The name \"Ford–Fulkerson\" is often also used for the Edmonds–Karp algorithm, which is a specialization of Ford–Fulkerson.\nThe idea behind the algorithm is as follows: as long as there is a path from the source (start node) to the sink (end node), with available capacity on all edges in the path, we send flow along one of the paths. Then we find another path, and so on. A path with available capacity is called an augmenting path.","name":"Ford–Fulkerson algorithm","categories":["Articles with example pseudocode","Graph algorithms","Network flow"],"tag_line":"The Ford–Fulkerson method or Ford–Fulkerson algorithm (FFA) is an algorithm that computes the maximum flow in a flow network."}}
,{"_index":"throwtable","_type":"algorithm","_id":"disparity-filter-algorithm-of-weighted-network","_score":0,"_source":{"description":"Disparity filter is a network reduction algorithm to extract the backbone structure of undirected weighted network. Many real world networks such as citation networks, food web, airport networks display heavy tailed statistical distribution of nodes' weight and strength. Disparity filter can sufficiently reduce the network without destroying the multi-scale nature of the network. The algorithm is developed by M. Angeles Serrano, Marian Boguna and Alessandro Vespignani.","name":"Disparity filter algorithm of weighted network","categories":["Graph algorithms"],"tag_line":"Disparity filter is a network reduction algorithm to extract the backbone structure of undirected weighted network."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fkt-algorithm","_score":0,"_source":{"description":"The FKT algorithm, named after Fisher, Kasteleyn, and Temperley, counts the number of perfect matchings in a planar graph in polynomial time. This same task is #P-complete for general graphs. Counting the number of matchings, even for planar graphs, is also #P-complete. The key idea is to convert the problem into a Pfaffian computation of a skew-symmetric matrix derived from a planar embedding of the graph. The Pfaffian of this matrix is then computed efficiently using standard determinant algorithms.","name":"FKT algorithm","categories":["Graph algorithms","Planar graphs"],"tag_line":"The FKT algorithm, named after Fisher, Kasteleyn, and Temperley, counts the number of perfect matchings in a planar graph in polynomial time."}}
,{"_index":"throwtable","_type":"algorithm","_id":"girvan–newman-algorithm","_score":0,"_source":{"description":"The Girvan–Newman algorithm (named after Michelle Girvan and Mark Newman) is a hierarchical method used to detect communities in complex systems.","name":"Girvan–Newman algorithm","categories":["Graph algorithms","Network analysis","Networks"],"tag_line":"The Girvan–Newman algorithm (named after Michelle Girvan and Mark Newman) is a hierarchical method used to detect communities in complex systems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"proof-number-search","_score":0,"_source":{"description":"Proof-number search (short: PN search) is a game tree search algorithm invented by Victor Allis, with applications mostly in endgame solvers, but also for sub-goals during games.\nUsing a binary goal (e.g. first player wins the game), game trees of two-person perfect-information games can be mapped to an and–or tree. Maximizing nodes become OR-nodes, minimizing nodes are mapped to AND-nodes. For all nodes proof and disproof numbers are stored, and updated during the search.\nTo each node of the partially expanded game tree the proof number and disproof number are associated. A proof number represents the minimum number of leaf nodes which have to be proved in order to prove the node. Analogously, a disproof number represents the minimum number of leaves which have to be disproved in order to disprove the node. Because the goal of the tree is to prove a forced win, winning nodes are regarded as proved. Therefore, they have proof number 0 and disproof number ∞. Lost or drawn nodes are regarded as disproved. They have proof number ∞ and disproof number 0. Unknown leaf nodes have a proof and disproof number of unity. The proof number of an internal AND node is equal to the sum of its childrens’ proof numbers, since to prove an AND node all the children have to be proved. The disproof number of an AND node is equal to the minimum of its childrens’ disproof numbers. The disproof number of an internal OR node is equal to the sum of its childrens’ disproof numbers, since to disprove an OR node all the children have to be disproved. Its proof number is equal to the minimum of its childrens’ proof numbers.\nThe procedure of selecting the most-proving node to expand is the following. We start at the root. Then, at each OR node the child with the lowest proof number is selected as successor, and at each AND node the child with the lowest disproof number is selected as successor. Finally, when a leaf node is reached, it is expanded and its children are evaluated.\nThe proof and disproof numbers represent lower bounds on the number of nodes to be evaluated to prove (or disprove) certain nodes. By always selecting the most proving (disproving) node to expand, an efficient search is generated.\nSome variants of proof number search like dfPN, PN2, PDS-PN have been developed to address the quite big memory requirements of the algorithm.","name":"Proof-number search","categories":["Game artificial intelligence","Graph algorithms","Search algorithms"],"tag_line":"Proof-number search (short: PN search) is a game tree search algorithm invented by Victor Allis, with applications mostly in endgame solvers, but also for sub-goals during games."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bubble-sort","_score":0,"_source":{"description":"Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list to be sorted, compares each pair of adjacent items and swaps them if they are in the wrong order. The pass through the list is repeated until no swaps are needed, which indicates that the list is sorted. The algorithm, which is a comparison sort, is named for the way smaller elements \"bubble\" to the top of the list. Although the algorithm is simple, it is too slow and impractical for most problems even when compared to insertion sort. It can be practical if the input is usually in sort order but may occasionally have some out-of-order elements nearly in position.\n\n","name":"Bubble sort","categories":["All articles with unsourced statements","Articles with example pseudocode","Articles with unsourced statements from August 2015","Commons category with local link same as on Wikidata","Comparison sorts","Pages with syntax highlighting errors","Sorting algorithms","Stable sorts","Wikipedia articles needing clarification from October 2014"],"tag_line":"Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list to be sorted, compares each pair of adjacent items and swaps them if they are in the wrong order."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bucket-sort","_score":0,"_source":{"description":"Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a distribution sort, and is a cousin of radix sort in the most to least significant digit flavour. Bucket sort is a generalization of pigeonhole sort. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity estimates involve the number of buckets.\nBucket sort works as follows:\nSet up an array of initially empty \"buckets\".\nScatter: Go over the original array, putting each object in its bucket.\nSort each non-empty bucket.\nGather: Visit the buckets in order and put all elements back into the original array.","name":"Bucket sort","categories":["All articles needing expert attention","Articles needing expert attention from November 2008","Articles needing expert attention with no reason or talk parameter","Articles with example pseudocode","Computer science articles needing expert attention","Sorting algorithms","Stable sorts"],"tag_line":"Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets."}}
,{"_index":"throwtable","_type":"algorithm","_id":"kruskal's-algorithm","_score":0,"_source":{"description":"Kruskal's algorithm is a minimum-spanning-tree algorithm which finds an edge of the least possible weight that connects any two trees in the forest. It is a greedy algorithm in graph theory as it finds a minimum spanning tree for a connected weighted graph adding increasing cost arcs at each step. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. If the graph is not connected, then it finds a minimum spanning forest (a minimum spanning tree for each connected component).\nThis algorithm first appeared in Proceedings of the American Mathematical Society, pp. 48–50 in 1956, and was written by Joseph Kruskal.\nOther algorithms for this problem include Prim's algorithm, Reverse-delete algorithm, and Borůvka's algorithm.","name":"Kruskal's algorithm","categories":["All articles lacking in-text citations","Articles containing proofs","Articles lacking in-text citations from June 2013","Articles with example pseudocode","Graph algorithms","Spanning tree"],"tag_line":"Kruskal's algorithm is a minimum-spanning-tree algorithm which finds an edge of the least possible weight that connects any two trees in the forest."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bead-sort","_score":0,"_source":{"description":"Bead sort, also called gravity sort is a natural sorting algorithm, developed by Joshua J. Arulanandham, Cristian S. Calude and Michael J. Dinneen in 2002, and published in The Bulletin of the European Association for Theoretical Computer Science. Both digital and analog hardware implementations of bead sort can achieve a sorting time of O(n); however, the implementation of this algorithm tends to be significantly slower in software and can only be used to sort lists of positive integers. Also, it would seem that even in the best case, the algorithm requires O(n2) space.","name":"Bead sort","categories":["Sorting algorithms"],"tag_line":"Bead sort, also called gravity sort is a natural sorting algorithm, developed by Joshua J. Arulanandham, Cristian S. Calude and Michael J. Dinneen in 2002, and published in The Bulletin of the European Association for Theoretical Computer Science."}}
,{"_index":"throwtable","_type":"algorithm","_id":"network-simplex-algorithm","_score":0,"_source":{"description":"In mathematical optimization, the network simplex algorithm is a graph theoretic specialization of the simplex algorithm. The algorithm is usually formulated in terms of a standard problem, minimum-cost flow problem and can be efficiently solved in polynomial time. The network simplex method works very well in practice, typically 200 to 300 times faster than the simplex method applied to general linear program of same dimensions.","name":"Network simplex algorithm","categories":["All articles with unsourced statements","Articles with unsourced statements from May 2015","Computational problems in graph theory","Graph algorithms","Linear programming","Mathematical problems","Network flow","Network theory","Operations research","Optimization algorithms and methods","Polynomial-time problems"],"tag_line":"In mathematical optimization, the network simplex algorithm is a graph theoretic specialization of the simplex algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"block-sort","_score":0,"_source":{"description":"Block sort, or block merge sort, is a sorting algorithm combining at least two merge operations with an insertion sort to arrive at O(n log n) in-place stable sorting. It gets its name from the observation that merging two sorted lists, A and B, is equivalent to breaking A into evenly sized blocks, inserting each A block into B under special rules, and merging AB pairs.\nOne practical algorithm for block sort was proposed by Pok-Son Kim and Arne Kutzner in 2008.","name":"Block sort","categories":["Articles with example pseudocode","Comparison sorts","Sorting algorithms","Stable sorts"],"tag_line":"Block sort, or block merge sort, is a sorting algorithm combining at least two merge operations with an insertion sort to arrive at O(n log n) in-place stable sorting."}}
,{"_index":"throwtable","_type":"algorithm","_id":"journal-of-graph-algorithms-and-applications","_score":0,"_source":{"description":"The Journal of Graph Algorithms and Applications is an open access peer-reviewed scientific journal covering the subject of graph algorithms and graph drawing. The journal was established in 1997 and the editor-in-chief is Giuseppe Liotta (University of Perugia). It is abstracted and indexed by Scopus and MathSciNet.","name":"Journal of Graph Algorithms and Applications","categories":["Computer science journals","English-language journals","Graph algorithms","Graph drawing","Mathematics journals","Publications established in 1997"],"tag_line":"The Journal of Graph Algorithms and Applications is an open access peer-reviewed scientific journal covering the subject of graph algorithms and graph drawing."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sethi–ullman-algorithm","_score":0,"_source":{"description":"In computer science, the Sethi–Ullman algorithm is an algorithm named after Ravi Sethi and Jeffrey D. Ullman, its inventors, for translating abstract syntax trees into machine code that uses as few registers as possible.","name":"Sethi–Ullman algorithm","categories":["Compiler construction","Graph algorithms"],"tag_line":"In computer science, the Sethi–Ullman algorithm is an algorithm named after Ravi Sethi and Jeffrey D. Ullman, its inventors, for translating abstract syntax trees into machine code that uses as few registers as possible."}}
,{"_index":"throwtable","_type":"algorithm","_id":"brotli","_score":0,"_source":{"description":"Brotli is an open source data compression library developed by Jyrki Alakuijala and Zoltan Szabadka. Brotli is based on a modern variant of the LZ77 algorithm, Huffman coding and 2nd order context modeling. Replacing deflate with brotli typically gives an increase of 20% in compression density for text files, while compression and decompression speeds are roughly unchanged.\nThe first release of brotli in 2013 was built for off-line compression of web fonts. The version of brotli released in September 2015 has been extended to perform competitively in generic lossless data compression, with particular emphasis on use for HTTP compression. The encoder has been partly rewritten, the compression ratio improved, both the encoder and the decoder have been sped up, the streaming API was improved, more compression quality levels have been added, performance has been improved across platforms, decoding memory use has been reduced, and more use cases are taken into account.\nBrotli uses a pre-defined static dictionary of more than 13,000 strings to \"warm up\" its internal state. The dictionary contains common words, phrases and other substrings derived from a large corpus of text and HTML documents.\nStreams compressed with Brotli have the proposed content encoding type \"br\".\nLike zopfli, another compression algorithm from Google, brotli is named after a Swiss bakery product, brötli.\n\n","name":"Brotli","categories":["Algorithms and data structures stubs","All articles lacking reliable references","All stub articles","Articles lacking reliable references from September 2015","Computer science stubs","Free computer libraries","Lossless compression algorithms","Pages containing cite templates with deprecated parameters"],"tag_line":"Brotli is an open source data compression library developed by Jyrki Alakuijala and Zoltan Szabadka."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dutch-national-flag-problem","_score":0,"_source":{"description":"The Dutch national flag problem (DNF) is a computer science programming problem proposed by Edsger Dijkstra. The flag of the Netherlands consists of three colours: red, white and blue. Given balls of these three colours arranged randomly in a line (the actual number of balls does not matter), the task is to arrange them such that all balls of the same colour are together and their collective colour groups are in the correct order.\nThe solution to this problem is of interest for designing sorting algorithms. In particular, variants of the quicksort algorithm that must be robust to repeated elements need a three-way partitioning function that groups items less than a given key (red), equal to the key (white) and greater than the key (blue). Several solutions exist that have varying performance characteristics, tailored to sorting arrays with either small or large numbers of repeated elements.","name":"Dutch national flag problem","categories":["All articles needing additional references","Articles needing additional references from July 2014","Computational problems","Sorting algorithms"],"tag_line":"The Dutch national flag problem (DNF) is a computer science programming problem proposed by Edsger Dijkstra."}}
,{"_index":"throwtable","_type":"algorithm","_id":"timsort","_score":0,"_source":{"description":"Timsort is a hybrid stable sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data. It was invented by Tim Peters in 2002 for use in the Python programming language. The algorithm finds subsets of the data that are already ordered, and uses that knowledge to sort the remainder more efficiently. This is done by merging an identified subset, called a run, with existing runs until certain criteria are fulfilled. Timsort has been Python's standard sorting algorithm since version 2.3. It is also used to sort arrays of non-primitive type in Java SE 7, on the Android platform, and in GNU Octave.","name":"Timsort","categories":["All articles with dead external links","Articles with dead external links from June 2013","Comparison sorts","Sorting algorithms","Stable sorts"],"tag_line":"Timsort is a hybrid stable sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cocktail-sort","_score":0,"_source":{"description":"Cocktail sort, also known as bidirectional bubble sort, cocktail shaker sort, shaker sort (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is a variation of bubble sort that is both a stable sorting algorithm and a comparison sort. The algorithm differs from a bubble sort in that it sorts in both directions on each pass through the list. This sorting algorithm is only marginally more difficult to implement than a bubble sort, and solves the problem of turtles in bubble sorts. It provides only marginal performance improvements, and does not improve asymptotic performance; like the bubble sort, it is not of practical interest (insertion sort is preferred for simple sorts), though it finds some use in education.","name":"Cocktail sort","categories":["Articles with example pseudocode","Comparison sorts","Sorting algorithms","Stable sorts"],"tag_line":"Cocktail sort, also known as bidirectional bubble sort, cocktail shaker sort, shaker sort (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is a variation of bubble sort that is both a stable sorting algorithm and a comparison sort."}}
,{"_index":"throwtable","_type":"algorithm","_id":"merge-sort","_score":0,"_source":{"description":"In computer science, merge sort (also commonly spelled mergesort) is an O(n log n) comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the implementation preserves the input order of equal elements in the sorted output. Mergesort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and Neumann as early as 1948.","name":"Merge sort","categories":["All articles with dead external links","All articles with unsourced statements","Articles with dead external links from June 2013","Articles with example pseudocode","Articles with inconsistent citation formats","Articles with unsourced statements from April 2014","Articles with unsourced statements from June 2008","Articles with unsourced statements from March 2014","Comparison sorts","Sorting algorithms","Stable sorts"],"tag_line":"In computer science, merge sort (also commonly spelled mergesort) is an O(n log n) comparison-based sorting algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cubesort","_score":0,"_source":{"description":"Cubesort is a parallel sorting algorithm that builds a self-balancing multi-dimensional array from the keys to be sorted. As the axes are of similar length the structure resembles a cube. After each key is inserted the cube can be rapidly converted to an array.\nA cubesort implementation written in C was published in 2014.\n^ Cypher, Robert; Sanz, Jorge L.C (1992). \"Cubesort: A parallel algorithm for sorting N data items with S-sorters\". \n^ \"Cubesort\".","name":"Cubesort","categories":["Algorithms and data structures stubs","All articles lacking reliable references","All articles with topics of unclear notability","All stub articles","Articles lacking reliable references from September 2014","Articles with topics of unclear notability from September 2014","Comparison sorts","Computer science stubs","Online sorts","Sorting algorithms","Stable sorts"],"tag_line":"Cubesort is a parallel sorting algorithm that builds a self-balancing multi-dimensional array from the keys to be sorted."}}
,{"_index":"throwtable","_type":"algorithm","_id":"tree-sort","_score":0,"_source":{"description":"A tree sort is a sort algorithm that builds a binary search tree from the keys to be sorted, and then traverses the tree (in-order) so that the keys come out in sorted order. Its typical use is sorting elements adaptively: after each insertion, the set of elements seen so far is available in sorted order.","name":"Tree sort","categories":["All articles lacking sources","All articles with unsourced statements","Articles lacking sources from September 2014","Articles with unsourced statements from September 2014","Sorting algorithms"],"tag_line":"A tree sort is a sort algorithm that builds a binary search tree from the keys to be sorted, and then traverses the tree (in-order) so that the keys come out in sorted order."}}
,{"_index":"throwtable","_type":"algorithm","_id":"median-cut","_score":0,"_source":{"description":"Median cut is an algorithm to sort data of an arbitrary number of dimensions into series of sets by recursively cutting each set of data at the median point along the longest dimension. Median cut is typically used for color quantization. For example, to reduce a 64k-colour image to 256 colours, median cut is used to find 256 colours that match the original data well.","name":"Median cut","categories":["Sorting algorithms"],"tag_line":"Median cut is an algorithm to sort data of an arbitrary number of dimensions into series of sets by recursively cutting each set of data at the median point along the longest dimension."}}
,{"_index":"throwtable","_type":"algorithm","_id":"comb-sort","_score":0,"_source":{"description":"Comb sort is a relatively simple sorting algorithm originally designed by Włodzimierz Dobosiewicz in 1980. Later it was rediscovered by Stephen Lacey and Richard Box in 1991. Comb sort improves on bubble sort.\n\n","name":"Comb sort","categories":["All articles needing additional references","Articles needing additional references from March 2011","Articles with example pseudocode","Comparison sorts","Sorting algorithms"],"tag_line":"Comb sort is a relatively simple sorting algorithm originally designed by Włodzimierz Dobosiewicz in 1980."}}
,{"_index":"throwtable","_type":"algorithm","_id":"canonical-huffman-code","_score":0,"_source":{"description":"A canonical Huffman code is a particular type of Huffman code with unique properties which allow it to be described in a very compact manner.\nData compressors generally work in one of two ways. Either the decompressor can infer what codebook the compressor has used from previous context, or the compressor must tell the decompressor what the codebook is. Since a canonical Huffman codebook can be stored especially efficiently, most compressors start by generating a \"normal\" Huffman codebook, and then convert it to canonical Huffman before using it.\nIn order for a symbol code scheme such as the Huffman code to be decompressed, the same model that the encoding algorithm used to compress the source data must be provided to the decoding algorithm so that it can use it to decompress the encoded data. In standard Huffman coding this model takes the form of a tree of variable-length codes, with the most frequent symbols located at the top of the structure and being represented by the fewest number of bits.\nHowever, this code tree introduces two critical inefficiencies into an implementation of the coding scheme. Firstly, each node of the tree must store either references to its child nodes or the symbol that it represents. This is expensive in memory usage and if there is a high proportion of unique symbols in the source data then the size of the code tree can account for a significant amount of the overall encoded data. Secondly, traversing the tree is computationally costly, since it requires the algorithm to jump randomly through the structure in memory as each bit in the encoded data is read in.\nCanonical Huffman codes address these two issues by generating the codes in a clear standardized format; all the codes for a given length are assigned their values sequentially. This means that instead of storing the structure of the code tree for decompression only the lengths of the codes are required, reducing the size of the encoded data. Additionally, because the codes are sequential, the decoding algorithm can be dramatically simplified so that it is computationally efficient.","name":"Canonical Huffman code","categories":["All Wikipedia articles needing context","All articles lacking in-text citations","All articles needing expert attention","All articles that are too technical","All pages needing cleanup","Articles lacking in-text citations from March 2014","Articles needing expert attention from June 2011","Coding theory","Lossless compression algorithms","Wikipedia articles needing context from June 2011","Wikipedia articles that are too technical from June 2011","Wikipedia introduction cleanup from June 2011"],"tag_line":"A canonical Huffman code is a particular type of Huffman code with unique properties which allow it to be described in a very compact manner."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lempel–ziv–markov-chain-algorithm","_score":0,"_source":{"description":"The Lempel–Ziv–Markov chain algorithm (LZMA) is an algorithm used to perform lossless data compression. It has been under development either since 1996 or 1998 and was first used in the 7z format of the 7-Zip archiver. This algorithm uses a dictionary compression scheme somewhat similar to the LZ77 algorithm published by Abraham Lempel and Jacob Ziv in 1977 and features a high compression ratio (generally higher than bzip2)) and a variable compression-dictionary size (up to 4 GB), while still maintaining decompression speed similar to other commonly used compression algorithms.\nLZMA2 is a simple container format that can include both uncompressed data and LZMA data, possibly with multiple different LZMA encoding parameters. LZMA2 supports arbitrarily scalable multithreaded compression and decompression and efficient compression of data which is partially incompressible.\n\n","name":"Lempel–Ziv–Markov chain algorithm","categories":["All articles covered by WikiProject Wikify","All articles needing additional references","All articles needing style editing","All articles that may contain original research","All articles with unsourced statements","All pages needing cleanup","Articles covered by WikiProject Wikify from July 2014","Articles needing additional references from July 2010","Articles that may contain original research from April 2012","Articles with unsourced statements from June 2013","Lossless compression algorithms","Wikipedia articles needing style editing from July 2014","Wikipedia introduction cleanup from July 2014"],"tag_line":"The Lempel–Ziv–Markov chain algorithm (LZMA) is an algorithm used to perform lossless data compression."}}
,{"_index":"throwtable","_type":"algorithm","_id":"deflate","_score":0,"_source":{"description":"In computing, deflate is a data compression algorithm that uses a combination of the LZ77 algorithm and Huffman coding. It was originally defined by Phil Katz for version 2 of his PKZIP archiving tool and was later specified in RFC 1951.\nThe original algorithm as designed by Katz was patented as U.S. Patent 5,051,745 and assigned to PKWARE, Inc. As stated in the RFC document, Deflate is widely thought to be implementable in a manner not covered by patents. This has led to its widespread use, for example in gzip compressed files, PNG image files and the .ZIP file format for which Katz originally designed it.","name":"DEFLATE","categories":["All articles needing additional references","Articles needing additional references from January 2009","Lossless compression algorithms"],"tag_line":"In computing, deflate is a data compression algorithm that uses a combination of the LZ77 algorithm and Huffman coding."}}
,{"_index":"throwtable","_type":"algorithm","_id":"move-to-front-transform","_score":0,"_source":{"description":"The move-to-front (MTF) transform is an encoding of data (typically a stream of bytes) designed to improve the performance of entropy encoding techniques of compression. When efficiently implemented, it is fast enough that its benefits usually justify including it as an extra step in data compression algorithms.","name":"Move-to-front transform","categories":["All Wikipedia articles needing clarification","All articles lacking in-text citations","All articles needing additional references","Articles lacking in-text citations from May 2011","Articles needing additional references from May 2011","Data compression","Lossless compression algorithms","Transforms","Wikipedia articles needing clarification from February 2011","Wikipedia articles needing clarification from February 2012","Wikipedia articles needing clarification from July 2015"],"tag_line":"The move-to-front (MTF) transform is an encoding of data (typically a stream of bytes) designed to improve the performance of entropy encoding techniques of compression."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adaptive-scalable-texture-compression","_score":0,"_source":{"description":"Adaptive Scalable Texture Compression (ASTC) is a lossy block-based texture compression algorithm developed by Jørn Nystad et al. of ARM Ltd.\nFull details of ASTC were first presented publicly at the High Performance Graphics 2012 conference, in a paper by Olson et al. entitled \"Adaptive Scalable Texture Compression\"\nASTC was adopted as an official extension for both OpenGL and OpenGL ES by the Khronos Group on 6 August 2012. It's also a part of Direct3D 11.3 and Direct3D 12 in Windows 10.\n\n","name":"Adaptive Scalable Texture Compression","categories":["Lossy compression algorithms","Texture compression"],"tag_line":"Adaptive Scalable Texture Compression (ASTC) is a lossy block-based texture compression algorithm developed by Jørn Nystad et al."}}
,{"_index":"throwtable","_type":"algorithm","_id":"prediction-by-partial-matching","_score":0,"_source":{"description":"Prediction by partial matching (PPM) is an adaptive statistical data compression technique based on context modeling and prediction. PPM models use a set of previous symbols in the uncompressed symbol stream to predict the next symbol in the stream. PPM algorithms can also be used to cluster data into predicted groupings in cluster analysis.","name":"Prediction by partial matching","categories":["All articles lacking in-text citations","Articles lacking in-text citations from November 2015","Articles with Russian-language external links","Lossless compression algorithms","Wikipedia external links cleanup from November 2015","Wikipedia spam cleanup from November 2015"],"tag_line":"Prediction by partial matching (PPM) is an adaptive statistical data compression technique based on context modeling and prediction."}}
,{"_index":"throwtable","_type":"algorithm","_id":"byte-pair-encoding","_score":0,"_source":{"description":"Byte pair encoding or digram coding is a simple form of data compression in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur within that data. A table of the replacements is required to rebuild the original data. The algorithm was first described publicly by Philip Gage in a February 1994 article \"A New Algorithm for Data Compression\" in the C Users Journal.","name":"Byte pair encoding","categories":["Lossless compression algorithms"],"tag_line":"Byte pair encoding or digram coding is a simple form of data compression in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur within that data."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sequitur-algorithm","_score":0,"_source":{"description":"Sequitur (or Nevill-Manning algorithm) is a recursive algorithm developed by Craig Nevill-Manning and Ian H. Witten in 1997 that infers a hierarchical structure (context-free grammar) from a sequence of discrete symbols. The algorithm operates in linear space and time. It can be used in data compression software applications.","name":"Sequitur algorithm","categories":["Lossless compression algorithms"],"tag_line":"Sequitur (or Nevill-Manning algorithm) is a recursive algorithm developed by Craig Nevill-Manning and Ian H. Witten in 1997 that infers a hierarchical structure (context-free grammar) from a sequence of discrete symbols."}}
,{"_index":"throwtable","_type":"algorithm","_id":"incompressible-string","_score":0,"_source":{"description":"An incompressible string is one that cannot be compressed because it lacks sufficient repeating sequences. Whether a string is compressible will often depend on the algorithm being used. Some strings are incompressible by any algorithm — see Kolmogorov complexity.","name":"Incompressible string","categories":["All articles lacking sources","Articles lacking sources from December 2009","Lossless compression algorithms","String (computer science)"],"tag_line":"An incompressible string is one that cannot be compressed because it lacks sufficient repeating sequences."}}
,{"_index":"throwtable","_type":"algorithm","_id":"double-dabble","_score":0,"_source":{"description":"In computer science, the double dabble algorithm is used to convert binary numbers into binary-coded decimal (BCD) notation. It is also known as the shift and add 3 algorithm, and can be implemented using a small number of gates in computer hardware, but at the expense of high latency. The algorithm operates as follows:\nSuppose the original number to be converted is stored in a register that is n bits wide. Reserve a scratch space wide enough to hold both the original number and its BCD representation; 4×ceil(n/3) bits will be enough. It takes a maximum of 4 bits in binary to store each decimal digit.\nThen partition the scratch space into BCD digits (on the left) and the original register (on the right). For example, if the original number to be converted is eight bits wide, the scratch space would be partitioned as follows:\n\n100s Tens Ones   Original\n0010 0100 0011   11110011\n\nThe diagram above shows the binary representation of 24310 in the original register, and the BCD representation of 243 on the left.\nThe scratch space is initialized to all zeros, and then the value to be converted is copied into the \"original register\" space on the right.\n\n0000 0000 0000   11110011\n\nThe algorithm then iterates n times. On each iteration, the entire scratch space is left-shifted one bit. However, before the left-shift is done, any BCD digit which is greater than 4 is incremented by 3. The increment ensures that a value of 5, incremented and left-shifted, becomes 16, thus correctly \"carrying\" into the next BCD digit.\nThe double-dabble algorithm, performed on the value 24310, looks like this:\n\n0000 0000 0000   11110011   Initialization\n0000 0000 0001   11100110   Shift\n0000 0000 0011   11001100   Shift\n0000 0000 0111   10011000   Shift\n0000 0000 1010   10011000   Add 3 to ONES, since it was 7\n0000 0001 0101   00110000   Shift\n0000 0001 1000   00110000   Add 3 to ONES, since it was 5\n0000 0011 0000   01100000   Shift\n0000 0110 0000   11000000   Shift\n0000 1001 0000   11000000   Add 3 to TENS, since it was 6\n0001 0010 0001   10000000   Shift\n0010 0100 0011   00000000   Shift\n   2    4    3\n       BCD\n\nNow eight shifts have been performed, so the algorithm terminates. The BCD digits to the left of the \"original register\" space display the BCD encoding of the original value 243.\nAnother example for the double dabble algorithm - value 6524410.\n\n 104  103  102   101  100    Original binary\n0000 0000 0000 0000 0000   1111111011011100   Initialization\n0000 0000 0000 0000 0001   1111110110111000   Shift left (1st)\n0000 0000 0000 0000 0011   1111101101110000   Shift left (2nd)\n0000 0000 0000 0000 0111   1111011011100000   Shift left (3rd)\n0000 0000 0000 0000 1010   1111011011100000   Add 3 to 100, since it was 7\n0000 0000 0000 0001 0101   1110110111000000   Shift left (4th)\n0000 0000 0000 0001 1000   1110110111000000   Add 3 to 100, since it was 5\n0000 0000 0000 0011 0001   1101101110000000   Shift left (5th)\n0000 0000 0000 0110 0011   1011011100000000   Shift left (6th)\n0000 0000 0000 1001 0011   1011011100000000   Add 3 to 101, since it was 6\n0000 0000 0001 0010 0111   0110111000000000   Shift left (7th)\n0000 0000 0001 0010 1010   0110111000000000   Add 3 to 100, since it was 7\n0000 0000 0010 0101 0100   1101110000000000   Shift left (8th)\n0000 0000 0010 1000 0100   1101110000000000   Add 3 to 101, since it was 5\n0000 0000 0101 0000 1001   1011100000000000   Shift left (9th)\n0000 0000 1000 0000 1001   1011100000000000   Add 3 to 102, since it was 5\n0000 0000 1000 0000 1100   1011100000000000   Add 3 to 100, since it was 9\n0000 0001 0000 0001 1001   0111000000000000   Shift left (10th)\n0000 0001 0000 0001 1100   0111000000000000   Add 3 to 100, since it was 9\n0000 0010 0000 0011 1000   1110000000000000   Shift left (11th)\n0000 0010 0000 0011 1011   1110000000000000   Add 3 to 100, since it was 8\n0000 0100 0000 0111 0111   1100000000000000   Shift left (12th)\n0000 0100 0000 1010 0111   1100000000000000   Add 3 to 101, since it was 7\n0000 0100 0000 1010 1010   1100000000000000   Add 3 to 100, since it was 7\n0000 1000 0001 0101 0101   1000000000000000   Shift left (13th)\n0000 1011 0001 0101 0101   1000000000000000   Add 3 to 103, since it was 8\n0000 1011 0001 1000 0101   1000000000000000   Add 3 to 101, since it was 5\n0000 1011 0001 1000 1000   1000000000000000   Add 3 to 100, since it was 5\n0001 0110 0011 0001 0001   0000000000000000   Shift left (14th)\n0001 1001 0011 0001 0001   0000000000000000   Add 3 to 103, since it was 6\n0011 0010 0110 0010 0010   0000000000000000   Shift left (15th)\n0011 0010 1001 0010 0010   0000000000000000   Add 3 to 102, since it was 6\n0110 0101 0010 0100 0100   0000000000000000   Shift left (16th)\n   6    5    2    4    4\n            BCD\n\nSixteen shifts have been performed, so the algorithm terminates. The BCD digits is: 6*104 + 5*103 + 2*102 + 4*101 + 4*100 = 65244.","name":"Double dabble","categories":["Articles with example C code","Binary arithmetic","Computer arithmetic algorithms"],"tag_line":"In computer science, the double dabble algorithm is used to convert binary numbers into binary-coded decimal (BCD) notation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"addition-chain-exponentiation","_score":0,"_source":{"description":"In mathematics and computer science, optimal addition-chain exponentiation is a method of exponentiation by positive integer powers that requires a minimal number of multiplications. It works by creating the shortest addition chain that generates the desired exponent. Each exponentiation in the chain can be evaluated by multiplying two of the earlier exponentiation results. More generally, addition-chain exponentiation may also refer to exponentiation by non-minimal addition chains constructed by a variety of algorithms (since a shortest addition chain is very difficult to find).\nThe shortest addition-chain algorithm requires no more multiplications than binary exponentiation and usually less. The first example of where it does better is for a15, where the binary method needs six multiplications but a shortest addition chain requires only five:\n (binary, 6 multiplications)\n (shortest addition chain, 5 multiplications).\nOn the other hand, the determination of a shortest addition chain is hard: no efficient optimal methods are currently known for arbitrary exponents, and the related problem of finding a shortest addition chain for a given set of exponents has been proven NP-complete. Even given a shortest chain, addition-chain exponentiation requires more memory than the binary method, because it must potentially store many previous exponents from the chain. So in practice, shortest addition-chain exponentiation is primarily used for small fixed exponents for which a shortest chain can be precomputed and is not too large.\nThere are also several methods to approximate a shortest addition chain, and which often require fewer multiplications than binary exponentiation; binary exponentiation itself is a suboptimal addition-chain algorithm. The optimal algorithm choice depends on the context (such as the relative cost of the multiplication and the number of times a given exponent is re-used).\nThe problem of finding the shortest addition chain cannot be solved by dynamic programming, because it does not satisfy the assumption of optimal substructure. That is, it is not sufficient to decompose the power into smaller powers, each of which is computed minimally, since the addition chains for the smaller powers may be related (to share computations). For example, in the shortest addition chain for a15 above, the subproblem for a6 must be computed as (a3)2 since a3 is re-used (as opposed to, say, a6 = a2(a2)2, which also requires three multiplies).","name":"Addition-chain exponentiation","categories":["Addition chains","Computer arithmetic algorithms","Exponentials"],"tag_line":"In mathematics and computer science, optimal addition-chain exponentiation is a method of exponentiation by positive integer powers that requires a minimal number of multiplications."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fürer's-algorithm","_score":0,"_source":{"description":"Fürer's algorithm is an integer multiplication algorithm for very large numbers possessing a very low asymptotic complexity. It was created in 2007 by Swiss mathematician Martin Fürer of Pennsylvania State University as an asymptotically faster (when analysed on a multitape Turing machine) algorithm than its predecessor, the Schönhage–Strassen algorithm published in 1971.\nThe predecessor to the Fürer algorithm, the Schönhage–Strassen algorithm, used fast Fourier transforms to compute integer products in time  (in big O notation) and its authors, Arnold Schönhage and Volker Strassen, also conjectured a lower bound for the problem of . Here,  denotes the total number of bits in the two input numbers. Fürer's algorithm reduces the gap between these two bounds: it can be used to multiply binary integers of length  in time  (or by a circuit with that many logic gates), where log*n represents the iterated logarithm operation. However, the difference between the  and  factors in the time bounds of the Schönhage–Strassen algorithm and the Fürer algorithm for realistic values of  is very small.\nIn 2008, Anindya De, Chandan Saha, Piyush Kurur and Ramprasad Saptharishi gave a similar algorithm that relies on modular arithmetic instead of complex arithmetic to achieve the same running time.\nIn 2014, David Harvey, Joris van der Hoeven, and Grégoire Lecerf showed that an optimized version of Fürer's algorithm achieves a running time of , making the implied constant in the  exponent explicit. They also gave a modification to Fürer's algorithm that achieves \nIn 2015 Svyatoslav Covanov and Emmanuel Thomé provided another modifications that achieves same running time. Yet, as all the other implementation, it's still not practical.","name":"Fürer's algorithm","categories":["Algorithms and data structures stubs","All articles with unsourced statements","All stub articles","Articles with unsourced statements from June 2015","Computer arithmetic algorithms","Computer science stubs"],"tag_line":"Fürer's algorithm is an integer multiplication algorithm for very large numbers possessing a very low asymptotic complexity."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bloom-filter","_score":0,"_source":{"description":"A Bloom filter is a space-efficient probabilistic data structure, conceived by Burton Howard Bloom in 1970, that is used to test whether an element is a member of a set. False positive matches are possible, but false negatives are not, thus a Bloom filter has a 100% recall rate. In other words, a query returns either \"possibly in set\" or \"definitely not in set\". Elements can be added to the set, but not removed (though this can be addressed with a \"counting\" filter). The more elements that are added to the set, the larger the probability of false positives.\nBloom proposed the technique for applications where the amount of source data would require an impractically large amount of memory if \"conventional\" error-free hashing techniques were applied. He gave the example of a hyphenation algorithm for a dictionary of 500,000 words, out of which 90% follow simple hyphenation rules, but the remaining 10% require expensive disk accesses to retrieve specific hyphenation patterns. With sufficient core memory, an error-free hash could be used to eliminate all unnecessary disk accesses; on the other hand, with limited core memory, Bloom's technique uses a smaller hash area but still eliminates most unnecessary accesses. For example, a hash area only 15% of the size needed by an ideal error-free hash still eliminates 85% of the disk accesses, an 85–15 form of the Pareto principle (Bloom (1970)).\nMore generally, fewer than 10 bits per element are required for a 1% false positive probability, independent of the size or number of elements in the set (Bonomi et al. (2006)).","name":"Bloom filter","categories":["All articles lacking in-text citations","All articles with dead external links","Articles lacking in-text citations from November 2009","Articles with dead external links from June 2010","Commons category with local link same as on Wikidata","Hashing","Lossy compression algorithms","Probabilistic data structures"],"tag_line":"A Bloom filter is a space-efficient probabilistic data structure, conceived by Burton Howard Bloom in 1970, that is used to test whether an element is a member of a set."}}
,{"_index":"throwtable","_type":"algorithm","_id":"binary-splitting","_score":0,"_source":{"description":"In mathematics, binary splitting is a technique for speeding up numerical evaluation of many types of series with rational terms. In particular, it can be used to evaluate hypergeometric series at rational points. Given a series\n\nwhere pn and qn are integers, the goal of binary splitting is to compute integers P(a, b) and Q(a, b) such that\n\nThe splitting consists of setting m = [(a + b)/2] and recursively computing P(a, b) and Q(a, b) from P(a, m), P(m, b), Q(a, m), and Q(m, b). When a and b are sufficiently close, P(a, b) and Q(a, b) can be computed directly from pa...pb and qa...qb.\nBinary splitting requires more memory than direct term-by-term summation, but is asymptotically faster since the sizes of all occurring subproducts are reduced. Additionally, whereas the most naive evaluation scheme for a rational series uses a full-precision division for each term in the series, binary splitting requires only one final division at the target precision; this is not only faster, but conveniently eliminates rounding errors. To take full advantage of the scheme, fast multiplication algorithms such as Toom–Cook and Schönhage–Strassen must be used; with ordinary O(n2) multiplication, binary splitting may render no speedup at all or be slower.\nSince all subdivisions of the series can be computed independently of each other, binary splitting lends well to parallelization and checkpointing.\nIn a less specific sense, binary splitting may also refer to any divide and conquer algorithm that always divides the problem in two halves.","name":"Binary splitting","categories":["Computer arithmetic algorithms"],"tag_line":"In mathematics, binary splitting is a technique for speeding up numerical evaluation of many types of series with rational terms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"spigot-algorithm","_score":0,"_source":{"description":"A spigot algorithm is an algorithm for computing the value of a mathematical constant such as π or e which generates output digits left to right, with limited intermediate storage.\nThe name comes from a \"spigot\", meaning a tap or valve controlling the flow of a liquid.\nInterest in such algorithms was spurred in the early days of computational mathematics by extreme constraints on memory, and an algorithm for calculating the digits of e appears in a paper by Sale in 1968. The name \"Spigot algorithm\" appears to have been coined by Stanley Rabinowitz and Stan Wagon, whose algorithm for calculating the digits of π is sometimes referred to as \"the spigot algorithm for π\".\nThe spigot algorithm of Rabinowitz and Wagon is bounded, in the sense that the number of required digits must be specified in advance. Jeremy Gibbons (2004) uses the term \"streaming algorithm\" to mean one which can be run indefinitely, without a prior bound. A further refinement is an algorithm which can compute a single arbitrary digit, without first computing the preceding digits: an example is the Bailey-Borwein-Plouffe formula, a digit extraction algorithm for π which produces hexadecimal digits.","name":"Spigot algorithm","categories":["Computer arithmetic algorithms"],"tag_line":"A spigot algorithm is an algorithm for computing the value of a mathematical constant such as π or e which generates output digits left to right, with limited intermediate storage."}}
,{"_index":"throwtable","_type":"algorithm","_id":"s2tc","_score":0,"_source":{"description":"S2TC (short for Super Simple Texture Compression) is a texture compression algorithm designed to be compatible with existing patented S3TC decompressors while avoiding any need for patent licensing fees. According to the authors, compressed textures produced by a good S2TC implementation are similar in quality to compressed textures produced by a bad S3TC implementation. The S2TC reference implementation is also capable of decompressing S3TC compressed textures, but instead of implementing the patented aspects of the algorithm, the S2TC decompressor picks colors at random. S2TC was created to provide an alternative to S3TC for open source OpenGL implementations which are legally constrained from implementing patented algorithms.","name":"S2TC","categories":["Lossy compression algorithms","Texture compression"],"tag_line":"S2TC (short for Super Simple Texture Compression) is a texture compression algorithm designed to be compatible with existing patented S3TC decompressors while avoiding any need for patent licensing fees."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fractal-compression","_score":0,"_source":{"description":"Fractal compression is a lossy compression method for digital images, based on fractals. The method is best suited for textures and natural images, relying on the fact that parts of an image often resemble other parts of the same image. Fractal algorithms convert these parts into mathematical data called \"fractal codes\" which are used to recreate the encoded image.","name":"Fractal compression","categories":["All articles with unsourced statements","Articles with unsourced statements from August 2009","Articles with unsourced statements from March 2008","Fractals","Image compression","Lossy compression algorithms","Pages containing cite templates with deprecated parameters"],"tag_line":"Fractal compression is a lossy compression method for digital images, based on fractals."}}
,{"_index":"throwtable","_type":"algorithm","_id":"spinlock","_score":0,"_source":{"description":"In software engineering, a spinlock is a lock which causes a thread trying to acquire it to simply wait in a loop (\"spin\") while repeatedly checking if the lock is available. Since the thread remains active but is not performing a useful task, the use of such a lock is a kind of busy waiting. Once acquired, spinlocks will usually be held until they are explicitly released, although in some implementations they may be automatically released if the thread being waited on (that which holds the lock) blocks, or \"goes to sleep\".\nBecause they avoid overhead from operating system process rescheduling or context switching, spinlocks are efficient if threads are likely to be blocked for only short periods. For this reason, operating-system kernels often use spinlocks. However, spinlocks become wasteful if held for longer durations, as they may prevent other threads from running and require rescheduling. The longer a thread holds a lock, the greater the risk that the thread will be interrupted by the OS scheduler while holding the lock. If this happens, other threads will be left \"spinning\" (repeatedly trying to acquire the lock), while the thread holding the lock is not making progress towards releasing it. The result is an indefinite postponement until the thread holding the lock can finish and release it. This is especially true on a single-processor system, where each waiting thread of the same priority is likely to waste its quantum (allocated time where a thread can run) spinning until the thread that holds the lock is finally finished.\nImplementing spin locks correctly offers challenges because programmers must take into account the possibility of simultaneous access to the lock, which could cause race conditions. Generally, such implementation is possible only with special assembly-language instructions, such as atomic test-and-set operations, and cannot be easily implemented in programming languages not supporting truly atomic operations. On architectures without such operations, or if high-level language implementation is required, a non-atomic locking algorithm may be used, e.g. Peterson's algorithm. But note that such an implementation may require more memory than a spinlock, be slower to allow progress after unlocking, and may not be implementable in a high-level language if out-of-order execution is allowed.","name":"Spinlock","categories":["All articles needing additional references","Articles needing additional references from October 2012","Concurrency control algorithms","Programming constructs"],"tag_line":"In software engineering, a spinlock is a lock which causes a thread trying to acquire it to simply wait in a loop (\"spin\") while repeatedly checking if the lock is available."}}
,{"_index":"throwtable","_type":"algorithm","_id":"block-cipher-mode-of-operation","_score":0,"_source":{"description":"In cryptography, a mode of operation is an algorithm that uses a block cipher to provide an information service such as confidentiality or authenticity. A block cipher by itself is only suitable for the secure cryptographic transformation (encryption or decryption) of one fixed-length group of bits called a block. A mode of operation describes how to repeatedly apply a cipher's single-block operation to securely transform amounts of data larger than a block.\nMost modes require a unique binary sequence, often called an initialization vector (IV), for each encryption operation. The IV has to be non-repeating and, for some modes, random as well. The initialization vector is used to ensure distinct ciphertexts are produced even when the same plaintext is encrypted multiple times independently with the same key. Block ciphers have one or more block size(s), but during transformation the block size is always fixed. Block cipher modes operate on whole blocks and require that the last part of the data be padded to a full block if it is smaller than the current block size. There are, however, modes that do not require padding because they effectively use a block cipher as a stream cipher.\nHistorically, encryption modes have been studied extensively in regard to their error propagation properties under various scenarios of data modification. Later development regarded integrity protection as an entirely separate cryptographic goal. Some modern modes of operation combine confidentiality and authenticity in an efficient way, and are known as authenticated encryption modes.","name":"Block cipher mode of operation","categories":["Block cipher modes of operation","Cryptographic algorithms"],"tag_line":"In cryptography, a mode of operation is an algorithm that uses a block cipher to provide an information service such as confidentiality or authenticity."}}
,{"_index":"throwtable","_type":"algorithm","_id":"non-blocking-algorithm","_score":0,"_source":{"description":"In computer science, an algorithm is called non-blocking if failure or suspension of any thread cannot cause failure or suspension of another thread; for some operations, these algorithms provide a useful alternative to traditional blocking implementations. A non-blocking algorithm is lock-free if there is guaranteed system-wide progress, and wait-free if there is also guaranteed per-thread progress.\nThe word \"non-blocking\" was traditionally used to describe telecommunications networks that could route a connection through a set of relays \"without having to re-arrange existing calls\", see Clos network. Also, if the telephone exchange \"is not defective, it can always make the connection\", see Nonblocking minimal spanning switch.\n^ Göetz, Brian; Peierls, Tim; Bloch, Joshua; Bowbeer, Joseph; Holmes, David; Lea, Doug (2006). Java concurrency in practice. Upper Saddle River, NJ: Addison-Wesley. p. 41. ISBN 9780321349606.","name":"Non-blocking algorithm","categories":["All articles needing additional references","All articles needing style editing","All articles with unsourced statements","Articles needing additional references from August 2010","Articles with unsourced statements from June 2014","CS1 maint: Date and year","Concurrency control","Concurrency control algorithms","Synchronization","Wikipedia articles needing style editing from October 2012"],"tag_line":"In computer science, an algorithm is called non-blocking if failure or suspension of any thread cannot cause failure or suspension of another thread; for some operations, these algorithms provide a useful alternative to traditional blocking implementations."}}
,{"_index":"throwtable","_type":"algorithm","_id":"chandra–toueg-consensus-algorithm","_score":0,"_source":{"description":"The Chandra–Toueg consensus algorithm, published by Tushar Deepak Chandra and Sam Toueg in 1996, is an algorithm for solving consensus in a network of unreliable processes equipped with an eventually strong failure detector. The failure detector is an abstract version of timeouts; it signals to each process when other processes may have crashed. An eventually strong failure detector is one that never identifies some specific good process as having failed after some initial period of confusion, and at the same time eventually identifies all bad processes as failed. The algorithm itself is similar to the Paxos algorithm, which also relies on failure detectors. Both algorithms assume the number of faulty processes is less than n/2, where n is the total number of processes.","name":"Chandra–Toueg consensus algorithm","categories":["All articles needing additional references","All articles needing style editing","Articles needing additional references from October 2011","Distributed algorithms","Fault-tolerant computer systems","Fault tolerance","Wikipedia articles needing style editing from October 2011"],"tag_line":"The Chandra–Toueg consensus algorithm, published by Tushar Deepak Chandra and Sam Toueg in 1996, is an algorithm for solving consensus in a network of unreliable processes equipped with an eventually strong failure detector."}}
,{"_index":"throwtable","_type":"algorithm","_id":"synchronizer-(algorithm)","_score":0,"_source":{"description":"In computer science, a synchronizer is an algorithm that can be used to run a synchronous algorithm on top of an asynchronous processor network, so enabling the asynchronous system to run as a synchronous network.\nThe concept was originally proposed in (Awerbuch, 1985) along with three synchronizer algorithms named alpha, beta and gamma which provided different tradeoffs in terms of time and message complexity. Essentially, they are a solution to the problem of asynchronous algorithms (which operate in a network with no global clock) being harder to design and often less efficient than the equivalent synchronous algorithms. By using a synchronizer, algorithm designers can deal with the simplified \"ideal network\" and then later mechanically produce a version that operates in more realistic asynchronous cases.\n\n","name":"Synchronizer (algorithm)","categories":["Distributed algorithms"],"tag_line":"In computer science, a synchronizer is an algorithm that can be used to run a synchronous algorithm on top of an asynchronous processor network, so enabling the asynchronous system to run as a synchronous network."}}
,{"_index":"throwtable","_type":"algorithm","_id":"raft-(computer-science)","_score":0,"_source":{"description":"Raft is a consensus algorithm designed as an alternative to Paxos. It was meant to be more understandable than Paxos by means of separation of logic, but it is also formally proven safe and offers some new features. Raft offers a generic way to distribute a state machine across a cluster of computing systems, ensuring that each node in the cluster agrees upon the same series of state transitions. It has a number of open-source reference implementations, with full-spec implementations in Go, C++, Java, and Scala.\n\n","name":"Raft (computer science)","categories":["All stub articles","Computer science stubs","Distributed algorithms","Fault-tolerant computer systems"],"tag_line":"Raft is a consensus algorithm designed as an alternative to Paxos."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bully-algorithm","_score":0,"_source":{"description":"The bully algorithm is a programming mechanism that applies a hierarchy to nodes on a system, making a process coordinator or slave. This is used as a method in distributed computing for dynamically electing a coordinator by process ID number. The process with the highest process ID number is selected as the coordinator.","name":"Bully algorithm","categories":["All Wikipedia articles needing clarification","All self-contradictory articles","Distributed algorithms","Self-contradictory articles from January 2015","Wikipedia articles needing clarification from January 2015"],"tag_line":"The bully algorithm is a programming mechanism that applies a hierarchy to nodes on a system, making a process coordinator or slave."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lamport's-distributed-mutual-exclusion-algorithm","_score":0,"_source":{"description":"Lamport's Distributed Mutual Exclusion Algorithm is a contention-based algorithm for mutual exclusion on a distributed system.","name":"Lamport's distributed mutual exclusion algorithm","categories":["All articles to be merged","All stub articles","Articles to be merged from October 2013","Computer science stubs","Concurrency control algorithms"],"tag_line":"Lamport's Distributed Mutual Exclusion Algorithm is a contention-based algorithm for mutual exclusion on a distributed system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dual-ec-drbg","_score":0,"_source":{"description":"Dual Elliptic Curve Deterministic Random Bit Generator (Dual_EC_DRBG) is an algorithm from the branch of cryptography known as elliptic curve cryptography that implements a cryptographically secure pseudorandom number generator (CSPRNG) capable of generating a random bit stream. The algorithm is based on the mathematics of the elliptic curve discrete logarithm problem (ECDLP). Despite public criticism, it was for some time one of the four (now three) CSPRNGs standardized in NIST SP 800-90A as originally published circa March 2007.\nWeaknesses in the cryptographic security of the algorithm were known and publicly criticised well before the algorithm became part of a formal standard endorsed by the ANSI, ISO, and formerly by the National Institute of Standards and Technology (NIST). One of the weaknesses publicly identified was the potential of the algorithm to harbour a backdoor advantageous to the algorithm's designers—the United States government's National Security Agency (NSA)—and no-one else. In 2013, the New York Times reported that documents in their possession but never released to the public \"appear to confirm\" that the backdoor was real, and had been deliberately inserted by the NSA as part of the NSA's Bullrun decryption program. In December 2013, a Reuters news article alleged that in 2004, before NIST standardized Dual_EC_DRBG, NSA paid RSA Security $10 million in a secret deal to use Dual_EC_DRBG as the default in the RSA BSAFE cryptography library, which resulted in RSA Security becoming the most important distributor of the insecure algorithm. RSA responded that they \"categorically deny\" that they had ever knowingly colluded with the NSA to adopt an algorithm that was known to be flawed, saying \"we have never kept [our] relationship [with the NSA] a secret\".\nSometime before its first known publication in 2004, a possible backdoor was discovered with the Dual_EC_DRBG's design, with the design of Dual_EC_DRBG having the unusual property that it was theoretically impossible for anyone but Dual_EC_DRBG's designers (NSA) to confirm the backdoor's existence. Bruce Schneier concluded shortly after standardization that the \"rather obvious\" backdoor (along with other deficiencies) would mean that nobody would use Dual_EC_DRBG. The backdoor would allow NSA to decrypt for example SSL/TLS encryption which used Dual_EC_DRBG as a CSPRNG.\nMembers of the ANSI standard group, to which Dual_EC_DRBG was first submitted, were aware of the exact mechanism of the potential backdoor and how to disable it, but did not take sufficient steps to unconditionally disable the backdoor or to widely publicize it. The general cryptographic community was initially not aware of the potential backdoor, until Dan Shumow and Niels Ferguson's publication, or of Certicom's Daniel R. L. Brown and Scott Vanstone's 2005 patent application describing the backdoor mechanism.\nIn September 2013, The New York Times reported that internal NSA memos leaked by Edward Snowden indicated that the NSA had worked during the standardization process to eventually become the sole editor of the Dual_EC_DRBG standard, and concluded that the Dual_EC_DRBG standard did indeed contain a backdoor for the NSA. As response, NIST stated that \"NIST would not deliberately weaken a cryptographic standard.\" According to the New York Times story, the NSA spends $250 million per year to insert backdoors in software and hardware as part of the Bullrun program. A Presidential advisory committee subsequently set up to examine NSA's conduct recommended among other things that the US government \"fully support and not undermine efforts to create encryption standards\".\nIn April 21, 2014, NIST withdrew Dual_EC_DRBG from its draft guidance on random number generators recommending \"current users of Dual_EC_DRBG transition to one of the three remaining approved algorithms as quickly as possible.\"","name":"Dual EC DRBG","categories":["Articles with underscores in the title","Broken cryptography algorithms","Conspiracy theories","Cryptographically secure pseudorandom number generators","Kleptography","National Institute of Standards and Technology","National Security Agency","Pseudorandom number generators"],"tag_line":"Dual Elliptic Curve Deterministic Random Bit Generator (Dual_EC_DRBG) is an algorithm from the branch of cryptography known as elliptic curve cryptography that implements a cryptographically secure pseudorandom number generator (CSPRNG) capable of generating a random bit stream."}}
,{"_index":"throwtable","_type":"algorithm","_id":"feedback-with-carry-shift-registers","_score":0,"_source":{"description":"In sequence design, a Feedback with Carry Shift Register (or FCSR) is the arithmetic or with carry analog of a Linear feedback shift register (LFSR). If  is an integer, then an N-ary FCSR of length  is a finite state device with a state  consisting of a vector of elements  in  and an integer . The state change operation is determined by a set of coefficients  and is defined as follows: compute . Express s as  with  in . Then the new state is . By iterating the state change an FCSR generates an infinite, eventually period sequence of numbers in .\nFCSRs have been used in the design of stream ciphers (such as the F-FCSR generator), in the cryptanalyis of the summation combiner stream cipher (the reason Goresky and Klapper invented them), and in generating pseudorandom numbers for quasi-Monte Carlo (under the name Multiply With Carry (MWC) generator - invented by Couture and L'Ecuyer,) generalizing work of Marsaglia and Zaman.\nFCSRs are analyzed using number theory. Associated with the FCSR is a connection integer . Associated with the output sequence is the N-adic number  The fundamental theorem of FCSRs says that there is an integer  so that , a rational number. The output sequence is strictly periodic if and only if  is between  and . It is possible to express u as a simple quadratic polynomial involving the initial state and the qi.\nThere is also an exponential representation of FCSRs: if  is the inverse of , and the output sequence is strictly periodic, then , where  is an integer. It follows that the period is at most the order of N in the multiplicative group of units modulo q. This is maximized when q is prime and N is a primitive element modulo q. In this case, the period is . In this case the output sequence is called an l-sequence (for \"long sequence\").\nl-sequences have many excellent statistical properties that make them candidates for use in applications, including near uniform distribution of sub-blocks, ideal arithmetic autocorrelations, and the arithmetic shift and add property. They are the with-carry analog of m-sequences or maximum length sequences.\nThere are efficient algorithms for FCSR synthesis. This is the problem: given a prefix of a sequence, construct a minimal length FCSR that outputs the sequence. This can be solved with a variant of Mahler and De Weger's lattice based analysis of N-adic numbers when ; by a variant of the Euclidean algorithm when N is prime; and in general by Xu's adaptation of the Berlekamp-Massey algorithm. If L is the size of the smallest FCSR that outputs the sequence (called the N-adic complexity of the sequence), then all these algorithms require a prefix of length about  to be successful and have quadratic time complexity. It follows that, as with LFSRs and linear complexity, any stream cipher whose N-adic complexity is low should not be used for cryptography.\nFCSRs and LFSRs are special cases of a very general algebraic construction of sequence generators called Algebraic Feedback Shift Registers (AFSRs) in which the integers are replaced by an arbitrary ring R and N is replaced by an arbitrary non-unit in R. A general reference on the subject of LFSRs, FCSRs, and AFSRs is the book.","name":"Feedback with Carry Shift Registers","categories":["Cryptographic algorithms","Cryptography","Digital registers","Pseudorandom number generators","Stream ciphers"],"tag_line":"In sequence design, a Feedback with Carry Shift Register (or FCSR) is the arithmetic or with carry analog of a Linear feedback shift register (LFSR)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ring-learning-with-errors-key-exchange","_score":0,"_source":{"description":"In cryptography, a public key exchange is a cryptographic algorithm which allows two parties to create and share a secret key which they use to encrypt messages between themselves. The Ring Learning with Errors Key Exchange (RLWE-KEX) is one of a new class of public key exchange algorithms that are designed to be secure against an adversary that possesses a quantum computer. This is important because all of the public key algorithms in use today are easily broken by a quantum computer and scientists are making steady progress toward creating such a computer. The RLWE-KEX is one of a set of Post Quantum cryptographic algorithms which are based on the difficulty of solving certain mathematical problems involving lattices. Unlike older lattice based cryptographic algorithms, the RLWE-KEX is provably reducible to a known hard problem in lattices.","name":"Ring learning with errors key exchange","categories":["All articles needing expert attention","All articles that are too technical","Articles needing expert attention from June 2015","Cryptographic algorithms","Wikipedia articles that are too technical from June 2015"],"tag_line":"In cryptography, a public key exchange is a cryptographic algorithm which allows two parties to create and share a secret key which they use to encrypt messages between themselves."}}
,{"_index":"throwtable","_type":"algorithm","_id":"industrial-grade-prime","_score":0,"_source":{"description":"Industrial-grade primes (the term is apparently due to Henri Cohen) are integers for which primality has not been certified (i.e. rigorously proven), but they have undergone probable prime tests such as the Miller-Rabin primality test, which has a positive, but negligible, failure rate, or the Baillie-PSW primality test, which no composites are known to pass.\nIndustrial-grade primes are sometimes used instead of certified primes in algorithms such as RSA encryption, which require the user to generate large prime numbers. Certifying the primality of large numbers (over 100 digits for instance) is significantly harder than showing they are industrial-grade primes. The latter can be done almost instantly with a failure rate so low that it is highly unlikely to ever fail in practice. In other words, the number is believed to be prime with very high, but not absolute, confidence.","name":"Industrial-grade prime","categories":["All stub articles","Cryptographic algorithms","Number theory stubs","Prime numbers"],"tag_line":"Industrial-grade primes (the term is apparently due to Henri Cohen) are integers for which primality has not been certified (i.e."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pr-cpa-advantage","_score":0,"_source":{"description":"The plaintext-recovery-under-chosen-plaintext-attack advantage (PR-CPA advantage) is defined as the probability that an algorithm with fixed computational resources can use a chosen-plaintext attack to decrypt a randomly selected message that has been encrypted with a symmetric cipher. It is regarded as a fundamental quantity in cryptography since every symmetric encryption scheme must obviously must have a very low PR-CPA advantage to be secure. Though having a low susceptibility to this sort of attack is a necessary condition for an encryption scheme's security, it is not sufficient to ensure security. This is because partial information about the plaintext can often be recovered (for example the least significant bit of the message).","name":"PR-CPA advantage","categories":["Cryptographic algorithms","Theory of cryptography"],"tag_line":"The plaintext-recovery-under-chosen-plaintext-attack advantage (PR-CPA advantage) is defined as the probability that an algorithm with fixed computational resources can use a chosen-plaintext attack to decrypt a randomly selected message that has been encrypted with a symmetric cipher."}}
,{"_index":"throwtable","_type":"algorithm","_id":"k-medoids","_score":0,"_source":{"description":"The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers (medoids or exemplars) and works with an arbitrary matrix of distances between datapoints instead of . This method was proposed in 1987 for the work with  norm and other distances.\nk-medoid is a classical partitioning technique of clustering that clusters the data set of n objects into k clusters known a priori. A useful tool for determining k is the silhouette.\nIt is more robust to noise and outliers as compared to k-means because it minimizes a sum of pairwise dissimilarities instead of a sum of squared Euclidean distances.\nA medoid can be defined as the object of a cluster whose average dissimilarity to all the objects in the cluster is minimal. i.e. it is a most centrally located point in the cluster.","name":"K-medoids","categories":["All articles needing style editing","All articles with unsourced statements","Articles with unsourced statements from May 2015","Data clustering algorithms","Statistical algorithms","Wikipedia articles needing style editing from September 2015"],"tag_line":"The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"optics-algorithm","_score":0,"_source":{"description":"Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander. Its basic idea is similar to DBSCAN, but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. In order to do so, the points of the database are (linearly) ordered such that points which are spatially closest become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that needs to be accepted for a cluster in order to have both points belong to the same cluster. This is represented as a dendrogram.","name":"OPTICS algorithm","categories":["Articles with specifically marked weasel-worded phrases from October 2014","Data clustering algorithms"],"tag_line":"Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data."}}
,{"_index":"throwtable","_type":"algorithm","_id":"k-svd","_score":0,"_source":{"description":"In applied mathematics, K-SVD is a dictionary learning algorithm for creating a dictionary for sparse representations, via a singular value decomposition approach. K-SVD is a generalization of the k-means clustering method, and it works by iteratively alternating between sparse coding the input data based on the current dictionary, and updating the atoms in the dictionary to better fit the data. K-SVD can be found widely in use in applications such as image processing, audio processing, biology, and document analysis.","name":"K-SVD","categories":["All articles lacking reliable references","All articles with close paraphrasing","All pages needing cleanup","Articles lacking reliable references from May 2014","Articles needing cleanup from May 2014","Articles with close paraphrasing from May 2014","Data clustering algorithms","Linear algebra","Norms (mathematics)"],"tag_line":"In applied mathematics, K-SVD is a dictionary learning algorithm for creating a dictionary for sparse representations, via a singular value decomposition approach."}}
,{"_index":"throwtable","_type":"algorithm","_id":"k-means++","_score":0,"_source":{"description":"In data mining, k-means++ is an algorithm for choosing the initial values (or \"seeds\") for the k-means clustering algorithm. It was proposed in 2007 by David Arthur and Sergei Vassilvitskii, as an approximation algorithm for the NP-hard k-means problem—a way of avoiding the sometimes poor clusterings found by the standard k-means algorithm. It is similar to the first of three seeding methods proposed, in independent work, in 2006 by Rafail Ostrovsky, Yuval Rabani, Leonard Schulman and Chaitanya Swamy. (The distribution of the first seed is different.)","name":"K-means++","categories":["All articles with dead external links","Articles with dead external links from May 2013","Data clustering algorithms","Statistical algorithms"],"tag_line":"In data mining, k-means++ is an algorithm for choosing the initial values (or \"seeds\") for the k-means clustering algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"flame-clustering","_score":0,"_source":{"description":"Fuzzy clustering by Local Approximation of MEmberships (FLAME) is a data clustering algorithm that defines clusters in the dense parts of a dataset and performs cluster assignment solely based on the neighborhood relationships among objects. The key feature of this algorithm is that the neighborhood relationships among neighboring objects in the feature space are used to constrain the memberships of neighboring objects in the fuzzy membership space.","name":"FLAME clustering","categories":["Data clustering algorithms","Wikipedia articles with possible conflicts of interest from August 2010"],"tag_line":"Fuzzy clustering by Local Approximation of MEmberships (FLAME) is a data clustering algorithm that defines clusters in the dense parts of a dataset and performs cluster assignment solely based on the neighborhood relationships among objects."}}
,{"_index":"throwtable","_type":"algorithm","_id":"brownboost","_score":0,"_source":{"description":"BrownBoost is a boosting algorithm that may be robust to noisy datasets. BrownBoost is an adaptive version of the boost by majority algorithm. As is true for all boosting algorithms, BrownBoost is used in conjunction with other machine learning methods. BrownBoost was introduced by Yoav Freund in 2001.","name":"BrownBoost","categories":["Classification algorithms","Ensemble learning"],"tag_line":"BrownBoost is a boosting algorithm that may be robust to noisy datasets."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fsa-red-algorithm","_score":0,"_source":{"description":"FSA-Red Algorithm is an algorithm for data reduction which is suitable to build strong association rule using data mining method such as Apriori algorithm.","name":"FSA-Red Algorithm","categories":["Data analysis","Data mining algorithms","Formal sciences"],"tag_line":"FSA-Red Algorithm is an algorithm for data reduction which is suitable to build strong association rule using data mining method such as Apriori algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"kernel-method","_score":0,"_source":{"description":"In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over pairs of data points in raw representation.\nKernel methods owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the \"kernel trick\". Kernel functions have been introduced for sequence data, graphs, text, images, as well as vectors.\nAlgorithms capable of operating with kernels include the kernel perceptron, support vector machines (SVM), Gaussian processes, principal components analysis (PCA), canonical correlation analysis, ridge regression, spectral clustering, linear adaptive filters and many others. Any linear model can be turned into a non-linear model by applying the kernel trick to the model: replacing its features (predictors) by a kernel function.\nMost kernel algorithms are based on convex optimization or eigenproblems and are statistically well-founded. Typically, their statistical properties are analyzed using statistical learning theory (for example, using Rademacher complexity).","name":"Kernel method","categories":["All articles lacking in-text citations","Articles lacking in-text citations from January 2011","Classification algorithms","Geostatistics","Kernel methods for machine learning"],"tag_line":"In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"goertzel-algorithm","_score":0,"_source":{"description":"The Goertzel algorithm is a Digital Signal Processing (DSP) technique that provides a means for efficient evaluation of individual terms of the Discrete Fourier Transform (DFT), thus making it useful in certain practical applications, such as recognition of DTMF tones produced by the buttons pushed on a telephone keypad. The algorithm was first described by Gerald Goertzel in 1958.\nLike the DFT, the Goertzel algorithm analyses one selectable frequency component from a discrete signal. Unlike direct DFT calculations, the Goertzel algorithm applies a single real-valued coefficient at each iteration, using real-valued arithmetic for real-valued input sequences. For covering a full spectrum, the Goertzel algorithm has a higher order of complexity than Fast Fourier Transform (FFT) algorithms; but for computing a small number of selected frequency components, it is more numerically efficient. The simple structure of the Goertzel algorithm makes it well suited to small processors and embedded applications, though not limited to these.\nThe Goertzel algorithm can also be used \"in reverse\" as a sinusoid synthesis function, which requires only 1 multiplication and 1 subtraction per generated sample.","name":"Goertzel algorithm","categories":["All articles needing additional references","All articles needing cleanup","All articles with specifically marked weasel-worded phrases","All articles with unsourced statements","Articles needing additional references from February 2014","Articles needing cleanup from February 2014","Articles with specifically marked weasel-worded phrases from February 2014","Articles with unsourced statements from February 2014","Cleanup tagged articles with a reason field from February 2014","Digital signal processing","FFT algorithms","Pages using citations with accessdate and no URL","Pages using web citations with no URL","Wikipedia pages needing cleanup from February 2014"],"tag_line":"The Goertzel algorithm is a Digital Signal Processing (DSP) technique that provides a means for efficient evaluation of individual terms of the Discrete Fourier Transform (DFT), thus making it useful in certain practical applications, such as recognition of DTMF tones produced by the buttons pushed on a telephone keypad."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sort-merge-join","_score":0,"_source":{"description":"The sort-merge join (also known as merge join) is a join algorithm and is used in the implementation of a relational database management system.\nThe basic problem of a join algorithm is to find, for each distinct value of the join attribute, the set of tuples in each relation which display that value. The key idea of the Sort-merge algorithm is to first sort the relations by the join attribute, so that interleaved linear scans will encounter these sets at the same time.\nIn practice, the most expensive part of performing a sort-merge join is arranging for both inputs to the algorithm to be presented in sorted order. This can be achieved via an explicit sort operation (often an external sort), or by taking advantage of a pre-existing ordering in one or both of the join relations. The latter condition can occur because an input to the join might be produced by an index scan of a tree-based index, another merge join, or some other plan operator that happens to produce output sorted on an appropriate key.\nLet's say that we have two relations  and  and .  fits in  pages memory and  fits in  pages memory. So, in the worst case Sort-Merge Join will run in  I/Os. In the case that  and  are not ordered the worst case time cost will contain additional terms of sorting time: , which equals  (as linearithmic terms outweigh the linear terms, see Big O notation – Orders of common functions).\n\n","name":"Sort-merge join","categories":["All articles lacking sources","Articles lacking sources from December 2009","Join algorithms"],"tag_line":"The sort-merge join (also known as merge join) is a join algorithm and is used in the implementation of a relational database management system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"random-forest","_score":0,"_source":{"description":"Random forests is a notion of the general technique of random decision forests that are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\nThe algorithm for inducing Breiman's random forest was developed by Leo Breiman and Adele Cutler, and \"Random Forests\" is their trademark. The method combines Breiman's \"bagging\" idea and the random selection of features, introduced independently by Ho and Amit and Geman in order to construct a collection of decision trees with controlled variance.\nThe selection of a random subset of features is an example of the random subspace method, which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.","name":"Random forest","categories":["All articles to be merged","All articles with unsourced statements","Articles to be merged from May 2015","Articles with unsourced statements from June 2015","Classification algorithms","Decision trees","Ensemble learning","Pages using duplicate arguments in template calls"],"tag_line":"Random forests is a notion of the general technique of random decision forests that are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bruun's-fft-algorithm","_score":0,"_source":{"description":"Bruun's algorithm is a fast Fourier transform (FFT) algorithm based on an unusual recursive polynomial-factorization approach, proposed for powers of two by G. Bruun in 1978 and generalized to arbitrary even composite sizes by H. Murakami in 1996. Because its operations involve only real coefficients until the last computation stage, it was initially proposed as a way to efficiently compute the discrete Fourier transform (DFT) of real data. Bruun's algorithm has not seen widespread use, however, as approaches based on the ordinary Cooley–Tukey FFT algorithm have been successfully adapted to real data with at least as much efficiency. Furthermore, there is evidence that Bruun's algorithm may be intrinsically less accurate than Cooley–Tukey in the face of finite numerical precision (Storn, 1993).\nNevertheless, Bruun's algorithm illustrates an alternative algorithmic framework that can express both itself and the Cooley–Tukey algorithm, and thus provides an interesting perspective on FFTs that permits mixtures of the two algorithms and other generalizations.","name":"Bruun's FFT algorithm","categories":["FFT algorithms"],"tag_line":"Bruun's algorithm is a fast Fourier transform (FFT) algorithm based on an unusual recursive polynomial-factorization approach, proposed for powers of two by G. Bruun in 1978 and generalized to arbitrary even composite sizes by H. Murakami in 1996."}}
,{"_index":"throwtable","_type":"algorithm","_id":"least-mean-squares-filter","_score":0,"_source":{"description":"Least mean squares (LMS) algorithms are a class of adaptive filter used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean squares of the error signal (difference between the desired and the actual signal). It is a stochastic gradient descent method in that the filter is only adapted based on the error at the current time. It was invented in 1960 by Stanford University professor Bernard Widrow and his first Ph.D. student, Ted Hoff.","name":"Least mean squares filter","categories":["Digital signal processing","Filter theory","Stochastic algorithms"],"tag_line":"Least mean squares (LMS) algorithms are a class of adaptive filter used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean squares of the error signal (difference between the desired and the actual signal)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"nearest-centroid-classifier","_score":0,"_source":{"description":"In machine learning, a nearest centroid classifier or nearest prototype classifier is a classification model that assigns to observations the label of the class of training samples whose mean (centroid) is closest to the observation.\nWhen applied to text classification using tf*idf vectors to represent documents, the nearest centroid classifier is known as the Rocchio classifier because of its similarity to the Rocchio algorithm for relevance feedback.\nAn extended version of the nearest centroid classifier has found applications in the medical domain, specifically classification of tumors.","name":"Nearest centroid classifier","categories":["Classification algorithms"],"tag_line":"In machine learning, a nearest centroid classifier or nearest prototype classifier is a classification model that assigns to observations the label of the class of training samples whose mean (centroid) is closest to the observation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"floyd–steinberg-dithering","_score":0,"_source":{"description":"Floyd–Steinberg dithering is an image dithering algorithm first published in 1976 by Robert W. Floyd and Louis Steinberg. It is commonly used by image manipulation software, for example when an image is converted into GIF format that is restricted to a maximum of 256 colors.\nThe algorithm achieves dithering using error diffusion, meaning it pushes (adds) the residual quantization error of a pixel onto its neighboring pixels, to be dealt with later. It spreads the debt out according to the distribution (shown as a map of the neighboring pixels):\n\nThe pixel indicated with a star (*) indicates the pixel currently being scanned, and the blank pixels are the previously-scanned pixels. The algorithm scans the image from left to right, top to bottom, quantizing pixel values one by one. Each time the quantization error is transferred to the neighboring pixels, while not affecting the pixels that already have been quantized. Hence, if a number of pixels have been rounded downwards, it becomes more likely that the next pixel is rounded upwards, such that on average, the quantization error is close to zero.\nThe diffusion coefficients have the property that if the original pixel values are exactly halfway in between the nearest available colors, the dithered result is a checkerboard pattern. For example 50% grey data could be dithered as a black-and-white checkerboard pattern. For optimal dithering, the counting of quantization errors should be in sufficient accuracy to prevent rounding errors from affecting the result.\nIn some implementations, the horizontal direction of scan alternates between lines; this is called \"serpentine scanning\" or boustrophedon transform dithering.\nIn pseudocode:\n\nfor each y from top to bottom\n   for each x from left to right\n      oldpixel  := pixel[x][y]\n      newpixel  := find_closest_palette_color(oldpixel)\n      pixel[x][y]  := newpixel\n      quant_error  := oldpixel - newpixel\n      pixel[x+1][y  ] := pixel[x+1][y  ] + quant_error * 7/16\n      pixel[x-1][y+1] := pixel[x-1][y+1] + quant_error * 3/16\n      pixel[x  ][y+1] := pixel[x  ][y+1] + quant_error * 5/16\n      pixel[x+1][y+1] := pixel[x+1][y+1] + quant_error * 1/16\n\nWhen converting 16 bit greyscale to 8 bit, find_closest_palette_color() may perform just a simple rounding, for example:\n\nfind_closest_palette_color(oldpixel) = floor(oldpixel / 256)","name":"Floyd–Steinberg dithering","categories":["Articles with example code","Articles with example pseudocode","Computer graphics algorithms","Image processing"],"tag_line":"Floyd–Steinberg dithering is an image dithering algorithm first published in 1976 by Robert W. Floyd and Louis Steinberg."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cellular-evolutionary-algorithm","_score":0,"_source":{"description":"A Cellular Evolutionary Algorithm (cEA) is a kind of evolutionary algorithm (EA) in which individuals cannot mate arbitrarily, but every one interacts with its closer neighbors on which a basic EA is applied (selection, variation, replacement).\n\nThe cellular model simulates Natural evolution from the point of view of the individual, which encodes a tentative (optimization, learning, search) problem solution. The essential idea of this model is to provide the EA population with a special structure defined as a connected graph, in which each vertex is an individual who communicates with his nearest neighbors. Particularly, individuals are conceptually set in a toroidal mesh, and are only allowed to recombine with close individuals. This leads us to a kind of locality known as isolation by distance. The set of potential mates of an individual is called its neighborhood. It is known that, in this kind of algorithm, similar individuals tend to cluster creating niches, and these groups operate as if they were separate sub-populations (islands). Anyway, there is no clear borderline between adjacent groups, and close niches could be easily colonized by competitive niches and maybe merge solution contents during the process. Simultaneously, farther niches can be affected more slowly.","name":"Cellular evolutionary algorithm","categories":["Evolutionary algorithms"],"tag_line":"A Cellular Evolutionary Algorithm (cEA) is a kind of evolutionary algorithm (EA) in which individuals cannot mate arbitrarily, but every one interacts with its closer neighbors on which a basic EA is applied (selection, variation, replacement)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bowyer–watson-algorithm","_score":0,"_source":{"description":"In computational geometry, the Bowyer–Watson algorithm is a method for computing the Delaunay triangulation of a finite set of points in any number of dimensions. The algorithm can be used to obtain a Voronoi diagram of the points, which is the dual graph of the Delaunay triangulation.\nThe Bowyer–Watson algorithm is an incremental algorithm. It works by adding points, one at a time, to a valid Delaunay triangulation of a subset of the desired points. After every insertion, any triangles whose circumcircles contain the new point are deleted, leaving a star-shaped polygonal hole which is then re-triangulated using the new point. By using the connectivity of the triangulation to efficiently locate triangles to remove, the algorithm can take O(N log N) operations to triangulate N points, although special degenerate cases exist where this goes up to O(N2).\nThe algorithm is sometimes known just as the Bowyer Algorithm or the Watson Algorithm. Adrian Bowyer and David Watson devised it independently of each other at the same time, and each published a paper on it in the same issue of The Computer Journal (see below).","name":"Bowyer–Watson algorithm","categories":["Geometric algorithms"],"tag_line":"In computational geometry, the Bowyer–Watson algorithm is a method for computing the Delaunay triangulation of a finite set of points in any number of dimensions."}}
,{"_index":"throwtable","_type":"algorithm","_id":"evolutionary-music","_score":0,"_source":{"description":"Evolutionary music is the audio counterpart to Evolutionary art, whereby algorithmic music is created using an evolutionary algorithm. The process begins with a population of individuals which by some means or other produce audio (e.g. a piece, melody, or loop), which is either initialized randomly or based on human-generated music. Then through the repeated application of computational steps analogous to biological selection, recombination and mutation the aim is for the produced audio to become more musical. Evolutionary sound synthesis is a related technique for generating sounds or synthesizer instruments. Evolutionary music is typically generated using an interactive evolutionary algorithm where the fitness function is the user or audience, as it is difficult to capture the aesthetic qualities of music computationally. However, research into automated measures of musical quality is also active. Evolutionary computation techniques have also been applied to harmonization and accompaniment tasks. The most commonly used evolutionary computation techniques are genetic algorithms and genetic programming.","name":"Evolutionary music","categories":["Electronic music","Evolutionary algorithms"],"tag_line":"Evolutionary music is the audio counterpart to Evolutionary art, whereby algorithmic music is created using an evolutionary algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cooley–tukey-fft-algorithm","_score":0,"_source":{"description":"The Cooley–Tukey algorithm, named after J.W. Cooley and John Tukey, is the most common fast Fourier transform (FFT) algorithm. It re-expresses the discrete Fourier transform (DFT) of an arbitrary composite size N = N1N2 in terms of smaller DFTs of sizes N1 and N2, recursively, to reduce the computation time to O(N log N) for highly composite N (smooth numbers). Because of the algorithm's importance, specific variants and implementation styles have become known by their own names, as described below.\nBecause the Cooley-Tukey algorithm breaks the DFT into smaller DFTs, it can be combined arbitrarily with any other algorithm for the DFT. For example, Rader's or Bluestein's algorithm can be used to handle large prime factors that cannot be decomposed by Cooley–Tukey, or the prime-factor algorithm can be exploited for greater efficiency in separating out relatively prime factors.\nThe algorithm, along with its recursive application, was invented by Carl Friedrich Gauss. Cooley and Tukey independently rediscovered and popularized it 160 years later.","name":"Cooley–Tukey FFT algorithm","categories":["Articles with example pseudocode","FFT algorithms"],"tag_line":"The Cooley–Tukey algorithm, named after J.W."}}
,{"_index":"throwtable","_type":"algorithm","_id":"largest-empty-rectangle","_score":0,"_source":{"description":"In computational geometry, the largest empty rectangle problem, maximal empty rectangle problem or maximum empty rectangle problem, is the problem of finding a rectangle of maximal size to be placed among obstacles in the plane. There are a number of variants of the problem, depending on the particularities of this generic formulation, in particular, depending on the measure of the \"size\", domain (type of obstacles), and the orientation of the rectangle.\nThe problems of this kind arise e.g., in electronic design automation, in design and verification of physical layout of integrated circuits.\nA maximal empty rectangle (MER) is a rectangle which is not contained in another empty rectangle. Each side of a MER abuts an obstacle (otherwise the side may be shifted outwards, increasing the empty rectangle). An application of this kind is enumeration of \"maximal white rectangles\" in image segmentation R&D of image processing and pattern recognition. In the contexts of many algorithms for largest empty rectangles, \"maximal empty rectangles\" are candidate solutions to be considered by the algorithm, since it is easily proven that, e.g., a maximum-area empty rectangle is a maximal empty rectangle.","name":"Largest empty rectangle","categories":["Geometric algorithms"],"tag_line":"In computational geometry, the largest empty rectangle problem, maximal empty rectangle problem or maximum empty rectangle problem, is the problem of finding a rectangle of maximal size to be placed among obstacles in the plane."}}
,{"_index":"throwtable","_type":"algorithm","_id":"k-independent-hashing","_score":0,"_source":{"description":"A family of hash functions is said to be -independent or -universal if selecting a hash function at random from the family guarantees that the hash codes of any designated  keys are independent random variables (see precise mathematical definitions below). Such families allow good average case performance in randomized algorithms or data structures, even if the input data is chosen by an adversary. The trade-offs between the degree of independence and the efficiency of evaluating the hash function are well studied, and many -independent families have been proposed.","name":"K-independent hashing","categories":["Error detection and correction","Hash functions","Pages containing cite templates with deprecated parameters","Search algorithms"],"tag_line":"A family of hash functions is said to be -independent or -universal if selecting a hash function at random from the family guarantees that the hash codes of any designated  keys are independent random variables (see precise mathematical definitions below)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bat-algorithm","_score":0,"_source":{"description":"Bat-inspired algorithm is a metaheuristic optimization algorithm developed by Xin-She Yang in 2010. This bat algorithm is based on the echolocation behaviour of microbats with varying pulse rates of emission and loudness.\n\n","name":"Bat algorithm","categories":["Evolutionary algorithms","Heuristic algorithms"],"tag_line":"Bat-inspired algorithm is a metaheuristic optimization algorithm developed by Xin-She Yang in 2010."}}
,{"_index":"throwtable","_type":"algorithm","_id":"painter's-algorithm","_score":0,"_source":{"description":"The painter's algorithm, also known as a priority fill, is one of the simplest solutions to the visibility problem in 3D computer graphics. When projecting a 3D scene onto a 2D plane, it is necessary at some point to decide which polygons are visible, and which are hidden.\nThe name \"painter's algorithm\" refers to the technique employed by many painters of painting distant parts of a scene before parts which are nearer thereby covering some areas of distant parts. The painter's algorithm sorts all the polygons in a scene by their depth and then paints them in this order, farthest to closest. It will paint over the parts that are normally not visible — thus solving the visibility problem — at the cost of having painted invisible areas of distant objects. The ordering used by the algorithm is called a 'depth order', and does not have to respect the numerical distances to the parts of the scene: the essential property of this ordering is, rather, that if one object obscures part of another then the first object is painted after the object that it obscures. Thus, a valid ordering can be described as a topological ordering of a directed acyclic graph representing occlusions between objects.\n\nThe algorithm can fail in some cases, including cyclic overlap or piercing polygons. In the case of cyclic overlap, as shown in the figure to the right, Polygons A, B, and C overlap each other in such a way that it is impossible to determine which polygon is above the others. In this case, the offending polygons must be cut to allow sorting. Newell's algorithm, proposed in 1972, provides a method for cutting such polygons. Numerous methods have also been proposed in the field of computational geometry.\nThe case of piercing polygons arises when one polygon intersects another. As with cyclic overlap, this problem may be resolved by cutting the offending polygons.\nIn basic implementations, the painter's algorithm can be inefficient. It forces the system to render each point on every polygon in the visible set, even if that polygon is occluded in the finished scene. This means that, for detailed scenes, the painter's algorithm can overly tax the computer hardware.\nA reverse painter's algorithm is sometimes used, in which objects nearest to the viewer are painted first — with the rule that paint must never be applied to parts of the image that are already painted (unless they are partially transparent). In a computer graphic system, this can be very efficient, since it is not necessary to calculate the colors (using lighting, texturing and such) for parts of the more distant scene that are hidden by nearby objects. However, the reverse algorithm suffers from many of the same problems as the standard version.\nThese and other flaws with the algorithm led to the development of Z-buffer techniques, which can be viewed as a development of the painter's algorithm, by resolving depth conflicts on a pixel-by-pixel basis, reducing the need for a depth-based rendering order. Even in such systems, a variant of the painter's algorithm is sometimes employed. As Z-buffer implementations generally rely on fixed-precision depth-buffer registers implemented in hardware, there is scope for visibility problems due to rounding error. These are overlaps or gaps at joins between polygons. To avoid this, some graphics engine implementations \"overrender\", drawing the affected edges of both polygons in the order given by painter's algorithm. This means that some pixels are actually drawn twice (as in the full painter's algorithm) but this happens on only small parts of the image and has a negligible performance effect.","name":"Painter's algorithm","categories":["3D computer graphics","All articles with unsourced statements","Articles with unsourced statements from January 2008","Commons category with local link same as on Wikidata","Computer graphics algorithms"],"tag_line":"The painter's algorithm, also known as a priority fill, is one of the simplest solutions to the visibility problem in 3D computer graphics."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lloyd's-algorithm","_score":0,"_source":{"description":"In computer science and electrical engineering, Lloyd's algorithm, also known as Voronoi iteration or relaxation, is an algorithm named after Stuart P. Lloyd for finding evenly spaced sets of points in subsets of Euclidean spaces, and partitions of these subsets into well-shaped and uniformly sized convex cells. Like the closely related k-means clustering algorithm, it repeatedly finds the centroid of each set in the partition, and then re-partitions the input according to which of these centroids is closest. However, Lloyd's algorithm differs from k-means clustering in that its input is a continuous geometric region rather than a discrete set of points. Thus, when re-partitioning the input, Lloyd's algorithm uses Voronoi diagrams rather than simply determining the nearest center to each of a finite set of points as the k-means algorithm does.\nAlthough the algorithm may be applied most directly to the Euclidean plane, similar algorithms may also be applied to higher-dimensional spaces or to spaces with other non-Euclidean metrics. Lloyd's algorithm can be used to construct close approximations to centroidal Voronoi tessellations of the input, which can be used for quantization, dithering, and stippling. Other applications of Lloyd's algorithm include smoothing of triangle meshes in the finite element method.","name":"Lloyd's algorithm","categories":["Geometric algorithms","Mathematical optimization"],"tag_line":"In computer science and electrical engineering, Lloyd's algorithm, also known as Voronoi iteration or relaxation, is an algorithm named after Stuart P. Lloyd for finding evenly spaced sets of points in subsets of Euclidean spaces, and partitions of these subsets into well-shaped and uniformly sized convex cells."}}
,{"_index":"throwtable","_type":"algorithm","_id":"velocity-obstacle","_score":0,"_source":{"description":"In robotics and motion planning, a velocity obstacle, commonly abbreviated VO, is the set of all velocities of a robot that will result in a collision with another robot at some moment in time, assuming that the other robot maintains its current velocity. If the robot chooses a velocity inside the velocity obstacle then the two robots will eventually collide, if it chooses a velocity outside the velocity obstacle, such a collision is guaranteed not to occur.\nThis algorithm for robot collision avoidance has been repeatedly rediscovered and published under different names: in 1989 as a maneuvering-board approach, in 1993 it was first introduced as the \"velocity obstacle\", in 1998 as collision cones, and in 2009 as forbidden velocity maps. The same algorithm has been used in maritime port navigation since at least 1903.\nThe velocity obstacle for a robot  induced by a robot  may be formally written as\n\nwhere  has position  and radius , and  has position , radius , and velocity . The notation  represents a disc with center  and radius .\nVariations include common velocity obstacles (CVO), finite-time-interval velocity obstacles (FVO), generalized velocity obstacles (GVO), hybrid reciprocal velocity obstacles (HRVO), nonlinear velocity obstacles (NLVO), reciprocal velocity obstacles (RVO), and recursive probabilistic velocity obstacles (PVO).","name":"Velocity obstacle","categories":["All stub articles","Geometric algorithms","Multi-robot systems","Robot kinematics","Robotics stubs"],"tag_line":"In robotics and motion planning, a velocity obstacle, commonly abbreviated VO, is the set of all velocities of a robot that will result in a collision with another robot at some moment in time, assuming that the other robot maintains its current velocity."}}
,{"_index":"throwtable","_type":"algorithm","_id":"nesting-algorithm","_score":0,"_source":{"description":"Nesting algorithms are used to make the most efficient use of material or space by evaluating many different possible combinations via recursion.\nLinear (1-dimensional): The simplest of the algorithms illustrated here. For an existing set there is only one position where a new cut can be placed – at the end of the last cut. Validation of a combination involves a simple Stock - Yield - Kerf = Scrap calculation.\nPlate (2-dimensional): These algorithms are significantly more complex. For an existing set, there may be as many as eight positions where a new cut may be introduced next to each existing cut, and if the new cut is not perfectly square then different rotations may need to be checked. Validation of a potential combination involves checking for intersections between two-dimensional objects.\nPacking (3-dimensional): These algorithms are the most complex illustrated here due to the larger number of possible combinations. Validation of a potential combination involves checking for intersections between three-dimensional objects.\n\n^ a b Herrmann, Jeffrey; Delalio, David. \"Algorithms for Sheet Metal Nesting\" (PDF). IEEE TRANSACTIONS ON ROBOTICS AND AUTOMATION. Retrieved 29 August 2015.","name":"Nesting algorithm","categories":["All articles needing additional references","All orphaned articles","All stub articles","Articles needing additional references from September 2015","Geometric algorithms","Orphaned articles from February 2013","Science software stubs"],"tag_line":"Nesting algorithms are used to make the most efficient use of material or space by evaluating many different possible combinations via recursion."}}
,{"_index":"throwtable","_type":"algorithm","_id":"planar-straight-line-graph","_score":0,"_source":{"description":"Planar straight-line graph (PSLG) is a term used in computational geometry for an embedding of a planar graph in the plane such that its edges are mapped into straight line segments. Fáry's theorem (1948) states that every planar graph has this kind of embedding.\nIn computational geometry PSLGs have often been called planar subdivisions, with an assumption or assertion that subdivisions are polygonal.\nA PSLG without vertices of degree 1 defines a subdivision of the plane into polygonal regions and vice versa. The absence of vertices of degree 1 simplifies descriptions of various algorithms, but it is not essential.\nPSLGs may serve as representations of various maps, e.g., geographical maps in geographical information systems.\nSpecial cases of PSLGs are triangulations (polygon triangulation, point set triangulation). Point set triangulations are maximal PSLGs in the sense that it is impossible to add straight edges to them. Triangulations have numerous applications in various areas.\nPSLGs may be seen as a special kind of Euclidean graphs. However in discussions involving Euclidean graphs the primary interest is their metric properties, i.e., distances between vertices, while for PSLGs the primary interest is the topological properties. For some graphs, such as Delaunay triangulations, both metric and topological properties are of importance.","name":"Planar straight-line graph","categories":["Geometric algorithms","Geometric graphs","Planar graphs"],"tag_line":"Planar straight-line graph (PSLG) is a term used in computational geometry for an embedding of a planar graph in the plane such that its edges are mapped into straight line segments."}}
,{"_index":"throwtable","_type":"algorithm","_id":"line-drawing-algorithm","_score":0,"_source":{"description":"A line drawing algorithm is a graphical algorithm for approximating a line segment on discrete graphical media. On discrete media, such as pixel-based displays and printers, line drawing requires such an approximation (in nontrivial cases). Basic algorithms rasterize lines in one color. A better representation with multiple color gradations requires an advanced process, anti-aliasing.\nOn continuous media, by contrast, no algorithm is necessary to draw a line. For example, oscilloscopes use natural phenomena to draw lines and curves.\nThe Cartesian slope-intercept equation for a straight line is  With m representing the slope of the line and b as the y intercept. Given that the two endpoints of the line segment are specified at positions  and . we can determine values for the slope m and y intercept b with the following calculations,  so, .","name":"Line drawing algorithm","categories":["All articles to be expanded","Articles to be expanded from December 2009","Computer graphics algorithms","Featured articles needing translation from German Wikipedia","Science articles needing translation from German Wikipedia"],"tag_line":"A line drawing algorithm is a graphical algorithm for approximating a line segment on discrete graphical media."}}
,{"_index":"throwtable","_type":"algorithm","_id":"stencil-jumping","_score":0,"_source":{"description":"Stencil jumping, at times called stencil walking, is an algorithm to locate the grid element enclosing a given point for any structured mesh. In simple words, given a point and a structured mesh, this algorithm will help locate the grid element that will enclose the given point.\nThis algorithm finds extensive use in Computational Fluid Dynamics (CFD) in terms of holecutting and interpolation when two meshes lie one inside the other. The other variations of the problem would be something like this: Given a place, at which latitude and longitude does it lie? The brute force algorithm would find the distance of the point from every mesh point and see which is smallest. Another approach would be to use a binary search algorithm which would yield a result comparable in speed to the stencil jumping algorithm. A combination of both the binary search and the stencil jumping algorithm will yield an optimum result in the minimum possible time.","name":"Stencil jumping","categories":["All articles with dead external links","Articles with dead external links from October 2010","Geometric algorithms"],"tag_line":"Stencil jumping, at times called stencil walking, is an algorithm to locate the grid element enclosing a given point for any structured mesh."}}
,{"_index":"throwtable","_type":"algorithm","_id":"proximity-problems","_score":0,"_source":{"description":"Proximity problems is a class of problems in computational geometry which involve estimation of distances between geometric objects.\nA subset of these problems stated in terms of points only are sometimes referred to as closest point problems, although the term \"closest point problem\" is also used synonymously to the nearest neighbor search.\nA common trait for many of these problems is the possibility to establish the Θ(n log n) lower bound on their computational complexity by reduction from the element uniqueness problem basing on an observation that if there is an efficient algorithm to compute some kind of minimal distance for a set of objects, it is trivial to check whether this distance equals to 0.","name":"Proximity problems","categories":["Geometric algorithms"],"tag_line":"Proximity problems is a class of problems in computational geometry which involve estimation of distances between geometric objects."}}
,{"_index":"throwtable","_type":"algorithm","_id":"jon-bentley","_score":0,"_source":{"description":"Jon Louis Bentley (born February 20, 1953 in Long Beach, California) is an American computer scientist who is credited with the heuristic based partitioning algorithm k-d tree.\nBentley received a B.S. in mathematical sciences from Stanford University in 1974, and M.S. and Ph.D in 1976 from the University of North Carolina at Chapel Hill; while a student, he also held internships at the Xerox Palo Alto Research Center and Stanford Linear Accelerator Center. After receiving his Ph.D., he joined the faculty at Carnegie Mellon University as an assistant professor of computer science and mathematics. At CMU, his students included Brian Reid, John Ousterhout, Jeff Eppinger, Joshua Bloch, and James Gosling, and he was one of Charles Leiserson's advisors. Later, Bentley moved to Bell Laboratories.\nHe found an optimal solution for the two dimensional case of Klee's measure problem: given a set of n rectangles, find the area of their union. He and Thomas Ottmann invented the Bentley–Ottmann algorithm, an efficient algorithm for finding all intersecting pairs among a collection of line segments. He wrote the Programming Pearls column for the Communications of the ACM magazine, and later collected the articles into two books of the same name.\nBentley received the Dr. Dobb's Excellence in Programming award in 2004.","name":"Jon Bentley","categories":["1953 births","American computer programmers","American computer scientists","Carnegie Mellon University faculty","Living people","People from Long Beach, California","Researchers in geometric algorithms","Stanford University alumni","University of North Carolina at Chapel Hill alumni","Wikipedia articles with BNF identifiers","Wikipedia articles with ISNI identifiers","Wikipedia articles with LCCN identifiers","Wikipedia articles with VIAF identifiers"],"tag_line":"Jon Louis Bentley (born February 20, 1953 in Long Beach, California) is an American computer scientist who is credited with the heuristic based partitioning algorithm k-d tree."}}
,{"_index":"throwtable","_type":"algorithm","_id":"scanline-rendering","_score":0,"_source":{"description":"Scanline rendering is an algorithm for visible surface determination, in 3D computer graphics, that works on a row-by-row basis rather than a polygon-by-polygon or pixel-by-pixel basis. All of the polygons to be rendered are first sorted by the top y coordinate at which they first appear, then each row or scanline of the image is computed using the intersection of a scanline with the polygons on the front of the sorted list, while the sorted list is updated to discard no-longer-visible polygons as the active scan line is advanced down the picture.\nThe main advantage of this method is that sorting vertices along the normal of the scanning plane reduces the number of comparisons between edges. Another advantage is that it is not necessary to translate the coordinates of all vertices from the main memory into the working memory—only vertices defining edges that intersect the current scan line need to be in active memory, and each vertex is read in only once. The main memory is often very slow compared to the link between the central processing unit and cache memory, and thus avoiding re-accessing vertices in main memory can provide a substantial speedup.\nThis kind of algorithm can be easily integrated with many other graphics techniques, such as the Phong reflection model or the Z-buffer algorithm.","name":"Scanline rendering","categories":["3D rendering","All articles with unsourced statements","Articles with unsourced statements from November 2015","Computer graphics algorithms","Optics"],"tag_line":"Scanline rendering is an algorithm for visible surface determination, in 3D computer graphics, that works on a row-by-row basis rather than a polygon-by-polygon or pixel-by-pixel basis."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pierre-rosenstiehl","_score":0,"_source":{"description":"Pierre Rosenstiehl (born 1933) is a French mathematician recognized for his work in graph theory, planar graphs, and graph drawing.\nThe Fraysseix-Rosenstiehl's planarity criterion is at the origin of the left-right planarity algorithm implemented in Pigale software, which is considered as the fastest implemented planarity testing algorithm.\nRosenstiehl was directeur d’études at the École des Hautes Études en Sciences Sociales in Paris, before his retirement. He is co-editor in chief of the European Journal of Combinatorics. Rosenstiehl, Giuseppe Di Battista, Peter Eades and Roberto Tamassia organized in 1992 at Marino (Italy) a meeting devoted to graph drawing which initiated a long series of international conferences, the International Symposia on Graph Drawing.\nHe has been a member of the French literary group Oulipo since 1992. He married the French author and illustrator Agnès Rosenstiehl.","name":"Pierre Rosenstiehl","categories":["1933 births","20th-century mathematicians","21st-century mathematicians","Academic journal editors","All stub articles","Combinatorialists","French male writers","French mathematician stubs","French mathematicians","Graph drawing people","Graph theorists","Living people","Oulipo members","Researchers in geometric algorithms","Wikipedia articles with BNF identifiers","Wikipedia articles with LCCN identifiers","Wikipedia articles with VIAF identifiers"],"tag_line":"Pierre Rosenstiehl (born 1933) is a French mathematician recognized for his work in graph theory, planar graphs, and graph drawing."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ray-casting","_score":0,"_source":{"description":"Ray casting is the use of ray–surface intersection tests to solve a variety of problems in computer graphics and computational geometry. The term was first used in computer graphics in a 1982 paper by Scott Roth to describe a method for rendering constructive solid geometry models.\nRay casting can refer to a variety of problems and techniques:\nthe general problem of determining the first object intersected by a ray,\na technique for hidden surface removal based on finding the first intersection of a ray cast from the eye through each pixel of an image,\na non-recursive ray tracing rendering algorithm that only casts primary rays, or\na direct volume rendering method, also called volume ray casting, in which the ray is \"pushed through\" the object and the 3D scalar field of interest is sampled along the ray inside the object. No secondary rays are spawned in this method.\nAlthough \"ray casting\" and \"ray tracing\" were often used interchangeably in early computer graphics literature, more recent usage tries to distinguish the two. The distinction is that ray casting is a rendering algorithm that never recursively traces secondary rays, whereas other ray tracing-based rendering algorithms may do so.","name":"Ray casting","categories":["All articles to be expanded","Articles to be expanded from May 2010","CS1 errors: chapter ignored","Computer graphics algorithms"],"tag_line":"Ray casting is the use of ray–surface intersection tests to solve a variety of problems in computer graphics and computational geometry."}}
,{"_index":"throwtable","_type":"algorithm","_id":"raimund-seidel","_score":0,"_source":{"description":"Raimund G. Seidel is a German and Austrian theoretical computer scientist and an expert in computational geometry.\nSeidel was born in Graz, Austria, and studied with Hermann Maurer at the Graz University of Technology. He received his Ph.D. in 1987 from Cornell University under the supervision of John Gilbert. After teaching at the University of California, Berkeley, he moved in 1994 to Saarland University. In 1997 he and Christoph M. Hoffmann were program chairs for the Symposium on Computational Geometry. In 2014, he became director of the Leibniz Center for Informatics at Schloss Dagstuhl.\nSeidel invented backwards analysis of randomized algorithms and used it to analyze a simple linear programming algorithm that runs in linear time for problems of bounded dimension. With his student Cecilia R. Aragon in 1989 he devised the treap data structure, and he is also known for the Kirkpatrick–Seidel algorithm for computing two-dimensional convex hulls.","name":"Raimund Seidel","categories":["Austrian computer scientists","Cornell University alumni","German computer scientists","Living people","Researchers in geometric algorithms","University of California, Berkeley faculty","Year of birth missing (living people)"],"tag_line":"Raimund G. Seidel is a German and Austrian theoretical computer scientist and an expert in computational geometry."}}
,{"_index":"throwtable","_type":"algorithm","_id":"nimrod-megiddo","_score":0,"_source":{"description":"Nimrod Megiddo (Hebrew: נמרוד מגידו‎) is a mathematician and computer scientist. He is a research scientist at the IBM Almaden Research Center.\nHis interests include optimization, algorithm design and analysis, game theory, and machine learning.\nMegiddo received his Ph.D. in mathematics from the Hebrew University of Jerusalem.\nMegiddo received the 2014 John von Neumann Theory Prize and is a 1992 Frederick W. Lanchester Prize recipient.","name":"Nimrod Megiddo","categories":["American computer scientists","American operations researchers","Articles containing Hebrew-language text","Game theorists","Hebrew University of Jerusalem alumni","Israeli operations researchers","John von Neumann Theory Prize winners","Living people","Numerical analysts","Researchers in geometric algorithms","Wikipedia articles with GND identifiers","Wikipedia articles with ISNI identifiers","Wikipedia articles with LCCN identifiers","Wikipedia articles with VIAF identifiers","Year of birth missing (living people)"],"tag_line":"Nimrod Megiddo (Hebrew: נמרוד מגידו‎) is a mathematician and computer scientist."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pankaj-k.-agarwal","_score":0,"_source":{"description":"Pankaj Kumar Agarwal is an Indian computer scientist and mathematician researching algorithms in computational geometry and related areas. He is the RJR Nabisco Professor of Computer Science and Mathematics at Duke University, where he has been chair of the computer science department since 2004. He obtained his Ph.D. in Computer Science in 1989 from the Courant Institute, New York, under the supervision of Micha Sharir.","name":"Pankaj K. Agarwal","categories":["Courant Institute of Mathematical Sciences alumni","Duke University faculty","Fellows of the Association for Computing Machinery","Indian mathematicians","Living people","Researchers in geometric algorithms","Year of birth missing (living people)"],"tag_line":"Pankaj Kumar Agarwal is an Indian computer scientist and mathematician researching algorithms in computational geometry and related areas."}}
,{"_index":"throwtable","_type":"algorithm","_id":"subhash-suri","_score":0,"_source":{"description":"Subhash Suri (born July 7, 1960) is an Indian-American computer scientist, a professor at the University of California, Santa Barbara. He is known for his research in computational geometry, computer networks, and algorithmic game theory.","name":"Subhash Suri","categories":["1960 births","American computer scientists","Fellow Members of the IEEE","Fellows of the American Association for the Advancement of Science","Fellows of the Association for Computing Machinery","Indian computer scientists","Indian mathematicians","Johns Hopkins University alumni","Living people","Researchers in geometric algorithms","University of California, Santa Barbara faculty","Washington University in St. Louis faculty"],"tag_line":"Subhash Suri (born July 7, 1960) is an Indian-American computer scientist, a professor at the University of California, Santa Barbara."}}
,{"_index":"throwtable","_type":"algorithm","_id":"loss-functions-for-classification","_score":0,"_source":{"description":"In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems. Given  as the vector space of all possible inputs, and Y = {–1,1} as the vector space of all possible outputs, we wish to find a function  which best maps  to . However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same  to generate different . As a result, the goal of the learning problem is to minimize expected risk, defined as\n\nwhere  represents the loss function, and  represents the probability distribution of the data, which can equivalently be written using Bayes' theorem as\n\nIn practice, the probability distribution  is unknown. Consequently, utilizing a training set of  independently and identically distributed samples\n\ndrawn from the data sample space, one seeks to minimize empirical risk\n\nas a proxy for expected risk. (See statistical learning theory for a more detailed description.)\nFor computational ease, it is standard practice to write loss functions as functions of only one variable. Within classification, loss functions are generally written solely in terms of the product of the true classifier  and the predicted value . Selection of a loss function within this framework\n\nimpacts the optimal  which minimizes empirical risk, as well as the computational complexity of the learning algorithm.\nGiven the binary nature of classification, a natural selection for a loss function (assuming equal cost for false positives and false negatives) would be the 0–1 indicator function which takes the value of 0 if the predicted classification equals that of the true class or a 1 if the predicted classification does not match the true class. This selection is modeled by\n\nwhere  indicates the Heaviside step function. However, this loss function is non-convex and non-smooth, and solving for the optimal solution is an NP-hard combinatorial optimization problem. As a result, it is better to substitute continuous, convex loss function surrogates which are tractable for commonly used learning algorithms. In addition to their computational tractability, one can show that the solutions to the learning problem using these loss surrogates allows for the recovery of the actual solution to the original classification problem. Some of these surrogates are described below.","name":"Loss functions for classification","categories":["Machine learning algorithms"],"tag_line":"In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"2-opt","_score":0,"_source":{"description":"In optimization, 2-opt is a simple local search algorithm first proposed by Croes in 1958 for solving the traveling salesman problem. The main idea behind it is to take a route that crosses over itself and reorder it so that it does not.\n\n - A   B -             - A - B -\n     X         ==>     \n - C   D -             - C - D -\n\nA complete 2-opt local search will compare every possible valid combination of the swapping mechanism. This technique can be applied to the travelling salesman problem as well as many related problems. These include the vehicle routing problem (VRP) as well as the capacitated VRP, which require minor modification of the algorithm.\nThis is the mechanism by which the 2-opt swap manipulates a given route:\n\n   2optSwap(route, i, k) {\n       1. take route[1] to route[i-1] and add them in order to new_route\n       2. take route[i] to route[k] and add them in reverse order to new_route\n       3. take route[k+1] to end and add them in order to new_route\n       return new_route;\n   }\n\nHere is an example of the above with arbitrary input:\n\n   example route: A ==> B ==> C ==> D ==> E ==> F ==> G ==> H ==> A\n   example i = 4, example k = 7\n   new_route:\n       1. (A ==> B ==> C)\n       2. A ==> B ==> C ==> (G ==> F ==> E ==> D)\n       3. A ==> B ==> C ==> G ==> F ==> E ==> D (==> H ==> A)\n\nThis is the complete 2-opt swap making use of the above mechanism:\n\n   repeat until no improvement is made {\n       start_again:\n       best_distance = calculateTotalDistance(existing_route)\n       for (i = 0; i < number of nodes eligible to be swapped - 1; i++) {\n           for (k = i + 1; k < number of nodes eligible to be swapped; k++) {\n               new_route = 2optSwap(existing_route, i, k)\n               new_distance = calculateTotalDistance(new_route)\n               if (new_distance < best_distance) {\n                   existing_route = new_route\n                   goto start_again\n               }\n           }\n       }\n   }\n\nNote: If you start/end at a particular node or depot, then you must remove this from the search as an eligible candidate for swapping, as reversing the order will cause an invalid path.\nFor example, with depot at A:\n\n   A ==> B ==> C ==> D ==> A\n\nSwapping using node[0] and node[2] would yield\n\n   C ==> B ==> A ==> D ==> A \n\nwhich is not valid (does not leave from A, the depot).","name":"2-opt","categories":["All stub articles","Heuristic algorithms","Mathematical analysis stubs","Mathematical optimization","Travelling salesman problem"],"tag_line":"In optimization, 2-opt is a simple local search algorithm first proposed by Croes in 1958 for solving the traveling salesman problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"freivalds'-algorithm","_score":0,"_source":{"description":"Freivalds' algorithm (named after Rusins Freivalds) is a probabilistic randomized algorithm used to verify matrix multiplication. Given three n × n matrices A, B, and C, a general problem is to verify whether A × B = C. A naïve algorithm would compute the product A × B explicitly and compare term by term whether this product equals C. However, the best known matrix multiplication algorithm runs in O(n2.3729) time. Freivalds' algorithm utilizes randomization in order to reduce this time bound to O(n2)  with high probability. In O(kn2) time the algorithm can verify a matrix product with probability of failure less than .\n\n","name":"Freivalds' algorithm","categories":["Articles containing proofs","Matrix multiplication algorithms","Matrix theory","Randomized algorithms"],"tag_line":"Freivalds' algorithm (named after Rusins Freivalds) is a probabilistic randomized algorithm used to verify matrix multiplication."}}
,{"_index":"throwtable","_type":"algorithm","_id":"almeida–pineda-recurrent-backpropagation","_score":0,"_source":{"description":"Almeida–Pineda recurrent backpropagation is an extension to the backpropagation algorithm that is applicable to recurrent neural networks. It is a type of supervised learning.\nA recurrent neural network for this algorithm consists of some input units, some output units and eventually some hidden units.\nFor a given set of (input, target) states, the network is trained to settle into a stable activation state with the output units in the target state, based on a given input state clamped on the input units.","name":"Almeida–Pineda recurrent backpropagation","categories":["All stub articles","Machine learning algorithms","Neuroscience","Neuroscience stubs"],"tag_line":"Almeida–Pineda recurrent backpropagation is an extension to the backpropagation algorithm that is applicable to recurrent neural networks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"3-opt","_score":0,"_source":{"description":"In optimization, 3-opt is a simple local search algorithm for solving the travelling salesman problem and related network optimization problems.\n3-opt analysis involves deleting 3 connections (or edges) in a network (or tour), reconnecting the network in all other possible ways, and then evaluating each reconnection method to find the optimum one. This process is then repeated for a different set of 3 connections.","name":"3-opt","categories":["All stub articles","Combinatorics stubs","Heuristic algorithms","Mathematical optimization","Travelling salesman problem"],"tag_line":"In optimization, 3-opt is a simple local search algorithm for solving the travelling salesman problem and related network optimization problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pvlv","_score":0,"_source":{"description":"The primary value learned value (PVLV) model is a possible explanation for the reward-predictive firing properties of dopamine (DA) neurons. It simulates behavioral and neural data on Pavlovian conditioning and the midbrain dopaminergic neurons that fire in proportion to unexpected rewards. It is an alternative to the temporal-differences (TD) algorithm.\nIt is used as part of Leabra.","name":"PVLV","categories":["All stub articles","Machine learning algorithms","Neuroscience","Neuroscience stubs"],"tag_line":"The primary value learned value (PVLV) model is a possible explanation for the reward-predictive firing properties of dopamine (DA) neurons."}}
,{"_index":"throwtable","_type":"algorithm","_id":"strassen-algorithm","_score":0,"_source":{"description":"In linear algebra, the Strassen algorithm, named after Volker Strassen, is an algorithm used for matrix multiplication. It is faster than the standard matrix multiplication algorithm and is useful in practice for large matrices, but would be slower than the fastest known algorithms for extremely large matrices.\n\n","name":"Strassen algorithm","categories":["All articles needing additional references","Articles needing additional references from January 2015","Matrix multiplication algorithms"],"tag_line":"In linear algebra, the Strassen algorithm, named after Volker Strassen, is an algorithm used for matrix multiplication."}}
,{"_index":"throwtable","_type":"algorithm","_id":"luleå-algorithm","_score":0,"_source":{"description":"The Luleå algorithm of computer science, designed by Degermark et al. (1997), is a technique for storing and searching internet routing tables efficiently. It is named after the Luleå University of Technology, the home institute of the technique's authors. The name of the algorithm does not appear in the original paper describing it, but was used in a message from Craig Partridge to the Internet Engineering Task Force describing that paper prior to its publication.\nThe key task to be performed in internet routing is to match a given IPv4 address (viewed as a sequence of 32 bits) to the longest prefix of the address for which routing information is available. This prefix matching problem may be solved by a trie, but trie structures use a significant amount of space (a node for each bit of each address) and searching them requires traversing a sequence of nodes with length proportional to the number of bits in the address. The Luleå algorithm shortcuts this process by storing only the nodes at three levels of the trie structure, rather than storing the entire trie.\nThe main advantage of the Luleå algorithm for the routing task is that it uses very little memory, averaging 4–5 bytes per entry for large routing tables. This small memory footprint often allows the entire data structure to fit into the routing processor's cache, speeding operations. However, it has the disadvantage that it cannot be modified easily: small changes to the routing table may require most or all of the data structure to be reconstructed.\nThe Luleå algorithm is patented in the United States (Degermark et al. 2001).","name":"Luleå algorithm","categories":["Internet architecture","Networking algorithms","Routing algorithms","Routing software"],"tag_line":"The Luleå algorithm of computer science, designed by Degermark et al."}}
,{"_index":"throwtable","_type":"algorithm","_id":"slob","_score":0,"_source":{"description":"The SLOB (Simple List Of Blocks) allocator is one of three available memory allocators in the Linux kernel. (The other two are SLAB and SLUB.) The SLOB allocator is designed to require little memory for the implementation and housekeeping, for use in small systems such as embedded systems. Unfortunately, a major limitation of the SLOB allocator is that it suffers greatly from internal fragmentation.\nSLOB currently uses a first-fit algorithm, which uses the first available space for memory. In 2008, a reply from Linus Torvalds on a Linux mailing list was made where he suggested the use of a best-fit algorithm, which tries to find a memory block which suits needs best. Best fit finds the smallest space which fits the required amount available, avoiding loss of performance, both by fragmentation and consolidation of memory.\nBy default, Linux kernel used a SLAB Allocation system until version 2.6.23, when SLUB allocation became the default. When the CONFIG_SLAB flag is disabled, the kernel falls back to using the \"SLOB\" allocator. The SLOB allocator was used in DSLinux on Nintendo DS handheld console.\n^ http://lxr.free-electrons.com/source/mm/slob.c SLOB Allocator Documentation and code. Retrieved 12 November 2010\n^ http://lwn.net/Articles/157944/ slob: introduce the SLOB Allocator . Retrieved 12 November 2010.","name":"SLOB","categories":["All articles lacking in-text citations","All articles needing cleanup","All stub articles","Articles lacking in-text citations from August 2008","Articles needing cleanup from August 2008","Cleanup tagged articles without a reason field from August 2008","Linux kernel","Linux stubs","Memory management algorithms","Use dmy dates from August 2012","Wikipedia pages needing cleanup from August 2008"],"tag_line":"The SLOB (Simple List Of Blocks) allocator is one of three available memory allocators in the Linux kernel."}}
,{"_index":"throwtable","_type":"algorithm","_id":"interleaved-polling-with-adaptive-cycle-time","_score":0,"_source":{"description":"Interleaved Polling with Adaptive Cycle Time (IPACT) is an algorithm designed by Glen Kramer, Biswanath Mukherjee and Gerry Pesavento Advanced Technology Lab.at the University of California, Davis. IPACT is a dynamic bandwidth allocation algorithm for use in Ethernet passive optical networks (EPONs).\nIPACT uses the Gate and Report messages provided by the EPON Multi-Point Control Protocol (MPCP) to allocate bandwidth to Optical Network Units (ONUs). If the optical line terminal grants bandwidth to an ONU and waits until it has received that particular ONU's transmission before granting bandwidth to another ONU, then time equivalent to a whole messaging round-trip is wasted during which the upstream may remain idle. IPACT eliminates this idle time by sending downstream grant messages to succeeding ONUs while receiving transmissions from previously granted ONUs. It accomplishes this by calculating the time at which a transmission grant allocated to a previous ONU ends.","name":"Interleaved polling with adaptive cycle time","categories":["All articles lacking sources","All stub articles","Articles lacking sources from June 2007","Computer network stubs","Network scheduling algorithms"],"tag_line":"Interleaved Polling with Adaptive Cycle Time (IPACT) is an algorithm designed by Glen Kramer, Biswanath Mukherjee and Gerry Pesavento Advanced Technology Lab.at the University of California, Davis."}}
,{"_index":"throwtable","_type":"algorithm","_id":"diffusing-update-algorithm","_score":0,"_source":{"description":"DUAL, the Diffusing Update ALgorithm, is the algorithm used by Cisco's EIGRP routing protocol to ensure that a given route is recalculated globally whenever it might cause a routing loop. It was developed by J.J. Garcia-Luna-Aceves at SRI International. According to Cisco, the full name of the algorithm is DUAL finite-state machine (DUAL FSM). EIGRP is responsible for the routing within an autonomous system and DUAL responds to changes in the routing topology and dynamically adjusts the routing tables of the router automatically.\nEIGRP uses a feasibility condition to ensure that only loop-free routes are ever selected. The feasibility condition is conservative: when the condition is true, no loops can occur, but the condition might under some circumstances reject all routes to a destination although some are loop-free.\nWhen no feasible route to a destination is available, the DUAL algorithm  invokes a Diffusing Computation  to ensure that all traces of the problematic route are eliminated from the network. At which point the normal Bellman–Ford algorithm is used to recover a new route.","name":"Diffusing update algorithm","categories":["Routing algorithms","Routing protocols","SRI International software"],"tag_line":"DUAL, the Diffusing Update ALgorithm, is the algorithm used by Cisco's EIGRP routing protocol to ensure that a given route is recalculated globally whenever it might cause a routing loop."}}
,{"_index":"throwtable","_type":"algorithm","_id":"local-replacement-algorithm","_score":0,"_source":{"description":"With multiple processes competing for frames, page-replacement algorithms can be classified into two broad categories: global replacement and local replacement. Global replacement allows a process to select a replacement frame from the set of all frames, even if that frame is currently allocated to some other process; that is; one process can take a frame from another. Local replacement requires that each process select from only its own set of allocated frames.","name":"Local replacement algorithm","categories":["All articles covered by WikiProject Wikify","All articles with too few wikilinks","Articles covered by WikiProject Wikify from October 2012","Articles with too few wikilinks from October 2012","Memory management algorithms","Use dmy dates from August 2012","Virtual memory"],"tag_line":"With multiple processes competing for frames, page-replacement algorithms can be classified into two broad categories: global replacement and local replacement."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adaptive-replacement-cache","_score":0,"_source":{"description":"Adaptive Replacement Cache (ARC) is a page replacement algorithm with better performance than LRU (Least Recently Used) developed at the IBM Almaden Research Center. This is accomplished by keeping track of both Frequently Used and Recently Used pages plus a recent eviction history for both. In 2006, IBM was granted a patent for the adaptive replacement cache policy.\n\n","name":"Adaptive replacement cache","categories":["Memory management algorithms","Use dmy dates from August 2012","Virtual memory"],"tag_line":"Adaptive Replacement Cache (ARC) is a page replacement algorithm with better performance than LRU (Least Recently Used) developed at the IBM Almaden Research Center."}}
,{"_index":"throwtable","_type":"algorithm","_id":"deficit-round-robin","_score":0,"_source":{"description":"Deficit Round Robin (DRR), also Deficit Weighted Round Robin (DWRR), is a scheduling algorithm for the network scheduler. DRR is, like weighted fair queuing (WFQ), a packet-based implementation of the ideal Generalized Processor Sharing (GPS) policy. It was proposed by M. Shreedhar and G. Varghese in 1995 as an efficient (with O(1) complexity) and fair algorithm.","name":"Deficit round robin","categories":["Network scheduling algorithms"],"tag_line":"Deficit Round Robin (DRR), also Deficit Weighted Round Robin (DWRR), is a scheduling algorithm for the network scheduler."}}
,{"_index":"throwtable","_type":"algorithm","_id":"exponential-backoff","_score":0,"_source":{"description":"Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, in order to gradually find an acceptable rate.\n\n","name":"Exponential backoff","categories":["Ethernet","Networking algorithms","Scheduling algorithms","Use dmy dates from July 2013","Wikipedia articles incorporating text from the Federal Standard 1037C"],"tag_line":"Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, in order to gradually find an acceptable rate.\n\n"}}
,{"_index":"throwtable","_type":"algorithm","_id":"chung-kwei-(algorithm)","_score":0,"_source":{"description":"Chung Kwei is a spam filtering algorithm based on the TEIRESIAS Algorithm for finding coding genes within bulk DNA.","name":"Chung Kwei (algorithm)","categories":["Algorithms and data structures stubs","All stub articles","Anti-spam","Computer science stubs","Networking algorithms"],"tag_line":"Chung Kwei is a spam filtering algorithm based on the TEIRESIAS Algorithm for finding coding genes within bulk DNA."}}
,{"_index":"throwtable","_type":"algorithm","_id":"euclidean-algorithm","_score":0,"_source":{"description":"In mathematics, the Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two numbers, the largest number that divides both of them without leaving a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in Euclid's Elements (c. 300 BC). It is an example of an algorithm, a step-by-step procedure for performing a calculation according to well-defined rules, and is one of the oldest numerical algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.\nThe Euclidean algorithm is based on the principle that the greatest common divisor of two numbers does not change if the larger number is replaced by its difference with the smaller number. For example, 21 is the GCD of 252 and 105 (252 = 21 × 12 and 105 = 21 × 5), and the same number 21 is also the GCD of 105 and 147 = 252 − 105. Since this replacement reduces the larger of the two numbers, repeating this process gives successively smaller pairs of numbers until one of the two numbers reaches zero. When that occurs, the other number (the one that is not zero) is the GCD of the original two numbers. By reversing the steps, the GCD can be expressed as a sum of the two original numbers each multiplied by a positive or negative integer, e.g., 21 = 5 × 105 + (−2) × 252. The fact that the GCD can always be expressed in this way is known as Bézout's identity.\nThe version of the Euclidean algorithm described above (and by Euclid) can take many subtraction steps to find the GCD when one of the given numbers is much bigger than the other. A more efficient version of the algorithm shortcuts these steps, instead replacing the larger of the two numbers by its remainder when divided by the smaller of the two. With this improvement, the algorithm never requires more steps than five times the number of digits (base 10) of the smaller integer. This was proven by Gabriel Lamé in 1844, and marks the beginning of computational complexity theory. Additional methods for improving the algorithm's efficiency were developed in the 20th century.\nThe Euclidean algorithm has many theoretical and practical applications. It is used for reducing fractions to their simplest form and for performing division in modular arithmetic. Computations using this algorithm form part of the cryptographic protocols that are used to secure internet communications, and in methods for breaking these cryptosystems by factoring large composite numbers. The Euclidean algorithm may be used to solve Diophantine equations, such as finding numbers that satisfy multiple congruences according to the Chinese remainder theorem, to construct continued fractions, and to find accurate rational approximations to real numbers. Finally, it is a basic tool for proving theorems in number theory such as Lagrange's four-square theorem and the uniqueness of prime factorizations. The original algorithm was described only for natural numbers and geometric lengths (real numbers), but the algorithm was generalized in the 19th century to other types of numbers, such as Gaussian integers and polynomials of one variable. This led to modern abstract algebraic notions such as Euclidean domains.","name":"Euclidean algorithm","categories":["Articles containing proofs","Articles with example pseudocode","CS1 German-language sources (de)","Commons category with local link same as on Wikidata","Euclid","Featured articles","Number theoretic algorithms"],"tag_line":"In mathematics, the Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two numbers, the largest number that divides both of them without leaving a remainder."}}
,{"_index":"throwtable","_type":"algorithm","_id":"distance-vector-routing-protocol","_score":0,"_source":{"description":"In computer communication theory relating to packet-switched networks, a distance-vector routing protocol is one of the two major classes of intra domain routing protocols, the other major class being the link-state protocol. Distance-vector routing protocols use the Bellman–Ford algorithm, Ford–Fulkerson algorithm, or DUAL FSM (in the case of Cisco Systems's protocols) to calculate paths.\nA distance-vector routing protocol requires that a router inform its neighbors of topology changes periodically. Compared to link-state protocols, which require a router to inform all the nodes in a network of topology changes, distance-vector routing protocols have less computational complexity and message overhead.\nThe term distance vector refers to the fact that the protocol manipulates vectors (arrays) of distances to other nodes in the network. The vector distance algorithm was the original ARPANET routing algorithm and was also used in the internet under the name of RIP (Routing Information Protocol).\nExamples of distance-vector routing protocols include RIPv1 and RIPv2 and IGRP.","name":"Distance-vector routing protocol","categories":["All articles lacking in-text citations","All articles needing expert attention","All articles that are too technical","All articles with unsourced statements","Articles lacking in-text citations from September 2010","Articles needing expert attention from November 2013","Articles with unsourced statements from January 2009","Articles with unsourced statements from October 2015","Routing algorithms","Routing protocols","Wikipedia articles that are too technical from November 2013"],"tag_line":"In computer communication theory relating to packet-switched networks, a distance-vector routing protocol is one of the two major classes of intra domain routing protocols, the other major class being the link-state protocol."}}
,{"_index":"throwtable","_type":"algorithm","_id":"link-state-routing-protocol","_score":0,"_source":{"description":"Link-state routing protocols are one of the two main classes of routing protocols used in packet switching networks for computer communications, the other being distance-vector routing protocols. Examples of link-state routing protocols include open shortest path first (OSPF) and intermediate system to intermediate system (IS-IS).\nThe link-state protocol is performed by every switching node in the network (i.e., nodes that are prepared to forward packets; in the Internet, these are called routers). The basic concept of link-state routing is that every node constructs a map of the connectivity to the network, in the form of a graph, showing which nodes are connected to which other nodes. Each node then independently calculates the next best logical path from it to every possible destination in the network. The collection of best paths will then form the node's routing table.\nThis contrasts with distance-vector routing protocols, which work by having each node share its routing table with its neighbours. In a link-state protocol the only information passed between nodes is connectivity related.\nLink-state algorithms are sometimes characterized informally as each router 'telling the world about its neighbours\"","name":"Link-state routing protocol","categories":["All articles lacking in-text citations","All articles with unsourced statements","Articles lacking in-text citations from September 2010","Articles with unsourced statements from February 2013","Routing algorithms","Routing protocols"],"tag_line":"Link-state routing protocols are one of the two main classes of routing protocols used in packet switching networks for computer communications, the other being distance-vector routing protocols."}}
,{"_index":"throwtable","_type":"algorithm","_id":"extended-euclidean-algorithm","_score":0,"_source":{"description":"In arithmetic and computer programming, the extended Euclidean algorithm is an extension to the Euclidean algorithm, which computes, besides the greatest common divisor of integers a and b, the coefficients of Bézout's identity, that is integers x and y such that\n\nIt allows one to compute also, with almost no extra cost, the quotients of a and b by their greatest common divisor.\nExtended Euclidean algorithm also refers to a very similar algorithm for computing the polynomial greatest common divisor and the coefficients of Bézout's identity of two univariate polynomials.\nThe extended Euclidean algorithm is particularly useful when a and b are coprime, since x is the modular multiplicative inverse of a modulo b, and y is the modular multiplicative inverse of b modulo a. Similarly, the polynomial extended Euclidean algorithm allows one to compute the multiplicative inverse in algebraic field extensions and, in particular in finite fields of non prime order. It follows that both extended Euclidean algorithms are widely used in cryptography. In particular, the computation of the modular multiplicative inverse is an essential step in RSA public-key encryption method.","name":"Extended Euclidean algorithm","categories":["Articles with example pseudocode","Euclid","Number theoretic algorithms"],"tag_line":"In arithmetic and computer programming, the extended Euclidean algorithm is an extension to the Euclidean algorithm, which computes, besides the greatest common divisor of integers a and b, the coefficients of Bézout's identity, that is integers x and y such that\n\nIt allows one to compute also, with almost no extra cost, the quotients of a and b by their greatest common divisor."}}
,{"_index":"throwtable","_type":"algorithm","_id":"chakravala-method","_score":0,"_source":{"description":"The chakravala method (Sanskrit: चक्रवाल विधि) is a cyclic algorithm to solve indeterminate quadratic equations, including Pell's equation. It is commonly attributed to Bhāskara II, (c. 1114 – 1185 CE) although some attribute it to Jayadeva (c. 950 ~ 1000 CE). Jayadeva pointed out that Brahmagupta's approach to solving equations of this type could be generalized, and he then described this general method, which was later refined by Bhāskara II in his Bijaganita treatise. He called it the Chakravala method: chakra meaning \"wheel\" in Sanskrit, a reference to the cyclic nature of the algorithm. E. O. Selenius held that no European performances at the time of Bhāskara, nor much later, exceeded its marvellous height of mathematical complexity.\nThis method is also known as the cyclic method and contains traces of mathematical induction.","name":"Chakravala method","categories":["Articles containing Sanskrit-language text","Brahmagupta","Diophantine equations","Indian mathematics","Number theoretic algorithms"],"tag_line":"The chakravala method (Sanskrit: चक्रवाल विधि) is a cyclic algorithm to solve indeterminate quadratic equations, including Pell's equation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"klee–minty-cube","_score":0,"_source":{"description":"The Klee–Minty cube (named after Victor Klee and George J. Minty) is a unit cube whose corners have been slightly perturbed. Klee and Minty demonstrated that Dantzig's simplex algorithm has poor worst-case performance when initialized at one corner of their \"squashed cube\".\nIn particular, many optimization algorithms for linear optimization exhibit poor performance when applied to the Klee–Minty cube. In 1973 Klee and Minty showed that Dantzig's simplex algorithm was not a polynomial-time algorithm when applied to their cube. Later, modifications of the Klee–Minty cube have shown poor behavior both for other basis-exchange pivoting algorithms and also for interior-point algorithms.","name":"Klee–Minty cube","categories":["All articles to be expanded","All articles with unsourced statements","Analysis of algorithms","Articles to be expanded from April 2011","Articles with unsourced statements from October 2015","Computational complexity theory","Convex geometry","Cubes","Linear programming","Mathematical optimization","Pages containing links to subscription-only content","Pages using duplicate arguments in template calls"],"tag_line":"The Klee–Minty cube (named after Victor Klee and George J. Minty) is a unit cube whose corners have been slightly perturbed."}}
,{"_index":"throwtable","_type":"algorithm","_id":"multicanonical-ensemble","_score":0,"_source":{"description":"In statistics and physics, multicanonical ensemble (also called multicanonical sampling or flat histogram) is a Markov chain Monte Carlo sampling technique that uses the Metropolis–Hastings algorithm to compute integrals where the integrand has a rough landscape with multiple local minima. It samples states according to the inverse of the density of states, which has to be known a priori or be computed using other techniques like the Wang and Landau algorithm. Multicanonical sampling is an important technique for spin systems like the Ising model or spin glasses.","name":"Multicanonical ensemble","categories":["Computational physics","Monte Carlo methods","Statistical algorithms"],"tag_line":"In statistics and physics, multicanonical ensemble (also called multicanonical sampling or flat histogram) is a Markov chain Monte Carlo sampling technique that uses the Metropolis–Hastings algorithm to compute integrals where the integrand has a rough landscape with multiple local minima."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rrqr-factorization","_score":0,"_source":{"description":"An RRQR factorization or rank-revealing QR factorization is a matrix decomposition algorithm based on the QR factorization which can be used to determine the rank of a matrix. The SVD can be used to generate an RRQR, but it is not an efficient method to do so. A RRQR implementation is available for MATLAB.","name":"RRQR factorization","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Linear algebra stubs","Matrix decompositions","Numerical linear algebra","Use dmy dates from April 2011"],"tag_line":"An RRQR factorization or rank-revealing QR factorization is a matrix decomposition algorithm based on the QR factorization which can be used to determine the rank of a matrix."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lemke's-algorithm","_score":0,"_source":{"description":"In mathematical optimization, Lemke's algorithm is a procedure for solving linear complementarity problems, and more generally mixed linear complementarity problems.\nLemke's algorithm is of pivoting or basis-exchange type. Similar algorithms can compute Nash equilibria for two-person matrix and bimatrix games.","name":"Lemke's algorithm","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Mathematical optimization","Optimization algorithms and methods"],"tag_line":"In mathematical optimization, Lemke's algorithm is a procedure for solving linear complementarity problems, and more generally mixed linear complementarity problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"newton's-method","_score":0,"_source":{"description":"In numerical analysis, Newton's method (also known as the Newton–Raphson method), named after Isaac Newton and Joseph Raphson, is a method for finding successively better approximations to the roots (or zeroes) of a real-valued function.\n\nThe Newton–Raphson method in one variable is implemented as follows:\nGiven a function ƒ defined over the reals x, and its derivative ƒ', we begin with a first guess x0 for a root of the function f. Provided the function satisfies all the assumptions made in the derivation of the formula, a better approximation x1 is\n\nGeometrically, (x1, 0) is the intersection with the x-axis of the tangent to the graph of f at (x0, f (x0)).\nThe process is repeated as\n\nuntil a sufficiently accurate value is reached.\nThis algorithm is first in the class of Householder's methods, succeeded by Halley's method. The method can also be extended to complex functions and to systems of equations.","name":"Newton's method","categories":["All articles lacking in-text citations","All articles needing additional references","Articles lacking in-text citations from February 2014","Articles needing additional references from February 2014","Articles needing additional references from November 2013","Articles with inconsistent citation formats","Commons category with local link same as on Wikidata","Optimization algorithms and methods","Root-finding algorithms","Use dmy dates from January 2012"],"tag_line":"In numerical analysis, Newton's method (also known as the Newton–Raphson method), named after Isaac Newton and Joseph Raphson, is a method for finding successively better approximations to the roots (or zeroes) of a real-valued function."}}
,{"_index":"throwtable","_type":"algorithm","_id":"list-update-problem","_score":0,"_source":{"description":"The List Update or the List Access problem is a simple model used in the study of competitive analysis of online algorithms. Given a set of items in a list where the cost of accessing an item is proportional to its distance from the head of the list, e.g. a Linked List, and a request sequence of accesses, the problem is to come up with a strategy of reordering the list so that the total cost of accesses is minimized. The reordering can be done at any time but incurs a cost. The standard model includes two reordering actions:\nA free transposition of the item being accessed anywhere ahead of its current position;\nA paid transposition of a unit cost for exchanging any two items in the list. Performance of algorithms depend on the construction of request sequences by adversaries under various Adversary models\nAn online algorithm for this problem has to reorder the elements and serve requests based only on the knowledge of previously requested items and hence its strategy may not have the optimum cost as compared to an offline algorithm that gets to see the entire request sequence and devise a complete strategy before serving the first request.","name":"List update problem","categories":["Analysis of algorithms","Online algorithms","Randomized algorithms"],"tag_line":"The List Update or the List Access problem is a simple model used in the study of competitive analysis of online algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"soundex","_score":0,"_source":{"description":"Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English. The goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. Soundex is the most widely known of all phonetic algorithms (in part because it is a standard feature of popular database software such as DB2, PostgreSQL, MySQL, Ingres, MS SQL Server and Oracle) and is often used (incorrectly) as a synonym for \"phonetic algorithm\". Improvements to Soundex are the basis for many modern phonetic algorithms.","name":"Soundex","categories":["All articles with unsourced statements","Articles with unsourced statements from August 2007","Phonetic algorithms"],"tag_line":"Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English."}}
,{"_index":"throwtable","_type":"algorithm","_id":"daitch–mokotoff-soundex","_score":0,"_source":{"description":"Daitch–Mokotoff Soundex (D–M Soundex) is a phonetic algorithm invented in 1985 by Jewish genealogists Gary Mokotoff and Randy Daitch. It is a refinement of the Russell and American Soundex algorithms designed to allow greater accuracy in matching of Slavic and Yiddish surnames with similar pronunciation but differences in spelling.\nDaitch–Mokotoff Soundex is sometimes referred to as \"Jewish Soundex\" and \"Eastern European Soundex\", although the authors discourage use of these nicknames for the algorithm because the algorithm itself is independent of the fact the motivation for creating the new system was the poor results of predecessor systems when dealing with Slavic and Yiddish surnames.\n\n","name":"Daitch–Mokotoff Soundex","categories":["Genealogy","Phonetic algorithms"],"tag_line":"Daitch–Mokotoff Soundex (D–M Soundex) is a phonetic algorithm invented in 1985 by Jewish genealogists Gary Mokotoff and Randy Daitch."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lehmer–schur-algorithm","_score":0,"_source":{"description":"In mathematics, the Lehmer–Schur algorithm (named after Derrick Henry Lehmer and Issai Schur) is a root-finding algorithm extending the idea of enclosing roots like in the one-dimensional bisection method to the complex plane. It uses the Schur–Cohn test to test increasingly smaller disks for the presence or absence of roots. Alternative methods like Wilf's algorithm apply different tests to differently shaped areas but keep the idea of descent by subdivision.","name":"Lehmer–Schur algorithm","categories":["All articles lacking reliable references","Articles lacking reliable references from June 2011","Root-finding algorithms"],"tag_line":"In mathematics, the Lehmer–Schur algorithm (named after Derrick Henry Lehmer and Issai Schur) is a root-finding algorithm extending the idea of enclosing roots like in the one-dimensional bisection method to the complex plane."}}
,{"_index":"throwtable","_type":"algorithm","_id":"redos","_score":0,"_source":{"description":"The regular expression denial of service (ReDoS) is an algorithmic complexity attack that produces a denial-of-service by providing a regular expression that takes a very long time to evaluate. The attack exploits the fact that most regular expression implementations have exponential time worst case complexity: the time taken can grow exponentially in relation to input size. An attacker can thus cause a program to spend an effectively infinite amount of time processing by providing such a regular expression, either slowing down or becoming unresponsive.","name":"ReDoS","categories":["Algorithmic complexity attacks","Denial-of-service attacks","Pattern matching","Regular expressions"],"tag_line":"The regular expression denial of service (ReDoS) is an algorithmic complexity attack that produces a denial-of-service by providing a regular expression that takes a very long time to evaluate."}}
,{"_index":"throwtable","_type":"algorithm","_id":"graeffe's-method","_score":0,"_source":{"description":"In mathematics, Graeffe's method or Dandelin–Graeffe method is an algorithm for finding all of the roots of a polynomial. It was developed independently by Germinal Pierre Dandelin in 1826 and Karl Heinrich Gräffe in 1837. Lobachevsky in 1834 also discovered the principal idea of the method. The method separates the roots of a polynomial by squaring them repeatedly. This squaring of the roots is done implicitly, that is, only working on the coefficients of the polynomial. Finally, Viète's formulas are used in order to approximate the roots.","name":"Graeffe's method","categories":["Root-finding algorithms"],"tag_line":"In mathematics, Graeffe's method or Dandelin–Graeffe method is an algorithm for finding all of the roots of a polynomial."}}
,{"_index":"throwtable","_type":"algorithm","_id":"reteoo","_score":0,"_source":{"description":"ReteOO is an improved version of the Rete algorithm.\nRete supports only boolean, first order logic.","name":"ReteOO","categories":["Algorithms and data structures stubs","All orphaned articles","All stub articles","Computer science stubs","Expert systems","Orphaned articles from February 2013","Pattern matching"],"tag_line":"ReteOO is an improved version of the Rete algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bitap-algorithm","_score":0,"_source":{"description":"The bitap algorithm (also known as the shift-or, shift-and or Baeza-Yates–Gonnet algorithm) is an approximate string matching algorithm. The algorithm tells whether a given text contains a substring which is \"approximately equal\" to a given pattern, where approximate equality is defined in terms of Levenshtein distance — if the substring and pattern are within a given distance k of each other, then the algorithm considers them equal. The algorithm begins by precomputing a set of bitmasks containing one bit for each element of the pattern. Then it is able to do most of the work with bitwise operations, which are extremely fast.\nThe bitap algorithm is perhaps best known as one of the underlying algorithms of the Unix utility agrep, written by Udi Manber, Sun Wu, and Burra Gopal. Manber and Wu's original paper gives extensions of the algorithm to deal with fuzzy matching of general regular expressions.\nDue to the data structures required by the algorithm, it performs best on patterns less than a constant length (typically the word length of the machine in question), and also prefers inputs over a small alphabet. Once it has been implemented for a given alphabet and word length m, however, its running time is completely predictable — it runs in O(mn) operations, no matter the structure of the text or the pattern.\nThe bitap algorithm for exact string searching was invented by Bálint Dömölki in 1964[1][2] and extended by R. K. Shyamasundar in 1977[3], before being reinvented in the context of fuzzy string searching by Manber and Wu in 1991[4][5] based on work done by Ricardo Baeza-Yates and Gaston Gonnet[6]. The algorithm was improved by Baeza-Yates and Navarro in 1996[7] and later by Gene Myers for long patterns in 1998[8].","name":"Bitap algorithm","categories":["Articles with example C code","String matching algorithms"],"tag_line":"The bitap algorithm (also known as the shift-or, shift-and or Baeza-Yates–Gonnet algorithm) is an approximate string matching algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"string-searching-algorithm","_score":0,"_source":{"description":"In computer science, string searching algorithms, sometimes called string matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.\nLet Σ be an alphabet (finite set). Formally, both the pattern and searched text are vectors of elements of Σ. The Σ may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (Σ = {0,1}) or DNA alphabet (Σ = {A,C,G,T}) in bioinformatics.\nIn practice, how the string is encoded can affect the feasible string search algorithms. In particular if a variable width encoding is in use then it is slow (time proportional to N) to find the Nth character. This will significantly slow down many of the more advanced search algorithms. A possible solution is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.","name":"String searching algorithm","categories":["All articles needing additional references","Articles needing additional references from July 2013","String matching algorithms"],"tag_line":"In computer science, string searching algorithms, sometimes called string matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text."}}
,{"_index":"throwtable","_type":"algorithm","_id":"boyer–moore-string-search-algorithm","_score":0,"_source":{"description":"In computer science, the Boyer–Moore string search algorithm is an efficient string searching algorithm that is the standard benchmark for practical string search literature. It was developed by Robert S. Boyer and J Strother Moore in 1977. The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.","name":"Boyer–Moore string search algorithm","categories":["Algorithms on strings","Articles with example C code","Articles with example Java code","Articles with example Python code","String matching algorithms"],"tag_line":"In computer science, the Boyer–Moore string search algorithm is an efficient string searching algorithm that is the standard benchmark for practical string search literature."}}
,{"_index":"throwtable","_type":"algorithm","_id":"boyer–moore–horspool-algorithm","_score":0,"_source":{"description":"In computer science, the Boyer–Moore–Horspool algorithm or Horspool's algorithm is an algorithm for finding substrings in strings. It was published by Nigel Horspool in 1980.\nIt is a simplification of the Boyer–Moore string search algorithm which is related to the Knuth–Morris–Pratt algorithm. The algorithm trades space for time in order to obtain an average-case complexity of O(N) on random text, although it has O(MN) in the worst case, where the length of the pattern is M and the length of the search string is N.","name":"Boyer–Moore–Horspool algorithm","categories":["All accuracy disputes","All articles needing additional references","All articles to be merged","Articles needing additional references from October 2015","Articles to be merged from March 2015","Articles with disputed statements from June 2015","Articles with example C code","Pages using citations with accessdate and no URL","String matching algorithms"],"tag_line":"In computer science, the Boyer–Moore–Horspool algorithm or Horspool's algorithm is an algorithm for finding substrings in strings."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pseudo-polynomial-time","_score":0,"_source":{"description":"In computational complexity theory, a numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input, but is exponential in the length of the input – the number of bits required to represent it.\nAn NP-complete problem with known pseudo-polynomial time algorithms is called weakly NP-complete. An NP-complete problem is called strongly NP-complete if it is proven that it cannot be solved by a pseudo-polynomial time algorithm unless P=NP. The strong/weak kinds of NP-hardness are defined analogously.\n\n","name":"Pseudo-polynomial time","categories":["Analysis of algorithms","Complexity classes","Computational complexity theory","Pseudo-polynomial time algorithms"],"tag_line":"In computational complexity theory, a numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input, but is exponential in the length of the input – the number of bits required to represent it."}}
,{"_index":"throwtable","_type":"algorithm","_id":"raita-algorithm","_score":0,"_source":{"description":"In computer science, the Raita algorithm is a string searching algorithm which improves the performance of Boyer-Moore-Horspool algorithm. This algorithm preprocesses the string being searched for the pattern, which is similar to Boyer-Moore string search algorithm. The searching pattern of particular sub-string in a given string is different from Boyer-Moore-Horspool algorithm. This algorithm was published by Tim Raita in 1991.","name":"Raita Algorithm","categories":["All articles needing additional references","All articles to be merged","All articles with topics of unclear notability","All orphaned articles","Articles needing additional references from March 2015","Articles to be merged from March 2015","Articles with topics of unclear notability from March 2015","Orphaned articles from May 2013","String matching algorithms"],"tag_line":"In computer science, the Raita algorithm is a string searching algorithm which improves the performance of Boyer-Moore-Horspool algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"atlantic-city-algorithm","_score":0,"_source":{"description":"An Atlantic City algorithm is a probabilistic polynomial-time algorithm that answers correctly at least 75% of the time (or, in some versions, some other value greater than 50%). The term \"Atlantic City\" was first introduced in 1982 by J. Finn in an unpublished manuscript entitled Comparison of probabilistic tests for primality.\nTwo other common classes of probabilistic algorithms are Monte Carlo algorithms and Las Vegas algorithms. Monte Carlo algorithms are always fast, but only probably correct. On the other hand, Las Vegas algorithms are always correct, but only probably fast. The Atlantic City algorithms which are bounded probabilistic polynomial time algorithms are probably correct and probably fast.\n\n","name":"Atlantic City algorithm","categories":["Cryptography","Randomized algorithms"],"tag_line":"An Atlantic City algorithm is a probabilistic polynomial-time algorithm that answers correctly at least 75% of the time (or, in some versions, some other value greater than 50%)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quantum-phase-estimation-algorithm","_score":0,"_source":{"description":"In quantum computing, the quantum phase estimation algorithm is a quantum algorithm that finds many applications as a subroutine in other algorithms. The quantum phase estimation algorithm allows one to estimate the eigenphase of an eigenvector of a unitary gate, given access to a quantum state proportional to the eigenvector and a procedure to implement the unitary conditionally.","name":"Quantum phase estimation algorithm","categories":["All Wikipedia articles needing context","All articles needing expert attention","All pages needing cleanup","Articles needing expert attention from September 2010","Articles needing expert attention with no reason or talk parameter","Articles needing unspecified expert attention","Quantum algorithms","Wikipedia articles needing context from September 2010","Wikipedia introduction cleanup from September 2010"],"tag_line":"In quantum computing, the quantum phase estimation algorithm is a quantum algorithm that finds many applications as a subroutine in other algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hidden-subgroup-problem","_score":0,"_source":{"description":"The hidden subgroup problem (HSP) is a topic of research in mathematics and theoretical computer science. The framework captures problems like factoring, graph isomorphism, and the shortest vector problem. This makes it especially important in the theory of quantum computing because Shor's quantum algorithm for factoring is essentially equivalent to the hidden subgroup problem for finite Abelian groups, while the other problems correspond to finite groups that are not Abelian.","name":"Hidden subgroup problem","categories":["Group theory","Quantum algorithms"],"tag_line":"The hidden subgroup problem (HSP) is a topic of research in mathematics and theoretical computer science."}}
,{"_index":"throwtable","_type":"algorithm","_id":"amplitude-amplification","_score":0,"_source":{"description":"Amplitude amplification is a technique in quantum computing which generalizes the idea behind the Grover's search algorithm, and gives rise to a family of quantum algorithms. It was discovered by Gilles Brassard and Peter Høyer in 1997,  and independently rediscovered by Lov Grover in 1998. \nIn a quantum computer, amplitude amplification can be used to obtain a quadratic speedup over several classical algorithms.","name":"Amplitude amplification","categories":["Quantum algorithms","Search algorithms"],"tag_line":"Amplitude amplification is a technique in quantum computing which generalizes the idea behind the Grover's search algorithm, and gives rise to a family of quantum algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"entropy-compression","_score":0,"_source":{"description":"In mathematics and theoretical computer science, entropy compression is an information theoretic method for proving that a random process terminates, originally used by Robin Moser to prove an algorithmic version of the Lovász local lemma.","name":"Entropy compression","categories":["Analysis of algorithms","Probabilistic complexity theory","Randomized algorithms"],"tag_line":"In mathematics and theoretical computer science, entropy compression is an information theoretic method for proving that a random process terminates, originally used by Robin Moser to prove an algorithmic version of the Lovász local lemma."}}
,{"_index":"throwtable","_type":"algorithm","_id":"deadline-monotonic-scheduling","_score":0,"_source":{"description":"Deadline-monotonic priority assignment is a priority assignment policy used with fixed priority pre-emptive scheduling.\nWith deadline-monotonic priority assignment, tasks are assigned priorities according to their deadlines; the task with the shortest deadline being assigned the highest priority.\nThis priority assignment policy is optimal for a set of periodic or sporadic tasks which comply with the following restrictive system model:\nAll tasks have deadlines less than or equal to their minimum inter-arrival times (or periods).\nAll tasks have worst-case execution times (WCET) that are less than or equal to their deadlines.\nAll tasks are independent and so do not block each other's execution (for example by accessing mutually exclusive shared resources).\nNo task voluntarily suspends itself.\nThere is some point in time, referred to as a critical instant, where all of the tasks become ready to execute simultaneously.\nScheduling overheads (switching from one task to another) are zero.\nAll tasks have zero release jitter (the time from the task arriving to it becoming ready to execute).\nIf restriction 7 is lifted, then \"deadline minus jitter\" monotonic priority assignment is optimal.\nIf restriction 1. is lifted allowing deadlines greater than periods, then Audsley's optimal priority assignment algorithm may be used to find the optimal priority assignment.\nDeadline monotonic priority assignment is not optimal for fixed priority non-pre-emptive scheduling.\nA fixed priority assignment policy P is referred to as optimal if no task set exists which is schedulable using a different priority assignment policy which is not also schedulable using priority assignment policy P. Or in other words: Deadline-monotonic priority assignment (DMPA) policy is optimal if any process set, Q, that is schedulable by priority scheme, W, is also schedulable by DMPA","name":"Deadline-monotonic scheduling","categories":["All articles covered by WikiProject Wikify","All articles with too few wikilinks","All stub articles","Articles covered by WikiProject Wikify from December 2012","Articles with too few wikilinks from December 2012","Computer science stubs","Processor scheduling algorithms"],"tag_line":"Deadline-monotonic priority assignment is a priority assignment policy used with fixed priority pre-emptive scheduling."}}
,{"_index":"throwtable","_type":"algorithm","_id":"interval-scheduling","_score":0,"_source":{"description":"Interval scheduling is a class of problems in computer science, particularly in the area of algorithm design. The problems consider a set of tasks. Each task is represented by an interval describing the time in which it needs to be executed. For instance, task A might run from 2:00 to 5:00, task B might run from 4:00 to 10:00 and task C might run from 9:00 to 11:00. A subset of intervals is compatible if no two intervals overlap. For example, the subset {A,C} is compatible, as is the subset {B}; but neither {A,B} nor {B,C} are compatible subsets, because the corresponding intervals within each subset overlap.\nThe interval scheduling maximization problem (ISMP) is to find a largest compatible set - a set of non-overlapping intervals of maximum size. The goal here is to execute as many tasks as possible.\nIn an upgraded version of the problem, the intervals are partitioned into groups. A subset of intervals is compatible if no two intervals overlap, and moreover, no two intervals belong to the same group (i.e. the subset contains at most a single representative interval of each group).\nThe group interval scheduling decision problem (GISDP) is to decide whether there exists a compatible set in which all groups are represented. The goal here is to execute a single representative task from each group. GISDPk is a restricted version of GISDP in which the number of intervals in each group is at most k.\nThe group interval scheduling maximization problem (GISMP) is to find a largest compatible set - a set of non-overlapping representatives of maximum size. The goal here is to execute a representative task from as many groups as possible. GISMPk is a restricted version of GISMP in which the number of intervals in each group is at most k. This problem is often called JISPk, where J stands for Job.\nGISMP is the most general problem; the other two problems can be seen as special cases of it:\nISMP is the special case in which each task belongs to its own group (i.e. it is equal to GISMP1).\nGISDP is the problem of deciding whether the maximum is exactly equal to the number of groups.","name":"Interval scheduling","categories":["Scheduling algorithms"],"tag_line":"Interval scheduling is a class of problems in computer science, particularly in the area of algorithm design."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shortest-job-next","_score":0,"_source":{"description":"Shortest job next (SJN), also known as Shortest Job First (SJF) or Shortest Process Next (SPN), is a scheduling policy that selects the waiting process with the smallest execution time to execute next. SJN is a non-preemptive algorithm. Shortest remaining time is a preemptive variant of SJN.\nShortest job next is advantageous because of its simplicity and because it minimizes the average amount of time each process has to wait until its execution is complete. However, it has the potential for process starvation for processes which will require a long time to complete if short processes are continually added. Highest response ratio next is similar but provides a solution to this problem.\nAnother disadvantage of using shortest job next is that the total execution time of a job must be known before execution. While it is not possible to perfectly predict execution time, several methods can be used to estimate the execution time for a job, such as a weighted average of previous execution times.\nShortest job next can be effectively used with interactive processes which generally follow a pattern of alternating between waiting for a command and executing it. If the execution burst of a process is regarded as a separate \"job\", past behaviour can indicate which process to run next, based on an estimate of its running time.\nShortest job next is used in specialized environments where accurate estimates of running time are available. Estimating the running time of queued processes is sometimes done using a technique called aging.","name":"Shortest job next","categories":["Processor scheduling algorithms"],"tag_line":"Shortest job next (SJN), also known as Shortest Job First (SJF) or Shortest Process Next (SPN), is a scheduling policy that selects the waiting process with the smallest execution time to execute next."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fibonacci-search-technique","_score":0,"_source":{"description":"In computer science, the Fibonacci search technique is a method of searching a sorted array using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers. Compared to binary search, Fibonacci search examines locations whose addresses have lower dispersion. Therefore, when the elements being searched have non-uniform access memory storage (i.e., the time needed to access a storage location varies depending on the location previously accessed), the Fibonacci search has an advantage over binary search in slightly reducing the average time needed to access a storage location. The typical example of non-uniform access storage is that of a magnetic tape, where the time to access a particular element is proportional to its distance from the element currently under the tape's head. Note, however, that large arrays not fitting in CPU cache or even in RAM can also be considered as non-uniform access examples. Fibonacci search has a complexity of O(log(n)) (see Big O notation).\nFibonacci search was first devised by Jack Kiefer (1953) as a minimax search for the maximum (minimum) of a unimodal function in an interval.","name":"Fibonacci search technique","categories":["All articles needing expert attention","All articles that are too technical","Articles needing expert attention from July 2013","Search algorithms","Wikipedia articles that are too technical from July 2013"],"tag_line":"In computer science, the Fibonacci search technique is a method of searching a sorted array using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"difference-map-algorithm","_score":0,"_source":{"description":"The difference-map algorithm is a search algorithm for general constraint satisfaction problems. It is a meta-algorithm in the sense that it is built from more basic algorithms that perform projections onto constraint sets. From a mathematical perspective, the difference-map algorithm is a dynamical system based on a mapping of Euclidean space. Solutions are encoded as fixed points of the mapping.\nAlthough originally conceived as a general method for solving the phase problem, the difference-map algorithm has been used for the boolean satisfiability problem, protein structure prediction, Ramsey numbers, diophantine equations, and Sudoku, as well as sphere- and disk-packing problems. Since these applications include NP-complete problems, the scope of the difference map is that of an incomplete algorithm. Whereas incomplete algorithms can efficiently verify solutions (once a candidate is found), they cannot prove that a solution does not exist.\nThe difference-map algorithm is a generalization of two iterative methods: Fienup's Hybrid input output (HIO) algorithm for phase retrieval  and the Douglas-Rachford algorithm for convex optimization. Iterative methods, in general, have a long history in phase retrieval and convex optimization. The use of this style of algorithm for hard, non-convex problems is a more recent development.","name":"Difference-map algorithm","categories":["Constraint programming","Search algorithms"],"tag_line":"The difference-map algorithm is a search algorithm for general constraint satisfaction problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"alpha–beta-pruning","_score":0,"_source":{"description":"Alpha–beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Go, etc.). It stops completely evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision.\n\n","name":"Alpha–beta pruning","categories":["All articles with dead external links","Articles with dead external links from September 2010","Articles with example pseudocode","Articles with inconsistent citation formats","CS1 errors: dates","Game artificial intelligence","Graph algorithms","Optimization algorithms and methods","Search algorithms"],"tag_line":"Alpha–beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree."}}
,{"_index":"throwtable","_type":"algorithm","_id":"god's-algorithm","_score":0,"_source":{"description":"God's algorithm is a notion originating in discussions of ways to solve the Rubik's Cube puzzle, but which can also be applied to other combinatorial puzzles and mathematical games. It refers to any algorithm which produces a solution having the fewest possible number of moves, the idea being that an omniscient being would know an optimal step from any given configuration.","name":"God's algorithm","categories":["Logic puzzles","Mathematical games","Rubik's Cube","Search algorithms"],"tag_line":"God's algorithm is a notion originating in discussions of ways to solve the Rubik's Cube puzzle, but which can also be applied to other combinatorial puzzles and mathematical games."}}
,{"_index":"throwtable","_type":"algorithm","_id":"backjumping","_score":0,"_source":{"description":"In backtracking algorithms, backjumping is a technique that reduces search space, therefore increasing efficiency. While backtracking always goes up one level in the search tree when all values for a variable have been tested, backjumping may go up more levels. In this article, a fixed order of evaluation of variables  is used, but the same considerations apply to a dynamic order of evaluation.","name":"Backjumping","categories":["Constraint programming","Search algorithms"],"tag_line":"In backtracking algorithms, backjumping is a technique that reduces search space, therefore increasing efficiency."}}
,{"_index":"throwtable","_type":"algorithm","_id":"critical-path-method","_score":0,"_source":{"description":"The critical path method (CPM) is an algorithm for scheduling a set of project activities.","name":"Critical path method","categories":["All articles needing additional references","Articles needing additional references from May 2009","Business terms","Management","Network theory","Operations research","Production and manufacturing","Project management","Scheduling algorithms"],"tag_line":"The critical path method (CPM) is an algorithm for scheduling a set of project activities."}}
,{"_index":"throwtable","_type":"algorithm","_id":"focused-crawler","_score":0,"_source":{"description":"A focused crawler is a web crawler that collects Web pages that satisfy some specific property, by carefully prioritizing the crawl frontier and managing the hyperlink exploration process. Some predicates may be based on simple, deterministic and surface properties. For example, a crawler's mission may be to crawl pages from only the .jp domain. Other predicates may be softer or comparative, e.g., \"crawl pages with large PageRank\", or \"crawl pages about baseball\". An important page property pertains to topics, leading to topical crawlers. For example, a topical crawler may be deployed to collect pages about solar power, or swine flu, while minimizing resources spent fetching pages on other topics. Crawl frontier management may not be the only device used by focused crawlers; they may use a Web directory, an Web text index, backlinks, or any other Web artifact.\nA focused crawler must predict the probability that an unvisited page will be relevant before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton  in a crawler developed in the early days of the Web. Topical crawling was first introduced by Filippo Menczer Chakrabarti et al. coined the term focused crawler and used a text classifier to prioritize the crawl frontier. Andrew McCallum and co-authors also used reinforcement learning to focus crawlers. Diligenti 'et al. traced the context graph leading up to relevant pages, and their text content, to train classifiers. A form of online reinforcement learning has been used along with features extracted from the DOM tree and text of linking pages, to continually train classifiers that guide the crawl. In a review of topical crawling algorithms, Menczer et al.  show that such simple strategies are very effective for short crawls, while more sophisticated techniques such as reinforcement learning and evolutionary adaptation can give the best performance over longer crawls.\nAnother type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. In addition, ontologies can be automatically updated in the crawling process. Dong et al. introduced such an ontology-learning-based crawler using support vector machine to update the content of ontological concepts when crawling Web Pages.\nCrawlers are also focused on page properties other than topics. Cho et al. study a variety of crawl prioritization policies and their effects on the link popularity of fetched pages. Najork and Weiner show that breadth-first crawling, starting from popular seed pages, leads to collecting large-PageRank pages early in the crawl. Refinements involving detection of stale (poorly maintained) pages have been reported by Eiron et al.\nThe performance of a focused crawler depends on the richness of links in the specific topic being searched, and focused crawling usually relies on a general web search engine for providing starting points. Davison presented studies on Web links and text that explain why focused crawling succeeds on broad topics; similar studies were presented by Chakrabarti et al. Seed selection can be important for focused crawlers and significantly influence the crawling efficiency. A whitelist strategy is to start the focus crawl from a list of high quality seed URLs and limit the crawling scope to the domains of these URLs. These high quality seeds should be selected based on a list of URL candidates which are accumulated over a sufficient long period of general web crawling. The whitelist should be updated periodically after it is created.","name":"Focused crawler","categories":["Internet search algorithms","Web crawlers","World Wide Web"],"tag_line":"A focused crawler is a web crawler that collects Web pages that satisfy some specific property, by carefully prioritizing the crawl frontier and managing the hyperlink exploration process."}}
,{"_index":"throwtable","_type":"algorithm","_id":"variation-(game-tree)","_score":0,"_source":{"description":"A Variation can refer to a specific sequence of successive moves in a turn-based game, often used to specify a hypothetical future state of a game that is being played. Although the term is most commonly used in the context of Chess analysis, it has been applied to other games. It also is a useful term used when describing computer tree-search algorithms (for example minimax) for playing games such as Go or Chess.\nA variation can be any number of steps as long as each step would be legal if it were to be played. It is often as far ahead as a human or computer can calculate; or however long is necessary to reach a particular position of interest. It may also lead to a terminal state in the game, in which case the term \"Winning Variation\" or \"Losing Variation\" is sometimes used.","name":"Variation (game tree)","categories":["Combinatorial game theory","Computer chess","Game artificial intelligence","Search algorithms","Trees (graph theory)"],"tag_line":"A Variation can refer to a specific sequence of successive moves in a turn-based game, often used to specify a hypothetical future state of a game that is being played."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fast-walsh–hadamard-transform","_score":0,"_source":{"description":"In computational mathematics, the Hadamard ordered fast Walsh–Hadamard transform (FWHTh) is an efficient algorithm to compute the Walsh–Hadamard transform (WHT). A naive implementation of the WHT would have a computational complexity of O(). The FWHTh requires only  additions or subtractions.\nThe FWHTh is a divide and conquer algorithm that recursively breaks down a WHT of size  into two smaller WHTs of size . This implementation follows the recursive definition of the  Hadamard matrix :\n\nThe  normalization factors for each stage may be grouped together or even omitted.\nThe sequency ordered, also known as Walsh ordered, fast Walsh–Hadamard transform, FWHTw, is obtained by computing the FWHTh as above, and then rearranging the outputs.","name":"Fast Walsh–Hadamard transform","categories":["Algorithms and data structures stubs","All articles lacking in-text citations","All stub articles","Articles lacking in-text citations from September 2015","Computer science stubs","Digital signal processing","Signal processing stubs"],"tag_line":"In computational mathematics, the Hadamard ordered fast Walsh–Hadamard transform (FWHTh) is an efficient algorithm to compute the Walsh–Hadamard transform (WHT)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"buzen's-algorithm","_score":0,"_source":{"description":"In queueing theory, a discipline within the mathematical theory of probability, Buzen's algorithm (or convolution algorithm) is an algorithm for calculating the normalization constant G(N) in the Gordon–Newell theorem. This method was first proposed by Jeffrey P. Buzen in 1973. Computing G(N) is required to compute the stationary probability distribution of a closed queueing network.\nPerforming a naïve computation of the normalising constant requires enumeration of all states. For a system with N jobs and M states there are  states. Buzen's algorithm \"computes G(1), G(2), ..., G(N) using a total of NM multiplications and NM additions.\" This is a significant improvement and allows for computations to be performed with much larger networks.","name":"Buzen's algorithm","categories":["Queueing theory","Statistical algorithms","Stochastic processes"],"tag_line":"In queueing theory, a discipline within the mathematical theory of probability, Buzen's algorithm (or convolution algorithm) is an algorithm for calculating the normalization constant G(N) in the Gordon–Newell theorem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"expectation–maximization-algorithm","_score":0,"_source":{"description":"In statistics, an expectation–maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.","name":"Expectation–maximization algorithm","categories":["Data clustering algorithms","Estimation theory","Machine learning algorithms","Missing data","Optimization algorithms and methods","Statistical algorithms"],"tag_line":"In statistics, an expectation–maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables."}}
,{"_index":"throwtable","_type":"algorithm","_id":"algorithms-for-calculating-variance","_score":0,"_source":{"description":"Algorithms for calculating variance play a major role in computational statistics. A key problem in the design of good algorithms for this problem is that formulas for the variance may involve sums of squares, which can lead to numerical instability as well as to arithmetic overflow when dealing with large values.","name":"Algorithms for calculating variance","categories":["Articles with example Python code","Articles with example pseudocode","Statistical algorithms","Statistical deviation and dispersion"],"tag_line":"Algorithms for calculating variance play a major role in computational statistics."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rapidly-exploring-dense-trees","_score":0,"_source":{"description":"Rapidly exploring dense trees is a family of planning algorithms that includes the rapidly exploring random tree.","name":"Rapidly exploring dense trees","categories":["All articles covered by WikiProject Wikify","All articles with too few wikilinks","Articles covered by WikiProject Wikify from October 2013","Articles with too few wikilinks from October 2013","Search algorithms"],"tag_line":"Rapidly exploring dense trees is a family of planning algorithms that includes the rapidly exploring random tree."}}
,{"_index":"throwtable","_type":"algorithm","_id":"null-move-heuristic","_score":0,"_source":{"description":"In computer chess programs, the null-move heuristic is a heuristic technique used to enhance the speed of the alpha-beta pruning algorithm.","name":"Null-move heuristic","categories":["Computer chess","Heuristics","Search algorithms"],"tag_line":"In computer chess programs, the null-move heuristic is a heuristic technique used to enhance the speed of the alpha-beta pruning algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ransac","_score":0,"_source":{"description":"Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data which contains outliers. It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981.They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations.\nA basic assumption is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and \"outliers\" which are data that do not fit the model. The outliers can come, e.g., from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data.","name":"RANSAC","categories":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from September 2014","Articles with example pseudocode","Articles with unsourced statements from September 2014","Geometry in computer vision","Robust statistics","SRI International","Statistical algorithms","Statistical outliers","Wikipedia articles needing clarification from April 2014","Wikipedia articles needing clarification from September 2014"],"tag_line":"Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data which contains outliers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"winepi","_score":0,"_source":{"description":"In data mining, the WINEPI algorithm is an influential algorithm for episode mining, which helps discover the knowledge hidden in an event sequence.\nWINEPI derives part of its name from the fact that it uses a sliding window to go through the event sequence.\nThe outcome of the algorithm are episode rules describe temporal relationships between events and form an extension of association rules.","name":"WINEPI","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Data mining algorithms"],"tag_line":"In data mining, the WINEPI algorithm is an influential algorithm for episode mining, which helps discover the knowledge hidden in an event sequence."}}
,{"_index":"throwtable","_type":"algorithm","_id":"group-method-of-data-handling","_score":0,"_source":{"description":"Group method of data handling (GMDH) is a family of inductive algorithms for computer-based mathematical modeling of multi-parametric datasets that features fully automatic structural and parametric optimization of models.\nGMDH is used in such fields as data mining, knowledge discovery, prediction, complex systems modeling, optimization and pattern recognition.\nGMDH algorithms are characterized by inductive procedure that performs sorting-out of gradually complicated polynomial models and selecting the best solution by means of the so-called external criterion.\nA GMDH model with multiple inputs and one output is a subset of components of the base function (1):\n\nwhere f are elementary functions dependent on different sets of inputs, a are coefficients and m is the number of the base function components.\nIn order to find the best solution GMDH algorithms consider various component subsets of the base function (1) called partial models. Coefficients of these models are estimated by the least squares method. GMDH algorithms gradually increase the number of partial model components and find a model structure with optimal complexity indicated by the minimum value of an external criterion. This process is called self-organization of models.\nThe most popular base function used in GMDH is the gradually complicated Kolmogorov-Gabor polynomial (2):\n\nThe resulting models are also known as polynomial neural networks. Jürgen Schmidhuber cites GDMH as one of the earliest deep learning methods, remarking that it was used to train eight-layer neural nets as early as 1971.","name":"Group method of data handling","categories":["All articles with failed verification","Articles with failed verification from July 2015","Artificial neural networks","Classification algorithms","Computational statistics","Regression variable selection"],"tag_line":"Group method of data handling (GMDH) is a family of inductive algorithms for computer-based mathematical modeling of multi-parametric datasets that features fully automatic structural and parametric optimization of models."}}
,{"_index":"throwtable","_type":"algorithm","_id":"chase-(algorithm)","_score":0,"_source":{"description":"The Chase is a simple fixed-point algorithm testing and enforcing implication of data dependencies in database systems. It plays important roles in database theory as well as in practice. It is used, directly or indirectly, on an everyday basis by people who design databases, and it is used in commercial systems to reason about the consistency and correctness of a data design. New applications of the chase in meta-data management and data exchange are still being discovered.\nThe Chase has its origins in two seminal papers, one by Alfred V. Aho, Catriel Beeri, and Jeffrey D. Ullman and the other by David Maier, Alberto O. Mendelzon, and Yehoshua Sagiv.\nIn its simplest application the chase is used for testing whether the projection of a relation schema constrained by some functional dependencies onto a given decomposition can be recovered by rejoining the projections. Let t be a tuple in  where R is a relation and F is a set of functional dependencies (FD). If tuples in R are represented as t1, ..., tk, the join of the projections of each ti should agree with t on  where i = 1, 2, ..., k. If ti is not on , the value is unknown.\nThe chase can be done by drawing a tableau (which is the same formalism used in tableau query). Suppose R has attributes A, B, ... and components of t are a, b, .... For ti use the same letter as t in the components that are in Si but subscript the letter with i if the component is not in i. Then, ti will agree with t if it is in Si and will have a unique value otherwise.\nThe chase process is confluent.","name":"Chase (algorithm)","categories":["All articles with unsourced statements","Articles with unsourced statements from November 2012","Database algorithms","Database theory"],"tag_line":"The Chase is a simple fixed-point algorithm testing and enforcing implication of data dependencies in database systems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"write-ahead-logging","_score":0,"_source":{"description":"In computer science, write-ahead logging (WAL) is a family of techniques for providing atomicity and durability (two of the ACID properties) in database systems.\nIn a system using WAL, all modifications are written to a log before they are applied. Usually both redo and undo information is stored in the log.\nThe purpose of this can be illustrated by an example. Imagine a program that is in the middle of performing some operation when the machine it is running on loses power. Upon restart, that program might well need to know whether the operation it was performing succeeded, half-succeeded, or failed. If a write-ahead log is used, the program can check this log and compare what it was supposed to be doing when it unexpectedly lost power to what was actually done. On the basis of this comparison, the program could decide to undo what it had started, complete what it had started, or keep things as they are.\nWAL allows updates of a database to be done in-place. Another way to implement atomic updates is with shadow paging, which is not in-place. The main advantage of doing updates in-place is that it reduces the need to modify indexes and block lists.\nARIES is a popular algorithm in the WAL family.\nFile systems typically use a variant of WAL for at least file system metadata called journaling.","name":"Write-ahead logging","categories":["All stub articles","Database algorithms","Database stubs"],"tag_line":"In computer science, write-ahead logging (WAL) is a family of techniques for providing atomicity and durability (two of the ACID properties) in database systems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"types-of-artificial-neural-networks","_score":0,"_source":{"description":"There are many types of artificial neural networks (ANN).\nArtificial neural networks are computational models inspired by biological neural networks, and are used to approximate functions that are generally unknown. Particularly, they are inspired by the behaviour of neurons and the electrical signals they convey between input (such as from the eyes or nerve endings in the hand), processing, and output from the brain (such as reacting to light, touch, or heat). The way neurons semantically communicate is an area of ongoing research. Most artificial neural networks bear only some resemblance to their more complex biological counterparts, but are very effective at their intended tasks (e.g. classification or segmentation).\nSome ANNs are adaptive systems and are used for example to model populations and environments, which constantly change.\nNeural networks can be hardware- (neurons are represented by physical components) or software-based (computer models), and can use a variety of topologies and learning algorithms.","name":"Types of artificial neural networks","categories":["All articles with unsourced statements","All copied and pasted articles and sections","Articles with unsourced statements from October 2008","Artificial neural networks","Classification algorithms","Computational neuroscience","Computational statistics","Copied and pasted articles and sections with url provided from November 2014"],"tag_line":"There are many types of artificial neural networks (ANN)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"block-nested-loop","_score":0,"_source":{"description":"A block-nested loop (BNL) is an algorithm used to join two relations in a relational database.\nThis algorithm is a variation on the simple nested loop join used to join two relations  and  (the \"outer\" and \"inner\" join operands, respectively). Suppose . In a traditional nested loop join,  will be scanned once for every tuple of . If there are many qualifying  tuples, and particularly if there is no applicable index for the join key on , this operation will be very expensive.\nThe block nested loop join algorithm improves on the simple nested loop join by only scanning  once for every group of  tuples. For example, one variant of the block nested loop join reads an entire page of  tuples into memory and loads them into a hash table. It then scans , and probes the hash table to find  tuples that match any of the tuples in the current page of . This reduces the number of scans of  that are necessary.\nA more aggressive variant of this algorithm loads as many pages of  as can be fit in the available memory, loading all such tuples into a hash table, and then repeatedly scans . This further reduces the number of scans of  that are necessary. In fact, this algorithm is essentially a special-case of the classic hash join algorithm.\nThe block nested loop runs in  I/Os where  is the number of available pages of internal memory and  and  is size of  and  respectively in pages. Note that block nested loop runs in  I/Os if  fits in the available internal memory.","name":"Block nested loop","categories":["All articles with unsourced statements","Articles with unsourced statements from August 2015","Join algorithms"],"tag_line":"A block-nested loop (BNL) is an algorithm used to join two relations in a relational database."}}
,{"_index":"throwtable","_type":"algorithm","_id":"large-margin-nearest-neighbor","_score":0,"_source":{"description":"Large margin nearest neighbor (LMNN) classification is a statistical machine learning algorithm. It learns a Pseudometric designed for k-nearest neighbor classification. The algorithm is based on semidefinite programming, a sub-class of convex optimization.\nThe goal of supervised learning (more specifically classification) is to learn a decision rule that can categorize data instances into pre-defined classes. The k-nearest neighbor rule assumes a training data set of labeled instances (i.e. the classes are known). It classifies a new data instance with the class obtained from the majority vote of the k closest (labeled) training instances. Closeness is measured with a pre-defined metric. Large Margin Nearest Neighbors is an algorithm that learns this global (pseudo-)metric in a supervised fashion to improve the classification accuracy of the k-nearest neighbor rule.","name":"Large margin nearest neighbor","categories":["Classification algorithms","Machine learning"],"tag_line":"Large margin nearest neighbor (LMNN) classification is a statistical machine learning algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"multiclass-classification","_score":0,"_source":{"description":"Not to be confused with multi-label classification.\nIn machine learning, multiclass or multinomial classification is the problem of classifying instances into one of the more than two classes (classifying instances into one of the two classes is called binary classification).\nWhile some classification algorithms naturally permit the use of more than two classes, others are by nature binary algorithms; these can, however, be turned into multinomial classifiers by a variety of strategies.\nMulticlass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance.","name":"Multiclass classification","categories":["All stub articles","Artificial intelligence stubs","Classification algorithms","Statistical classification"],"tag_line":"Not to be confused with multi-label classification."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hash-join","_score":0,"_source":{"description":"The hash join is an example of a join algorithm and is used in the implementation of a relational database management system.\nThe task of a join algorithm is to find, for each distinct value of the join attribute, the set of tuples in each relation which have that value.\nHash joins require an equijoin predicate (a predicate comparing values from one table with values from the other table using the equals operator '=').","name":"Hash join","categories":["Hashing","Join algorithms"],"tag_line":"The hash join is an example of a join algorithm and is used in the implementation of a relational database management system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"genetic-representation","_score":0,"_source":{"description":"In computer programming, genetic representation is a way of representing solutions/individuals in evolutionary computation methods. Genetic representation can encode appearance, behavior, physical qualities of individuals. Designing a good genetic representation that is expressive and evolvable is a hard problem in evolutionary computation. Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation.\nGenetic algorithms use linear binary representations. The most standard one is an array of bits. Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size. This facilitates simple crossover operation. Variable length representations were also explored in Genetic algorithms, but crossover implementation is more complex in this case.\nEvolution strategy uses linear real-valued representations, e.g. an array of real values. It uses mostly gaussian mutation and blending/averaging crossover.\nGenetic programming (GP) pioneered tree-like representations and developed genetic operators suitable for such representations. Tree-like representations are used in GP to represent and evolve functional programs with desired properties.\nHuman-based genetic algorithm (HBGA) offers a way to avoid solving hard representation problems by outsourcing all genetic operators to outside agents, in this case, humans. The algorithm has no need for knowledge of a particular fixed genetic representation as long as there are enough external agents capable of handling those representations, allowing for free-form and evolving genetic representations.\n\n","name":"Genetic representation","categories":["All articles needing additional references","Articles needing additional references from December 2009","Evolutionary algorithms"],"tag_line":"In computer programming, genetic representation is a way of representing solutions/individuals in evolutionary computation methods."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fast-fourier-transform","_score":0,"_source":{"description":"A fast Fourier transform (FFT) algorithm computes the discrete Fourier transform (DFT) of a sequence, or its inverse. Fourier analysis converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa. An FFT rapidly computes such transformations by factorizing the DFT matrix into a product of sparse (mostly zero) factors. As a result, it manages to reduce the complexity of computing the DFT from , which arises if one simply applies the definition of DFT, to , where  is the data size.\nFast Fourier transforms are widely used for many applications in engineering, science, and mathematics. The basic ideas were popularized in 1965, but some algorithms had been derived as early as 1805. In 1994 Gilbert Strang described the FFT as \"the most important numerical algorithm of our lifetime\" and it was included in Top 10 Algorithms of 20th Century by the IEEE journal Computing in Science & Engineering.","name":"Fast Fourier transform","categories":["All articles needing additional references","Articles needing additional references from June 2015","Articles with inconsistent citation formats","Digital signal processing","Discrete transforms","FFT algorithms","Unsolved problems in computer science"],"tag_line":"A fast Fourier transform (FFT) algorithm computes the discrete Fourier transform (DFT) of a sequence, or its inverse."}}
,{"_index":"throwtable","_type":"algorithm","_id":"computer-automated-design","_score":0,"_source":{"description":"Design Automation usually refers to electronic design automation, or Design Automation which is a Product configurator. Extending Computer-Aided Design (CAD), automated design and Computer-Automated Design (CAutoD)  are more concerned with a broader range of applications, such as automotive engineering, civil engineering, composite material design, control engineering, dynamic system identification, financial systems, industrial equipment, mechatronic systems, steel construction, structural optimisation, and the invention of novel systems.\nThe concept of CAutoD perhaps first appeared in 1963, in the IBM Journal of Research and Development [1], where a computer program was written (1) to search for logic circuits having certain constraints on hardware design and (2) to evaluate these logics in terms of their discriminating ability over samples of the character set they are expected to recognize. More recently, traditional CAD simulation is seen to be transformed to CAutoD by biologically-inspired machine learning or search techniques such as evolutionary computation, including swarm intelligence algorithms[3].","name":"Computer-automated design","categories":["Computer-aided design","Design","Evolutionary algorithms","Evolutionary computation"],"tag_line":"Design Automation usually refers to electronic design automation, or Design Automation which is a Product configurator."}}
,{"_index":"throwtable","_type":"algorithm","_id":"firefly-algorithm","_score":0,"_source":{"description":"The firefly algorithm (FA) is a metaheuristic algorithm, inspired by the flashing behaviour of fireflies. The primary purpose for a firefly's flash is to act as a signal system to attract other fireflies. Xin-She Yang formulated this firefly algorithm by assuming:\nAll fireflies are unisexual, so that any individual firefly will be attracted to all other fireflies;\nAttractiveness is proportional to their brightness, and for any two fireflies, the less bright one will be attracted by (and thus move towards) the brighter one; however, the intensity (apparent brightness) decrease as their mutual distance increases;\nIf there are no fireflies brighter than a given firefly, it will move randomly.\nThe brightness should be associated with the objective function.\nFirefly algorithm is a nature-inspired metaheuristic optimization algorithm.","name":"Firefly algorithm","categories":["Evolutionary algorithms","Image processing","Metaheuristic","Optimization algorithms and methods","Swarm Intelligence"],"tag_line":"The firefly algorithm (FA) is a metaheuristic algorithm, inspired by the flashing behaviour of fireflies."}}
,{"_index":"throwtable","_type":"algorithm","_id":"memetic-algorithm","_score":0,"_source":{"description":"Memetic algorithms (MA) represent one of the recent growing areas of research in evolutionary computation. The term MA is now widely used as a synergy of evolutionary or any population-based approach with separate individual learning or local improvement procedures for problem search. Quite often, MA are also referred to in the literature as Baldwinian evolutionary algorithms (EA), Lamarckian EAs, cultural algorithms, or genetic local search.","name":"Memetic algorithm","categories":["All Wikipedia articles needing context","All articles with unsourced statements","All pages needing cleanup","Articles with unsourced statements from September 2014","Evolutionary algorithms","Wikipedia articles needing context from February 2011","Wikipedia introduction cleanup from February 2011"],"tag_line":"Memetic algorithms (MA) represent one of the recent growing areas of research in evolutionary computation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ramer–douglas–peucker-algorithm","_score":0,"_source":{"description":"The Ramer–Douglas–Peucker algorithm (RDP) is an algorithm for reducing the number of points in a curve that is approximated by a series of points. The initial form of the algorithm was independently suggested in 1972 by Urs Ramer and 1973 by David Douglas and Thomas Peucker and several others in the following decade. This algorithm is also known under the names Douglas–Peucker algorithm, iterative end-point fit algorithm and split-and-merge algorithm.\n\n","name":"Ramer–Douglas–Peucker algorithm","categories":["Articles with example pseudocode","Computer graphics algorithms","Digital signal processing","Geometric algorithms"],"tag_line":"The Ramer–Douglas–Peucker algorithm (RDP) is an algorithm for reducing the number of points in a curve that is approximated by a series of points."}}
,{"_index":"throwtable","_type":"algorithm","_id":"nested-loop-join","_score":0,"_source":{"description":"A nested loop join is a naive algorithm that joins two sets by using two nested loops. Join operations are important to database management.","name":"Nested loop join","categories":["All articles lacking sources","All articles needing expert attention","All stub articles","Articles lacking sources from January 2010","Articles needing expert attention from March 2011","Articles needing expert attention with no reason or talk parameter","Computer science stubs","Join algorithms","Mathematics articles needing expert attention"],"tag_line":"A nested loop join is a naive algorithm that joins two sets by using two nested loops."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bresenham's-line-algorithm","_score":0,"_source":{"description":"Bresenham's line algorithm is an algorithm that determines the points of an n-dimensional raster that should be selected in order to form a close approximation to a straight line between two points. It is commonly used to draw lines on a computer screen, as it uses only integer addition, subtraction and bit shifting, all of which are very cheap operations in standard computer architectures. It is one of the earliest algorithms developed in the field of computer graphics. An extension to the original algorithm may be used for drawing circles.\nWhile algorithms such as Wu's algorithm are also frequently used in modern computer graphics because they can support antialiasing, the speed and simplicity of Bresenham's line algorithm means that it is still important. The algorithm is used in hardware such as plotters and in the graphics chips of modern graphics cards. It can also be found in many software graphics libraries. Because the algorithm is very simple, it is often implemented in either the firmware or the graphics hardware of modern graphics cards.\nThe label \"Bresenham\" is used today for a family of algorithms extending or modifying Bresenham's original algorithm.","name":"Bresenham's line algorithm","categories":["All articles needing additional references","All articles to be expanded","All articles with unsourced statements","Articles needing additional references from August 2012","Articles to be expanded from September 2011","Articles with example pseudocode","Articles with unsourced statements from December 2010","Commons category with local link same as on Wikidata","Computer graphics algorithms","Digital geometry"],"tag_line":"Bresenham's line algorithm is an algorithm that determines the points of an n-dimensional raster that should be selected in order to form a close approximation to a straight line between two points."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gilbert–johnson–keerthi-distance-algorithm","_score":0,"_source":{"description":"The Gilbert–Johnson–Keerthi distance algorithm is a method of determining the minimum distance between two convex sets. Unlike many other distance algorithms, it does not require that the geometry data be stored in any specific format, but instead relies solely on a support function to iteratively generate closer simplices to the correct answer using the Minkowski sum (CSO) of two convex shapes.\n\"Enhanced GJK\" algorithms use edge information to speed up the algorithm by following edges when looking for the next simplex. This improves performance substantially for polytopes with large numbers of vertices.\nGJK algorithms are often used incrementally in simulation systems and video games. In this mode, the final simplex from a previous solution is used as the initial guess in the next iteration, or \"frame\". If the positions in the new frame are close to those in the old frame, the algorithm will converge in one or two iterations. This yields collision detection systems which operate in near-constant time.\nThe algorithm's stability, speed, and small storage footprint make it popular for realtime collision detection, especially in physics engines for video games.","name":"Gilbert–Johnson–Keerthi distance algorithm","categories":["All stub articles","Applied mathematics stubs","Convex geometry","Geometric algorithms"],"tag_line":"The Gilbert–Johnson–Keerthi distance algorithm is a method of determining the minimum distance between two convex sets."}}
,{"_index":"throwtable","_type":"algorithm","_id":"geomipmapping","_score":0,"_source":{"description":"Geomipmapping or geometrical mipmapping is a real-time block-based terrain rendering algorithm developed by W.H. de Boer in 2000 that aims to reduce CPU processing time which is a common bottleneck in level of detail approaches to terrain rendering. [1]\nPrior to geomipmapping, techniques such as quadtree rendering were used to divide the terrain into square tiles created by binary division with quadratically diminishing size. The subdivision step is typically performed on the CPU which creates a bottleneck as geometry commands are buffered to the GPU. Unlike quadtrees which send 1x1 polygon units to the GPU, to reduce the CPU processing time geomipmapping divides the terrain into grid-based tiles which are themselves regularly subdivided. Typically, a fixed number of vertex buffer objects (VBOs) are stored on the GPU at different grid resolutions, such as 10x10 and 20x20, and then placed at major terrain regions selectively chosen by the CPU. A vertex shader is then used to reposition the vertices for a given VBO, all on the GPU. Overall, this results in a major reduction in CPU processing, and reduced CPU-to-GPU bandwidth as the GPU then performs most of the work. Geoclipmaps and GPU raycasting are two other modern alternatives to geomipmapping for interactive rendering of terrain.","name":"Geomipmapping","categories":["All stub articles","Computer graphics algorithms","Computer graphics stubs"],"tag_line":"Geomipmapping or geometrical mipmapping is a real-time block-based terrain rendering algorithm developed by W.H."}}
,{"_index":"throwtable","_type":"algorithm","_id":"line-segment-intersection","_score":0,"_source":{"description":"In computational geometry, the line segment intersection problem supplies a list of line segments in the Euclidean plane and asks whether any two of them intersect, or cross.\nSimple algorithms examine each pair of segments. However, if a large number of possibly intersecting segments are to be checked, this becomes increasingly inefficient since most pairs of segments are not close to one another in a typical input sequence. The most common, more efficient way to solve this problem for a high number of segments is to use a sweep line algorithm, where we imagine a line sliding across the line segments and we track which line segments it intersects at each point in time using a dynamic data structure based on binary search trees. The Shamos–Hoey algorithm applies this principle to solve the line segment intersection detection problem, as stated above, of determining whether or not a set of line segments has an intersection; the Bentley–Ottmann algorithm works by the same principle to list all intersections in logarithmic time per intersection.","name":"Line segment intersection","categories":["Algorithms and data structures stubs","All stub articles","CS1 errors: chapter ignored","Computer science stubs","Geometric algorithms"],"tag_line":"In computational geometry, the line segment intersection problem supplies a list of line segments in the Euclidean plane and asks whether any two of them intersect, or cross."}}
,{"_index":"throwtable","_type":"algorithm","_id":"even–odd-rule","_score":0,"_source":{"description":"The even–odd rule is an algorithm implemented in vector-based graphic software, like the PostScript language and Scalable Vector Graphics (SVG), which determines how a graphical shape with more than one closed outline will be filled. Unlike the nonzero-rule algorithm, this algorithm will alternatively color and leave uncolored shapes defined by nested closed paths irrespective of their winding.\nThe SVG specification says: \"This rule determines the \"insideness\" of a point on the canvas by drawing a ray from that point to infinity in any direction and counting the number of path segments from the given shape that the ray crosses. If this number is odd, the point is inside; if even, the point is outside.\"\nThe rule can be seen in effect in many vector graphic programs (like Freehand or Illustrator), where a crossing of an outline with itself causes shapes to fill in strange ways.\nOn a simple curve, the even–odd rule reduces to a decision algorithm for the point in polygon problem.\n\n","name":"Even–odd rule","categories":["All stub articles","Computer graphics algorithms","Computer programming stubs","Parity (mathematics)"],"tag_line":"The even–odd rule is an algorithm implemented in vector-based graphic software, like the PostScript language and Scalable Vector Graphics (SVG), which determines how a graphical shape with more than one closed outline will be filled."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bounding-sphere","_score":0,"_source":{"description":"In mathematics, given a non-empty set of objects of finite extension in n-dimensional space, for example a set of points, a bounding sphere, enclosing sphere or enclosing ball for that set is an n-dimensional solid sphere containing all of these objects.\nIn the plane the terms bounding or enclosing circle are used.\nUsed in computer graphics and computational geometry, a bounding sphere is a special type of bounding volume. There are several fast and simple bounding sphere construction algorithms with a high practical value in real-time computer graphics applications.\nIn statistics and operations research, the objects are typically points, and generally the sphere of interest is the minimal bounding sphere, that is, the sphere with minimal radius among all bounding spheres. It may be proven that such a sphere is unique: If there are two of them, then the objects in question lie within their intersection. But an intersection of two non-coinciding spheres of equal radius is contained in a sphere of smaller radius.\nThe problem of computing the center of a minimal bounding sphere is also known as the \"unweighted Euclidean 1-center problem\".","name":"Bounding sphere","categories":["All articles with unsourced statements","Articles with unsourced statements from November 2015","Geometric algorithms"],"tag_line":"In mathematics, given a non-empty set of objects of finite extension in n-dimensional space, for example a set of points, a bounding sphere, enclosing sphere or enclosing ball for that set is an n-dimensional solid sphere containing all of these objects."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sweep-line-algorithm","_score":0,"_source":{"description":"In computational geometry, a sweep line algorithm or plane sweep algorithm is a type of algorithm that uses a conceptual sweep line or sweep surface to solve various problems in Euclidean space. It is one of the key techniques in computational geometry.\nThe idea behind algorithms of this type is to imagine that a line (often a vertical line) is swept or moved across the plane, stopping at some points. Geometric operations are restricted to geometric objects that either intersect or are in the immediate vicinity of the sweep line whenever it stops, and the complete solution is available once the line has passed over all objects.","name":"Sweep line algorithm","categories":["All articles with unsourced statements","Articles with unsourced statements from May 2009","Geometric algorithms"],"tag_line":"In computational geometry, a sweep line algorithm or plane sweep algorithm is a type of algorithm that uses a conceptual sweep line or sweep surface to solve various problems in Euclidean space."}}
,{"_index":"throwtable","_type":"algorithm","_id":"flood-fill","_score":0,"_source":{"description":"Flood fill, also called seed fill, is an algorithm that determines the area connected to a given node in a multi-dimensional array. It is used in the \"bucket\" fill tool of paint programs to fill connected, similarly-colored areas with a different color, and in games such as Go and Minesweeper for determining which pieces are cleared. When applied on an image to fill a particular bounded area with color, it is also known as boundary fill.","name":"Flood fill","categories":["All articles lacking sources","Articles lacking sources from August 2009","Articles with example pseudocode","Computer graphics algorithms"],"tag_line":"Flood fill, also called seed fill, is an algorithm that determines the area connected to a given node in a multi-dimensional array."}}
,{"_index":"throwtable","_type":"algorithm","_id":"meta-optimization","_score":0,"_source":{"description":"In numerical optimization, meta-optimization is the use of one optimization method to tune another optimization method. Meta-optimization is reported to have been used as early as in the late 1970s by Mercer and Sampson  for finding optimal parameter settings of a genetic algorithm. Meta-optimization is also known in the literature as meta-evolution, super-optimization, automated parameter calibration, hyper-heuristics, etc.","name":"Meta-optimization","categories":["Evolutionary algorithms","Heuristics","Mathematical optimization","Optimization algorithms and methods"],"tag_line":"In numerical optimization, meta-optimization is the use of one optimization method to tune another optimization method."}}
,{"_index":"throwtable","_type":"algorithm","_id":"matthew-t.-dickerson","_score":0,"_source":{"description":"Matthew T. Dickerson is a professor of computer science at Middlebury College in Vermont, a scholar of the fiction of J. R. R. Tolkien and the Inklings, a novelist, a blues musician and historian of music, a fly fisherman, a maple sugar farmer, and a beekeeper.\nDickerson received an A.B. from Dartmouth College in 1985, and a Ph.D. in Computer Science from Cornell University, under the supervision of Dexter Kozen, in 1989. His Ph.D. research was in symbolic computation, but since then he has worked primarily in computational geometry; his most frequently cited computer science papers concern k-nearest neighbor algorithms and minimum-weight triangulation. He has been on the Middlebury faculty since receiving his Ph.D.\nHe is also the author of six non-technical books, most of them about fantasy fiction. His 2003 book Following Gandalf: Epic Battles and Moral Victory in The Lord of the Rings (Brazos Press, 2003, ISBN 978-1-58743-085-5), a study of the moral and Christian values expressed by Tolkien's works, highlights the contrasts between moral and physical victories, and between heroism and violence; it points out the necessity of having free will in order to make moral choices. It was shortlisted for the Mythopoeic Society's 2004 and 2005 Mythopoeic Scholarship Awards. He has also written a pair of books on Tolkien, C. S. Lewis, and environmentalism, Ents, Elves, and Eriador: The Environmental Vision of J.R.R. Tolkien (with Jonathan Evans, The University Press of Kentucky, 2006, ISBN 978-0-8131-2418-6) and Narnia and the Fields of Arbol: The Environmental Vision of C. S. Lewis (with David L. O'Hara, The University Press of Kentucky, 2009, ISBN 978-0-8131-2522-0). Despite giving the first of these two books an overall negative review, reviewer Patrick Curry writes that it is \"a major new contribution to the subject of Tolkien's work\". His other books include The Finnsburg Encounter (Crossway Books, 1991, ISBN 978-0-89107-604-9), a work of historical fiction, translated into German as Licht uber Friesland (Verlag Schulte & Gerth, 1996, ISBN 3-89437-422-5), Hammers and Nails: The Life and Music of Mark Heard (Cornerstone Press, 2003, ISBN 978-0-940895-49-2), a biography of musician Mark Heard, and From Homer to Harry Potter: A Handbook on Myth and Fantasy (with David L. O'Hara, Brazos Press, 2006, ISBN 978-1-58743-133-3).\nFrom 1997 to 2001 Dickerson published a biweekly column on fishing and the outdoors in the Addison Independent, a local newspaper. Since 2002 he has been the director of the New England Young Writers Conference, an annual four-day conference for high school students in Bread Loaf, Vermont that is associated with Middlebury College. He is also the founding director of the Vermont Conference on Christianity and the Arts. He plays bass in a Vermont-based blues band, Deep Freyed.","name":"Matthew T. Dickerson","categories":["American computer scientists","Cornell University alumni","Dartmouth College alumni","Living people","Middlebury College faculty","Researchers in geometric algorithms","Tolkien studies","Year of birth missing (living people)"],"tag_line":"Matthew T. Dickerson is a professor of computer science at Middlebury College in Vermont, a scholar of the fiction of J. R. R. Tolkien and the Inklings, a novelist, a blues musician and historian of music, a fly fisherman, a maple sugar farmer, and a beekeeper."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mark-overmars","_score":0,"_source":{"description":"Markus Hendrik \"Mark\" Overmars (Dutch pronunciation: [ˈmɑrkʏs ˈɦɛndrɪk ˈmɑrk ˈoːvərˌmɑrs], born 29 September 1958 in Zeist, Netherlands) is a Dutch computer scientist and teacher of game programming known for his game development application Game Maker. Game Maker lets people create computer games using a drag-and-drop interface. He is the former head of the Center for Geometry, Imaging, and Virtual Environments at Utrecht University, in the Netherlands. This research center concentrates on computational geometry and its application in areas like computer graphics, robotics, geographic information systems, imaging, multimedia, virtual environments, and games.\nOvermars received his Ph.D. in 1983 from Utrecht University under the supervision of Jan van Leeuwen, and has since been a member of the faculty of the same university. Overmars has published over 100 journal papers, largely on computational geometry, and is the co-author of several books including a widely used computational geometry text.\nOvermars has also worked in robotics. He was the first to develop the probabilistic roadmap method in 1992, which was later independently discovered by Kavraki and Latombe in 1994. Their joint paper, Probabilistic roadmaps for path planning in high-dimensional configuration spaces, is considered one of the most influential studies in motion planning, and has been widely cited (more than 2500 times as of 2014 according to Google Scholar).\n^ Curriculum vitae, archived from the Utrecht University web site on October 2, 2011.\n^ Former colleagues, GIVE Center, retrieved 2014-01-16.\n^ Markus (Mark) Hendrik Overmars at the Mathematics Genealogy Project.\n^ Kavraki, L.E.; Svestka, P.; Latombe, J.C.; Overmars, M.H. (1996). \"Probabilistic roadmaps for path planning in high-dimensional configuration spaces\". Robotics and Automation, IEEE Transactions on 12 (4): 566–580. doi:10.1109/70.508439. \n^ Karaman, Sertac; Frazzoli, Emilio (2011), \"Sampling-based algorithms for optimal motion planning\", International Journal of Robotics Research 30 (7): 846–894, doi:10.1177/0278364911406761, Arguably, the most influential sampling-based motion planning algorithms to date include probabilistic roadmaps \n^ Citations to probabilistic roadmaps, Google Scholar, retrieved 2014-01-17.","name":"Mark Overmars","categories":["1958 births","Dutch computer programmers","Dutch computer scientists","Living people","People from Zeist","Researchers in geometric algorithms","Roboticists","Utrecht University alumni","Utrecht University faculty","Wikipedia articles with ISNI identifiers","Wikipedia articles with LCCN identifiers","Wikipedia articles with VIAF identifiers"],"tag_line":"Markus Hendrik \"Mark\" Overmars (Dutch pronunciation: [ˈmɑrkʏs ˈɦɛndrɪk ˈmɑrk ˈoːvərˌmɑrs], born 29 September 1958 in Zeist, Netherlands) is a Dutch computer scientist and teacher of game programming known for his game development application Game Maker."}}
,{"_index":"throwtable","_type":"algorithm","_id":"michael-ian-shamos","_score":0,"_source":{"description":"Michael Ian \"Mike\" Shamos (born April 21, 1947) is an American mathematician, attorney, book author, journal editor, consultant and company director. He is (with Franco P. Preparata) the author of Computational Geometry (Springer-Verlag, 1985), which was for many years the standard textbook in computational geometry, and is known for the Shamos–Hoey sweep line algorithm for line segment intersection detection and for the rotating calipers technique for finding the width and diameter of a geometric figure. His publications also include works on electronic voting, the game of billiards, and intellectual property law in the digital age.\nHe was a fellow of Sigma Xi (1974–83), had an IBM Fellowship at Yale University (1974–75), was SIAM National Lecturer (1977–78), distinguished lecturer in computer science at the University of Rochester (1978), visited McGill University (1979), and belonged to the Duquesne University Law Review (1980–81). He won the first annual Black & White Scotch Achiever’s Award in 1991 for contributions to bagpipe musicography, and the Industry Service Award of the Billiard and Bowling Institute of America, 1996, for contributions to billiard history. Since 2001 he is a Billiard Worldcup Association official referee.\nHe has been editor in chief of the Journal of Privacy Technology (2003–2006), a member of the editorial boards of Electronic Commerce Research Journal and the Pittsburgh Journal of Technology, Law and Policy, and a contributing editor of Billiards Digest magazine.\nShamos is the author of The New Illustrated Encyclopedia of Billiards (Lyons, 1999) among other related works, and is the curator of the Billiards Museum and Archive.\nMichael Shamos is the Director of the MS in IT eBusiness Technology program at Carnegie Mellon University.","name":"Michael Ian Shamos","categories":["1947 births","20th-century American mathematicians","21st-century American mathematicians","Cue sports writers and broadcasters","Duquesne University faculty","Living people","Researchers in geometric algorithms","University of Rochester faculty","Voting theorists","Wikipedia articles with BNF identifiers","Wikipedia articles with ISNI identifiers","Wikipedia articles with VIAF identifiers"],"tag_line":"Michael Ian \"Mike\" Shamos (born April 21, 1947) is an American mathematician, attorney, book author, journal editor, consultant and company director."}}
,{"_index":"throwtable","_type":"algorithm","_id":"jörg-rüdiger-sack","_score":0,"_source":{"description":"Jörg-Rüdiger Wolfgang Sack (born in Duisburg, Germany) is a professor of computer science at Carleton University, where he holds the SUN–NSERC chair in Applied Parallel Computing. Sack received a masters degree from the University of Bonn in 1979 and a Ph.D. in 1984 from McGill University, under the supervision of Godfried Toussaint. He is co-editor-in-chief of the journals Computational Geometry: Theory and Applications and the Journal of Spatial Information Science, co-editor of the Handbook of Computational Geometry (Elsevier, 2000, ISBN 978-0-444-82537-7), and co-editor of the proceedings of the biennial Algorithms and Data Structures Symposium (WADS). Sack's research interests include computational geometry, parallel algorithms, and geographic information systems.","name":"Jörg-Rüdiger Sack","categories":["All stub articles","Canadian computer scientists","Canadian computer specialist stubs","Canadian people stubs","Carleton University faculty","Computer scientist stubs","German computer scientists","Living people","McGill University alumni","Researchers in geometric algorithms","University of Bonn alumni","Year of birth missing (living people)"],"tag_line":"Jörg-Rüdiger Wolfgang Sack (born in Duisburg, Germany) is a professor of computer science at Carleton University, where he holds the SUN–NSERC chair in Applied Parallel Computing."}}
,{"_index":"throwtable","_type":"algorithm","_id":"joseph-o'rourke-(professor)","_score":0,"_source":{"description":"Joseph O'Rourke is the Olin Professor of Computer Science at Smith College and the founding chair of the Smith computer science department. His main research interest is computational geometry.\nO'Rourke was the first person to publish an algorithm to determine the minimum bounding box of a point set in three dimensions.\nIn 1985, O'Rourke was the program chair of the first annual Symposium on Computational Geometry. He was formerly the arXiv moderator for computational geometry and discrete mathematics.\nIn 2012 O'Rourke was named a fellow of the Association for Computing Machinery.","name":"Joseph O'Rourke (professor)","categories":["All stub articles","American computer scientists","BLP articles lacking sources from December 2012","Computer scientist stubs","Fellows of the Association for Computing Machinery","Guggenheim Fellows","Living people","Researchers in geometric algorithms","Smith College faculty","Wikipedia articles with BNF identifiers","Wikipedia articles with ISNI identifiers","Wikipedia articles with VIAF identifiers","Year of birth missing (living people)"],"tag_line":"Joseph O'Rourke is the Olin Professor of Computer Science at Smith College and the founding chair of the Smith computer science department."}}
,{"_index":"throwtable","_type":"algorithm","_id":"constructing-skill-trees","_score":0,"_source":{"description":"Constructing skill trees (CST) is a hierarchical reinforcement learning algorithm which can build skill trees from a set of sample solution trajectories obtained from demonstration. CST uses an incremental MAP(maximum a posteriori ) change point detection algorithm to segment each demonstration trajectory into skills and integrate the results into a skill tree. CST was introduced by George Konidaris, Scott Kuindersma, Andrew Barto and Roderic Grupen in 2010.","name":"Constructing skill trees","categories":["All articles needing additional references","Articles needing additional references from January 2012","CS1 errors: missing author or editor","Machine learning algorithms"],"tag_line":"Constructing skill trees (CST) is a hierarchical reinforcement learning algorithm which can build skill trees from a set of sample solution trajectories obtained from demonstration."}}
,{"_index":"throwtable","_type":"algorithm","_id":"david-g.-kirkpatrick","_score":0,"_source":{"description":"David Galer Kirkpatrick is a professor of computer science at the University of British Columbia. He is known for the Kirkpatrick–Seidel algorithm and his work on polygon triangulation, and for co-inventing α-shapes and the β-skeleton. He received his PhD from the University of Toronto in 1974.","name":"David G. Kirkpatrick","categories":["All stub articles","Canadian computer scientists","Canadian computer specialist stubs","Canadian people stubs","Fellows of the Royal Society of Canada","Living people","Researchers in geometric algorithms","University of British Columbia faculty","University of Toronto alumni","Wikipedia articles with VIAF identifiers","Year of birth missing (living people)"],"tag_line":"David Galer Kirkpatrick is a professor of computer science at the University of British Columbia."}}
,{"_index":"throwtable","_type":"algorithm","_id":"generec","_score":0,"_source":{"description":"GeneRec is a generalization of the Recirculation algorithm, and approximates Almeida-Pineda recurrent backpropagation. It is used as part of the Leabra algorithm for error-driven learning.\nThe symmetric, midpoint version of GeneRec is equivalent to the contrastive Hebbian learning algorithm (CHL).","name":"GeneRec","categories":["All stub articles","Machine learning algorithms","Neuroscience","Neuroscience stubs"],"tag_line":"GeneRec is a generalization of the Recirculation algorithm, and approximates Almeida-Pineda recurrent backpropagation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"reinforcement-learning","_score":0,"_source":{"description":"Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics, and genetic algorithms. In the operations research and control literature, the field where reinforcement learning methods are studied is called approximate dynamic programming. The problem has been studied in the theory of optimal control, though most studies are concerned with the existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.\nIn machine learning, the environment is typically formulated as a Markov decision process (MDP) as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical techniques and reinforcement learning algorithms is that the latter do not need knowledge about the MDP and they target large MDPs where exact methods become infeasible.\nReinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs.","name":"Reinforcement learning","categories":["Belief revision","CS1 errors: missing author or editor","Machine learning algorithms","Markov models","Pages containing cite templates with deprecated parameters"],"tag_line":"Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward."}}
,{"_index":"throwtable","_type":"algorithm","_id":"temporal-difference-learning","_score":0,"_source":{"description":"Temporal difference (TD) learning is a prediction-based machine learning method. It has primarily been used for the reinforcement learning problem, and is said to be \"a combination of Monte Carlo ideas and dynamic programming (DP) ideas.\" TD resembles a Monte Carlo method because it learns by sampling the environment according to some policy, and is related to dynamic programming techniques as it approximates its current estimate based on previously learned estimates (a process known as bootstrapping). The TD learning algorithm is related to the temporal difference model of animal learning.\nAs a prediction method, TD learning considers that subsequent predictions are often correlated in some sense. In standard supervised predictive learning, one learns only from actually observed values: A prediction is made, and when the observation is available, the prediction is adjusted to better match the observation. As elucidated by Richard Sutton, the core idea of TD learning is that one adjusts predictions to match other, more accurate, predictions about the future. This procedure is a form of bootstrapping, as illustrated with the following example:\n\"Suppose you wishes to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday - and thus be able to change, say, Monday's model before Saturday arrives.\"\nMathematically speaking, both in a standard and a TD approach, one would try to optimize some cost function, related to the error in our predictions of the expectation of some random variable, E[z]. However, while in the standard approach one in some sense assumes E[z] = z (the actual observed value), in the TD approach we use a model. For the particular case of reinforcement learning, which is the major application of TD methods, z is the total return and E[z] is given by the Bellman equation of the return.","name":"Temporal difference learning","categories":["Computational neuroscience","Machine learning algorithms"],"tag_line":"Temporal difference (TD) learning is a prediction-based machine learning method."}}
,{"_index":"throwtable","_type":"algorithm","_id":"page-replacement-algorithm","_score":0,"_source":{"description":"In a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which memory pages to page out (swap out, write to disk) when a page of memory needs to be allocated. Paging happens when a page fault occurs and a free page cannot be used to satisfy the allocation, either because there are none, or because the number of free pages is lower than some threshold.\nWhen the page that was selected for replacement and paged out is referenced again it has to be paged in (read in from disk), and this involves waiting for I/O completion. This determines the quality of the page replacement algorithm: the less time waiting for page-ins, the better the algorithm. A page replacement algorithm looks at the limited information about accesses to the pages provided by hardware, and tries to guess which pages should be replaced to minimize the total number of page misses, while balancing this with the costs (primary storage and processor time) of the algorithm itself.\nThe page replacing problem is a typical online problem from the competitive analysis perspective in the sense that the optimal deterministic algorithm is known.","name":"Page replacement algorithm","categories":["All Wikipedia articles needing clarification","All articles with unsourced statements","Articles with unsourced statements from June 2008","Memory management algorithms","Online algorithms","Use dmy dates from August 2012","Virtual memory","Wikipedia articles needing clarification from August 2011","Wikipedia articles needing clarification from January 2014"],"tag_line":"In a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which memory pages to page out (swap out, write to disk) when a page of memory needs to be allocated."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quickprop","_score":0,"_source":{"description":"Quickprop is an iterative method for determining the minimum of the loss function of an artificial neural network, following an algorithm inspired by the Newton's method. Sometimes, the algorithm is classified to the group of the second order learning methods. It follows a quadratic approximation of the previous gradient step and the current gradient, which is expected to be closed to the minimum of the loss function, under the assumption that the loss function is locally approximately square, trying to describe it by means of an upwardly open parabola. The minimum is sought in the vertex of the parabola. The procedure requires only local information of the artificial neuron to which it is applied. The k-th approximation step is given by:\n\nBeing  the neuron j weight of its i input and E is the loss function.\nThe Quickprop algorithm is an implementation of the error backpropagation algorithm, but the network can behave chaotically during the learning phase due to large step sizes.","name":"Quickprop","categories":["Artificial neural networks","Computational neuroscience","Machine learning algorithms"],"tag_line":"Quickprop is an iterative method for determining the minimum of the loss function of an artificial neural network, following an algorithm inspired by the Newton's method."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cache-algorithms","_score":0,"_source":{"description":"In computing, cache algorithms (also frequently called cache replacement algorithms or cache replacement policies) are optimizing instructions—​or algorithms—​that a computer program or a hardware-maintained structure can follow in order to manage a cache of information stored on the computer. When the cache is full, the algorithm must choose which items to discard to make room for the new ones.","name":"Cache algorithms","categories":["Cache (computing)","Memory management algorithms","Use dmy dates from August 2012"],"tag_line":"In computing, cache algorithms (also frequently called cache replacement algorithms or cache replacement policies) are optimizing instructions—​or algorithms—​that a computer program or a hardware-maintained structure can follow in order to manage a cache of information stored on the computer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"least-frequently-used","_score":0,"_source":{"description":"Least Frequently Used (LFU) is a type of cache algorithm used to manage memory within a computer. The standard characteristics of this method involve the system keeping track of the number of times a block is referenced in memory. When the cache is full and requires more room the system will purge the item with the lowest reference frequency.\nLFU is sometimes combined with a Least Recently Used algorithm and called LRFU.\n\n","name":"Least frequently used","categories":["Memory management algorithms","Online algorithms","Use dmy dates from August 2012","Virtual memory"],"tag_line":"Least Frequently Used (LFU) is a type of cache algorithm used to manage memory within a computer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"geographic-routing","_score":0,"_source":{"description":"Geographic routing (also called georouting or position-based routing) is a routing principle that relies on geographic position information. It is mainly proposed for wireless networks and based on the idea that the source sends a message to the geographic location of the destination instead of using the network address. The idea of using position information for routing was first proposed in the 1980s in the area of packet radio networks  and interconnection networks. Geographic routing requires that each node can determine its own location and that the source is aware of the location of the destination. With this information a message can be routed to the destination without knowledge of the network topology or a prior route discovery.\nThere are various approaches, such as single-path, multi-path and flooding-based strategies (see  for a survey). Most single-path strategies rely on two techniques: greedy forwarding and face routing. Greedy forwarding tries to bring the message closer to the destination in each step using only local information. Thus, each node forwards the message to the neighbor that is most suitable from a local point of view. The most suitable neighbor can be the one who minimizes the distance to the destination in each step (Greedy). Alternatively, one can consider another notion of progress, namely the projected distance on the source-destination-line (MFR, NFP), or the minimum angle between neighbor and destination (Compass Routing). Not all of these strategies are loop-free, i.e. a message can circulate among nodes in a certain constellation. It is known that the basic greedy strategy and MFR are loop free, while NFP and Compass Routing are not .\n\nGreedy forwarding can lead into a dead end, where there is no neighbor closer to the destination. Then, face routing helps to recover from that situation and find a path to another node, where greedy forwarding can be resumed. A recovery strategy such as face routing is necessary to assure that a message can be delivered to the destination. The combination of greedy forwarding and face routing was first proposed in 1999 under the name GFG (Greedy-Face-Greedy). It guarantees delivery in the so-called unit disk graph network model. Various variants, which were proposed later  , also for non-unit disk graphs, are based on the principles of GFG .\nAlthough originally developed as a routing scheme that uses the physical positions of each node, geographic routing algorithms have also been applied to networks in which each node is associated with a point in a virtual space, unrelated to its physical position. The process of finding a set of virtual positions for the nodes of a network such that geographic routing using these positions is guaranteed to succeed is called greedy embedding.","name":"Geographic routing","categories":["All stub articles","Computer network stubs","Pages containing cite templates with deprecated parameters","Pages with citations having bare URLs","Pages with citations lacking titles","Routing algorithms","Routing protocols","Wireless networking"],"tag_line":"Geographic routing (also called georouting or position-based routing) is a routing principle that relies on geographic position information."}}
,{"_index":"throwtable","_type":"algorithm","_id":"karn's-algorithm","_score":0,"_source":{"description":"Karn's algorithm addresses the problem of getting accurate estimates of the round-trip time for messages when using the Transmission Control Protocol (TCP) in computer networking. The algorithm was proposed by Phil Karn in 1987.\nAccurate round trip estimates in TCP can be difficult to calculate because of an ambiguity created by retransmitted segments. The round trip time is estimated as the difference between the time that a segment was sent and the time that its acknowledgment was returned to the sender, but when packets are re-transmitted there is an ambiguity: the acknowledgment may be a response to the first transmission of the segment or to a subsequent re-transmission.\nKarn's Algorithm ignores retransmitted segments when updating the round trip time estimate. Round trip time estimation is based only on unambiguous acknowledgments, which are acknowledgments for segments that were sent only once.\nThis simplistic implementation of Karn's algorithm can lead to problems as well. Consider what happens when TCP sends a segment after a sharp increase in delay. Using the prior round trip time estimate, TCP computes a timeout and retransmits a segment. If TCP ignores the round trip time of all retransmitted packets, the round trip estimate will never be updated, and TCP will continue retransmitting every segment, never adjusting to the increased delay.\nA solution to this problem is to incorporate transmission timeouts with a timer backoff strategy. The timer backoff strategy computes an initial timeout. If the timer expires and causes a retransmission, TCP increases the timeout generally by a factor of 2. This algorithm has proven to be extremely effective in networks with high packet loss.","name":"Karn's algorithm","categories":["Networking algorithms","Transmission Control Protocol","Wikipedia articles needing page number citations from March 2015"],"tag_line":"Karn's algorithm addresses the problem of getting accurate estimates of the round-trip time for messages when using the Transmission Control Protocol (TCP) in computer networking."}}
,{"_index":"throwtable","_type":"algorithm","_id":"generic-cell-rate-algorithm","_score":0,"_source":{"description":"The generic cell rate algorithm (GCRA) is a leaky bucket-type scheduling algorithm for the network scheduler that is used in Asynchronous Transfer Mode (ATM) networks. It is used to measure the timing of cells on virtual channels (VCs) and or Virtual Paths (VPs) against bandwidth and jitter limits contained in a traffic contract for the VC or VP to which the cells belong. Cells that do not conform to the limits given by the traffic contract may then be re-timed (delayed) in traffic shaping, or may be dropped (discarded) or reduced in priority (demoted) in traffic policing. Nonconforming cells that are reduced in priority may then be dropped, in preference to higher priority cells, by downstream components in the network that are experiencing congestion. Alternatively they may reach their destination (VC or VP termination) if there is enough capacity for them, despite them being excess cells as far as the contract is concerned: see priority control.\nThe GCRA is given as the reference for checking the traffic on connections in the network, i.e. usage/network parameter control (UPC/NPC) at user–network interfaces (UNI) or inter-network interfaces or network-network interfaces (INI/NNI) . It is also given as the reference for the timing of cells transmitted (ATM PDU Data_Requests) onto an ATM network by a network interface card (NIC) in a host, i.e. on the user side of the UNI . This ensures that cells are not then discarded by UPC/NCP in the network, i.e. on the network side of the UNI. However, as the GCRA is only given as a reference, the network providers and users may use any other algorithm that gives the same result.","name":"Generic cell rate algorithm","categories":["Network scheduling algorithms","Networking algorithms","Teletraffic"],"tag_line":"The generic cell rate algorithm (GCRA) is a leaky bucket-type scheduling algorithm for the network scheduler that is used in Asynchronous Transfer Mode (ATM) networks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"network-scheduler","_score":0,"_source":{"description":"On a node in packet switching communication network, a network scheduler, also called packet scheduler, is an arbiter program that manages the sequence of network packets in the transmit and receive queues of the network interface controller, which is a circular data buffer. There are several network schedulers available for the different operating system kernels, that implement many of the existing network scheduling algorithms.\nThe network scheduler logic decides, in a way similar to statistical multiplexers, which network packet to forward next from the buffer. The buffer works as a queuing system, storing the network packets temporarily until they are transmitted. The buffer space may be divided into different queues, with each of them holding the packets of one flow according to configured packet classification rules; for example, packets can be divided into flows by their source and destination IP addresses. Network scheduling algorithms and their associated settings determine how the network scheduler manages the buffer.\nAlso, network schedulers are enabling accomplishment of the active queue management and traffic shaping.","name":"Network scheduler","categories":["All articles with unsourced statements","All pages needing cleanup","Articles needing cleanup from September 2014","Articles with sections that need to be turned into prose from September 2014","Articles with unsourced statements from September 2014","Linux kernel features","Network performance","Network scheduling algorithms","Network theory","Wikipedia articles needing clarification from June 2014"],"tag_line":"On a node in packet switching communication network, a network scheduler, also called packet scheduler, is an arbiter program that manages the sequence of network packets in the transmit and receive queues of the network interface controller, which is a circular data buffer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"max-min-fairness","_score":0,"_source":{"description":"In communication networks, multiplexing and the division of scarce resources, max-min fairness is said to be achieved by an allocation if and only if the allocation is feasible and an attempt to increase the allocation of any participant necessarily results in the decrease in the allocation of some other participant with an equal or smaller allocation.\nIn best-effort statistical multiplexing, a first-come first-served (FCFS) scheduling policy is often used. The advantage with max-min fairness over FCFS is that it results in traffic shaping, meaning that an ill-behaved flow, consisting of large data packets or bursts of many packets, will only punish itself and not other flows. Network congestion is consequently to some extent avoided.\nFair queuing is an example of a max-min fair packet scheduling algorithm for statistical multiplexing and best effort packet-switched networks, since it gives scheduling priority to users that have achieved lowest data rate since they became active. In case of equally sized data packets, round-robin scheduling is max-min fair.","name":"Max-min fairness","categories":["Network scheduling algorithms","Routing algorithms","Wikipedia articles needing clarification from November 2012"],"tag_line":"In communication networks, multiplexing and the division of scarce resources, max-min fairness is said to be achieved by an allocation if and only if the allocation is feasible and an attempt to increase the allocation of any participant necessarily results in the decrease in the allocation of some other participant with an equal or smaller allocation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"proportionally-fair","_score":0,"_source":{"description":"Proportional fair is a compromise-based scheduling algorithm. It is based upon maintaining a balance between two competing interests: Trying to maximize total [wired/wireless network] throughput while at the same time allowing all users at least a minimal level of service. This is done by assigning each data flow a data rate or a scheduling priority (depending on the implementation) that is inversely proportional to its anticipated resource consumption.","name":"Proportionally fair","categories":["Mobile telecommunications","Network scheduling algorithms","Pages containing cite templates with deprecated parameters","Pages using citations with accessdate and no URL","Radio resource management","Use dmy dates from September 2010","Use dmy dates from September 2013","Wireless"],"tag_line":"Proportional fair is a compromise-based scheduling algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"flooding-(computer-networking)","_score":0,"_source":{"description":"Flooding is a simple computer network routing algorithm in which every incoming packet is sent through every outgoing link except the one it arrived on.\nFlooding is used in bridging and in systems such as Usenet and peer-to-peer file sharing and as part of some routing protocols, including OSPF, DVMRP, and those used in ad-hoc wireless networks.","name":"Flooding (computer networking)","categories":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from October 2015","Articles with unsourced statements from January 2012","Routing algorithms"],"tag_line":"Flooding is a simple computer network routing algorithm in which every incoming packet is sent through every outgoing link except the one it arrived on."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lehmer's-gcd-algorithm","_score":0,"_source":{"description":"Lehmer's GCD algorithm, named after Derrick Henry Lehmer, is a fast GCD algorithm, an improvement on the simpler but slower Euclidean algorithm. It is mainly used for big integers that have a representation as a string of digits relative to some chosen numeral system base, say β = 1000 or β = 232.","name":"Lehmer's GCD algorithm","categories":["Number theoretic algorithms"],"tag_line":"Lehmer's GCD algorithm, named after Derrick Henry Lehmer, is a fast GCD algorithm, an improvement on the simpler but slower Euclidean algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cornacchia's-algorithm","_score":0,"_source":{"description":"In computational number theory, Cornacchia's algorithm is an algorithm for solving the Diophantine equation , where  and d and m are coprime. The algorithm was described in 1908 by Giuseppe Cornacchia.","name":"Cornacchia's algorithm","categories":["Number theoretic algorithms"],"tag_line":"In computational number theory, Cornacchia's algorithm is an algorithm for solving the Diophantine equation , where  and d and m are coprime."}}
,{"_index":"throwtable","_type":"algorithm","_id":"vegas-algorithm","_score":0,"_source":{"description":"The VEGAS algorithm, due to G. P. Lepage, is a method for reducing error in Monte Carlo simulations by using a known or approximate probability distribution function to concentrate the search in those areas of the integrand that make the greatest contribution to the final integral.\nThe VEGAS algorithm is based on importance sampling. It samples points from the probability distribution described by the function , so that the points are concentrated in the regions that make the largest contribution to the integral.\nIn general, if the Monte Carlo integral of  is sampled with points distributed according to a probability distribution described by the function , we obtain an estimate ,\n.\nThe variance of the new estimate is then\n\nwhere  is the variance of the original estimate, .\nIf the probability distribution is chosen as  then it can be shown that the variance  vanishes, and the error in the estimate will be zero. In practice it is not possible to sample from the exact distribution g for an arbitrary function, so importance sampling algorithms aim to produce efficient approximations to the desired distribution.\nThe VEGAS algorithm approximates the exact distribution by making a number of passes over the integration region while histogramming the function f. Each histogram is used to define a sampling distribution for the next pass. Asymptotically this procedure converges to the desired distribution. In order to avoid the number of histogram bins growing like  with dimension d the probability distribution is approximated by a separable function:  so that the number of bins required is only Kd. This is equivalent to locating the peaks of the function from the projections of the integrand onto the coordinate axes. The efficiency of VEGAS depends on the validity of this assumption. It is most efficient when the peaks of the integrand are well-localized. If an integrand can be rewritten in a form which is approximately separable this will increase the efficiency of integration with VEGAS.","name":"VEGAS algorithm","categories":["Computational physics","Monte Carlo methods","Statistical algorithms","Variance reduction"],"tag_line":"The VEGAS algorithm, due to G. P. Lepage, is a method for reducing error in Monte Carlo simulations by using a known or approximate probability distribution function to concentrate the search in those areas of the integrand that make the greatest contribution to the final integral."}}
,{"_index":"throwtable","_type":"algorithm","_id":"metropolis-light-transport","_score":0,"_source":{"description":"The Metropolis light transport (MLT) is an application of a variant of the Monte Carlo method called the Metropolis-Hastings algorithm to the rendering equation for generating images from detailed physical descriptions of three-dimensional scenes.\nThe procedure constructs paths from the eye to a light source using bidirectional path tracing, then constructs slight modifications to the path. Some careful statistical calculation (the Metropolis algorithm) is used to compute the appropriate distribution of brightness over the image. This procedure has the advantage, relative to bidirectional path tracing, that once a path has been found from light to eye, the algorithm can then explore nearby paths; thus difficult-to-find light paths can be explored more thoroughly with the same number of simulated photons. In short, the algorithm generates a path and stores the path's 'nodes' in a list. It can then modify the path by adding extra nodes and creating a new light path. While creating this new path, the algorithm decides how many new 'nodes' to add and whether or not these new nodes will actually create a new path.\nMetropolis Light Transport is an unbiased method that, in some cases (but not always), converges to a solution of the rendering equation faster than other unbiased algorithms such as path tracing or bidirectional path tracing.","name":"Metropolis light transport","categories":["All articles lacking in-text citations","All articles with unsourced statements","All stub articles","Articles lacking in-text citations from February 2014","Articles with unsourced statements from July 2010","Computing stubs","Global illumination algorithms","Monte Carlo methods"],"tag_line":"The Metropolis light transport (MLT) is an application of a variant of the Monte Carlo method called the Metropolis-Hastings algorithm to the rendering equation for generating images from detailed physical descriptions of three-dimensional scenes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"odds-algorithm","_score":0,"_source":{"description":"The odds-algorithm is a mathematical method for computing optimal strategies for a class of problems that belong to the domain of optimal stopping problems. Their solution follows from the odds-strategy, and the importance of the odds-strategy lies in its optimality, as explained below. This was used to devise betting strategies called martingales.\nThe odds-algorithm applies to a class of problems called last-success-problems. Formally, the objective in these problems is to maximize the probability of identifying in a sequence of sequentially observed independent events the last event satisfying a specific criterion (a \"specific event\"). This identification must be done at the time of observation. No revisiting of preceding observations is permitted. Usually, a specific event is defined by the decision maker as an event that is of true interest in the view of \"stopping\" to take a well-defined action. Such problems are encountered in several situations.","name":"Odds algorithm","categories":["Mathematical optimization","Optimal decisions","Statistical algorithms"],"tag_line":"The odds-algorithm is a mathematical method for computing optimal strategies for a class of problems that belong to the domain of optimal stopping problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"block-lanczos-algorithm","_score":0,"_source":{"description":"In computer science, the block Lanczos algorithm is an algorithm for finding the nullspace of a matrix over a finite field, using only multiplication of the matrix by long, thin matrices. Such matrices are considered as vectors of tuples of finite-field entries, and so tend to be called 'vectors' in descriptions of the algorithm.\nThe block Lanczos algorithm is amongst the most efficient methods known for finding nullspaces, which is the final stage in integer factorization algorithms such as the quadratic sieve and number field sieve, and its development has been entirely driven by this application.","name":"Block Lanczos algorithm","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Linear algebra stubs","Numerical linear algebra"],"tag_line":"In computer science, the block Lanczos algorithm is an algorithm for finding the nullspace of a matrix over a finite field, using only multiplication of the matrix by long, thin matrices."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pantelides-algorithm","_score":0,"_source":{"description":"Pantelides algorithm gives a systematic method for reducing high-index systems of differential-algebraic equations to lower index, by selectively adding differentiated forms of the equations already present in the system. It is possible for the algorithm to fail in some instances.\nPantelides algorithm is implemented in several significant equation-based simulation programs such as gPROMS, Modelica and EMSO.","name":"Pantelides algorithm","categories":["Algorithms and data structures stubs","All Wikipedia articles needing context","All pages needing cleanup","All stub articles","Computer science stubs","Numerical differential equations","Wikipedia articles needing context from August 2010","Wikipedia introduction cleanup from August 2010"],"tag_line":"Pantelides algorithm gives a systematic method for reducing high-index systems of differential-algebraic equations to lower index, by selectively adding differentiated forms of the equations already present in the system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"boyer–moore-majority-vote-algorithm","_score":0,"_source":{"description":"The Boyer-Moore Vote Algorithm solves the majority vote problem in linear time  and logarithmic space . The majority vote problem is to determine in any given sequence of choices whether there is a choice with more occurrences than all the others, and if so, to determine this choice. Mathematically, given a finite sequence (length n) of numbers, the object is to find the majority number defined as the number that appears more than ⌊ n/2 ⌋ times.","name":"Boyer–Moore majority vote algorithm","categories":["Algorithms","All orphaned articles","Orphaned articles from December 2014"],"tag_line":"The Boyer-Moore Vote Algorithm solves the majority vote problem in linear time  and logarithmic space ."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ternary-search","_score":0,"_source":{"description":"A ternary search algorithm is a technique in computer science for finding the minimum or maximum of a unimodal function. A ternary search determines either that the minimum or maximum cannot be in the first third of the domain or that it cannot be in the last third of the domain, then repeats on the remaining two-thirds. A ternary search is an example of a divide and conquer algorithm (see search algorithm).","name":"Ternary search","categories":["All articles lacking sources","Articles lacking sources from May 2007","Mathematical optimization","Search algorithms"],"tag_line":"A ternary search algorithm is a technique in computer science for finding the minimum or maximum of a unimodal function."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fast-inverse-square-root","_score":0,"_source":{"description":"Fast inverse square root (sometimes referred to as Fast InvSqrt() or by the hexadecimal constant 0x5f3759df) is a method of calculating x−½, the reciprocal (or multiplicative inverse) of a square root for a 32-bit floating point number in IEEE 754 floating point format. The algorithm was probably developed at Silicon Graphics in the early 1990s, and an implementation appeared in 1999 in the Quake III Arena source code, but the method did not appear on public forums such as Usenet until 2002 or 2003. (There is a discussion on Chinese developer forum CSDN back in 2000) At the time, the primary advantage of the algorithm came from avoiding computationally expensive floating point operations in favor of integer operations. Inverse square roots are used to compute angles of incidence and reflection for lighting and shading in computer graphics.\nThe algorithm accepts a 32-bit floating point number as the input and stores a halved value for later use. Then, treating the bits representing the floating point number as a 32-bit integer, a logical shift right of one bit is performed and the result subtracted from the magic number 0x5f3759df. This is the first approximation of the inverse square root of the input. Treating the bits again as floating point it runs one iteration of Newton's method to return a more precise approximation. This computes an approximation of the inverse square root of a floating point number approximately four times faster than floating point division.\nThe algorithm was originally attributed to John Carmack, but an investigation showed that the code had deeper roots in both the hardware and software side of computer graphics. Adjustments and alterations passed through both Silicon Graphics and 3dfx Interactive, with Gary Tarolli's implementation for the SGI Indigo as the earliest known use. It is not known how the constant was originally derived, though investigation has shed some light on possible methods.","name":"Fast inverse square root","categories":["All articles with unsourced statements","Articles with unsourced statements from April 2012","Good articles","Quake (series)","Root-finding algorithms","Source code"],"tag_line":"Fast inverse square root (sometimes referred to as Fast InvSqrt() or by the hexadecimal constant 0x5f3759df) is a method of calculating x−½, the reciprocal (or multiplicative inverse) of a square root for a 32-bit floating point number in IEEE 754 floating point format."}}
,{"_index":"throwtable","_type":"algorithm","_id":"root-finding-algorithm","_score":0,"_source":{"description":"A root-finding algorithm is a numerical method, or algorithm, for finding a value x such that f(x) = 0, for a given function f. Such an x is called a root of the function f.\nThis article is concerned with finding scalar, real or complex roots, approximated as floating point numbers. Finding integer roots or exact algebraic roots are separate problems, whose algorithms have little in common with those discussed here. (See: Diophantine equation for integer roots)\nFinding a root of f(x) − g(x) = 0 is the same as solving the equation f(x) = g(x). Here, x is called the unknown in the equation. Conversely, any equation can take the canonical form f(x) = 0, so equation solving is the same thing as computing (or finding) a root of a function.\nNumerical root-finding methods use iteration, producing a sequence of numbers that hopefully converge towards a limit, which is a root. The first values of this series are initial guesses. Many methods computes subsequent values by evaluating an auxiliary function on the preceding values. The limit is thus a fixed point of the auxiliary function, which is chosen for having the roots of the original equation as fixed points.\nThe behaviour of root-finding algorithms is studied in numerical analysis. Algorithms perform best when they take advantage of known characteristics of the given function. Thus an algorithm to find isolated real roots of a low-degree polynomial in one variable may bear little resemblance to an algorithm for complex roots of a \"black-box\" function which is not even known to be differentiable. Questions include ability to separate close roots, robustness against failures of continuity and differentiability, reliability despite inevitable numerical errors, and rate of convergence.","name":"Root-finding algorithm","categories":["Root-finding algorithms","Vague or ambiguous time from February 2014"],"tag_line":"A root-finding algorithm is a numerical method, or algorithm, for finding a value x such that f(x) = 0, for a given function f. Such an x is called a root of the function f.\nThis article is concerned with finding scalar, real or complex roots, approximated as floating point numbers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"nth-root-algorithm","_score":0,"_source":{"description":"The principal nth root  of a positive real number A, is the positive real solution of the equation\n\n(for integer n there are n distinct complex solutions to this equation if , but only one is positive and real).\nThere is a very fast-converging nth root algorithm for finding :\nMake an initial guess \nSet . In practice we do .\nRepeat step 2 until the desired precision is reached, i.e.  .\nA special case is the familiar square-root algorithm. By setting n = 2, the iteration rule in step 2 becomes the square root iteration rule:\n\nSeveral different derivations of this algorithm are possible. One derivation shows it is a special case of Newton's method (also called the Newton-Raphson method) for finding zeros of a function  beginning with an initial guess. Although Newton's method is iterative, meaning it approaches the solution through a series of increasingly accurate guesses, it converges very quickly. The rate of convergence is quadratic, meaning roughly that the number of bits of accuracy doubles on each iteration (so improving a guess from 1 bit to 64 bits of precision requires only 6 iterations). For this reason, this algorithm is often used in computers as a very fast method to calculate square roots.\nFor large n, the nth root algorithm is somewhat less efficient since it requires the computation of  at each step, but can be efficiently implemented with a good exponentiation algorithm.","name":"Nth root algorithm","categories":["Root-finding algorithms"],"tag_line":"The principal nth root  of a positive real number A, is the positive real solution of the equation\n\n(for integer n there are n distinct complex solutions to this equation if , but only one is positive and real)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ridders'-method","_score":0,"_source":{"description":"In numerical analysis, Ridders' method is a root-finding algorithm based on the false position method and the use of an exponential function to successively approximate a root of a function f. The method is due to C. Ridders.\nRidders' method is simpler than Muller's method or Brent's method but with similar performance. The formula below converges quadratically when the function is well-behaved, which implies that the number of additional significant digits found at each step approximately doubles; but the function has to be evaluated twice for each step, so the overall order of convergence of the method is √2. If the function is not well-behaved, the root remains bracketed and the length of the bracketing interval at least halves on each iteration, so convergence is guaranteed. The algorithm also makes use of square roots, which are slower than basic floating point operations.","name":"Ridders' method","categories":["All stub articles","Applied mathematics stubs","Root-finding algorithms"],"tag_line":"In numerical analysis, Ridders' method is a root-finding algorithm based on the false position method and the use of an exponential function to successively approximate a root of a function f. The method is due to C. Ridders."}}
,{"_index":"throwtable","_type":"algorithm","_id":"householder's-method","_score":0,"_source":{"description":"In mathematics, and more specifically in numerical analysis, Householder's methods are a class of root-finding algorithms that are used for functions of one real variable with continuous derivatives up to some order d+1. Each of these methods is characterized by the number d, which is known as the order of the method. The algorithm is iterative and has an rate of convergence of d+1.\nThese methods are named after the American mathematician Alston Scott Householder.","name":"Householder's method","categories":["All articles needing additional references","Articles needing additional references from November 2013","Root-finding algorithms"],"tag_line":"In mathematics, and more specifically in numerical analysis, Householder's methods are a class of root-finding algorithms that are used for functions of one real variable with continuous derivatives up to some order d+1."}}
,{"_index":"throwtable","_type":"algorithm","_id":"aberth-method","_score":0,"_source":{"description":"The Aberth method, or Aberth–Ehrlich method, named after Oliver Aberth and Louis W. Ehrlich, is a root-finding algorithm for simultaneous approximation of all the roots of a univariate polynomial.\nThe fundamental theorem of algebra states that for each polynomial with complex coefficients there are as many roots as the degree of the polynomial. This method converges cubically, an improvement over the Weierstrass–(Durand–Kerner) method, another numerical algorithm that approximates all roots at once, which converges quadratically. (However, both algorithms converge linearly at multiple zeros.)","name":"Aberth method","categories":["Root-finding algorithms"],"tag_line":"The Aberth method, or Aberth–Ehrlich method, named after Oliver Aberth and Louis W. Ehrlich, is a root-finding algorithm for simultaneous approximation of all the roots of a univariate polynomial."}}
,{"_index":"throwtable","_type":"algorithm","_id":"muller's-method","_score":0,"_source":{"description":"Muller's method is a root-finding algorithm, a numerical method for solving equations of the form f(x) = 0. It was first presented by David E. Muller in 1956.\nMuller's method is based on the secant method, which constructs at every iteration a line through two points on the graph of f. Instead, Muller's method uses three points, constructs the parabola through these three points, and takes the intersection of the x-axis with the parabola to be the next approximation.","name":"Muller's method","categories":["Root-finding algorithms"],"tag_line":"Muller's method is a root-finding algorithm, a numerical method for solving equations of the form f(x) = 0."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bairstow's-method","_score":0,"_source":{"description":"In numerical analysis, Bairstow's method is an efficient algorithm for finding the roots of a real polynomial of arbitrary degree. The algorithm first appeared in the appendix of the 1920 book \"Applied Aerodynamics\" by Leonard Bairstow. The algorithm finds the roots in complex conjugate pairs using only real arithmetic.\nSee root-finding algorithm for other algorithms.","name":"Bairstow's method","categories":["Root-finding algorithms"],"tag_line":"In numerical analysis, Bairstow's method is an efficient algorithm for finding the roots of a real polynomial of arbitrary degree."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rapidly-exploring-random-tree","_score":0,"_source":{"description":"A rapidly exploring random tree (RRT) is an algorithm designed to efficiently search nonconvex, high-dimensional spaces by randomly building a space-filling tree. The tree is constructed incrementally from samples drawn randomly from the search space and is inherently biased to grow towards large unsearched areas of the problem. RRTs were developed by Steven M. LaValle and James J. Kuffner Jr.  . They easily handle problems with obstacles and differential constraints (nonholonomic and kinodynamic) and have been widely used in autonomous robotic path planning.\nRRTs can be viewed as a technique to generate open-loop trajectories for nonlinear systems with state constraints. An RRT can also be considered as a Monte-Carlo method to bias search into the largest Voronoi regions of a graph in a configuration space. Some variations can even be considered stochastic fractals.","name":"Rapidly exploring random tree","categories":["Probabilistic data structures","Robot control","Search algorithms"],"tag_line":"A rapidly exploring random tree (RRT) is an algorithm designed to efficiently search nonconvex, high-dimensional spaces by randomly building a space-filling tree."}}
,{"_index":"throwtable","_type":"algorithm","_id":"knuth–morris–pratt-algorithm","_score":0,"_source":{"description":"In computer science, the Knuth–Morris–Pratt string searching algorithm (or KMP algorithm) searches for occurrences of a \"word\" W within a main \"text string\" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.\nThe algorithm was conceived in 1974 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. The three published it jointly in 1977.","name":"Knuth–Morris–Pratt algorithm","categories":["All articles needing additional references","Articles needing additional references from October 2009","Articles with example pseudocode","Donald Knuth","String matching algorithms"],"tag_line":"In computer science, the Knuth–Morris–Pratt string searching algorithm (or KMP algorithm) searches for occurrences of a \"word\" W within a main \"text string\" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters."}}
,{"_index":"throwtable","_type":"algorithm","_id":"wang-and-landau-algorithm","_score":0,"_source":{"description":"The Wang and Landau algorithm, proposed by Fugao Wang and David P. Landau, is a Monte Carlo method designed to calculate the density of states of a system. The method performs a non-markovian random walk to build the density of states by quickly visiting all the available energy spectrum. The Wang and Landau algorithm is an important method to obtain the density of states required to perform a multicanonical simulation.\nThe Wang–Landau algorithm can be applied to any system which is characterized by a cost (or energy) function. For instance, it has been applied to the solution of numerical integrals and the folding of proteins. The Wang-Landau Sampling is related to the Metadynamics algorithm.","name":"Wang and Landau algorithm","categories":["Articles with example Python code","Computational physics","Markov chain Monte Carlo","Statistical algorithms"],"tag_line":"The Wang and Landau algorithm, proposed by Fugao Wang and David P. Landau, is a Monte Carlo method designed to calculate the density of states of a system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"approximate-counting-algorithm","_score":0,"_source":{"description":"The approximate counting algorithm allows the counting of a large number of events using a small amount of memory. Invented in 1977 by Robert Morris (cryptographer) of Bell Labs, it uses probabilistic techniques to increment the counter. It was fully analyzed in the early 1980s by Philippe Flajolet of INRIA Rocquencourt, who coined the name Approximate Counting, and strongly contributed to its recognition among the research community. The algorithm is considered one of the precursors of streaming algorithms, and the more general problem of determining the frequency moments of a data stream has been central to the field.","name":"Approximate counting algorithm","categories":["Randomized algorithms"],"tag_line":"The approximate counting algorithm allows the counting of a large number of events using a small amount of memory."}}
,{"_index":"throwtable","_type":"algorithm","_id":"master-theorem","_score":0,"_source":{"description":"In the analysis of algorithms, the master theorem provides a solution in asymptotic terms (using Big O notation) for recurrence relations of types that occur in the analysis of many divide and conquer algorithms. It was popularized by the canonical algorithms textbook Introduction to Algorithms by Cormen, Leiserson, Rivest, and Stein, in which it is both introduced and proved. Not all recurrence relations can be solved with the use of the master theorem; its generalizations include the Akra–Bazzi method.","name":"Master theorem","categories":["Analysis of algorithms","Asymptotic analysis","Recurrence relations","Theorems in computational complexity theory"],"tag_line":"In the analysis of algorithms, the master theorem provides a solution in asymptotic terms (using Big O notation) for recurrence relations of types that occur in the analysis of many divide and conquer algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"expected-linear-time-mst-algorithm","_score":0,"_source":{"description":"A randomized algorithm for computing the minimum spanning forest of a weighted graph with no isolated vertices. It was developed by David Karger, Philip Klein, and Robert Tarjan. The algorithm relies on techniques from Borůvka's algorithm along with an algorithm for verifying a minimum spanning tree in linear time. It combines the design paradigms of divide and conquer algorithms, greedy algorithms, and randomized algorithms to achieve expected linear performance.\nDeterministic algorithms that find the minimum spanning tree include Prim's algorithm, Kruskal's algorithm, reverse-delete algorithm, and Borůvka's algorithm.","name":"Expected linear time MST algorithm","categories":["Randomized algorithms","Spanning tree"],"tag_line":"A randomized algorithm for computing the minimum spanning forest of a weighted graph with no isolated vertices."}}
,{"_index":"throwtable","_type":"algorithm","_id":"deutsch–jozsa-algorithm","_score":0,"_source":{"description":"The Deutsch–Jozsa algorithm is a quantum algorithm, proposed by David Deutsch and Richard Jozsa in 1992 with improvements by Richard Cleve, Artur Ekert, Chiara Macchiavello, and Michele Mosca in 1998. Although of little practical use, it is one of the first examples of a quantum algorithm that is exponentially faster than any possible deterministic classical algorithm. It is also a deterministic algorithm, meaning that it always produces an answer, and that answer is always correct.","name":"Deutsch–Jozsa algorithm","categories":["Quantum algorithms"],"tag_line":"The Deutsch–Jozsa algorithm is a quantum algorithm, proposed by David Deutsch and Richard Jozsa in 1992 with improvements by Richard Cleve, Artur Ekert, Chiara Macchiavello, and Michele Mosca in 1998."}}
,{"_index":"throwtable","_type":"algorithm","_id":"principle-of-deferred-decision","_score":0,"_source":{"description":"Principle of Deferred Decisions is a technique used in analysis of randomized algorithms.","name":"Principle of deferred decision","categories":["Algorithms and data structures stubs","All articles covered by WikiProject Wikify","All articles with too few wikilinks","All orphaned articles","All stub articles","Articles covered by WikiProject Wikify from December 2013","Articles with too few wikilinks from December 2013","Computer science stubs","Orphaned articles from January 2009","Randomized algorithms"],"tag_line":"Principle of Deferred Decisions is a technique used in analysis of randomized algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dynamic-priority-scheduling","_score":0,"_source":{"description":"Definition: Dynamic scheduling is a method in which the hardware determines which instructions to execute, as opposed to a statically scheduled machine, in which the compiler determines the order of execution. In essence, the processor is executing instructions out of order\nA major driving force in the microprocessor industry is the never ending desire to miniaturize things. Smaller transistors require less voltage to operate and thus consume less power and produce less heat. Smaller interconnect distances also allow for faster clock speeds. Perhaps most important of all, smaller die areas lead to cheaper processors since more chips can fit in a single wafer. The first microprocessor made by Intel was the 4004, which had 2300 transistors. Today's chips, on the other hand, incorporate 5 to 20 million transistors. So what do they do with all those transistors?\nA major hog of real estate is, of course, caches. Caches, and any IC (integrated circuit) based memory device, must have many wires running to and from the read and write ports. For on-chip caches, the load/store unit in the CPU must be able to access every location in the cache from both the read and write ports. The situation is even worse when there are more than one load/store units. That's a lot of wires! In the Pentium Pro, for example, a single package includes both the CPU chip and a L2 cache chip; the CPU chip has about 5 million transistors, while the cache chip has about 15 million transistors.\nHowever, even accounting for caches, there is still a large increase in the number of transistors in today's chips compared to the 4004. Obviously, microprocessors are becoming increasingly more complex. We can understand this increasing complexity since chip designers want to create fast processors which are at the same time affordable. As process technology improved and more transistors could be fitted in the same die area, it became cost effective to add newer or improved features to the processor in an attempt to increase its effective speed. One of these improvements is dynamic scheduling.\nAs its name implies, is a method in which the hardware determines which instructions to execute, as opposed to a statically scheduled machine, in which the compiler determines the order of execution. In essence, the processor is executing instructions out of order. Dynamic scheduling is akin to a data flow machine, in which instructions don't execute based on the order in which they appear, but rather on the availability of the source operands. Of course, a real processor also has to take into account the limited amount of resources available. Thus instructions execute based on the availability of the source operands as well as the availability of the requested functional units.\nDynamically scheduled machines can take advantage of parallelism which would not be visible at compile time. They are also more versatile as code does not necessarily have to be recompiled to run efficiently since the hardware takes care of much of the scheduling. In a statically scheduled machine, code would have to be recompiled to take advantage of the machine's particular hardware. (All of this is assuming the machines use the same instruction set architecture. Of course, the code would have to be recompiled no matter what if the machines used different ISAs.)\nDynamic priority scheduling is a type of scheduling algorithm in which the priorities are calculated during the execution of the system. The goal of dynamic priority scheduling is to adapt to dynamically changing progress and form an optimal configuration in self-sustained manner. It can be very hard to produce well-defined policies to achieve the goal depending on the difficulty of a given problem.\nEarliest deadline first scheduling and Least slack time scheduling are examples of Dynamic priority scheduling algorithms.","name":"Dynamic priority scheduling","categories":["All articles lacking sources","All articles needing expert attention","All stub articles","Articles lacking sources from August 2009","Articles needing expert attention from February 2009","Articles needing expert attention with no reason or talk parameter","Computer science articles needing expert attention","Computer science stubs","Scheduling algorithms"],"tag_line":"Definition: Dynamic scheduling is a method in which the hardware determines which instructions to execute, as opposed to a statically scheduled machine, in which the compiler determines the order of execution."}}
,{"_index":"throwtable","_type":"algorithm","_id":"finger-search-tree","_score":0,"_source":{"description":"In computer science, finger search trees are a type of binary search tree that keeps pointers to interior nodes, called fingers. The fingers speed up searches, insertions, and deletions for elements close to the fingers, giving amortized O(log n) lookups, and amortized O(1) insertions and deletions. It should not be confused with a finger tree nor a splay tree, although both can be used to implement finger search trees.\nGuibas et al. introduced ﬁnger search trees, by building upon B-trees. The original version supports ﬁnger searches in O(log d) time, where d is the number of elements between the ﬁnger and the search target. Updates take O(1) time, when only O(1) moveable ﬁngers are maintained. Moving a ﬁnger p positions requires O(log p) time. Huddleston and Mehlhorn refined this idea as level-linked B-trees.\nTsakalidis proposed a version based on AVL trees that facilitates searching from the ends of the tree; it can be used to implement a data structure with multiple fingers by using multiple of such trees.\nTo perform a finger search on a binary tree, the ideal way is to start from the finger, and search upwards to the root, until we reach the turning node or the least common ancestor of x and y, and then go downwards to find the element we're looking for. Determining if a node is the ancestor of another is non-trivial.\n\nTreaps, a randomized tree structure proposed by Seidel and Aragon, has the property that the expected path length of two elements of distance d is O(log d). For finger searching, they proposed adding pointers to determine the least common ancestor(LCA) quickly, or in every node maintain the minimum and maximum values of its subtree.\nA book chapter has been written that covers finger search trees in depth. In which, Brodal suggested an algorithm to perform finger search on treaps in O(log d) time, without needing any extra bookkeeping information; this algorithm accomplishes this by concurrently searching downward from the last candidate LCA.","name":"Finger search tree","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Pages using citations with accessdate and no URL","Search algorithms","Trees (data structures)"],"tag_line":"In computer science, finger search trees are a type of binary search tree that keeps pointers to interior nodes, called fingers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gang-scheduling","_score":0,"_source":{"description":"In computer science, gang scheduling is a scheduling algorithm for parallel systems that schedules related threads or processes to run simultaneously on different processors. Usually these will be threads all belonging to the same process, but they may also be from different processes, for example when the processes have a producer-consumer relationship, or when they all come from the same MPI program.\nGang scheduling is used so that if two or more threads or processes communicate with each other, they will all be ready to communicate at the same time. If they were not gang-scheduled, then one could wait to send or receive a message to another while it is sleeping, and vice versa. When processors are over-subscribed and gang scheduling is not used within a group of processes or threads which communicate with each other, it can lead to situations where each communication event suffers the overhead of a context switch.\nTechnically, gang scheduling is based on a data structure called the Ousterhout matrix. In this matrix each row represents a time slice, and each column a processor. The threads or processes of each job are packed into a single row of the matrix. During execution, coordinated context switching is performed across all nodes to switch from the processes in one row to those in the next row.\nGang scheduling is stricter than coscheduling. It requires all threads of the same process to run concurrently, while coscheduling allows for fragments, which are sets of threads that do not run concurrently with the rest of the gang.\nGang scheduling was implemented and used in production mode on several parallel machines, most notably the Connection Machine CM-5.","name":"Gang scheduling","categories":["All stub articles","Computer science stubs","Processor scheduling algorithms"],"tag_line":"In computer science, gang scheduling is a scheduling algorithm for parallel systems that schedules related threads or processes to run simultaneously on different processors."}}
,{"_index":"throwtable","_type":"algorithm","_id":"processor-affinity","_score":0,"_source":{"description":"Processor affinity, or CPU pinning enables the binding and unbinding of a process or a thread to a central processing unit (CPU) or a range of CPUs, so that the process or thread will execute only on the designated CPU or CPUs rather than any CPU. This can be viewed as a modification of the native central queue scheduling algorithm in a symmetric multiprocessing operating system. Each item in the queue has a tag indicating its kin processor. At the time of resource allocation, each task is allocated to its kin processor in preference to others.\nProcessor affinity takes advantage of the fact that some remnants of a process that was run on a given processor may remain in that processor's memory state (for example, data in the CPU cache) after another process is run on that CPU. Scheduling that process to execute on the same processor could result in an efficient use of process by reducing performance-degrading situations such as cache misses. A practical example of processor affinity is executing multiple instances of a non-threaded application, such as some graphics-rendering software.\nScheduling-algorithm implementations vary in adherence to processor affinity. Under certain circumstances, some implementations will allow a task to change to another processor if it results in higher efficiency. For example, when two processor-intensive tasks (A and B) have affinity to one processor while another processor remains unused, many schedulers will shift task B to the second processor in order to maximize processor use. Task B will then acquire affinity with the second processor, while task A will continue to have affinity with the original processor.","name":"Processor affinity","categories":["Processor scheduling algorithms"],"tag_line":"Processor affinity, or CPU pinning enables the binding and unbinding of a process or a thread to a central processing unit (CPU) or a range of CPUs, so that the process or thread will execute only on the designated CPU or CPUs rather than any CPU."}}
,{"_index":"throwtable","_type":"algorithm","_id":"best-first-search","_score":0,"_source":{"description":"Best-first search is a search algorithm which explores a graph by expanding the most promising node chosen according to a specified rule.\nJudea Pearl described best-first search as estimating the promise of node n by a \"heuristic evaluation function  which, in general, may depend on the description of n, the description of the goal, the information gathered by the search up to that point, and most important, on any extra knowledge about the problem domain.\"\nSome authors have used \"best-first search\" to refer specifically to a search with a heuristic that attempts to predict how close the end of a path is to a solution, so that paths which are judged to be closer to a solution are extended first. This specific type of search is called greedy best-first search or pure heuristic search.\nEfficient selection of the current best candidate for extension is typically implemented using a priority queue.\nThe A* search algorithm is an example of best-first search, as is B*. Best-first algorithms are often used for path finding in combinatorial search. (Note that neither A* nor B* is a greedy best-first search as they incorporate the distance from start in addition to estimated distances to the goal.)","name":"Best-first search","categories":["All articles with dead external links","Articles with dead external links from August 2014","Search algorithms"],"tag_line":"Best-first search is a search algorithm which explores a graph by expanding the most promising node chosen according to a specified rule."}}
,{"_index":"throwtable","_type":"algorithm","_id":"generalized-processor-sharing","_score":0,"_source":{"description":"Generalized processor sharing (GPS) is an ideal scheduling algorithm for network schedulers.\nIt is related to the Fair queuing principle, that groups the packets into classes, and share the service capacity between them. The GPS shares this capacity according to some fixed weights.\nIn processor scheduling, generalized processor sharing is \"an idealized scheduling algorithm that achieves perfect fairness. All practical schedulers approximate GPS and use it as a reference to measure fairness.\" Generalized processor sharing assumes that traffic is fluid (infinitesimal packet sizes), and can be arbitrarily split. There are several service disciplines which track the performance of GPS quite closely such as weighted fair queuing (WFQ), also known as packet-by-packet generalized processor sharing (PGPS).\n\n","name":"Generalized processor sharing","categories":["Scheduling algorithms"],"tag_line":"Generalized processor sharing (GPS) is an ideal scheduling algorithm for network schedulers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"look-algorithm","_score":0,"_source":{"description":"LOOK is a disk scheduling algorithm used to determine the order in which new disk read and write requests are processed.","name":"LOOK algorithm","categories":["All articles needing additional references","All articles that may contain original research","Articles needing additional references from April 2014","Articles that may contain original research from April 2014","Disk scheduling algorithms"],"tag_line":"LOOK is a disk scheduling algorithm used to determine the order in which new disk read and write requests are processed."}}
,{"_index":"throwtable","_type":"algorithm","_id":"any-angle-path-planning","_score":0,"_source":{"description":"Any-angle path planning algorithms search for paths on a cell decomposition of a continuous configuration space (such as a two-dimensional terrain).","name":"Any-angle path planning","categories":["Artificial intelligence","Robot navigation","Search algorithms"],"tag_line":"Any-angle path planning algorithms search for paths on a cell decomposition of a continuous configuration space (such as a two-dimensional terrain)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sss*","_score":0,"_source":{"description":"SSS* is a search algorithm, introduced by George Stockman in 1979, that conducts a state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithm.\nSSS* is based on the notion of solution trees. Informally, a solution tree can be formed from any arbitrary game tree by pruning the number of branches at each MAX node to one. Such a tree represents a complete strategy for MAX, since it specifies exactly one MAX action for every possible sequence of moves might be made by the opponent. Given a game tree, SSS* searches through the space of partial solution trees, gradually analyzing larger and larger subtrees, eventually producing a single solution tree with the same root and Minimax value as the original game tree. SSS* never examines a node that alpha-beta pruning would prune, and may prune some branches that alpha-beta would not. Stockman speculated that SSS* may therefore be a better general algorithm than alpha-beta. However, Igor Roizen and Judea Pearl have shown that the savings in the number of positions that SSS* evaluates relative to alpha/beta is limited and generally not enough to compensate for the increase in other resources (e.g., the storing and sorting of a list of nodes made necessary by the best-first nature of the algorithm). However, Aske Plaat, Jonathan Schaeffer, Wim Pijls and Arie de Bruin have shown that a sequence of null-window alpha-beta calls is equivalent to SSS* (i.e., it expands the same nodes in the same order) when alpha-beta is used with a transposition table, as is the case in all game-playing programs for chess, checkers, etc. Now the storing and sorting of the OPEN list were no longer necessary. This allowed the implementation of (an algorithm equivalent to) SSS* in tournament quality game-playing programs. Experiments showed that it did indeed perform better than Alpha-Beta in practice, but that it did not beat NegaScout.\nThe reformulation of a best-first algorithm as a sequence of depth-first calls prompted the formulation of a class of null-window alpha-beta algorithms, of which MTD-f is the best known example.\n^ Roizen, Igor; Judea Pearl (March 1983). \"A minimax algorithm better than alpha-beta?: Yes and No\". Artificial Intelligence 21 (1-2): 199–220. doi:10.1016/s0004-3702(83)80010-1. \n^ Plaat, Aske; Jonathan Schaeffer; Wim Pijls; Arie de Bruin (November 1996). \"Best-first Fixed-depth Minimax Algorithms\". Artificial Intelligence 87 (1-2): 255–293. doi:10.1016/0004-3702(95)00126-3.","name":"SSS*","categories":["All articles needing additional references","Articles needing additional references from February 2010","Search algorithms"],"tag_line":"SSS* is a search algorithm, introduced by George Stockman in 1979, that conducts a state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hill-climbing","_score":0,"_source":{"description":"In computer science, hill climbing is a mathematical optimization technique which belongs to the family of local search. It is an iterative algorithm that starts with an arbitrary solution to a problem, then attempts to find a better solution by incrementally changing a single element of the solution. If the change produces a better solution, an incremental change is made to the new solution, repeating until no further improvements can be found.\nFor example, hill climbing can be applied to the travelling salesman problem. It is easy to find an initial solution that visits all the cities but will be very poor compared to the optimal solution. The algorithm starts with such a solution and makes small improvements to it, such as switching the order in which two cities are visited. Eventually, a much shorter route is likely to be obtained.\nHill climbing is good for finding a local optimum (a solution that cannot be improved by considering a neighbouring configuration) but it is not necessarily guaranteed to find the best possible solution (the global optimum) out of all possible solutions (the search space). In convex problems, hill-climbing is optimal. Examples of algorithms that solve convex problems by hill-climbing include the simplex algorithm for linear programming and binary search.\nThe characteristic that only local optima are guaranteed can be cured by using restarts (repeated local search), or more complex schemes based on iterations, like iterated local search, on memory, like reactive search optimization and tabu search, or memory-less stochastic modifications, like simulated annealing.\nThe relative simplicity of the algorithm makes it a popular first choice amongst optimizing algorithms. It is used widely in artificial intelligence, for reaching a goal state from a starting node. Choice of next node and starting node can be varied to give a list of related algorithms. Although more advanced algorithms such as simulated annealing or tabu search may give better results, in some situations hill climbing works just as well. Hill climbing can often produce a better result than other algorithms when the amount of time available to perform a search is limited, such as with real-time systems. It is an anytime algorithm: it can return a valid solution even if it's interrupted at any time before it ends.","name":"Hill climbing","categories":["All articles that may contain original research","Articles that may contain original research from September 2007","Optimization algorithms and methods","Search algorithms"],"tag_line":"In computer science, hill climbing is a mathematical optimization technique which belongs to the family of local search."}}
,{"_index":"throwtable","_type":"algorithm","_id":"interpolation-search","_score":0,"_source":{"description":"Interpolation search (sometimes referred to as extrapolation search) is an algorithm for searching for a given key value in an indexed array that has been ordered by the values of the key. It parallels how humans search through a telephone book for a particular name, the key value by which the book's entries are ordered. In each search step it calculates where in the remaining search space the sought item might be, based on the key values at the bounds of the search space and the value of the sought key, usually via a linear interpolation. The key value actually found at this estimated position is then compared to the key value being sought. If it is not equal, then depending on the comparison, the remaining search space is reduced to the part before or after the estimated position. This method will only work if calculations on the size of differences between key values are sensible.\nBy comparison, the binary search always chooses the middle of the remaining search space, discarding one half or the other, again depending on the comparison between the key value found at the estimated position and the key value sought. The remaining search space is reduced to the part before or after the estimated position. The linear search uses equality only as it compares elements one-by-one from the start, ignoring any sorting.\nOn average the interpolation search makes about log(log(n)) comparisons (if the elements are uniformly distributed), where n is the number of elements to be searched. In the worst case (for instance where the numerical values of the keys increase exponentially) it can make up to O(n) comparisons.\nIn interpolation-sequential search, interpolation is used to find an item near the one being searched for, then linear search is used to find the exact item.","name":"Interpolation search","categories":["Search algorithms"],"tag_line":"Interpolation search (sometimes referred to as extrapolation search) is an algorithm for searching for a given key value in an indexed array that has been ordered by the values of the key."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mobilegeddon","_score":0,"_source":{"description":"Mobilegeddon is a name given by webmasters and web-developers to the Google's algorithm update of April 21, 2015. The main effect of this update is give priority to web sites that display well on smartphones and other mobile devices. The change does not affect searches made from a desktop computer or a laptop.\nGoogle announced its intention to make the change in February 2015. The Economist found the timing \"awkward\" because they said \"It comes less than a week after the European Union accused the firm...\" of anti-competitive behaviors.\nThe protologism is a blend word of \"mobile\" and \"Armageddon\" because the change \"could cause massive disruption to page rankings.\" But, writing for Forbes, Robert Hof says that concerns about the change were \"overblown\" in part because \"Google is providing a test to see if sites look good on smartphones\".\nSearch engine results pages on smartphones now show URLs in \"breadcrumb\" format, as opposed to the previous explicit format.","name":"Mobilegeddon","categories":["All orphaned articles","Google","Orphaned articles from July 2015","Search algorithms","Search engine optimization"],"tag_line":"Mobilegeddon is a name given by webmasters and web-developers to the Google's algorithm update of April 21, 2015."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mamf","_score":0,"_source":{"description":"MaMF, or Mammalian Motif Finder, is an algorithm for identifying motifs to which transcription factors bind.\nThe algorithm takes as input a set of promoter sequences, and a motif width(w), and as output, produces a ranked list of 30 predicted motifs(each motif is defined by a set of N sequences, where N is a parameter).\nThe algorithm firstly indexes each sub-sequence of length n, where n is a parameter around 4-6 base pairs, in each promoter, so they can be looked up efficiently. This index is then used to build a list of all pairs of sequences of length w, such that each sequence shares an n-mer, and each sequence forms an ungapped alignment with a substring of length w from the string of length 2w around the match, with a score exceeding a cut-off.\nThe pairs of sequences are then scored. The scoring function favours pairs which are very similar, but disfavours sequences which are very common in the target genome. The 1000 highest scoring pairs are kept, and the others are discarded. Each of these 1000 'seed' motifs are then used to search iteratively search for further sequences of length which maximise the score(a greedy algorithm), until N sequences for that motif are reached.\nVery similar motifs are discarded, and the 30 highest scoring motifs are returned as output.","name":"MaMF","categories":["All orphaned articles","Bioinformatics","Orphaned articles from October 2008","Search algorithms"],"tag_line":"MaMF, or Mammalian Motif Finder, is an algorithm for identifying motifs to which transcription factors bind."}}
,{"_index":"throwtable","_type":"algorithm","_id":"uniform-binary-search","_score":0,"_source":{"description":"Uniform binary search is an optimization of the classic binary search algorithm invented by Donald Knuth and given in Knuth's The Art of Computer Programming. It uses a lookup table to update a single array index, rather than taking the midpoint of an upper and a lower bound on each iteration; therefore, it is optimized for architectures (such as Knuth's MIX) on which\na table lookup is generally faster than an addition and a shift, and\nmany searches will be performed on the same array, or on several arrays of the same length","name":"Uniform binary search","categories":["All orphaned articles","Articles with example C code","Orphaned articles from February 2009","Search algorithms"],"tag_line":"Uniform binary search is an optimization of the classic binary search algorithm invented by Donald Knuth and given in Knuth's The Art of Computer Programming."}}
,{"_index":"throwtable","_type":"algorithm","_id":"linear-hashing","_score":0,"_source":{"description":"Linear hashing is a dynamic hash table algorithm invented by Witold Litwin (1980), and later popularized by Paul Larson. Linear hashing allows for the expansion of the hash table one slot at a time. The frequent single slot expansion can very effectively control the length of the collision chain. The cost of hash table expansion is spread out across each hash table insertion operation, as opposed to being incurred all at once. Linear hashing is therefore well suited for interactive applications.","name":"Linear hashing","categories":["Hashing","Search algorithms"],"tag_line":"Linear hashing is a dynamic hash table algorithm invented by Witold Litwin (1980), and later popularized by Paul Larson."}}
,{"_index":"throwtable","_type":"algorithm","_id":"proof-of-o(log*n)-time-complexity-of-union–find","_score":0,"_source":{"description":"In computer science, Union Find is an algorithm for doing certain operations on sets. This page is about proof of O(log*n) amortized time  of Union Find\nStatement: If m operations, either Union or Find, are applied to n elements, the total run time is O(m log*n), where log* is the iterated logarithm.","name":"Proof of O(log*n) time complexity of union–find","categories":["All articles lacking in-text citations","Articles lacking in-text citations from July 2012","Search algorithms"],"tag_line":"In computer science, Union Find is an algorithm for doing certain operations on sets."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bootstrapping-populations","_score":0,"_source":{"description":"Starting with a sample  observed from a random variable X having a given distribution law with a set of non fixed parameters which we denote with a vector , a parametric inference problem consists of computing suitable values – call them estimates – of these parameters precisely on the basis of the sample. An estimate is suitable if replacing it with the unknown parameter does not cause major damage in next computations. In Algorithmic inference, suitability of an estimate reads in terms of compatibility with the observed sample.\nIn this framework, resampling methods are aimed at generating a set of candidate values to replace the unknown parameters that we read as compatible replicas of them. They represent a population of specifications of a random vector   compatible with an observed sample, where the compatibility of its values has the properties of a probability distribution. By plugging parameters into the expression of the questioned distribution law, we bootstrap entire populations of random variables compatible with the observed sample.\nThe rationale of the algorithms computing the replicas, which we denote population bootstrap procedures, is to identify a set of statistics  exhibiting specific properties, denoting a well behavior, w.r.t. the unknown parameters. The statistics are expressed as functions of the observed values , by definition. The  may be expressed as a function of the unknown parameters and a random seed specification  through the sampling mechanism , in turn. Then, by plugging the second expression in the former, we obtain  expressions as functions of seeds and parameters – the master equations – that we invert to find values of the latter as a function of: i) the statistics, whose values in turn are fixed at the observed ones; and ii) the seeds, which are random according to their own distribution. Hence from a set of seed samples we obtain a set of parameter replicas.\n^ By default, capital letters (such as U, X) will denote random variables and small letters (u, x) their corresponding realizations.","name":"Bootstrapping populations","categories":["Algorithmic inference","All Wikipedia articles needing context","All articles needing cleanup","All pages needing cleanup","Articles needing cleanup from January 2009","Cleanup tagged articles without a reason field from January 2009","Computational statistics","Resampling (statistics)","Wikipedia articles needing context from October 2009","Wikipedia introduction cleanup from October 2009","Wikipedia pages needing cleanup from January 2009"],"tag_line":"Starting with a sample  observed from a random variable X having a given distribution law with a set of non fixed parameters which we denote with a vector , a parametric inference problem consists of computing suitable values – call them estimates – of these parameters precisely on the basis of the sample."}}
,{"_index":"throwtable","_type":"algorithm","_id":"yamartino-method","_score":0,"_source":{"description":"The Yamartino method (introduced by Robert J. Yamartino in 1984) is an algorithm for calculating an approximation to the standard deviation σθ of wind direction θ during a single pass through the incoming data. The standard deviation of wind direction is a measure of lateral turbulence, and is used in a method for estimating the Pasquill stability category.\nThe simple method for calculating standard deviation requires two passes through the list of values. The first pass determines the average of those values; the second pass determines the sum of the squares of the differences between the values and the average. This double-pass method requires access to all values. A single-pass method can be used for normal data but is unsuitable for angular data such as wind direction where the 0°/360° (or +180°/-180°) discontinuity forces special consideration. For example, the directions 1°, 0°, and 359° (or -1°) should not average to the direction 120°!\nThe Yamartino method solves both problems. The United States Environmental Protection Agency (EPA) has chosen it as the preferred way to compute the standard deviation of wind direction. A further discussion of the Yamartino method, along with other methods of estimating the standard deviation of wind direction can be found in Farrugia & Micallef.\nIt should be mentioned that it is also possible to calculate the exact standard deviation in one pass. However, that method needs slightly more calculation effort.","name":"Yamartino method","categories":["Atmospheric dispersion modeling","Boundary layer meteorology","Directional statistics","Statistical algorithms"],"tag_line":"The Yamartino method (introduced by Robert J. Yamartino in 1984) is an algorithm for calculating an approximation to the standard deviation σθ of wind direction θ during a single pass through the incoming data."}}
,{"_index":"throwtable","_type":"algorithm","_id":"parsing-expression-grammar","_score":0,"_source":{"description":"In computer science, a parsing expression grammar, or PEG, is a type of analytic formal grammar, i.e. it describes a formal language in terms of a set of rules for recognizing strings in the language. The formalism was introduced by Bryan Ford in 2004 and is closely related to the family of top-down parsing languages introduced in the early 1970s. Syntactically, PEGs also look similar to context-free grammars (CFGs), but they have a different interpretation: the choice operator selects the first match in PEG, while it is ambiguous in CFG. This is closer to how string recognition tends to be done in practice, e.g. by a recursive descent parser.\nUnlike CFGs, PEGs cannot be ambiguous; if a string parses, it has exactly one valid parse tree. It is conjectured that there exist context-free languages that cannot be parsed by a PEG, but this is not yet proven. PEGs are well-suited to parsing computer languages, but not natural languages where their performance is comparable to general CFG algorithms such as the Earley algorithm.","name":"Parsing expression grammar","categories":["All accuracy disputes","Articles with disputed statements from July 2014","Formal languages","Parsing algorithms","Wikipedia external links cleanup from September 2011","Wikipedia spam cleanup from September 2011"],"tag_line":"In computer science, a parsing expression grammar, or PEG, is a type of analytic formal grammar, i.e."}}
,{"_index":"throwtable","_type":"algorithm","_id":"continued-fraction","_score":0,"_source":{"description":"In mathematics, a continued fraction is an expression obtained through an iterative process of representing a number as the sum of its integer part and the reciprocal of another number, then writing this other number as the sum of its integer part and another reciprocal, and so on. In a finite continued fraction (or terminated continued fraction), the iteration/recursion is terminated after finitely many steps by using an integer in lieu of another continued fraction. In contrast, an infinite continued fraction is an infinite expression. In either case, all integers in the sequence, other than the first, must be positive. The integers ai are called the coefficients or terms of the continued fraction.\nContinued fractions have a number of remarkable properties related to the Euclidean algorithm for integers or real numbers. Every rational number p/q has two closely related expressions as a finite continued fraction, whose coefficients ai can be determined by applying the Euclidean algorithm to (p, q). The numerical value of an infinite continued fraction will be irrational; it is defined from its infinite sequence of integers as the limit of a sequence of values for finite continued fractions. Each finite continued fraction of the sequence is obtained by using a finite prefix of the infinite continued fraction's defining sequence of integers. Moreover, every irrational number α is the value of a unique infinite continued fraction, whose coefficients can be found using the non-terminating version of the Euclidean algorithm applied to the incommensurable values α and 1. This way of expressing real numbers (rational and irrational) is called their continued fraction representation.\nIt is generally assumed that the numerator of all of the fractions is 1. If arbitrary values and/or functions are used in place of one or more of the numerators or the integers in the denominators, the resulting expression is a generalized continued fraction. When it is necessary to distinguish the first form from generalized continued fractions, the former may be called a simple or regular continued fraction, or said to be in canonical form.\nThe term continued fraction may also refer to representations of rational functions, arising in their analytic theory. For this use of the term see Padé approximation and Chebyshev rational functions.\n\n","alt_names":["Continued_fraction"],"name":"Continued fraction","categories":["Continued fractions","Mathematical analysis"],"tag_line":"In mathematics, a continued fraction is an expression obtained through an iterative process of representing a number as the sum of its integer part and the reciprocal of another number, then writing this other number as the sum of its integer part and another reciprocal, and so on."}}
,{"_index":"throwtable","_type":"algorithm","_id":"knapsack-problem","_score":0,"_source":{"description":"The knapsack problem or rucksack problem is a problem in combinatorial optimization: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. It derives its name from the problem faced by someone who is constrained by a fixed-size knapsack and must fill it with the most valuable items.\nThe problem often arises in resource allocation where there are financial constraints and is studied in fields such as combinatorics, computer science, complexity theory, cryptography and applied mathematics.\nThe knapsack problem has been studied for more than a century, with early works dating as far back as 1897. It is not known how the name \"knapsack problem\" originated, though the problem was referred to as such in the early works of mathematician Tobias Dantzig (1884–1956), suggesting that the name could have existed in folklore before a mathematical problem had been fully defined.","alt_names":["Knapsack_problem"],"name":"Knapsack problem","categories":["Combinatorial optimization","Cryptography","Dynamic programming","NP-complete problems","Operations research","Packing problems","Pages with duplicate reference names","Pages with reference errors","Pseudo-polynomial time algorithms","Use dmy dates from September 2010","Weakly NP-complete problems"],"tag_line":"The knapsack problem or rucksack problem is a problem in combinatorial optimization: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible."}}
,{"_index":"throwtable","_type":"algorithm","_id":"function-prototype","_score":0,"_source":{"description":"In computer programming, a function prototype or function interface is a declaration of a function that specifies the function's name and type signature (arity, parameter types, and return type), but omits the function body. The term is particularly used in C and C++. While a function definition specifies how the function does what it does (the \"implementation\"), a function prototype merely specifies its interface, i.e. what data types go in and come out of it.\nIn a prototype, parameter names are optional (and in C/C++ have function prototype scope, meaning their scope ends at the end of the prototype), however, the type is necessary along with all modifiers (e.g. if it is a pointer or a const parameter).\nIn object-oriented programming, interfaces and abstract methods serve much the same purpose.\n\n","alt_names":["Function_prototype"],"name":"Function prototype","categories":["C++ articles","C programming language family","Computer programming","Subroutines"],"tag_line":"In computer programming, a function prototype or function interface is a declaration of a function that specifies the function's name and type signature (arity, parameter types, and return type), but omits the function body."}}
,{"_index":"throwtable","_type":"algorithm","_id":"easter","_score":0,"_source":{"description":"Easter (Old English usually Ēastrun, -on, or -an; also Ēastru, -o; and Ēostre), also called Pasch (derived, through Latin: Pascha and Greek Πάσχα Paskha, from Aramaic: פסחא‎, cognate to Hebrew: פֶּסַח‎ Pesaḥ) or Resurrection Sunday, is a festival and holiday celebrating the resurrection of Jesus Christ from the dead, described in the New Testament as having occurred on the third day of his burial after his crucifixion by Romans at Calvary c. 30 AD. It is the culmination of the Passion of Christ, preceded by Lent (or Great Lent), a forty-day period of fasting, prayer, and penance.\nThe week before Easter is called Holy Week, and it contains the days of the Easter Triduum, including Maundy Thursday, commemorating the Maundy and Last Supper, as well as Good Friday, commemorating the crucifixion and death of Jesus. In western Christianity, Eastertide, the Easter Season, begins on Easter Sunday and lasts seven weeks, ending with the coming of the fiftieth day, Pentecost Sunday. In Orthodoxy, the season of Pascha begins on Pascha and ends with the coming of the fortieth day, the Feast of the Ascension.\nEaster and the holidays that are related to it are moveable feasts in that they do not fall on a fixed date in the Gregorian or Julian calendars which follow only the cycle of the sun; rather, its date is determined on a lunisolar calendar similar to the Hebrew calendar. The First Council of Nicaea (325) established two rules, independence of the Jewish calendar and worldwide uniformity, which were the only rules for Easter explicitly laid down by the council. No details for the computation were specified; these were worked out in practice, a process that took centuries and generated a number of controversies. It has come to be the first Sunday after the full moon that occurs on or soonest after 21 March, but calculations vary in East and West. Details of this complicated computation are found below in the section Date.\nEaster is linked to the Jewish Passover by much of its symbolism, as well as by its position in the calendar. In many languages, the words for \"Easter\" and \"Passover\" are identical or very similar. Easter customs vary across the Christian world, and include sunrise services, exclaiming the Paschal greeting, clipping the church, and decorating Easter eggs, a symbol of the empty tomb. The Easter lily, a symbol of the resurrection, traditionally decorates the chancel area of churches on this day and for the rest of Eastertide. Additional customs that have become associated with Easter and are observed by both Christians and some non-Christians include egg hunting, the Easter Bunny, and Easter parades. There are also various traditional Easter foods that vary regionally.\n\n","name":"Easter","categories":["All articles with unsourced statements","April observances","Articles containing Ancient Greek-language text","Articles containing Aramaic-language text","Articles containing Hebrew-language text","Articles containing Latin-language text","Articles containing Old English-language text","Articles containing explicitly cited English-language text","Articles with Infobox holidays","Articles with Norwegian-language external links","Articles with unsourced statements from August 2014","CS1 Danish-language sources (da)","Christian festivals and holy days","Commons category with local link same as on Wikidata","Easter","Infobox holiday (other)","March observances","Moveable holidays (to check)","Pages using citations with accessdate and no URL","Use dmy dates from April 2012","Wikipedia articles with GND identifiers","Wikipedia indefinitely move-protected pages","Wikipedia indefinitely semi-protected pages"],"tag_line":"Easter (Old English usually Ēastrun, -on, or -an; also Ēastru, -o; and Ēostre), also called Pasch (derived, through Latin: Pascha and Greek Πάσχα Paskha, from Aramaic: פסחא‎, cognate to Hebrew: פֶּסַח‎ Pesaḥ) or Resurrection Sunday, is a festival and holiday celebrating the resurrection of Jesus Christ from the dead, described in the New Testament as having occurred on the third day of his burial after his crucifixion by Romans at Calvary c. 30 AD."}}
,{"_index":"throwtable","_type":"algorithm","_id":"moving-average","_score":0,"_source":{"description":"In statistics, a moving average (rolling average or running average) is a calculation to analyze data points by creating series of averages of different subsets of the full data set. It is also called a moving mean (MM) or rolling mean and is a type of finite impulse response filter. Variations include: simple, and cumulative, or weighted forms (described below).\nGiven a series of numbers and a fixed subset size, the first element of the moving average is obtained by taking the average of the initial fixed subset of the number series. Then the subset is modified by \"shifting forward\"; that is, excluding the first number of the series and including the next number following the original subset in the series. This creates a new subset of numbers, which is averaged. This process is repeated over the entire data series. The plot line connecting all the (fixed) averages is the moving average. A moving average is a set of numbers, each of which is the average of the corresponding subset of a larger set of datum points. A moving average may also use unequal weights for each datum value in the subset to emphasize particular values in the subset.\nA moving average is commonly used with time series data to smooth out short-term fluctuations and highlight longer-term trends or cycles. The threshold between short-term and long-term depends on the application, and the parameters of the moving average will be set accordingly. For example, it is often used in technical analysis of financial data, like stock prices, returns or trading volumes. It is also used in economics to examine gross domestic product, employment or other macroeconomic time series. Mathematically, a moving average is a type of convolution and so it can be viewed as an example of a low-pass filter used in signal processing. When used with non-time series data, a moving average filters higher frequency components without any specific connection to time, although typically some kind of ordering is implied. Viewed simplistically it can be regarded as smoothing the data.","alt_names":["Moving_average"],"name":"Moving average","categories":["All articles lacking in-text citations","All articles needing additional references","Articles lacking in-text citations from February 2010","Articles needing additional references from September 2013","Chart overlays","Mathematical finance","Statistical charts and diagrams","Time series analysis"],"tag_line":"In statistics, a moving average (rolling average or running average) is a calculation to analyze data points by creating series of averages of different subsets of the full data set."}}
,{"_index":"throwtable","_type":"algorithm","_id":"arithmetic-mean","_score":0,"_source":{"description":"In mathematics and statistics, the arithmetic mean (pronunciation: /ˌærɪθˈmɛtɪk ˈmiːn/, stress on third syllable of \"arithmetic\"), or simply the mean or average when the context is clear, is the sum of a collection of numbers divided by the number of numbers in the collection. The collection is often a set of results of an experiment, or a set of results from a survey. The term \"arithmetic mean\" is preferred in some contexts in mathematics and statistics because it helps distinguish it from other means, such as the geometric mean and the harmonic mean.\nIn addition to mathematics and statistics, the arithmetic mean is used frequently in fields such as economics, sociology, and history, and it is used in almost every academic field to some extent. For example, per capita income is the arithmetic average income of a nation's population.\nWhile the arithmetic mean is often used to report central tendencies, it is not a robust statistic, meaning that it is greatly influenced by outliers (values that are very much larger or smaller than most of the values). Notably, for skewed distributions, such as the distribution of income for which a few people's incomes are substantially greater than most people's, the arithmetic mean may not accord with one's notion of \"middle\", and robust statistics, such as the median, may be a better description of central tendency.\nIn a more obscure usage, any sequence of values that form an arithmetic sequence between two numbers x and y can be called \"arithmetic means between x and y.\"","alt_names":["arithmetic_mean"],"name":"Arithmetic mean","categories":["All articles needing additional references","Articles needing additional references from July 2013","Means","Use dmy dates from June 2013"],"tag_line":"In mathematics and statistics, the arithmetic mean (pronunciation: /ˌærɪθˈmɛtɪk ˈmiːn/, stress on third syllable of \"arithmetic\"), or simply the mean or average when the context is clear, is the sum of a collection of numbers divided by the number of numbers in the collection."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dot-product","_score":0,"_source":{"description":"In mathematics, the dot product or scalar product (sometimes inner product in the context of Euclidean space, or rarely projection product for emphasizing the geometric significance), is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number. This operation can be defined either algebraically or geometrically. Algebraically, it is the sum of the products of the corresponding entries of the two sequences of numbers. Geometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them. The name \"dot product\" is derived from the centered dot \" · \" that is often used to designate this operation; the alternative name \"scalar product\" emphasizes that the result is a scalar (rather than a vector).\nIn three-dimensional space, the dot product contrasts with the cross product of two vectors, which produces a pseudovector as the result. The dot product is directly related to the cosine of the angle between two vectors in Euclidean space of any number of dimensions.\n\n","alt_names":["Dot_product"],"name":"Dot product","categories":["Analytic geometry","Articles containing proofs","Bilinear forms","Linear algebra","Vectors (mathematics and physics)"],"tag_line":"In mathematics, the dot product or scalar product (sometimes inner product in the context of Euclidean space, or rarely projection product for emphasizing the geometric significance), is an algebraic operation that takes two equal-length sequences of numbers (usually coordinate vectors) and returns a single number."}}
,{"_index":"throwtable","_type":"algorithm","_id":"chess960","_score":0,"_source":{"description":"Chess960 (or Fischer Random Chess) is a variant of chess invented and advocated by former World Chess Champion Bobby Fischer, publicly announced on June 19, 1996 in Buenos Aires, Argentina. It employs the same board and pieces as standard chess; however, the starting position of the pieces on the players' home ranks is randomized. The name \"Chess960\" is derived from the number of possible starting positions. The random setup renders the prospect of obtaining an advantage through the memorization of opening lines impracticable, compelling players to rely on their talent and creativity.\nRandomizing the main pieces had long been known as Shuffle Chess; however, Chess960 introduces restrictions on the randomization, \"preserving the dynamic nature of the game by retaining bishops of opposite colours for each player and the right to castle for both sides\", resulting in 960 unique starting positions.\nIn 2008 FIDE added Chess960 to an appendix of the rules of chess.","name":"Chess960","categories":["1996 in chess","All articles needing additional references","All articles with specifically marked weasel-worded phrases","All articles with unsourced statements","Articles needing additional references from March 2015","Articles with specifically marked weasel-worded phrases from March 2015","Articles with unsourced statements from June 2008","Board games introduced in 1996","CS1 German-language sources (de)","Chess variants","Recurring events established in 2001","Use mdy dates from February 2013","Wikipedia articles with GND identifiers"],"tag_line":"Chess960 (or Fischer Random Chess) is a variant of chess invented and advocated by former World Chess Champion Bobby Fischer, publicly announced on June 19, 1996 in Buenos Aires, Argentina."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pangram","_score":0,"_source":{"description":"A pangram (Greek: παν γράμμα, pan gramma, \"every letter\") or holoalphabetic sentence for a given alphabet is a sentence using every letter of the alphabet at least once. Pangrams have been used to display typefaces, test equipment, and develop skills in handwriting, calligraphy, and keyboarding.\nThe best known English pangram is \"The quick brown fox jumps over the lazy dog.\" It has been used since at least the late 19th century, was utilized by Western Union to test Telex / TWX data communication equipment for accuracy and reliability, and is now used by a number of computer programs (most notably the font viewer built into Microsoft Windows) to display computer fonts.\nAn example in another language is the German Victor jagt zwölf Boxkämpfer quer über den großen Sylter Deich, containing all letters used in German, including every umlaut (ä, ö, ü) plus the ß. It has been used since before 1800.\nShort pangrams in English are more difficult to come up with and tend to use uncommon words, because the English language uses some letters (especially vowels) much more frequently than others. Longer pangrams may afford more opportunity for humor, cleverness, or thoughtfulness. In a sense, the pangram is the opposite of the lipogram, in which the aim is to omit one or more letters. A perfect pangram contains every letter of the alphabet only once and can be considered an anagram of the alphabet; it is the shortest possible pangram. An example is the phrase \"Cwm fjord bank glyphs vext quiz\" (cwm, a loan word from Welsh, means a steep-sided valley, particularly in Wales).","name":"Pangram","categories":["All articles needing additional references","Articles containing Chinese-language text","Articles containing German-language text","Articles containing Greek-language text","Articles containing Welsh-language text","Articles needing additional references from September 2013","Commons category with local link same as on Wikidata","Pangrams","Phrases","Test items","Typography","Word games"],"tag_line":"A pangram (Greek: παν γράμμα, pan gramma, \"every letter\") or holoalphabetic sentence for a given alphabet is a sentence using every letter of the alphabet at least once."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mad-libs","_score":0,"_source":{"description":"Mad Libs is a phrasal template word game where one player prompts others for a list of words to substitute for blanks in a story, before reading the – often comical or nonsensical – story aloud. The game is frequently played as a party game or as a pastime.\nThe game was invented in the United States, and more than 110 million copies of Mad Libs books have been sold since the series was first published in 1958.","alt_names":["Mad_Libs"],"name":"Mad Libs","categories":["1958 introductions","Random text generation","Word games"],"tag_line":"Mad Libs is a phrasal template word game where one player prompts others for a list of words to substitute for blanks in a story, before reading the – often comical or nonsensical – story aloud."}}
,{"_index":"throwtable","_type":"algorithm","_id":"teiresias-algorithm","_score":0,"_source":{"description":"The Teiresias algorithm is a combinatorial algorithm for the discovery of rigid patterns (motifs) in biological sequences. It is named after the Greek prophet Teiresias and was created in 1997 by Isidore Rigoutsos and Aris Floratos.\nThe problem of finding sequence similarities in the primary structure of related proteins or genes is one of the problems arising in the analysis of biological sequences. It can be shown that pattern discovery in its general form is NP-hard. The Teiresias algorithm, is based on the observation that if a pattern spans many positions and appears exactly k times in the input then all fragments (sub patterns) of the pattern have to appear at least k times in the input. The algorithm is able to produce all patterns that have a user-defined number of copies in the given input, and manages to be very efficient by avoiding the enumeration of the entire space. Finally, the algorithm reports motifs that are maximal in both length and composition.\nA new implementation of the Teiresias algorithm was recently made available by the Computational Medicine Center at Thomas Jefferson University . Teiresias is also accessible through an interactive web-based user interface by the same center. See external links for both.\n^ Rigoutsos, I, Floratos, A (1998) Combinatorial pattern discovery in biological sequences: The TEIRESIAS algorithm. Bioinformatics 14: 55-67\n^ Maier, D., \"The Complexity of Some Problems on Subsequences and Supersequences\", Journal of the ACM, 322-336, 1978","name":"Teiresias algorithm","categories":["Data mining algorithms","Official website not in Wikidata","Pattern matching"],"tag_line":"The Teiresias algorithm is a combinatorial algorithm for the discovery of rigid patterns (motifs) in biological sequences."}}
,{"_index":"throwtable","_type":"algorithm","_id":"k-medians-clustering","_score":0,"_source":{"description":"In statistics and data mining, k-medians clustering is a cluster analysis algorithm. It is a variation of k-means clustering where instead of calculating the mean for each cluster to determine its centroid, one instead calculates the median. This has the effect of minimizing error over all clusters with respect to the 1-norm distance metric, as opposed to the square of the 2-norm distance metric (which k-means does.)\nThis relates directly to the k-median problem which is the problem of finding k centers such that the clusters formed by them are the most compact. Formally, given a set of data points x, the k centers ci are to be chosen so as to minimize the sum of the distances from each x to the nearest ci.\nThe criterion function formulated in this way is sometimes a better criterion than that used in the k-means clustering algorithm, in which the sum of the squared distances is used. The sum of distances is widely used in applications such as facility location.\nThe proposed algorithm uses Lloyd-style iteration which alternates between an expectation (E) and maximization (M) step, making this an Expectation–maximization algorithm. In the E step, all objects are assigned to their nearest median. In the M step, the medians are recomputed by using the median in each single dimension.","name":"K-medians clustering","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Data clustering algorithms","Operations research","Statistical algorithms","Statistics stubs"],"tag_line":"In statistics and data mining, k-medians clustering is a cluster analysis algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"generalization-error","_score":0,"_source":{"description":"The generalization error of a machine learning model is a function that measures how well a learning machine generalizes to unseen data. It is measured as the distance between the error on the training set and the test set and is averaged over the entire set of possible training data that can be generated after each iteration of the learning process. It has this name because this function indicates the capacity of a machine that learns with the specified algorithm to infer a rule (or generalize) that is used by the teacher machine to generate data based only on a few examples.\nThe theoretical model assumes a probability distribution of the examples, and a function giving the exact target. The model can also include noise in the example (in the input and/or target output). The generalization error is usually defined as the expected value of the square of the difference between the learned function and the exact target (mean-square error). In practical cases, the distribution and target are unknown; statistical estimates are used.\nThe performance of a machine learning algorithm is measured by plots of the generalization error values through the learning process and are called learning curve.\nThe generalization error of a perceptron is the probability of the student perceptron to classify an example differently from the teacher and is given by the overlap of the student and teacher synaptic vectors and is a function of their scalar product.","name":"Generalization error","categories":["All stub articles","Classification algorithms","Robotics stubs"],"tag_line":"The generalization error of a machine learning model is a function that measures how well a learning machine generalizes to unseen data."}}
,{"_index":"throwtable","_type":"algorithm","_id":"coboosting","_score":0,"_source":{"description":"CoBoost is a semi-supervised training algorithm proposed by Collins and Singer in 1999. The original application for the algorithm was the task of Named Entity Classification using very weak learners. It can be used for performing semi-supervised learning in cases in which there exist redundancy in features.\nIt may be seen as a combination of co-training and boosting. Each example is available in two views (subsections of the feature set), and boosting is applied iteratively in alternation with each view using predicted labels produced in the alternate view on the previous iteration. CoBoosting is not a valid boosting algorithm in the PAC learning sense.\n\n","name":"CoBoosting","categories":["Classification algorithms"],"tag_line":"CoBoost is a semi-supervised training algorithm proposed by Collins and Singer in 1999."}}
,{"_index":"throwtable","_type":"algorithm","_id":"information-fuzzy-networks","_score":0,"_source":{"description":"Info Fuzzy Networks(IFN) is a greedy machine learning algorithm for supervised learning. The data structure produced by the learning algorithm is also called Info Fuzzy Network. IFN construction is quite similar to decision trees' construction. However, IFN constructs a directed graph and not a tree. IFN also uses the conditional mutual information metric in order to choose features during the construction stage while decision trees usually use other metrics like entropy or gini.\n\n","name":"Information Fuzzy Networks","categories":["All articles with topics of unclear notability","All orphaned articles","Articles with topics of unclear notability from May 2010","Classification algorithms","Orphaned articles from May 2010"],"tag_line":"Info Fuzzy Networks(IFN) is a greedy machine learning algorithm for supervised learning."}}
,{"_index":"throwtable","_type":"algorithm","_id":"apriori-algorithm","_score":0,"_source":{"description":"Apriori is an algorithm for frequent item set mining and association rule learning over transactional databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.","name":"Apriori algorithm","categories":["Articles with example pseudocode","Data mining algorithms"],"tag_line":"Apriori is an algorithm for frequent item set mining and association rule learning over transactional databases."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cure-data-clustering-algorithm","_score":0,"_source":{"description":"CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases that is more robust to outliers and identifies clusters having non-spherical shapes and size variances.\n\n","name":"CURE data clustering algorithm","categories":["All articles with unsourced statements","Articles with unsourced statements from May 2015","Data clustering algorithms"],"tag_line":"CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases that is more robust to outliers and identifies clusters having non-spherical shapes and size variances.\n\n"}}
,{"_index":"throwtable","_type":"algorithm","_id":"syntactic-pattern-recognition","_score":0,"_source":{"description":"Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features. This allows for representing pattern structures, taking into account more complex interrelationships between attributes than is possible in the case of flat, numerical feature vectors of fixed dimensionality, that are used in statistical classification.\nSyntactic pattern recognition can be used instead of statistical pattern recognition if there is clear structure in the patterns. One way to present such structure is by means of a strings of symbols from a formal language. In this case the differences in the structures of the classes are encoded as different grammars.\nAn example of this would be diagnosis of the heart with ECG measurements. ECG waveforms can be approximated with diagonal and vertical line segments. If normal and unhealthy waveforms can be described as formal grammars, measured ECG signal can be classified as healthy or unhealthy by first describing it in term of the basic line segments and then trying to parse the descriptions according to the grammars. Another example is tessellation of tiling patterns.\nA second way to represent relations are graphs, where nodes are connected if corresponding subpatterns are related. An item can be labeled as belonging to a class if its graph representation is isomorphic with prototype graphs of the class.\nTypically, patterns are constructed from simpler sub patterns in a hierarchical fashion. This helps in dividing the recognition task into easier subtask of first identifying sub patterns and only then the actual patterns.\nStructural methods provide descriptions of items, which may be useful in their own right. For example, syntactic pattern recognition can be used to find out what objects are present in an image. Furthermore, structural methods are strong in finding a correspondence mapping between two images of an object. Under natural conditions, corresponding features will be in different positions and/or may be occluded in the two images, due to camera-attitude and perspective, as in face recognition. A graph-matching algorithm will yield the optimal correspondence.","name":"Syntactic pattern recognition","categories":["Classification algorithms"],"tag_line":"Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features."}}
,{"_index":"throwtable","_type":"algorithm","_id":"support-vector-machine","_score":0,"_source":{"description":"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis. Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\nWhen data is not labeled, a supervised learning is not possible, and an unsupervised learning is required, that would find natural clustering of the data to groups, and map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering (SVC) is highly used in industrial applications either when data is not labeled or when only some data is labeled as a preprocessing for a classification pass; the clustering method was published.","name":"Support vector machine","categories":["All articles with unsourced statements","Articles with unsourced statements from February 2015","Articles with unsourced statements from June 2013","Classification algorithms","Statistical classification","Support vector machines","Wikipedia articles with GND identifiers"],"tag_line":"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis."}}
,{"_index":"throwtable","_type":"algorithm","_id":"multi-label-classification","_score":0,"_source":{"description":"In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple target labels must be assigned to each instance. Multi-label classification should not be confused with multiclass classification, which is the problem of categorizing instances into one of more than two classes. Formally, multi-label learning can be phrased as the problem of finding a model that maps inputs x to binary vectors y, rather than scalar outputs as in the ordinary classification problem.\nThere are two main methods for tackling the multi-label classification problem: problem transformation methods and algorithm adaptation methods. Problem transformation methods transform the multi-label problem into a set of binary classification problems, which can then be handled using single-class classifiers. Algorithm adaptation methods adapt the algorithms to directly perform multi-label classification. In other words, rather than trying to convert the problem to a simpler problem, they try to address the problem in its full form.\n\n","name":"Multi-label classification","categories":["Classification algorithms"],"tag_line":"In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple target labels must be assigned to each instance."}}
,{"_index":"throwtable","_type":"algorithm","_id":"random-subspace-method","_score":0,"_source":{"description":"Random subspace method  (or attribute bagging) is an ensemble classifier that consists of several classifiers each operating in a subspace of the original feature space, and outputs the class based on the outputs of these individual classifiers. Random subspace method has been used for decision trees (random decision forests), linear classifiers, support vector machines, nearest neighbours and other types of classifiers. This method is also applicable to one-class classifiers.\nThe algorithm is an attractive choice for classification problems where the number of features is much larger than the number of training objects, such as fMRI data or gene expression data.","name":"Random subspace method","categories":["CS1 errors: chapter ignored","Classification algorithms","Ensemble learning","Pages containing cite templates with deprecated parameters","Pages with citations having bare URLs","Pages with citations lacking titles"],"tag_line":"Random subspace method  (or attribute bagging) is an ensemble classifier that consists of several classifiers each operating in a subspace of the original feature space, and outputs the class based on the outputs of these individual classifiers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"perceptron","_score":0,"_source":{"description":"In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers: functions that can decide whether an input (represented by a vector of numbers) belongs to one class or another. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time.\nThe perceptron algorithm dates back to the late 1950s; its first implementation, in custom hardware, was one of the first artificial neural networks to be produced.\n\n","name":"Perceptron","categories":["All accuracy disputes","Articles with disputed statements from August 2015","Articles with disputed statements from June 2014","Articles with example Python code","Artificial neural networks","Classification algorithms"],"tag_line":"In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers: functions that can decide whether an input (represented by a vector of numbers) belongs to one class or another."}}
,{"_index":"throwtable","_type":"algorithm","_id":"winnow-(algorithm)","_score":0,"_source":{"description":"The winnow algorithm is a technique from machine learning for learning a linear classifier from labeled examples. It is very similar to the perceptron algorithm. However, the perceptron algorithm uses an additive weight-update scheme, while Winnow uses a multiplicative scheme that allows it to perform much better when many dimensions are irrelevant (hence its name). It is a simple algorithm that scales well to high-dimensional data. During training, Winnow is shown a sequence of positive and negative examples. From these it learns a decision hyperplane that can then be used to label novel examples as positive or negative. The algorithm can also be used in the online learning setting, where the learning and the classification phase are not clearly separated.","name":"Winnow (algorithm)","categories":["Classification algorithms"],"tag_line":"The winnow algorithm is a technique from machine learning for learning a linear classifier from labeled examples."}}
,{"_index":"throwtable","_type":"algorithm","_id":"margin-classifier","_score":0,"_source":{"description":"In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example. For instance, if a linear classifier (e.g. perceptron or linear discriminant analysis) is used, the distance (typically euclidean distance, though others may be used) of an example from the separating hyperplane is the margin of that example.\nThe notion of margin is important in several machine learning classification algorithms, as it can be used to bound the generalization error of the classifier. These bounds are frequently shown using the VC dimension. Of particular prominence is the generalization error bound on boosting algorithms and support vector machines.","name":"Margin classifier","categories":["Classification algorithms","Statistical classification"],"tag_line":"In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gaussian-adaptation","_score":0,"_source":{"description":"Gaussian adaptation (GA) (also referred to as normal or natural adaptation and sometimes abbreviated as NA) is an evolutionary algorithm designed for the maximization of manufacturing yield due to statistical deviation of component values of signal processing systems. In short, GA is a stochastic adaptive process where a number of samples of an n-dimensional vector x[xT = (x1, x2, ..., xn)] are taken from a multivariate Gaussian distribution, N(m, M), having mean m and moment matrix M. The samples are tested for fail or pass. The first- and second-order moments of the Gaussian restricted to the pass samples are m* and M*.\nThe outcome of x as a pass sample is determined by a function s(x), 0 < s(x) < q ≤ 1, such that s(x) is the probability that x will be selected as a pass sample. The average probability of finding pass samples (yield) is\n\nThen the theorem of GA states:\n\nFor any s(x) and for any value of P < q, there always exist a Gaussian p. d. f. that is adapted for maximum dispersion. The necessary conditions for a local optimum are m = m* and M proportional to M*. The dual problem is also solved: P is maximized while keeping the dispersion constant (Kjellström, 1991).\n\nProofs of the theorem may be found in the papers by Kjellström, 1970, and Kjellström & Taxén, 1981.\nSince dispersion is defined as the exponential of entropy/disorder/average information it immediately follows that the theorem is valid also for those concepts. Altogether, this means that Gaussian adaptation may carry out a simultaneous maximisation of yield and average information (without any need for the yield or the average information to be defined as criterion functions).\nThe theorem is valid for all regions of acceptability and all Gaussian distributions. It may be used by cyclic repetition of random variation and selection (like the natural evolution). In every cycle a sufficiently large number of Gaussian distributed points are sampled and tested for membership in the region of acceptability. The centre of gravity of the Gaussian, m, is then moved to the centre of gravity of the approved (selected) points, m*. Thus, the process converges to a state of equilibrium fulfilling the theorem. A solution is always approximate because the centre of gravity is always determined for a limited number of points.\nIt was used for the first time in 1969 as a pure optimization algorithm making the regions of acceptability smaller and smaller (in analogy to simulated annealing, Kirkpatrick 1983). Since 1970 it has been used for both ordinary optimization and yield maximization.","name":"Gaussian adaptation","categories":["All articles lacking reliable references","All articles needing additional references","All articles needing expert attention","Articles lacking reliable references from July 2008","Articles needing additional references from July 2008","Articles needing expert attention from January 2015","Articles needing expert attention with no reason or talk parameter","Creationism","Creativity","Evolutionary algorithms","Free will","Mathematics articles needing expert attention","Wikipedia articles with possible conflicts of interest from March 2009"],"tag_line":"Gaussian adaptation (GA) (also referred to as normal or natural adaptation and sometimes abbreviated as NA) is an evolutionary algorithm designed for the maximization of manufacturing yield due to statistical deviation of component values of signal processing systems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"evolved-antenna","_score":0,"_source":{"description":"In radio communications, an evolved antenna is an antenna designed fully or substantially by an automatic computer design program that uses an evolutionary algorithm that mimics Darwinian evolution. This sophisticated procedure has been used in recent years to design a few antennas for mission-critical applications involving stringent, conflicting, or unusual design requirements, such as unusual radiation patterns, for which none of the many existing antenna types are adequate.\nThe computer program starts with simple antenna shapes, then adds or modifies elements in a semirandom manner to create a number of new candidate antenna shapes. These are then evaluated to determine how well they fulfill the design requirements, and a numerical score is computed for each. Then, in a step similar to natural selection, a portion of the candidate antennas with the worst scores are discarded, leaving a small population of the highest-scoring designs. Using these antennas, the computer repeats the procedure, generating a population of even higher-scoring designs. After a number of iterations, the population of antennas is evaluated and the highest-scoring design is chosen. The resulting antenna often outperforms the best manual designs, because it has a complicated asymmetric shape that could not have been found with traditional manual design methods.\nThe first evolved antenna designs appeared in the mid-1990s from the work of Michielssen, Altshuler, Linden, Haupt, and Rahmat-Samii. Most practitioners use the genetic algorithm technique or some variant thereof to evolve antenna designs.\nAn example of an evolved antenna is an X-band antenna evolved for a 2006 NASA mission called Space Technology 5 (ST5). The mission consists of three satellites that will take measurements in Earth's magnetosphere. Each satellite has two communication antennas to talk to ground stations. The antenna has an unusual structure and was evolved to meet a challenging set of mission requirements, notably the combination of wide beamwidth for a circularly polarized wave and wide impedance bandwidth. For comparison, a traditional approach to meet the mission requirements might involve a helical antenna design, or specifically, a quadrifilar helix. The ST5 mission successfully launched on March 22, 2006, and so this evolved antenna represents the world's first artificially-evolved object to fly in space.","name":"Evolved antenna","categories":["Evolutionary algorithms","Radio frequency antenna types"],"tag_line":"In radio communications, an evolved antenna is an antenna designed fully or substantially by an automatic computer design program that uses an evolutionary algorithm that mimics Darwinian evolution."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cma-es","_score":0,"_source":{"description":"CMA-ES stands for Covariance Matrix Adaptation Evolution Strategy. Evolution strategies (ES) are stochastic, derivative-free methods for numerical optimization of non-linear or non-convex continuous optimization problems. They belong to the class of evolutionary algorithms and evolutionary computation. An evolutionary algorithm is broadly based on the principle of biological evolution, namely the repeated interplay of variation (via recombination and mutation) and selection: in each generation (iteration) new individuals (candidate solutions, denoted as ) are generated by variation, usually in a stochastic way, of the current parental individuals. Then, some individuals are selected to become the parents in the next generation based on their fitness or objective function value . Like this, over the generation sequence, individuals with better and better -values are generated.\nIn an evolution strategy, new candidate solutions are sampled according to a multivariate normal distribution in the . Recombination amounts to selecting a new mean value for the distribution. Mutation amounts to adding a random vector, a perturbation with zero mean. Pairwise dependencies between the variables in the distribution are represented by a covariance matrix. The covariance matrix adaptation (CMA) is a method to update the covariance matrix of this distribution. This is particularly useful, if the function  is ill-conditioned.\nAdaptation of the covariance matrix amounts to learning a second order model of the underlying objective function similar to the approximation of the inverse Hessian matrix in the Quasi-Newton method in classical optimization. In contrast to most classical methods, fewer assumptions on the nature of the underlying objective function are made. Only the ranking between candidate solutions is exploited for learning the sample distribution and neither derivatives nor even the function values themselves are required by the method.","name":"CMA-ES","categories":["Evolutionary algorithms","Optimization algorithms and methods","Stochastic optimization"],"tag_line":"CMA-ES stands for Covariance Matrix Adaptation Evolution Strategy."}}
,{"_index":"throwtable","_type":"algorithm","_id":"darwintunes","_score":0,"_source":{"description":"DarwinTunes is a research project into the use of natural selection to create music led by Bob MacCallum and Armand Leroi, scientists at Imperial College London. The project asks volunteers on the Internet to listen to automatically generated sound loops and rate them based on aesthetic preference. After the volunteers rate the loops on a five-point scale, software permits the highest rated loops to 'reproduce sexually' and populate the next generation of musical loops.\nIn a paper published in the Proceedings of the National Academies of Science, the DarwinTunes developers describe how their first experimental population derived from two randomly generated founding loops, allowed 100 generations of loops to evolve without any selection pressure before asking members of the public to rate the loops. The paper found that for the first 500 to 600 generations, aesthetic quality of the loops dramatically improved before reaching a stable equilibrium. They tested this using ratings by listeners and also by using sampling techniques used by music information retrieval technology—namely the Chordino and Rhythm Patterns algorithms, which measure the presence of chords used in Western music and the presence of rhythm respectively.","name":"DarwinTunes","categories":["All stub articles","Artificial life models","Computer music software","Evolutionary algorithms","Music software stubs","Music theory stubs"],"tag_line":"DarwinTunes is a research project into the use of natural selection to create music led by Bob MacCallum and Armand Leroi, scientists at Imperial College London."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rader's-fft-algorithm","_score":0,"_source":{"description":"Rader's algorithm (1968) is a fast Fourier transform (FFT) algorithm that computes the discrete Fourier transform (DFT) of prime sizes by re-expressing the DFT as a cyclic convolution (the other algorithm for FFTs of prime sizes, Bluestein's algorithm, also works by rewriting the DFT as a convolution).\nSince Rader's algorithm only depends upon the periodicity of the DFT kernel, it is directly applicable to any other transform (of prime order) with a similar property, such as a number-theoretic transform or the discrete Hartley transform.\nThe algorithm can be modified to gain a factor of two savings for the case of DFTs of real data, using a slightly modified re-indexing/permutation to obtain two half-size cyclic convolutions of real data (Chu & Burrus, 1982); an alternative adaptation for DFTs of real data, using the discrete Hartley transform, was described by Johnson & Frigo (2007).\nWinograd extended Rader's algorithm to include prime-power DFT sizes  (Winograd 1976; Winograd 1978), and today Rader's algorithm is sometimes described as a special case of Winograd's FFT algorithm, also called the multiplicative Fourier transform algorithm (Tolimieri et al., 1997), which applies to an even larger class of sizes. However, for composite sizes such as prime powers, the Cooley–Tukey FFT algorithm is much simpler and more practical to implement, so Rader's algorithm is typically only used for large-prime base cases of Cooley–Tukey's recursive decomposition of the DFT (Frigo and Johnson, 2005).","name":"Rader's FFT algorithm","categories":["FFT algorithms"],"tag_line":"Rader's algorithm (1968) is a fast Fourier transform (FFT) algorithm that computes the discrete Fourier transform (DFT) of prime sizes by re-expressing the DFT as a cyclic convolution (the other algorithm for FFTs of prime sizes, Bluestein's algorithm, also works by rewriting the DFT as a convolution)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"biogeography-based-optimization","_score":0,"_source":{"description":"Biogeography-based optimization (BBO) is an evolutionary algorithm (EA) that optimizes a function by stochastically and iteratively improving candidate solutions with regard to a given measure of quality, or fitness function. BBO belongs to the class of metaheuristics since it includes many variations, and since it does not make any assumptions about the problem and can therefore be applied to a wide class of problems.\nBBO is typically used to optimize multidimensional real-valued functions, but it does not use the gradient of the function, which means that it does not require the function to be differentiable as required by classic optimization methods such as gradient descent and quasi-newton methods. BBO can therefore be used on discontinuous functions.\nBBO optimizes a problem by maintaining a population of candidate solutions, and creating new candidate solutions by combining existing ones according to a simple formula. In this way the objective function is treated as a black box that merely provides a measure of quality given a candidate solution, and the function's gradient is not needed.\nLike many EAs, BBO was motivated by a natural process; in particular, BBO was motivated by biogeography, which is the study of the distribution of biological species through time and space. BBO was originally introduced by Dan Simon in 2008.","name":"Biogeography-based optimization","categories":["Evolutionary algorithms","Stochastic optimization"],"tag_line":"Biogeography-based optimization (BBO) is an evolutionary algorithm (EA) that optimizes a function by stochastically and iteratively improving candidate solutions with regard to a given measure of quality, or fitness function."}}
,{"_index":"throwtable","_type":"algorithm","_id":"constructive-cooperative-coevolution","_score":0,"_source":{"description":"The constructive cooperative coevolutionary algorithm (also called C3) is an global optimisation algorithm in artificial intelligence based on the multi-start architecture of the greedy randomized adaptive search procedure (GRASP). It incorporates the existing cooperative coevolutionary algorithm (CC). The considered problem is decomposed into subproblems. These subproblems are optimised separately while exchanging information in order to solve the complete problem. An optimisation algorithm, usually but not necessarily an evolutionary algorithm, is embedded in C3 for optimising those subproblems. The nature of the embedded optimisation algorithm determines whether C3's behaviour is deterministic or stochastic.\nThe C3 optimisation algorithm was originally designed for simulation-based optimisation but it can be used for global optimisation problems in general. Its strength over other optimisation algorithms, specifically cooperative coevolution, is that it is better able to handle non-separable optimisation problems.","name":"Constructive cooperative coevolution","categories":["Evolutionary algorithms","Evolutionary computation","Mathematical optimization","Optimization algorithms and methods"],"tag_line":"The constructive cooperative coevolutionary algorithm (also called C3) is an global optimisation algorithm in artificial intelligence based on the multi-start architecture of the greedy randomized adaptive search procedure (GRASP)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"artificial-bee-colony-algorithm","_score":0,"_source":{"description":"In computer science and operations research, the artificial bee colony algorithm (ABC) is an optimization algorithm based on the intelligent foraging behaviour of honey bee swarm, proposed by Karaboga in 2005.\n\n","name":"Artificial bee colony algorithm","categories":["All articles needing expert attention","Articles needing expert attention","Articles needing expert attention with no reason or talk parameter","Articles needing unspecified expert attention","Bees","Collective intelligence","Evolutionary algorithms","Optimization algorithms and methods"],"tag_line":"In computer science and operations research, the artificial bee colony algorithm (ABC) is an optimization algorithm based on the intelligent foraging behaviour of honey bee swarm, proposed by Karaboga in 2005.\n\n"}}
,{"_index":"throwtable","_type":"algorithm","_id":"cyclotomic-fast-fourier-transform","_score":0,"_source":{"description":"The cyclotomic fast Fourier transform is a type of fast Fourier transform algorithm over finite fields. This algorithm first decomposes a DFT into several circular convolutions, and then derives the DFT results from the circular convolution results. When applied to a DFT over , this algorithm has a very low multiplicative complexity. In practice, since there usually exist efficient algorithms for circular convolutions with specific lengths, this algorithm is very efficient.","name":"Cyclotomic fast Fourier transform","categories":["Discrete transforms","FFT algorithms"],"tag_line":"The cyclotomic fast Fourier transform is a type of fast Fourier transform algorithm over finite fields."}}
,{"_index":"throwtable","_type":"algorithm","_id":"geometric-design","_score":0,"_source":{"description":"Geometric design (GD), also known as geometric modelling, is a branch of computational geometry. It deals with the construction and representation of free-form curves, surfaces, or volumes. Core problems are curve and surface modelling and representation. GD studies especially the construction and manipulation of curves and surfaces given by a set of points using polynomial, rational, piecewise polynomial, or piecewise rational methods. The most important instruments here are parametric curves and parametric surfaces, such as Bézier curves, spline curves and surfaces. An important non-parametric approach is the level set method.\nApplication areas include shipbuilding, aircraft, and automotive industries, as well as architectural design. The modern ubiquity and power of computers means that even perfume bottles and shampoo dispensers are designed using techniques unheard of by shipbuilders of 1960s.\nGeometric models can be built for objects of any dimension in any geometric space. Both 2D and 3D geometric models are extensively used in computer graphics. 2D models are important in computer typography and technical drawing. 3D models are central to computer-aided design and manufacturing, and many applied technical fields such as geology and medical image processing.\nGeometric models are usually distinguished from procedural and object-oriented models, which define the shape implicitly by an algorithm. They are also contrasted with digital images and volumetric models; and with implicit mathematical models such as the zero set of an arbitrary polynomial. However, the distinction is often blurred: for instance, geometric shapes can be represented by objects; a digital image can be interpreted as a collection of colored squares; and geometric shapes such as circles are defined by implicit mathematical equations. Also, the modeling of fractal objects often requires a combination of geometric and procedural techniques.\nGeometric problems originating in architecture can lead to interesting research and results in geometry processing, computer-aided geometric design, and discrete differential geometry.","name":"Geometric design","categories":["Computational science","Computer-aided design","Geometric algorithms"],"tag_line":"Geometric design (GD), also known as geometric modelling, is a branch of computational geometry."}}
,{"_index":"throwtable","_type":"algorithm","_id":"particle-swarm-optimization","_score":0,"_source":{"description":"In computer science, particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. PSO optimizes a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. Each particle's movement is influenced by its local best known position but, is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions.\nPSO is originally attributed to Kennedy, Eberhart and Shi and was first intended for simulating social behaviour, as a stylized representation of the movement of organisms in a bird flock or fish school. The algorithm was simplified and it was observed to be performing optimization. The book by Kennedy and Eberhart describes many philosophical aspects of PSO and swarm intelligence. An extensive survey of PSO applications is made by Poli.\nPSO is a metaheuristic as it makes few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. However, metaheuristics such as PSO do not guarantee an optimal solution is ever found. More specifically, PSO does not use the gradient of the problem being optimized, which means PSO does not require that the optimization problem be differentiable as is required by classic optimization methods such as gradient descent and quasi-newton methods. PSO can therefore also be used on optimization problems that are partially irregular, noisy, change over time, etc.","name":"Particle swarm optimization","categories":["Evolutionary algorithms","Optimization algorithms and methods","Pages using citations with format and no URL"],"tag_line":"In computer science, particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality."}}
,{"_index":"throwtable","_type":"algorithm","_id":"marching-tetrahedra","_score":0,"_source":{"description":"Marching tetrahedra is an algorithm in the field of computer graphics to render implicit surfaces. It clarifies a minor ambiguity problem of the marching cubes algorithm with some cube configurations.\nSince more than 20 years have passed from the patent filing date of the marching cubes (June 5, 1985), the original algorithm can be used freely again, adding only the minor modification to circumvent the aforementioned ambiguity in some configurations.\nIn marching tetrahedra, each cube is split into six irregular tetrahedra by cutting the cube in half three times, cutting diagonally through each of the three pairs of opposing faces. In this way, the tetrahedra all share one of the main diagonals of the cube. Instead of the twelve edges of the cube, we now have nineteen edges: the original twelve, six face diagonals, and the main diagonal. Just like in marching cubes, the intersections of these edges with the isosurface are approximated by linearly interpolating the values at the grid points.\nAdjacent cubes share all edges in the connecting face, including the same diagonal. This is an important property to prevent cracks in the rendered surface, because interpolation of the two distinct diagonals of a face usually gives slightly different intersection points. An added benefit is that up to five computed intersection points can be reused when handling the neighbor cube. This includes the computed surface normals and other graphics attributes at the intersection points.\nEach tetrahedron has sixteen possible configurations, falling into three classes: no intersection, intersection in one triangle and intersection in two (adjacent) triangles. It is straightforward to enumerate all sixteen configurations and map them to vertex index lists defining the appropriate triangle strips.","name":"Marching tetrahedra","categories":["All articles needing additional references","Articles needing additional references from September 2012","Computer graphics algorithms"],"tag_line":"Marching tetrahedra is an algorithm in the field of computer graphics to render implicit surfaces."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shoelace-formula","_score":0,"_source":{"description":"The shoelace formula or shoelace algorithm (also known as Gauss's area formula and the surveyor's formula) is a mathematical algorithm to determine the area of a simple polygon whose vertices are described by ordered pairs in the plane. The user cross-multiplies corresponding coordinates to find the area encompassing the polygon, and subtracts it from the surrounding polygon to find the area of the polygon within. It is called the shoelace formula because of the constant cross-multiplying for the coordinates making up the polygon, like tying shoelaces. It is also sometimes called the shoelace method. It has applications in surveying and forestry, among other areas.\nThe formula was described by Meister (1724-1788) in 1769 and by Gauss in 1795. It can be verified by dividing the polygon into triangles, but it can also be seen as a special case of Green's theorem.\nThe area formula is derived by taking each edge AB, and calculating the (signed) area of triangle ABO with a vertex at the origin O, by taking the cross-product (which gives the area of a parallelogram) and dividing by 2. As one wraps around the polygon, these triangles with positive and negative area will overlap, and the areas between the origin and the polygon will be cancelled out and sum to 0, while only the area inside the reference triangle remains. This is why the formula is called the Surveyor's Formula, since the \"surveyor\" is at the origin; if going counterclockwise, positive area is added when going from left to right and negative area is added when going from right to left, from the perspective of the origin.\nThe area formula is valid for any non-self-intersecting (simple) polygon, which can be convex or concave.","name":"Shoelace formula","categories":["CS1 Latin-language sources (la)","Geometric algorithms","Surveying"],"tag_line":"The shoelace formula or shoelace algorithm (also known as Gauss's area formula and the surveyor's formula) is a mathematical algorithm to determine the area of a simple polygon whose vertices are described by ordered pairs in the plane."}}
,{"_index":"throwtable","_type":"algorithm","_id":"diamond-square-algorithm","_score":0,"_source":{"description":"The diamond-square algorithm is a method for generating heightmaps for computer graphics. It is a slightly better algorithm than the three-dimensional implementation of the midpoint displacement algorithm which produces two-dimensional landscapes. It is also known as the random midpoint displacement fractal, the cloud fractal or the plasma fractal, because of the plasma effect produced when applied.\nThe idea was first introduced by Fournier, Fussell and Carpenter at SIGGRAPH 1982. It was later analyzed by Gavin S. P. Miller in SIGGRAPH 1986 who described it as flawed because the algorithm produces noticeable vertical and horizontal \"creases\" due to the most significant perturbation taking place in a rectangular grid.\nThe algorithm starts with a 2D grid then randomly generates terrain height from four seed values arranged in a grid of points so that the entire plane is covered in squares.\n\n","name":"Diamond-square algorithm","categories":["Computer graphics algorithms","Fractals","Pages using citations with accessdate and no URL","Procedural generation"],"tag_line":"The diamond-square algorithm is a method for generating heightmaps for computer graphics."}}
,{"_index":"throwtable","_type":"algorithm","_id":"symmetrization-methods","_score":0,"_source":{"description":"In mathematics the symmetrization methods are algorithms of transforming a set  to a ball  with equal volume  and centered at the origin. B is called the symmetrized version of A, usually denoted . These algorithms show up in solving the classical isoperimetric inequality problem, which asks: Given all two-dimensional shapes of a given area, which of them has the minimal perimeter (for details see Isoperimetric inequality). The conjectured answer was the disk and Steiner in 1838 showed this to be true using the Steiner symmetrization method (described below). From this many other isoperimetric problems sprung and other symmetrization algorithms. For example, Rayleigh's conjecture is that the first eigenvalue of the Dirichlet problem is minimized for the ball (see Rayleigh–Faber–Krahn inequality for details). Another problem is that the Newtonian capacity of a set A is minimized by  and this was proved by Polya and G. Szego (1951) using circular symmetrization (described below).","name":"Symmetrization methods","categories":["Geometric algorithms","Geometric inequalities"],"tag_line":"In mathematics the symmetrization methods are algorithms of transforming a set  to a ball  with equal volume  and centered at the origin."}}
,{"_index":"throwtable","_type":"algorithm","_id":"marching-squares","_score":0,"_source":{"description":"Marching squares is a computer graphics algorithm that generates contours for a two-dimensional scalar field (rectangular array of individual numerical values). A similar method can be used to contour 2D triangle meshes.\nThe contours can be of two kinds:\nIsolines - lines following a single data level, or isovalue.\nIsobands - filled areas between isolines.\nTypical applications include the Contour lines on topographic maps or the generation of isobars for weather maps.\nMarching squares takes a similar approach to the 3D marching cubes algorithm:\nProcess each cell in the grid independently.\nCalculate a cell index using comparisons of the contour level(s) with the data values at the cell corners.\nUse a pre-built lookup table, keyed on the cell index, to describe the output geometry for the cell.\nApply linear interpolation along the boundaries of the cell to calculate the exact contour position.","name":"Marching squares","categories":["Computer graphics algorithms"],"tag_line":"Marching squares is a computer graphics algorithm that generates contours for a two-dimensional scalar field (rectangular array of individual numerical values)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cone-algorithm","_score":0,"_source":{"description":"In computational geometry, the cone algorithm is an algorithm for identifying the particles that are near the surface of an object composed of discrete particles. Its applications include computational surface science and computational nano science. The cone algorithm was first described in a publication about nanogold in 2005.\nThe cone algorithm works well with clusters in condensed phases, including solid and liquid phases. It can handle the situations when one configuration includes multiple clusters or when holes exist inside clusters. It can also be applied to a cluster iteratively to identify multiple sub-surface layers.","name":"Cone algorithm","categories":["Geometric algorithms","Molecular modelling software"],"tag_line":"In computational geometry, the cone algorithm is an algorithm for identifying the particles that are near the surface of an object composed of discrete particles."}}
,{"_index":"throwtable","_type":"algorithm","_id":"melomics","_score":0,"_source":{"description":"Melomics (derived from \"genomics of melodies\") is a computational system for the automatic composition of music (with no human intervention), based on bioinspired algorithms.","name":"Melomics","categories":["Biotechnology","Evolutionary algorithms","Music technology","Spanish Supercomputing Network"],"tag_line":"Melomics (derived from \"genomics of melodies\") is a computational system for the automatic composition of music (with no human intervention), based on bioinspired algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bernard-chazelle","_score":0,"_source":{"description":"Bernard Chazelle (born November 5, 1955) is the Eugene Higgins Professor of Computer Science at Princeton University. Much of his work is in computational geometry, where he has found many of the best-known algorithms, such as linear-time triangulation of a simple polygon, as well as major complexity results, such as lower bound techniques based on discrepancy theory. He is also known for his invention of the soft heap data structure and the most asymptotically efficient known algorithm for finding minimum spanning trees.\n^ http://www.cs.princeton.edu/~chazelle/\n^ Chazelle, Bernard (1991), \"Triangulating a Simple Polygon in Linear Time\", Discrete & Computational Geometry 6: 485–524, doi:10.1007/BF02574703, ISSN 0179-5376 \n^ Chazelle, Bernard (2000), The Discrepancy Method: Randomness and Complexity, Cambridge University Press, ISBN 978-0-521-00357-5 \n^ Chazelle, Bernard (2000), \"A minimum spanning tree algorithm with inverse-Ackermann type complexity\", Journal of the Association for Computing Machinery 47 (6): 1028–1047, doi:10.1145/355541.355562, MR 1866456 .","name":"Bernard Chazelle","categories":["1955 births","American computer scientists","Articles with hCards","BLP articles lacking sources from October 2012","Fellows of the Association for Computing Machinery","French computer scientists","Guggenheim Fellows","Infobox person using numbered parameter","Living people","People from Paris","Princeton University faculty","Researchers in geometric algorithms","Wikipedia articles with BNF identifiers","Wikipedia articles with ISNI identifiers","Wikipedia articles with VIAF identifiers","Yale University alumni"],"tag_line":"Bernard Chazelle (born November 5, 1955) is the Eugene Higgins Professor of Computer Science at Princeton University."}}
,{"_index":"throwtable","_type":"algorithm","_id":"newell's-algorithm","_score":0,"_source":{"description":"Newell's Algorithm is a 3D computer graphics procedure for elimination of polygon cycles in the depth sorting required in hidden surface removal. It was proposed in 1972 by brothers Martin Newell and Dick Newell, and Tom Sancha, while all three were working at CADCentre.\nIn the depth sorting phase of hidden surface removal, if two polygons have no overlapping extents or extreme minimum and maximum values in the x, y, and z directions, then they can be easily sorted. If two polygons, Q and P, do have overlapping extents in the Z direction, then it is possible that cutting is necessary.\n\nIn that case Newell's algorithm tests the following:\nTest for Z overlap; implied in the selection of the face Q from the sort list\nThe extreme coordinate values in X of the two faces do not overlap (minimax test in X)\nThe extreme coordinate values in Y of the two faces do not overlap (minimax test in Y)\nAll vertices of P lie deeper than the plane of Q\nAll vertices of Q lie closer to the viewpoint than the plane of P\nThe rasterisation of P and Q do not overlap\nNote that the tests are given in order of increasing computational difficulty.\nNote also that the polygons must be planar.\nIf the tests are all false, then the polygons must be split. Splitting is accomplished by selecting one polygon and cutting it along the line of intersection with the other polygon. The above tests are again performed, and the algorithm continues until all polygons pass the above tests.","name":"Newell's algorithm","categories":["3D computer graphics","All stub articles","Computer graphics algorithms","Computer graphics stubs"],"tag_line":"Newell's Algorithm is a 3D computer graphics procedure for elimination of polygon cycles in the depth sorting required in hidden surface removal."}}
,{"_index":"throwtable","_type":"algorithm","_id":"david-avis","_score":0,"_source":{"description":"David Michael Avis (born March 20, 1951) is a Canadian and British computer scientist known for his contributions to geometric computations. Avis is a professor in computational geometry and applied mathematics in the School of Computer Science, McGill University, in Montreal. Since 2010, he belongs to Department of Communications and Computer Engineering, School of Informatics, Kyoto University.\nAvis received his Ph.D. in 1977 from Stanford University. He has published more than 70 journal papers and articles. Writing with Komei Fukuda, Avis proposed a reverse-search algorithm for the vertex enumeration problem; their algorithm generates all of the vertices of a convex polytope.\nHe has a collaboration article with Paul Erdős. Therefore, his Erdős number is 1.\n^ David Avis at the Mathematics Genealogy Project\n^ Avis & Fukuda (1992)\n^ Avis & Fukuda (1996)\n^ David Avis, Paul Erdös and János Pach: \"Repeated distances in space\"(1988)","name":"David Avis","categories":["1951 births","20th-century British mathematicians","21st-century British mathematicians","All stub articles","Anglophone Quebec people","British mathematician stubs","Living people","McGill University faculty","Researchers in geometric algorithms","Stanford University alumni","Wikipedia articles with ISNI identifiers","Wikipedia articles with VIAF identifiers"],"tag_line":"David Michael Avis (born March 20, 1951) is a Canadian and British computer scientist known for his contributions to geometric computations."}}
,{"_index":"throwtable","_type":"algorithm","_id":"timothy-m.-chan","_score":0,"_source":{"description":"Timothy Moon-Yew Chan is Professor and University Research Chair in the David R. Cheriton School of Computer Science, University of Waterloo, Canada.\nHe graduated with BA (summa cum laude) from Rice University in 1992, and completed his Ph.D. in Computer Science at UBC in 1995 at the age of 19.\nHe was awarded the Governor General's Gold Medal (as Head of Graduating Class in the Faculty of Graduate Studies at the University of British Columbia during convocation), the NSERC doctoral prize, and the Premier's Research Excellence Award (PREA) of Ontario, Canada.\nHe is currently an associate editor for the ACM Transactions on Algorithms (TALG), and the International Journal of Computational Geometry and Applications. He is also a member of the editorial board of Algorithmica, Discrete and Computational Geometry, as well as Computational Geometry: Theory and Applications.\nChan has published extensively. His research covers Data Structures, Algorithms and Computational geometry.","name":"Timothy M. Chan","categories":["1976 births","Living people","Researchers in geometric algorithms","Rice University alumni","University of British Columbia alumni","University of Waterloo faculty"],"tag_line":"Timothy Moon-Yew Chan is Professor and University Research Chair in the David R. Cheriton School of Computer Science, University of Waterloo, Canada."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mclone","_score":0,"_source":{"description":"MClone, or Clonal Mosaic, is a pattern formation algorithm proposed in 1998 used specially for simulating the visible patches of color in the fur of giraffes and members of the Felidae family of the mammalians. It was primarily proposed as a 2D model and lately was extended to 3D. An important feature of the algorithm is that it is biologically plausible.\nSince the algorithm was created in order to address some of the problems with texture mapping, its main goal is to produce, with the same set of parameters, a variable number of color patterns for a 2D or 3D object model. This way, for a relatively big amount of different entities represented by the same model, instead of using the same texture (and, doing so, each object would be equal to the others), one could use the different color patterns created by the MClone algorithm. Another useful feature of MClone is that it can be used to create patterns along with growing data of the object model.","name":"MClone","categories":["All articles needing additional references","Articles needing additional references from June 2011","Computer graphics algorithms"],"tag_line":"MClone, or Clonal Mosaic, is a pattern formation algorithm proposed in 1998 used specially for simulating the visible patches of color in the fur of giraffes and members of the Felidae family of the mammalians."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lars-arge","_score":0,"_source":{"description":"Lars Allan Arge is a Danish computer scientist, the head of the Center for Massive Data Algorithmics (MADALGO) at Aarhus University, where he is also a professor of computer science. His research involves the study of algorithms and data structures for handling massive data, especially in graph algorithms and computational geometry.\nArge earned his Ph.D. in 1996 from Aarhus University, under the supervision of Erik Meineche Schmidt. He was a professor at Duke University before returning to Aarhus as a professor in 2004, and he continues to hold an adjunct professorship at Duke.\nArge is a member of the Royal Danish Academy of Sciences and Letters, and was elected to the presidium of the academy in 2015. In 2012, he was elected as a Fellow of the Association for Computing Machinery \"for contributions to massive data algorithmics\", becoming only the second ACM Fellow in Denmark. He also belongs to the Danish Academy of Technical Sciences.","name":"Lars Arge","categories":["Aarhus University alumni","Aarhus University faculty","Danish computer scientists","Duke University faculty","Fellows of the Association for Computing Machinery","Living people","Researchers in geometric algorithms","Year of birth missing (living people)"],"tag_line":"Lars Allan Arge is a Danish computer scientist, the head of the Center for Massive Data Algorithmics (MADALGO) at Aarhus University, where he is also a professor of computer science."}}
,{"_index":"throwtable","_type":"algorithm","_id":"progressive-refinement","_score":0,"_source":{"description":"Progressive refinement is a ray tracing algorithm that quickly reveals coarse structure of an image, and gradually reveals additional detail over time.\nThe first pixel is rendered as a single rectangle occupying the entire work area. The second through fourth each occupy a quarter of the work area. Sufficient progression will refine the image until the rendered rectangles correspond to a target resolution (for example, a screen resolution).\nRectangles are laid out with an overlap pattern so as to avoid unnecessary rendering.","name":"Progressive refinement","categories":["All articles lacking sources","All stub articles","Articles lacking sources from December 2009","Computer graphics algorithms","Computer graphics stubs"],"tag_line":"Progressive refinement is a ray tracing algorithm that quickly reveals coarse structure of an image, and gradually reveals additional detail over time."}}
,{"_index":"throwtable","_type":"algorithm","_id":"frances-yao","_score":0,"_source":{"description":"Frances Foong Chu Yao (儲楓) is a Chinese-born American mathematician and computer scientist. She was Chair Professor and Head of the Department of computer science at the City University of Hong Kong, where she is now an honorary professor.\nAfter receiving a B.S. in mathematics from National Taiwan University in 1969, Yao did her Ph.D. studies under the supervision of Michael J. Fischer at the Massachusetts Institute of Technology, receiving her Ph.D. in 1973. She then held positions at the University of Illinois at Urbana-Champaign, Brown University, and Stanford University, before joining the staff at the Xerox Palo Alto Research Center in 1979 where she stayed until her retirement in 1999.\nIn 2003, she came out of retirement to become the Head and a Chair Professor of the Department of Computer Science at City University, which she held until June 2011. She is a Fellow of the American Association for the Advancement of Science; in 1991, she and Ronald Graham won the Lester R. Ford Award of the Mathematical Association of America for their expository article, A Whirlwind Tour of Computational Geometry.\nYao's husband, Andrew Yao, is also a well-known theoretical computer scientist and Turing Award winner.\nMuch of Yao's research has been in the subject of computational geometry and combinatorial algorithms; she is known for her work with Mike Paterson on binary space partitioning, her work with Dan Greene on finite-resolution computational geometry, and her work with Alan Demers and Scott Shenker on scheduling algorithms for energy-efficient power management.\nMore recently she has been working in cryptography. Along with her husband Andrew Yao and Wang Xiaoyun, they found new attacks on the SHA-1 cryptographic hash function.\n\n","name":"Frances Yao","categories":["American computer scientists","Brown University faculty","Chinese emigrants to the United States","Faculty of the City University of Hong Kong","Fellows of the American Association for the Advancement of Science","Living people","Massachusetts Institute of Technology alumni","National Taiwan University alumni","Researchers in geometric algorithms","Stanford University Department of Computer Science faculty","University of Illinois at Urbana–Champaign faculty","Women computer scientists"],"tag_line":"Frances Foong Chu Yao (儲楓) is a Chinese-born American mathematician and computer scientist."}}
,{"_index":"throwtable","_type":"algorithm","_id":"michael-segal","_score":0,"_source":{"description":"Michael Segal (Hebrew: מיכאל סגל; Russian: Михаил Сегал, born 1972 in Kishinev, USSR) is a Professor of Communication Systems Engineering at Ben-Gurion University of the Negev, known for his work in ad-hoc and sensor networks.\nAfter completing his undergraduate studies at Ben-Gurion University in 1994, Segal received a Ph.D. in Mathematics and Computer Science from Ben-Gurion University in 2000 under the supervision of Klara Kedem. The topic of his PhD Dissertation was: Covering point sets and accompanying problems.\nAfter continuing his studies with David G. Kirkpatrick at University of British Columbia, and Pacific Istitute for the Mathematical Studies  he joined the faculty at Ben-Gurion University in 2000, where he also served as the head of the Communication Systems Engineering department between 2005-2010. He is known (equally with his coauthors) for being first to analyze the analytical performance of the well-known Least Cluster Change (LCC) algorithm that is widely used in ad hoc networks for re-clustering in order to reduce the number of modifications. He also was one of the first to introduce and analyze the construction of multi-criteria spanners for ad hoc networks.\nSegal has published over 140 scientific papers and was a recipient of the Toronto Prize for Research in 2010. He is serving as the Editor-in-Chief for the Journal of Computer and System Sciences. Along with his Ben-Gurion University professorship, he also is visiting professor at Cambridge University.","name":"Michael Segal","categories":["1972 births","Israeli computer scientists","Israeli mathematicians","Living people","Pages using infoboxes with thumbnail images","Pages using web citations with no URL","Pages with citations lacking titles","Researchers in geometric algorithms"],"tag_line":"Michael Segal (Hebrew: מיכאל סגל; Russian: Михаил Сегал, born 1972 in Kishinev, USSR) is a Professor of Communication Systems Engineering at Ben-Gurion University of the Negev, known for his work in ad-hoc and sensor networks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shang-hua-teng","_score":0,"_source":{"description":"Shang-Hua Teng (Chinese: 滕尚华; pinyin: Téng Shàng-huá, born 1964) is a Chinese-American computer scientist. He is the Seeley G. Mudd Professor of Computer Science and Mathematics at the University of Southern California. Previously, he was the chairman of the Computer Science Department at the Viterbi School of Engineering of the University of Southern California. In 2008 he was awarded the Gödel Prize for his joint work on smoothed analysis of algorithms with Daniel Spielman. They went to win the prize again in 2015 for their contribution on \"nearly-linear-time Laplacian solvers\". In 2009, he received the Fulkerson Prize given by the American Mathematical Society and the Mathematical Programming Society.","name":"Shang-Hua Teng","categories":["1964 births","All articles with dead external links","All stub articles","American computer scientists","Articles containing Chinese-language text","Articles with dead external links from October 2010","Boston University faculty","Carnegie Mellon University alumni","Chinese computer scientists","Chinese emigrants to the United States","Computer specialist stubs","Educators from Beijing","Fellows of the Association for Computing Machinery","Gödel Prize laureates","IBM employees","Intel people","Living people","Massachusetts Institute of Technology faculty","Microsoft people","Researchers in geometric algorithms","Shanghai Jiao Tong University alumni","Sloan Fellows","USC Viterbi School of Engineering alumni","University of Illinois at Urbana–Champaign faculty","University of Minnesota faculty","University of Southern California faculty","Wikipedia articles with GND identifiers","Wikipedia articles with LCCN identifiers","Wikipedia articles with VIAF identifiers","Xerox people"],"tag_line":"Shang-Hua Teng (Chinese: 滕尚华; pinyin: Téng Shàng-huá, born 1964) is a Chinese-American computer scientist."}}
,{"_index":"throwtable","_type":"algorithm","_id":"jiří-matoušek-(mathematician)","_score":0,"_source":{"description":"Jiří (Jirka) Matoušek (10 March 1963 – 9 March 2015) was a Czech mathematician working in computational geometry and algebraic topology. He was a professor at Charles University in Prague and the author of several textbooks and research monographs.\nMatoušek was born in Prague. In 1986, he received his Master's degree at Charles University under Miroslav Katětov. From 1986 until his death he was employed at the Department of Applied Mathematics of Charles University in Prague, holding a professor position since 2000. He was also a visiting and later full professor at ETH Zurich.\nIn 1996, he won the European Mathematical Society prize and in 2000 he won the Scientist award of the Learned Society of the Czech Republic. He became a fellow of the Learned Society of the Czech Republic in 2005.\nMatoušek's paper on computational aspects of algebraic topology won the Best Paper award at the 2012 ACM Symposium on Discrete Algorithms.\nAside from his own academic writing, he has translated the popularization book Mathematics: A Very Short Introduction by Timothy Gowers into Czech.\nHe was a supporter and signatory of the Cost of Knowledge protest. He died in 2015, aged 51.","name":"Jiří Matoušek (mathematician)","categories":["1963 births","2015 deaths","All stub articles","CS1 Czech-language sources (cs)","Charles University in Prague faculty","Czech mathematicians","European mathematician stubs","People from Prague","Researchers in geometric algorithms","Wikipedia articles with BNF identifiers","Wikipedia articles with GND identifiers","Wikipedia articles with ISNI identifiers","Wikipedia articles with LCCN identifiers","Wikipedia articles with VIAF identifiers"],"tag_line":"Jiří (Jirka) Matoušek (10 March 1963 – 9 March 2015) was a Czech mathematician working in computational geometry and algebraic topology."}}
,{"_index":"throwtable","_type":"algorithm","_id":"luus–jaakola","_score":0,"_source":{"description":"In computational engineering, Luus–Jaakola (LJ) denotes a heuristic for global optimization of a real-valued function. In engineering use, LJ is not an algorithm that terminates with an optimal solution; nor is it an iterative method that generates a sequence of points that converges to an optimal solution (when one exists). However, when applied to a twice continuously differentiable function, the LJ heuristic is a proper iterative method, that generates a sequence that has a convergent subsequence; for this class of problems, Newton's method is recommended and enjoys a quadratic rate of convergence, while no convergence rate analysis has been given for the LJ heuristic. In practice, the LJ heuristic has been recommended for functions that need be neither convex nor differentiable nor locally Lipschitz: The LJ heuristic does not use a gradient or subgradient when one be available, which allows its application to non-differentiable and non-convex problems.\nProposed by Luus and Jaakola, LJ generates a sequence of iterates. The next iterate is selected from a sample from a neighborhood of the current position using a uniform distribution. With each iteration, the neighborhood decreases, which forces a subsequence of iterates to converge to a cluster point.\nLuus has applied LJ in optimal control, transformer design, metallurgical processes, and chemical engineering.","name":"Luus–Jaakola","categories":["Heuristic algorithms","Optimization algorithms and methods"],"tag_line":"In computational engineering, Luus–Jaakola (LJ) denotes a heuristic for global optimization of a real-valued function."}}
,{"_index":"throwtable","_type":"algorithm","_id":"györgy-elekes","_score":0,"_source":{"description":"György Elekes (19 May 1949 – 29 September 2008) was a Hungarian mathematician and computer scientist who specialized in Combinatorial geometry and Combinatorial set theory. He may be best known for his work in the field that would eventually be called Additive Combinatorics. Particularly notable was his \"ingenious\" application of the Szemerédi–Trotter theorem to improve the best known lower bound for the sum-product problem. He also proved that any polynomial-time algorithm approximating the volume of convex bodies must have a multiplicative error, and the error grows exponentially on the dimension. With Micha Sharir he set up a framework which eventually led Guth and Katz to the solution of the Erdős distinct distances problem. (See below.)","name":"György Elekes","categories":["1949 births","2008 deaths","Combinatorialists","Hungarian computer scientists","Hungarian mathematicians","Number theorists","Researchers in geometric algorithms"],"tag_line":"György Elekes (19 May 1949 – 29 September 2008) was a Hungarian mathematician and computer scientist who specialized in Combinatorial geometry and Combinatorial set theory."}}
,{"_index":"throwtable","_type":"algorithm","_id":"david-eppstein","_score":0,"_source":{"description":"David Arthur Eppstein (born 1963) is an American computer scientist and mathematician. He is a Chancellor's Professor of computer science at University of California, Irvine. He is known for his work in computational geometry, graph algorithms, and recreational mathematics.\n^ 11011110 - User Profile\n^ \"UCI Chancellor's Professors\". Retrieved August 18, 2014.","name":"David Eppstein","categories":["1963 births","American computer scientists","British emigrants to the United States","Cellular automatists","Columbia School of Engineering and Applied Science alumni","Fellows of the Association for Computing Machinery","Graph drawing people","Graph theorists","Living people","Palo Alto High School alumni","People from Irvine, California","Recreational mathematicians","Researchers in geometric algorithms","Stanford University alumni","University of California, Irvine faculty","Wikipedia articles with BNF identifiers","Wikipedia articles with GND identifiers","Wikipedia articles with ISNI identifiers","Wikipedia articles with LCCN identifiers","Wikipedia articles with VIAF identifiers","Wikipedia indefinitely semi-protected biographies of living people"],"tag_line":"David Arthur Eppstein (born 1963) is an American computer scientist and mathematician."}}
,{"_index":"throwtable","_type":"algorithm","_id":"social-cognitive-optimization","_score":0,"_source":{"description":"Social cognitive optimization (SCO) is a population-based metaheuristic optimization algorithm which was developed in 2002. This algorithm is based on the social cognitive theory, and the key point of the ergodicity is the process of individual learning of a set of agents with their own memory and their social learning with the knowledge points in the social sharing library. It has been used for solving continuous optimization, integer programming, and combinatorial optimization problems. It has been incorporated into the NLPSolver extension of Calc in Apache OpenOffice.","name":"Social cognitive optimization","categories":["Collective intelligence","Heuristic algorithms","Metaheuristics","Optimization algorithms and methods"],"tag_line":"Social cognitive optimization (SCO) is a population-based metaheuristic optimization algorithm which was developed in 2002."}}
,{"_index":"throwtable","_type":"algorithm","_id":"monte-carlo-tree-search","_score":0,"_source":{"description":"In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm of making decisions in some decision processes, most notably employed in game playing. The leading example of its use is in contemporary computer Go programs, but it is also used in other board games, as well as real-time video games and non-deterministic games such as poker (see history section).","name":"Monte Carlo tree search","categories":["Combinatorial game theory","Heuristic algorithms","Monte Carlo methods","Wikipedia articles needing clarification from November 2015"],"tag_line":"In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm of making decisions in some decision processes, most notably employed in game playing."}}
,{"_index":"throwtable","_type":"algorithm","_id":"leabra","_score":0,"_source":{"description":"Leabra stands for \"Local, Error-driven and Associative, Biologically Realistic Algorithm\". It is a model of learning which is a balance between Hebbian and error-driven learning with other network-derived characteristics. This model is used to mathematically predict outcomes based on inputs and previous learning influences. This model is heavily influenced by and contributes to neural network designs and models. This algorithm is the default algorithm in Emergent (successor of PDP++) when making a new project, and is extensively used in various simulations.\nHebbian learning is performed using conditional principal components analysis (CPCA) algorithm with correction factor for sparse expected activity levels.\nError-driven learning is performed using GeneRec, which is a generalization of the Recirculation algorithm, and approximates Almeida-Pineda recurrent backpropagation. The symmetric, midpoint version of GeneRec is used, which is equivalent to the contrastive Hebbian learning algorithm (CHL). See O'Reilly (1996; Neural Computation) for more details.\nThe activation function is a point-neuron approximation with both discrete spiking and continuous rate-code output.\nLayer or unit-group level inhibition can be computed directly using a k-winners-take-all (KWTA) function, producing sparse distributed representations.\nThe net input is computed as an average, not a sum, over connections, based on normalized, sigmoidally transformed weight values, which are subject to scaling on a connection-group level to alter relative contributions. Automatic scaling is performed to compensate for differences in expected activity level in the different projections.\nDocumentation about this algorithm can be found in the book \"Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain\" published by MIT press. and in the Emergent Documentation","name":"Leabra","categories":["Artificial neural networks","Machine learning algorithms"],"tag_line":"Leabra stands for \"Local, Error-driven and Associative, Biologically Realistic Algorithm\"."}}
,{"_index":"throwtable","_type":"algorithm","_id":"minimum-redundancy-feature-selection","_score":0,"_source":{"description":"Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes and narrow down their relevance and is usually described in its pairing with relevant feature selection as Minimum Redundancy Maximum Relevance (mRMR).\nFeature selection, one of the basic problems in pattern recognition and machine learning, identifies subsets of data that are relevant to the parameters used and is normally called Maximum Relevance. These subsets often contain material which is relevant but redundant and mRMR attempts to address this problem by removing those redundant subsets. mRMR has a variety of applications in many areas such as cancer diagnosis and speech recognition.\nFeatures can be selected in many different ways. One scheme is to select features that correlate strongest to the classification variable. This has been called maximum-relevance selection. Many heuristic algorithms can be used, such as the sequential forward, backward, or floating selections.\nOn the other hand features can be selected to be mutually far away from each other while still having \"high\" correlation to the classification variable. This scheme, termed as Minimum Redundancy Maximum Relevance (mRMR) selection has been found to be more powerful than the maximum relevance selection.\nAs a special case, the \"correlation\" can be replaced by the statistical dependency between variables. Mutual information can be used to quantify the dependency. In this case, it is shown that mRMR is an approximation to maximizing the dependency between the joint distribution of the selected features and the classification variable.\nStudies have tried different measures for redundancy and relevance measures. A recent study compared several measures within the context of biomedical images.\n\n","name":"Minimum redundancy feature selection","categories":["All stub articles","Artificial intelligence stubs","Machine learning algorithms","Robotics stubs"],"tag_line":"Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes and narrow down their relevance and is usually described in its pairing with relevant feature selection as Minimum Redundancy Maximum Relevance (mRMR)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rprop","_score":0,"_source":{"description":"Rprop, short for resilient backpropagation, is a learning heuristic for supervised learning in feedforward artificial neural networks. This is a first-order optimization algorithm. This algorithm was created by Martin Riedmiller and Heinrich Braun in 1992.\nSimilarly to the Manhattan update rule, Rprop takes into account only the sign of the partial derivative over all patterns (not the magnitude), and acts independently on each \"weight\". For each weight, if there was a sign change of the partial derivative of the total error function compared to the last iteration, the update value for that weight is multiplied by a factor η−, where η− < 1. If the last iteration produced the same sign, the update value is multiplied by a factor of η+, where η+ > 1. The update values are calculated for each weight in the above manner, and finally each weight is changed by its own update value, in the opposite direction of that weight's partial derivative, so as to minimise the total error function. η+ is empirically set to 1.2 and η− to 0.5.\nNext to the cascade correlation algorithm and the Levenberg–Marquardt algorithm, Rprop is one of the fastest weight update mechanisms.\nRPROP is a batch update algorithm.","name":"Rprop","categories":["Artificial neural networks","Machine learning algorithms"],"tag_line":"Rprop, short for resilient backpropagation, is a learning heuristic for supervised learning in feedforward artificial neural networks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"multiple-kernel-learning","_score":0,"_source":{"description":"Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm. Reasons to use multiple kernel learning include a) the ability to select for an optimal kernel and parameters from a larger set of kernels, reducing bias due to kernel selection while allowing for more automated machine learning methods, and b) combining data from different sources (e.g. sound and images from a video) that have different notions of similarity and thus require different kernels. Instead of creating a new kernel, multiple kernel algorithms can be used to combine kernels already established for each individual data source.\nMultiple kernel learning approaches have been used in many applications, such as event recognition in video.., object recognition in images, and biomedical data fusion.","name":"Multiple kernel learning","categories":["All orphaned articles","Data mining","Machine learning algorithms","Orphaned articles from January 2015"],"tag_line":"Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"genetic-algorithm-for-rule-set-production","_score":0,"_source":{"description":"Genetic Algorithm for Rule Set Production (GARP) is a computer program based on genetic algorithm that creates ecological niche models for species. The generated models describe environmental conditions (precipitation, temperatures, elevation, etc.) under which the species should be able to maintain populations. As input, local observations of species and related environmental parameters are used which describe potential limits of the species' capabilities to survive. Such environmental parameters are commonly stored in geographical information systems. A GARP model is a random set of mathematical rules which can be read as limiting environmental conditions. Each rule is considered as a gene; the set of genes is combined in random ways to further generate many possible models describing the potential of the species to occur.","name":"Genetic Algorithm for Rule Set Production","categories":["All stub articles","Ecology stubs","Machine learning algorithms"],"tag_line":"Genetic Algorithm for Rule Set Production (GARP) is a computer program based on genetic algorithm that creates ecological niche models for species."}}
,{"_index":"throwtable","_type":"algorithm","_id":"state-action-reward-state-action","_score":0,"_source":{"description":"State-Action-Reward-State-Action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was introduced in a technical note  where the alternative name SARSA was only mentioned as a footnote.\nThis name simply reflects the fact that the main function for updating the Q-value depends on the current state of the agent \"S1\", the action the agent chooses \"A1\", the reward \"R\" the agent gets for choosing this action, the state \"S2\" that the agent will now be in after taking that action, and finally the next action \"A2\" the agent will choose in its new state. Taking every letter in the quintuple (st, at, rt, st+1, at+1) yields the word SARSA.","name":"State-Action-Reward-State-Action","categories":["Machine learning algorithms"],"tag_line":"State-Action-Reward-State-Action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning."}}
,{"_index":"throwtable","_type":"algorithm","_id":"local-outlier-factor","_score":0,"_source":{"description":"In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.\nLOF shares some concepts with DBSCAN and OPTICS such as the concepts of \"core distance\" and \"reachability distance\", which are used for local density estimation.","name":"Local outlier factor","categories":["Data mining","Machine learning algorithms","Statistical outliers"],"tag_line":"In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quadratic-unconstrained-binary-optimization","_score":0,"_source":{"description":"Quadratic unconstrained binary optimization (QUBO) is a pattern matching technique, common in machine learning applications. QUBO is an NP hard problem.\nQUBO problems may sometimes be well-suited to algorithms aided by quantum annealing.\nQUBO is given by the formula:","name":"Quadratic unconstrained binary optimization","categories":["All articles needing additional references","All stub articles","Articles needing additional references from September 2014","Artificial intelligence stubs","Machine learning algorithms"],"tag_line":"Quadratic unconstrained binary optimization (QUBO) is a pattern matching technique, common in machine learning applications."}}
,{"_index":"throwtable","_type":"algorithm","_id":"query-level-feature","_score":0,"_source":{"description":"A query level feature or QLF is a ranking feature utilized in a machine-learned ranking algorithm.\nExample QLFs:\nHow many times has this query been run in the last month?\nHow many words are in the query?\nWhat is the sum/average/min/max/median of the BM25F values for the query?","name":"Query level feature","categories":["All stub articles","Artificial intelligence stubs","Machine learning","Machine learning algorithms"],"tag_line":"A query level feature or QLF is a ranking feature utilized in a machine-learned ranking algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"edge-disjoint-shortest-pair-algorithm","_score":0,"_source":{"description":"Edge disjoint shortest pair algorithm is an algorithm in computer network routing. The algorithm is used for generating the shortest pair of edge disjoint paths between a given pair of vertices as follows:\nRun the shortest path algorithm for the given pair of vertices\nReplace each edge of the shortest path (equivalent to two oppositely directed arcs) by a single arc directed towards the source vertex\nMake the length of each of the above arcs negative\nRun the shortest path algorithm (Note: the algorithm should accept negative costs)\nErase the overlapping edges of the two paths found, and reverse the direction of the remaining arcs on the first shortest path such that each arc on it is directed towards the sink vertex now. The desired pair of paths results.\nSuurballe's algorithm solves the same problem more quickly by reweighting the edges of the graph to avoid negative costs, allowing Dijkstra's algorithm to be used for both shortest path steps.","name":"Edge disjoint shortest pair algorithm","categories":["All articles needing additional references","All stub articles","Articles needing additional references from January 2010","Computer network stubs","Routing algorithms"],"tag_line":"Edge disjoint shortest pair algorithm is an algorithm in computer network routing."}}
,{"_index":"throwtable","_type":"algorithm","_id":"active-queue-management","_score":0,"_source":{"description":"In Internet routers, active queue management (AQM) is the intelligent drop of network packets inside a buffer associated with a network interface controller (NIC), when that buffer becomes full or gets close to becoming full, often with the larger goal of reducing network congestion. This task is performed by the network scheduler, which for this purpose uses various algorithms such as random early detection (RED), Explicit Congestion Notification (ECN), or controlled delay (CoDel). RFC 7567 recommends active queue management as a best practice.","name":"Active queue management","categories":["All articles needing additional references","Articles needing additional references from December 2009","Network performance","Network scheduling algorithms","Packets (information technology)"],"tag_line":"In Internet routers, active queue management (AQM) is the intelligent drop of network packets inside a buffer associated with a network interface controller (NIC), when that buffer becomes full or gets close to becoming full, often with the larger goal of reducing network congestion."}}
,{"_index":"throwtable","_type":"algorithm","_id":"temporally-ordered-routing-algorithm","_score":0,"_source":{"description":"The Temporally Ordered Routing Algorithm (TORA) is an algorithm for routing data across Wireless Mesh Networks or Mobile ad hoc networks.\nIt was developed by Vincent Park and Scott Corson at the University of Maryland and the Naval Research Laboratory. Park has patented his work, and it was licensed by Nova Engineering, who are marketing a wireless router product based on Park's algorithm.","name":"Temporally ordered routing algorithm","categories":["Ad hoc routing protocols","All articles lacking sources","Articles lacking sources from July 2013","Routing algorithms","Wireless networking"],"tag_line":"The Temporally Ordered Routing Algorithm (TORA) is an algorithm for routing data across Wireless Mesh Networks or Mobile ad hoc networks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mark-compact-algorithm","_score":0,"_source":{"description":"In computer science, a mark-compact algorithm is a type of garbage collection algorithm used to reclaim unreachable memory. Mark-compact algorithms can be regarded as a combination of the mark-sweep algorithm and Cheney's copying algorithm. First, reachable objects are marked, then a compacting step relocates the reachable (marked) objects towards the beginning of the heap area. Compacting garbage collection is used by Microsoft's Common Language Runtime and by the Glasgow Haskell Compiler.","name":"Mark-compact algorithm","categories":["Automatic memory management","Memory management algorithms","Use dmy dates from August 2012"],"tag_line":"In computer science, a mark-compact algorithm is a type of garbage collection algorithm used to reclaim unreachable memory."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lirs-caching-algorithm","_score":0,"_source":{"description":"LIRS (Low Inter-reference Recency Set) is a page replacement algorithm with an improved performance over LRU (Least Recently Used) and many other newer replacement algorithms. This is achieved by using reuse distance as a metric for dynamically ranking accessed pages to make a replacement decision. The algorithm was developed by Song Jiang and Xiaodong Zhang.","name":"LIRS caching algorithm","categories":["Memory management algorithms","Online algorithms","Virtual memory"],"tag_line":"LIRS (Low Inter-reference Recency Set) is a page replacement algorithm with an improved performance over LRU (Least Recently Used) and many other newer replacement algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pseudo-lru","_score":0,"_source":{"description":"Pseudo-LRU is a cache algorithm created to improve the Least Recently Used (LRU) algorithm.\nPLRU usually refers to two cache replacement algorithms: tree-PLRU and bit-PLRU.","name":"Pseudo-LRU","categories":["All stub articles","Computer science stubs","Memory management algorithms","Use dmy dates from August 2012"],"tag_line":"Pseudo-LRU is a cache algorithm created to improve the Least Recently Used (LRU) algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"special-ordered-set","_score":0,"_source":{"description":"In discrete optimization, a special ordered set (SOS) is an ordered set of variables, used as an additional way to specify integrality conditions in an optimization model. Special order sets are basically a device or tool used in branch and bound methods for branching on sets of variables, rather than individual variables, as in ordinary mixed integer programming. Knowing that a variable is part of a set and that it is ordered gives the branch and bound algorithm a more intelligent way to face the optimization problem, helping to speed up the search procedure. The members of a special ordered set individually may be continuous or discrete variables in any combination. However, even when all the members are themselves continuous, a model containing one or more special ordered sets becomes a discrete optimization problem requiring a mixed integer optimizer for its solution.\nThe ‘only’ beneﬁt of using Special Ordered Sets compared with using only constraints, is that the search procedure will generally be noticeably faster.\nAs per J.A. Tomlin, Special Order Sets provide a powerful means of modeling nonconvex functions and discrete requirements, though there has been a tendency to think of them only in terms of multiple-choice zero-one programming.","name":"Special ordered set","categories":["Mathematical optimization","Optimization algorithms and methods"],"tag_line":"In discrete optimization, a special ordered set (SOS) is an ordered set of variables, used as an additional way to specify integrality conditions in an optimization model."}}
,{"_index":"throwtable","_type":"algorithm","_id":"odlyzko–schönhage-algorithm","_score":0,"_source":{"description":"In mathematics, the Odlyzko–Schönhage algorithm is a fast algorithm for evaluating the Riemann zeta function at many points, introduced by (Odlyzko & Schönhage 1988). The main point is the use of the fast Fourier transform to speed up the evaluation of a finite Dirichlet series of length N at O(N) equally spaced values from O(N2) to O(N1+ε) steps (at the cost of storing O(N1+ε) intermediate values). The Riemann–Siegel formula used for calculating the Riemann zeta function with imaginary part T uses a finite Dirichlet series with about N = T1/2 terms, so when finding about N values of the Riemann zeta function it is sped up by a factor of about T1/2. This reduces the time to find the zeros of the zeta function with imaginary part at most T from about T3/2+ε steps to about T1+ε steps.\nThe algorithm can be used not just for the Riemann zeta function, but also for many other functions given by Dirichlet series.\nThe algorithm was used by Gourdon (2004) to verify the Riemann hypothesis for the first 1013 zeros of the zeta function.","name":"Odlyzko–Schönhage algorithm","categories":["Algorithms and data structures stubs","All stub articles","Analytic number theory","Computational number theory","Computer science stubs","Zeta and L-functions"],"tag_line":"In mathematics, the Odlyzko–Schönhage algorithm is a fast algorithm for evaluating the Riemann zeta function at many points, introduced by (Odlyzko & Schönhage 1988)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"halley's-method","_score":0,"_source":{"description":"In numerical analysis, Halley’s method is a root-finding algorithm used for functions of one real variable with a continuous second derivative, i.e., C2 functions. It is named after its inventor Edmond Halley.\nThe algorithm is second in the class of Householder's methods, right after Newton's method. Like the latter, it produces iteratively a sequence of approximations to the root; their rate of convergence to the root is cubic. Multidimensional versions of this method exist.\nHalley's method can be viewed as exactly finding the roots of a linear-over-linear Padé approximation to the function, in contrast to Newton's method/Secant method (approximates/interpolates the function linearly) or Cauchy's method/Muller's method (approximates/interpolates the function quadratically).","name":"Halley's method","categories":["Root-finding algorithms","Use dmy dates from July 2013"],"tag_line":"In numerical analysis, Halley’s method is a root-finding algorithm used for functions of one real variable with a continuous second derivative, i.e., C2 functions."}}
,{"_index":"throwtable","_type":"algorithm","_id":"polylogarithmic-function","_score":0,"_source":{"description":"A polylogarithmic function in n is a polynomial in the logarithm of n,\n\nIn computer science, polylogarithmic functions occur as the order of memory used by some algorithms (e.g., \"it has polylogarithmic order\").\nAll polylogarithmic functions are\n\nfor every exponent ε > 0 (for the meaning of this symbol, see small o notation), that is, a polylogarithmic function grows more slowly than any positive exponent. This observation is the basis for the soft O notation Õ(n).","name":"Polylogarithmic function","categories":["All stub articles","Analysis of algorithms","Computer science stubs","Mathematical analysis","Mathematical analysis stubs","Polynomials"],"tag_line":"A polylogarithmic function in n is a polynomial in the logarithm of n,\n\nIn computer science, polylogarithmic functions occur as the order of memory used by some algorithms (e.g., \"it has polylogarithmic order\")."}}
,{"_index":"throwtable","_type":"algorithm","_id":"frank–wolfe-algorithm","_score":0,"_source":{"description":"The Frank–Wolfe algorithm is an iterative first-order optimization algorithm for constrained convex optimization. Also known as the conditional gradient method, reduced gradient algorithm and the convex combination algorithm, the method was originally proposed by Marguerite Frank and Philip Wolfe in 1956. In each iteration, the Frank–Wolfe algorithm considers a linear approximation of the objective function, and moves slightly towards a minimizer of this linear function (taken over the same domain).\n\n","name":"Frank–Wolfe algorithm","categories":["First order methods","Gradient methods","Iterative methods","Optimization algorithms and methods"],"tag_line":"The Frank–Wolfe algorithm is an iterative first-order optimization algorithm for constrained convex optimization."}}
,{"_index":"throwtable","_type":"algorithm","_id":"integer-relation-algorithm","_score":0,"_source":{"description":"An integer relation between a set of real numbers x1, x2, ..., xn is a set of integers a1, a2, ..., an, not all 0, such that\n\nAn integer relation algorithm is an algorithm for finding integer relations. Specifically, given a set of real numbers known to a given precision, an integer relation algorithm will either find an integer relation between them, or will determine that no integer relation exists with coefficients whose magnitudes are less than a certain upper bound.\n\n","name":"Integer relation algorithm","categories":["Number theoretic algorithms"],"tag_line":"An integer relation between a set of real numbers x1, x2, ..., xn is a set of integers a1, a2, ..., an, not all 0, such that\n\nAn integer relation algorithm is an algorithm for finding integer relations."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shuffled-frog-leaping-algorithm","_score":0,"_source":{"description":"The shuffled frog leaping algorithm (SFLA) is an optimization algorithm used in artificial intelligence (AI). It is like a genetic algorithm.\n\n^ Muzaffar Eusuff , Kevin Lansey & Fayzul Pasha (2006) Shuffled frog-leaping algorithm: a memetic meta-heuristic for discrete optimization, Engineering Optimization, 38:2, 129–154, DOI: 10.1080/03052150500384759","name":"Shuffled frog leaping algorithm","categories":["All stub articles","Artificial intelligence stubs","Mathematical optimization","Optimization algorithms and methods","Search algorithms"],"tag_line":"The shuffled frog leaping algorithm (SFLA) is an optimization algorithm used in artificial intelligence (AI)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"predictor–corrector-method","_score":0,"_source":{"description":"In mathematics, particularly numerical analysis, a predictor–corrector method is an algorithm that proceeds in two steps. First, the prediction step calculates a rough approximation of the desired quantity. Second, the corrector step refines the initial approximation using another means.","name":"Predictor–corrector method","categories":["Algorithms","Numerical analysis"],"tag_line":"In mathematics, particularly numerical analysis, a predictor–corrector method is an algorithm that proceeds in two steps."}}
,{"_index":"throwtable","_type":"algorithm","_id":"competitive-analysis-(online-algorithm)","_score":0,"_source":{"description":"Competitive analysis is a method invented for analyzing online algorithms, in which the performance of an online algorithm (which must satisfy an unpredictable sequence of requests, completing each request without being able to see the future) is compared to the performance of an optimal offline algorithm that can view the sequence of requests in advance. An algorithm is competitive if its competitive ratio—the ratio between its performance and the offline algorithm's performance—is bounded. Unlike traditional worst-case analysis, where the performance of an algorithm is measured only for \"hard\" inputs, competitive analysis requires that an algorithm perform well both on hard and easy inputs, where \"hard\" and \"easy\" are defined by the performance of the optimal offline algorithm.\nFor many algorithms, performance is dependent not only on the size of the inputs, but also on their values. One such example is the quicksort algorithm, which sorts an array of elements. Such data-dependent algorithms are analysed for average-case and worst-case data. Competitive analysis is a way of doing worst case analysis for on-line and randomized algorithms, which are typically data dependent.\nIn competitive analysis, one imagines an \"adversary\" that deliberately chooses difficult data, to maximize the ratio of the cost of the algorithm being studied and some optimal algorithm. Adversaries range in power from the oblivious adversary, which has no knowledge of the random choices made by the algorithm pitted against it, to the adaptive adversary that has full knowledge of how an algorithm works and its internal state at any point during its execution. Note that this distinction is only meaningful for randomized algorithms. For a deterministic algorithm, either adversary can simply compute what state that algorithm must have at any time in the future, and choose difficult data accordingly.\nFor example, the quicksort algorithm chooses one element, called the \"pivot\", that is, on average, not too far from the center value of the data being sorted. Quicksort then separates the data into two piles, one of which contains all elements with value less than the value of the pivot, and the other containing the rest of the elements. If quicksort chooses the pivot in some deterministic fashion (for instance, always choosing the first element in the list), then it is easy for an adversary to arrange the data beforehand so that quicksort will perform in worst-case time. If, however, quicksort chooses some random element to be the pivot, then an adversary without knowledge of what random numbers are coming up cannot arrange the data to guarantee worst-case execution time for quicksort.\nThe classic on-line problem first analysed with competitive analysis (Sleator & Tarjan 1985) is the list update problem: Given a list of items and a sequence of requests for the various items, minimize the cost of accessing the list where the elements closer to the front of the list cost less to access. (Typically, the cost of accessing an item is equal to its position in the list.) After an access, the list may be rearranged. Most rearrangements have a cost. The Move-To-Front algorithm simply moves the requested item to the front after the access, at no cost. The Transpose algorithm swaps the accessed item with the item immediately before it, also at no cost. Classical methods of analysis showed that Transpose is optimal in certain contexts. In practice, Move-To-Front performed much better. Competitive analysis was used to show that an adversary can make Transpose perform arbitrarily badly compared to an optimal algorithm, whereas Move-To-Front can never be made to incur more than twice the cost of an optimal algorithm.\nIn the case of online requests from a server, competitive algorithms are used to overcome uncertainties about the future. That is, the algorithm does not \"know\" the future, while the imaginary adversary (the \"competitor\") \"knows\". Similarly, competitive algorithms were developed for distributed systems, where the algorithm has to react to a request arriving at one location, without \"knowing\" what has just happened in a remote location. This setting was presented in (Awerbuch, Kutten & Peleg 1992).","name":"Competitive analysis (online algorithm)","categories":["Analysis of algorithms","Online algorithms"],"tag_line":"Competitive analysis is a method invented for analyzing online algorithms, in which the performance of an online algorithm (which must satisfy an unpredictable sequence of requests, completing each request without being able to see the future) is compared to the performance of an optimal offline algorithm that can view the sequence of requests in advance."}}
,{"_index":"throwtable","_type":"algorithm","_id":"apostolico–giancarlo-algorithm","_score":0,"_source":{"description":"In computer science, the Apostolico–Giancarlo algorithm is a variant of the Boyer–Moore string search algorithm, the basic application of which is searching for occurrences of a pattern  in a text . As with other comparison-based string searches, this is done by aligning  to a certain index of  and checking whether a match occurs at that index.  is then shifted relative to  according to the rules of the Boyer-Moore algorithm, and the process repeats until the end of  has been reached. Application of the Boyer-Moore shift rules often results in large chunks of the text being skipped entirely.\nWith regard to the shift operation, Apostolico-Giancarlo is exactly equivalent in functionality to Boyer-Moore. The utility of Apostolico-Giancarlo is to speed up the match-checking operation at any index. With Boyer-Moore, finding an occurrence of  in  requires that all  characters of  be explicitly matched. For certain patterns and texts, this is very inefficient - a simple example is when both pattern and text consist of the same repeated character, in which case Boyer-Moore runs in  where  is the length in characters of . Apostolico-Giancarlo speeds this up by recording the number of characters matched at the alignments of  in a table, which is combined with data gathered during the pre-processing of  to avoid redundant equality checking for sequences of characters that are known to match.","name":"Apostolico–Giancarlo algorithm","categories":["String matching algorithms"],"tag_line":"In computer science, the Apostolico–Giancarlo algorithm is a variant of the Boyer–Moore string search algorithm, the basic application of which is searching for occurrences of a pattern  in a text ."}}
,{"_index":"throwtable","_type":"algorithm","_id":"metaphone","_score":0,"_source":{"description":"Lawrence Philips redirects here. For the football player, see Lawrence Phillips.\nMetaphone is a phonetic algorithm, published by Lawrence Philips in 1990, for indexing words by their English pronunciation. It fundamentally improves on the Soundex algorithm by using information about variations and inconsistencies in English spelling and pronunciation to produce a more accurate encoding, which does a better job of matching words and names which sound similar. As with Soundex, similar sounding words should share the same keys. Metaphone is available as a built-in operator in a number of systems.\nThe original author later produced a new version of the algorithm, which he named Double Metaphone. Contrary to the original algorithm whose application is limited to English only, this version takes into account spelling peculiarities of a number of other languages. In 2009 Lawrence Philips released a third version, called Metaphone 3, which achieves an accuracy of approximately 99% for English words, non-English words familiar to Americans, and first names and family names commonly found in the United States, having been developed according to modern engineering standards against a test harness of prepared correct encodings.","name":"Metaphone","categories":["Phonetic algorithms"],"tag_line":"Lawrence Philips redirects here."}}
,{"_index":"throwtable","_type":"algorithm","_id":"solving-quadratic-equations-with-continued-fractions","_score":0,"_source":{"description":"In mathematics, a quadratic equation is a polynomial equation of the second degree. The general form is\n\nwhere a ≠ 0.\nThe quadratic equation can be solved using the well-known quadratic formula, which can be derived by completing the square. That formula always gives the roots of the quadratic equation, but the solutions are expressed in a form that often involves a quadratic irrational number, which is an algebraic fraction that can be evaluated as a decimal fraction only by applying an additional root extraction algorithm.\nIf the roots are real, there is an alternative technique that obtains a rational approximation to one of the roots by manipulating the equation directly. The method works in many cases, and long ago it stimulated further development of the analytical theory of continued fractions.","name":"Solving quadratic equations with continued fractions","categories":["Continued fractions","Elementary algebra","Equations","Mathematical analysis","Root-finding algorithms"],"tag_line":"In mathematics, a quadratic equation is a polynomial equation of the second degree."}}
,{"_index":"throwtable","_type":"algorithm","_id":"laguerre's-method","_score":0,"_source":{"description":"In numerical analysis, Laguerre's method is a root-finding algorithm tailored to polynomials. In other words, Laguerre's method can be used to numerically solve the equation\n\nfor a given polynomial p. One of the most useful properties of this method is that it is, from extensive empirical study, very close to being a \"sure-fire\" method, meaning that it is almost guaranteed to always converge to some root of the polynomial, no matter what initial guess is chosen. This method is named in honour of Edmond Laguerre, a French mathematician.","name":"Laguerre's method","categories":["Root-finding algorithms"],"tag_line":"In numerical analysis, Laguerre's method is a root-finding algorithm tailored to polynomials."}}
,{"_index":"throwtable","_type":"algorithm","_id":"zhu–takaoka-string-matching-algorithm","_score":0,"_source":{"description":"In computer science, the Zhu–Takaoka string matching algorithm is a variant of the Boyer–Moore string search algorithm. It uses two consecutive text characters to compute the bad character shift. It is faster when the alphabet or pattern is small, but the skip table grows quickly, slowing the pre-processing phase.","name":"Zhu–Takaoka string matching algorithm","categories":["String matching algorithms"],"tag_line":"In computer science, the Zhu–Takaoka string matching algorithm is a variant of the Boyer–Moore string search algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"estimation-of-distribution-algorithm","_score":0,"_source":{"description":"Estimation of distribution algorithms (EDAs), sometimes called probabilistic model-building genetic algorithms (PMBGAs), are stochastic optimization methods that guide the search for the optimum by building and sampling explicit probabilistic models of promising candidate solutions. Optimization is viewed as a series of incremental updates of a probabilistic model, starting with the model encoding the uniform distribution over admissible solutions and ending with the model that generates only the global optima.\nEDAs belong to the class of evolutionary algorithms. The main difference between EDAs and most conventional evolutionary algorithms is that evolutionary algorithms generate new candidate solutions using an implicit distribution defined by one or more variation operators, whereas EDAs use an explicit probability distribution encoded by a Bayesian network, a multivariate normal distribution, or another model class. Similarly as other evolutionary algorithms, EDAs can be used to solve optimization problems defined over a number of representations from vectors to LISP style S expressions, and the quality of candidate solutions is often evaluated using one or more objective functions.\nThe general procedure of an EDA is outlined in the following:\nt = 0\ninitialize model M(0) to represent uniform distribution over admissible solutions\nwhile (termination criteria not met)\n\nP = generate N>0 candidate solutions by sampling M(t)\nF = evaluate all candidate solutions in P\nM(t+1) = adjust_model(P,F,M(t))\nt = t + 1\n\nUsing explicit probabilistic models in optimization allowed EDAs to feasibly solve optimization problems that were notoriously difficult for most conventional evolutionary algorithms and traditional optimization techniques, such as problems with high levels of epistasis. Nonetheless, the advantage of EDAs is also that these algorithms provide an optimization practitioner with a series of probabilistic models that reveal a lot of information about the problem being solved. This information can in turn be used to design problem-speciﬁc neighborhood operators for local search, to bias future runs of EDAs on a similar problem, or to create an efficient computational model of the problem.\nFor example, if the population is represented by bit strings of length 4, the EDA can represent the population of promising solution using a single vector of four probabilities (p1, p2, p3, p4) where each component of p defines the probability of that position being a 1. Using this probability vector it is possible to create an arbitrary number of candidate solutions.\nBetter-known EDAs include\nPopulation-based incremental learning (PBIL)\nHill Climbing with Learning (HCwL)\nCompact Genetic Algorithm (cGA)\nUnivariate Marginal Distribution Algorithm (UMDA)\nEstimation of Multivariate Normal Algorithm (EMNA)\nMutual Information Maximization for Input Clustering (MIMIC)\nBivariate Marginal Distribution Algorithm (BMDA)\nExtended Compact Genetic Algorithm (ECGA)\nBayesian Optimization Algorithm (BOA)\nEstimation of Bayesian Networks Algorithm (EBNA)\nStochastic hill climbing with learning by vectors of normal distributions (SHCLVND)\nReal-coded PBIL\nProbabilistic Incremental Program Evolution (PIPE)\nEstimation of Gaussian Networks Algorithm (EGNA)","name":"Estimation of distribution algorithm","categories":["All Wikipedia articles needing context","All articles covered by WikiProject Wikify","All articles needing additional references","All articles needing references cleanup","All pages needing cleanup","Articles covered by WikiProject Wikify from September 2009","Articles needing additional references from January 2008","Evolutionary computation","Stochastic algorithms","Wikipedia articles needing context from September 2009","Wikipedia introduction cleanup from September 2009","Wikipedia references cleanup from September 2009"],"tag_line":"Estimation of distribution algorithms (EDAs), sometimes called probabilistic model-building genetic algorithms (PMBGAs), are stochastic optimization methods that guide the search for the optimum by building and sampling explicit probabilistic models of promising candidate solutions."}}
,{"_index":"throwtable","_type":"algorithm","_id":"stochastic-diffusion-search","_score":0,"_source":{"description":"Stochastic diffusion search (SDS) was first described in 1989 as a population-based, pattern-matching algorithm [Bishop, 1989]. It belongs to a family of swarm intelligence and naturally inspired search and optimisation algorithms which includes ant colony optimization, particle swarm optimization and genetic algorithms. Unlike stigmergetic communication employed in ant colony optimization, which is based on modification of the physical properties of a simulated environment, SDS uses a form of direct (one-to-one) communication between the agents similar to the tandem calling mechanism employed by one species of ants, Leptothorax acervorum.\nIn SDS agents perform cheap, partial evaluations of a hypothesis (a candidate solution to the search problem). They then share information about hypotheses (diffusion of information) through direct one-to-one communication. As a result of the diffusion mechanism, high-quality solutions can be identified from clusters of agents with the same hypothesis. The operation of SDS is most easily understood by means of a simple analogy – The Restaurant Game.","name":"Stochastic diffusion search","categories":["Artificial intelligence","Stochastic algorithms"],"tag_line":"Stochastic diffusion search (SDS) was first described in 1989 as a population-based, pattern-matching algorithm [Bishop, 1989]."}}
,{"_index":"throwtable","_type":"algorithm","_id":"monte-carlo-algorithm","_score":0,"_source":{"description":"In computing, a Monte Carlo algorithm is a randomized algorithm whose running time is deterministic, but whose output may be incorrect with a certain (typically small) probability.\nThe related class of Las Vegas algorithms are also randomized, but in a different way: they take an amount of time that varies randomly, but always produce the correct answer. A Monte Carlo algorithm can be converted into a Las Vegas algorithm whenever there exists a procedure to verify that the output produced by the algorithm is indeed correct. If so, then the resulting Las Vegas algorithm is merely to repeatedly run the Monte Carlo algorithm until one of the runs produces an output that can be verified to be correct.\nThe name refers to the grand casino in the Principality of Monaco at Monte Carlo, which is well-known around the world as an icon of gambling.\n\n","name":"Monte Carlo algorithm","categories":["All articles lacking in-text citations","Articles lacking in-text citations from August 2011","Randomized algorithms"],"tag_line":"In computing, a Monte Carlo algorithm is a randomized algorithm whose running time is deterministic, but whose output may be incorrect with a certain (typically small) probability."}}
,{"_index":"throwtable","_type":"algorithm","_id":"list-scheduling","_score":0,"_source":{"description":"The basic idea of list scheduling is to make an ordered list of processes by assigning them some priorities, and then repeatedly execute the following two steps until a valid schedule is obtained :\nSelect from the list, the process with the highest priority for scheduling.\nSelect a resource to accommodate this process.\nIf no resource can be found, we select the next process in the list.\nThe priorities are determined statically before scheduling process begins. The first step chooses the process with the highest priority, the second step selects the best possible resource. Some known list scheduling strategies are :\nHighest level first algorithm or HLF\nLongest path algorithm or LP\nLongest processing time\nCritical path method\nHeterogeneous Earliest Finish Time or HEFT. For the case heterogeneous workers.","name":"List scheduling","categories":["Pages using citations with accessdate and no URL","Scheduling algorithms"],"tag_line":"The basic idea of list scheduling is to make an ordered list of processes by assigning them some priorities, and then repeatedly execute the following two steps until a valid schedule is obtained :\nSelect from the list, the process with the highest priority for scheduling."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fscan","_score":0,"_source":{"description":"FScan is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests. It uses two subqueues. During the scan, all of the requests are in the first queue and all new requests are put into the second queue. Thus, service of new requests is deferred until all of the old requests have been processed. When the scan ends, the arm is taken to the first queue entries and is started all over again.","name":"FSCAN","categories":["All articles lacking sources","Articles lacking sources from December 2009","Disk scheduling algorithms"],"tag_line":"FScan is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests."}}
,{"_index":"throwtable","_type":"algorithm","_id":"stochastic-computing","_score":0,"_source":{"description":"Stochastic computing is a collection of techniques that represent continuous values by streams of random bits. Complex computations can then be computed by simple bit-wise operations on the streams.\nDespite the similarity in their names, stochastic computing is distinct from the study of randomized algorithms.","name":"Stochastic computing","categories":["All articles with unsourced statements","Articles with unsourced statements from October 2014","History of computing hardware","Models of computation","Stochastic algorithms"],"tag_line":"Stochastic computing is a collection of techniques that represent continuous values by streams of random bits."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bht-algorithm","_score":0,"_source":{"description":"The BHT algorithm is a quantum algorithm that solves the collision problem. In this problem, one is given n and an r-to-1 function  and needs to find two inputs that f maps to the same output. The BHT algorithm only makes  queries to f, which matches the lower bound of  in the black box model.\nThe algorithm was discovered by Brassard, Hoyer, and Tapp in 1997. It uses Grover's algorithm, which was discovered in the previous year.","name":"BHT algorithm","categories":["Quantum algorithms"],"tag_line":"The BHT algorithm is a quantum algorithm that solves the collision problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"atropos-scheduler","_score":0,"_source":{"description":"In computer science, Atropos is a real-time scheduling algorithm developed at Cambridge University. It combines the earliest deadline first algorithm with a best effort scheduler to make use of slack time, while exercising strict admission control.","name":"Atropos scheduler","categories":["All articles lacking sources","All stub articles","Articles lacking sources from December 2009","Computer science stubs","Real-time computing","Scheduling algorithms"],"tag_line":"In computer science, Atropos is a real-time scheduling algorithm developed at Cambridge University."}}
,{"_index":"throwtable","_type":"algorithm","_id":"brute-force-search","_score":0,"_source":{"description":"In computer science, brute-force search or exhaustive search, also known as generate and test, is a very general problem-solving technique that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement.\nA brute-force algorithm to find the divisors of a natural number n would enumerate all integers from 1 to n, and check whether each of them divides n without remainder. A brute-force approach for the eight queens puzzle would examine all possible arrangements of 8 pieces on the 64-square chessboard, and, for each arrangement, check whether each (queen) piece can attack any other.\nWhile a brute-force search is simple to implement, and will always find a solution if it exists, its cost is proportional to the number of candidate solutions – which in many practical problems tends to grow very quickly as the size of the problem increases. Therefore, brute-force search is typically used when the problem size is limited, or when there are problem-specific heuristics that can be used to reduce the set of candidate solutions to a manageable size. The method is also used when the simplicity of implementation is more important than speed.\nThis is the case, for example, in critical applications where any errors in the algorithm would have very serious consequences; or when using a computer to prove a mathematical theorem. Brute-force search is also useful as a baseline method when benchmarking other algorithms or metaheuristics. Indeed, brute-force search can be viewed as the simplest metaheuristic. Brute force search should not be confused with backtracking, where large sets of solutions can be discarded without being explicitly enumerated (as in the textbook computer solution to the eight queens problem above). The brute-force method for finding an item in a table — namely, check all entries of the latter, sequentially — is called linear search.","name":"Brute-force search","categories":["All articles needing additional references","Articles needing additional references from February 2008","Search algorithms"],"tag_line":"In computer science, brute-force search or exhaustive search, also known as generate and test, is a very general problem-solving technique that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement."}}
,{"_index":"throwtable","_type":"algorithm","_id":"knuth's-algorithm-x","_score":0,"_source":{"description":"\"Algorithm X\" is the name Donald Knuth used in his paper \"Dancing Links\" to refer to \"the most obvious trial-and-error approach\" for finding all solutions to the exact cover problem. Technically, Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm. While Algorithm X is generally useful as a succinct explanation of how the exact cover problem may be solved, Knuth's intent in presenting it was merely to demonstrate the utility of the dancing links technique via an efficient implementation he called DLX.\nThe exact cover problem is represented in Algorithm X using a matrix A consisting of 0s and 1s. The goal is to select a subset of the rows so that the digit 1 appears in each column exactly once.\nAlgorithm X functions as follows:\n\nThe nondeterministic choice of r means that the algorithm essentially clones itself into independent subalgorithms; each subalgorithm inherits the current matrix A, but reduces it with respect to a different row r. If column c is entirely zero, there are no subalgorithms and the process terminates unsuccessfully.\nThe subalgorithms form a search tree in a natural way, with the original problem at the root and with level k containing each subalgorithm that corresponds to k chosen rows. Backtracking is the process of traversing the tree in preorder, depth first.\nAny systematic rule for choosing column c in this procedure will find all solutions, but some rules work much better than others. To reduce the number of iterations, Knuth suggests that the column choosing algorithm select a column with the lowest number of 1s in it.","name":"Knuth's Algorithm X","categories":["Donald Knuth","Search algorithms"],"tag_line":"\"Algorithm X\" is the name Donald Knuth used in his paper \"Dancing Links\" to refer to \"the most obvious trial-and-error approach\" for finding all solutions to the exact cover problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"starvation-(computer-science)","_score":0,"_source":{"description":"In computer science, starvation is a problem encountered in concurrent computing where a process is perpetually denied necessary resources to process its work. Starvation may be caused by errors in a scheduling or mutual exclusion algorithm, but can also be caused by resource leaks, and can be intentionally caused via a denial-of-service attack such as a fork bomb.\nThe impossibility of starvation in a concurrent algorithm is called starvation-freedom, lockout-freedom or finite bypass, is an instance of liveness, and is one of the two requirements for any mutual exclusion algorithm (the other being correctness). The name \"finite bypass\" means that any process (concurrent part) of the algorithm is bypassed at most a finite number times before being allowed access to the shared resource.\n^ Tanenbaum, Andrew (2001). Modern Operating Systems. Prentice Hall. pp. 184–185. ISBN 0-13-092641-8. \n^ Herlihy, Maurice; Shavit, Nir (2012). The Art of Multiprocessor Programming. Elsevier. p. 24. ISBN 9780123977953. \n^ a b Raynal, Michel (2012). Concurrent Programming: Algorithms, Principles, and Foundations. Springer Science & Business Media. p. 10–11. ISBN 3642320279.","name":"Starvation (computer science)","categories":["All articles needing additional references","All stub articles","Articles needing additional references from January 2011","Computer science stubs","Concurrency (computer science)","Processor scheduling algorithms"],"tag_line":"In computer science, starvation is a problem encountered in concurrent computing where a process is perpetually denied necessary resources to process its work."}}
,{"_index":"throwtable","_type":"algorithm","_id":"beam-search","_score":0,"_source":{"description":"In computer science, beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic which attempts to predict how close a partial solution is to a complete solution (goal state). But in beam search, only a predetermined number of best partial solutions are kept as candidates.\nBeam search uses breadth-first search to build its search tree. At each level of the tree, it generates all successors of the states at the current level, sorting them in increasing order of heuristic cost. However, it only stores a predetermined number of best states at each level (called the beam width). Only those states are expanded next. The greater the beam width, the fewer states are pruned. With an infinite beam width, no states are pruned and beam search is identical to breadth-first search. The beam width bounds the memory required to perform the search. Since a goal state could potentially be pruned, beam search sacrifices completeness (the guarantee that an algorithm will terminate with a solution, if one exists). Beam search is not optimal (that is, there is no guarantee that it will find the best solution). It returns the first solution found.\nThe beam width can either be fixed or variable. One approach that uses a variable beam width starts with the width at a minimum. If no solution is found, the beam is widened and the procedure is repeated.","name":"Beam search","categories":["Search algorithms"],"tag_line":"In computer science, beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set."}}
,{"_index":"throwtable","_type":"algorithm","_id":"all-nearest-smaller-values","_score":0,"_source":{"description":"In computer science, the all nearest smaller values problem is the following task: for each position in a sequence of numbers, search among the previous positions for the last position that contains a smaller value. This problem can be solved efficiently both by parallel and non-parallel algorithms: Berkman, Schieber & Vishkin (1993), who first identified the procedure as a useful subroutine for other parallel programs, developed efficient algorithms to solve it in the Parallel Random Access Machine model; it may also be solved in linear time on a non-parallel computer using a stack-based algorithm. Later researchers have studied algorithms to solve it in other models of parallel computation.","name":"All nearest smaller values","categories":["Parallel computing","Search algorithms"],"tag_line":"In computer science, the all nearest smaller values problem is the following task: for each position in a sequence of numbers, search among the previous positions for the last position that contains a smaller value."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fast-folding-algorithm","_score":0,"_source":{"description":"In signal processing, the fast folding algorithm (Staelin, 1969) is an efficient algorithm for the detection of approximately-periodic events within time series data. It computes superpositions of the signal modulo various window sizes simultaneously.\nThe FFA is best known for its use in the detection of pulsars, as popularised by SETI@home and Astropulse.","name":"Fast folding algorithm","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Signal processing"],"tag_line":"In signal processing, the fast folding algorithm (Staelin, 1969) is an efficient algorithm for the detection of approximately-periodic events within time series data."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mtd-f","_score":0,"_source":{"description":"MTD(f), is a minimax search algorithm, developed in 1994 by Aske Plaat, Jonathan Schaeffer, Wim Pijls, and Arie de Bruin. Experiments with tournament-quality chess, checkers, and Othello programs show it to be the most efficient minimax algorithm. The name MTD(f) is an abbreviation for MTD(n,f) (Memory-enhanced Test Driver with node n and value f). It is an alternative to the alpha-beta pruning algorithm.","name":"MTD-f","categories":["Articles with example pseudocode","Search algorithms"],"tag_line":"MTD(f), is a minimax search algorithm, developed in 1994 by Aske Plaat, Jonathan Schaeffer, Wim Pijls, and Arie de Bruin."}}
,{"_index":"throwtable","_type":"algorithm","_id":"beam-stack-search","_score":0,"_source":{"description":"Beam Stack Search is a search algorithm that combines chronological backtracking (that is, depth-first search) with beam search and is similar to Depth-First Beam Search. Both search algorithms are anytime algorithms that find good but likely sub-optimal solutions quickly, like beam search, then backtrack and continue to find improved solutions until convergence to an optimal solution.\n\n","name":"Beam stack search","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Search algorithms"],"tag_line":"Beam Stack Search is a search algorithm that combines chronological backtracking (that is, depth-first search) with beam search and is similar to Depth-First Beam Search."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lr-parser","_score":0,"_source":{"description":"In computer science, LR parsers are a type of bottom-up parsers that efficiently handle deterministic context-free languages in guaranteed linear time. The LALR parsers and the SLR parsers are common variants of LR parsers. LR parsers are often mechanically generated from a formal grammar for the language by a parser generator tool. They are very widely used for the processing of computer languages, more than other kinds of generated parsers.\nThe name LR is an acronym. The L means that the parser reads input text in one direction without backing up; that direction is typically Left to right within each line, and top to bottom across the lines of the full input file. (This is true for most parsers.) The R means that the parser produces a reversed Rightmost derivation; it does a bottom-up parse, not a top-down LL parse or ad-hoc parse. The name LR is often followed by a numeric qualifier, as in LR(1) or sometimes LR(k). To avoid backtracking or guessing, the LR parser is allowed to peek ahead at k lookahead input symbols before deciding how to parse earlier symbols. Typically k is 1 and is not mentioned. The name LR is often preceded by other qualifiers, as in SLR and LALR.\nLR parsers are deterministic; they produce a single correct parse without guesswork or backtracking, in linear time. This is ideal for computer languages. But LR parsers are not suited for human languages which need more flexible but slower methods. Other parser methods (CYK algorithm, Earley parser, and GLR parser) that backtrack or yield multiple parses may take O(n2), O(n3) or even exponential time when they guess badly.\nThe above properties of L, R, and k are actually shared by all shift-reduce parsers, including precedence parsers. But by convention, the LR name stands for the form of parsing invented by Donald Knuth, and excludes the earlier, less powerful precedence methods (for example Operator-precedence parser). LR parsers can handle a larger range of languages and grammars than precedence parsers or top-down LL parsing. This is because the LR parser waits until it has seen an entire instance of some grammar pattern before committing to what it has found. An LL parser has to decide or guess what it is seeing much sooner, when it has only seen the leftmost input symbol of that pattern. LR is also better at error reporting. It detects syntax errors as early in the input stream as possible.","name":"LR parser","categories":["All articles with unsourced statements","Articles with unsourced statements from June 2012","Parsing algorithms"],"tag_line":"In computer science, LR parsers are a type of bottom-up parsers that efficiently handle deterministic context-free languages in guaranteed linear time."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lalr-parser","_score":0,"_source":{"description":"In computer science, an LALR parser or Look-Ahead LR parser is a simplified version of a canonical LR parser, to parse (separate and analyze) a text according to a set of production rules specified by a formal grammar for a computer language. (\"LR\" means left-to-right, rightmost derivation.)\nThe LALR parser was invented by Frank DeRemer in his 1969 PhD dissertation, Practical Translators for LR(k) languages, in his treatment of the practical difficulties at that time of implementing LR(1) parsers. He showed that the LALR parser has more language recognition power than the LR(0) parser, while requiring the same number of states as the LR(0) parser for a language that can be recognized by both parsers. This makes the LALR parser a memory-efficient alternative to the LR(1) parser for languages that are not LR(0). It was also proved that there exist LR(1) languages that are not LALR. Despite this weakness, the power of the LALR parser is enough for many mainstream computer languages, including Java, though the reference grammars for many languages fail to be LALR due to being ambiguous.\nThe original dissertation gave no algorithm for constructing such a parser given some formal grammar. The first algorithms for LALR parser generation were published in 1973. In 1982, DeRemer and Tom Pennello published an algorithm that generated highly memory-efficient LALR parsers. LALR parsers can be automatically generated from some grammar by an LALR parser generator such as Yacc or GNU Bison. The automatically generated code may be augmented by hand-written code to augment the power of the resulting parser.\n\n","name":"LALR parser","categories":["All articles with unsourced statements","Articles with unsourced statements from December 2012","Parsing algorithms","Use dmy dates from July 2012"],"tag_line":"In computer science, an LALR parser or Look-Ahead LR parser is a simplified version of a canonical LR parser, to parse (separate and analyze) a text according to a set of production rules specified by a formal grammar for a computer language."}}
,{"_index":"throwtable","_type":"algorithm","_id":"twisting-properties","_score":0,"_source":{"description":"Starting with a sample  observed from a random variable X having a given distribution law with a non-set parameter, a parametric inference problem consists of computing suitable values – call them estimates – of this parameter precisely on the basis of the sample. An estimate is suitable if replacing it with the unknown parameter does not cause major damage in next computations. In algorithmic inference, suitability of an estimate reads in terms of compatibility with the observed sample.\nIn turn, parameter compatibility is a probability measure that we derive from the probability distribution of the random variable to which the parameter refers. In this way we identify a random parameter Θ compatible with an observed sample. Given a sampling mechanism , the rationale of this operation lies in using the Z seed distribution law to determine both the X distribution law for the given θ, and the Θ distribution law given an X sample. Hence, we may derive the latter distribution directly from the former if we are able to relate domains of the sample space to subsets of Θ support. In more abstract terms, we speak about twisting properties of samples with properties of parameters and identify the former with statistics that are suitable for this exchange, so denoting a well behavior w.r.t. the unknown parameters. The operational goal is to write the analytic expression of the cumulative distribution function , in light of the observed value s of a statistic S, as a function of the S distribution law when the X parameter is exactly θ.","name":"Twisting properties","categories":["Algorithmic inference","All Wikipedia articles needing context","All articles lacking in-text citations","All pages needing cleanup","Articles lacking in-text citations from September 2009","Computational statistics","Wikipedia articles needing context from January 2009","Wikipedia introduction cleanup from January 2009"],"tag_line":"Starting with a sample  observed from a random variable X having a given distribution law with a non-set parameter, a parametric inference problem consists of computing suitable values – call them estimates – of this parameter precisely on the basis of the sample."}}
,{"_index":"throwtable","_type":"algorithm","_id":"earley-parser","_score":0,"_source":{"description":"In computer science, the Earley parser is an algorithm for parsing strings that belong to a given context-free language, though (depending on the variant) it may suffer problems with certain nullable grammars. The algorithm, named after its inventor, Jay Earley, is a chart parser that uses dynamic programming; it is mainly used for parsing in computational linguistics. It was first introduced in his dissertation in 1968 (and later appeared in abbreviated, more legible form in a journal).\nEarley parsers are appealing because they can parse all context-free languages, unlike LR parsers and LL parsers, which are more typically used in compilers but which can only handle restricted classes of languages. The Earley parser executes in cubic time in the general case , where n is the length of the parsed string, quadratic time for unambiguous grammars , and linear time for almost all LR(k) grammars. It performs particularly well when the rules are written left-recursively.","name":"Earley parser","categories":["Dynamic programming","Parsing algorithms"],"tag_line":"In computer science, the Earley parser is an algorithm for parsing strings that belong to a given context-free language, though (depending on the variant) it may suffer problems with certain nullable grammars."}}
,{"_index":"throwtable","_type":"algorithm","_id":"inside–outside-algorithm","_score":0,"_source":{"description":"In computer science, the inside–outside algorithm is a way of re-estimating production probabilities in a probabilistic context-free grammar. It was introduced James K. Baker in 1979 as a generalization of the forward–backward algorithm for parameter estimation on hidden Markov models to stochastic context-free grammars. It is used to compute expectations, for example as part of the expectation–maximization algorithm (an unsupervised learning algorithm).\n\n","name":"Inside–outside algorithm","categories":["Algorithms and data structures stubs","All Wikipedia articles needing context","All pages needing cleanup","All stub articles","Computer science stubs","Parsing algorithms","Wikipedia articles needing context from June 2012","Wikipedia introduction cleanup from June 2012"],"tag_line":"In computer science, the inside–outside algorithm is a way of re-estimating production probabilities in a probabilistic context-free grammar."}}
,{"_index":"throwtable","_type":"algorithm","_id":"wagner–fischer-algorithm","_score":0,"_source":{"description":"In computer science, the Wagner–Fischer algorithm is a dynamic programming algorithm that computes the edit distance between two strings of characters.\n\n","name":"Wagner–Fischer algorithm","categories":["Algorithms on strings","String similarity measures"],"tag_line":"In computer science, the Wagner–Fischer algorithm is a dynamic programming algorithm that computes the edit distance between two strings of characters.\n\n"}}
,{"_index":"throwtable","_type":"algorithm","_id":"selection-algorithm","_score":0,"_source":{"description":"In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. This includes the cases of finding the minimum, maximum, and median elements. There are O(n) (worst-case linear time) selection algorithms, and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. Selection is a subproblem of more complex problems like the nearest neighbor and shortest path problems. Many selection algorithms are derived by generalizing a sorting algorithm, and conversely some sorting algorithms can be derived as repeated application of selection.\nThe simplest case of a selection algorithm is finding the minimum (or maximum) element by iterating through the list, keeping track of the running minimum – the minimum so far – (or maximum) and can be seen as related to the selection sort. Conversely, the hardest case of a selection algorithm is finding the median, and this necessarily takes n/2 storage. In fact, a specialized median-selection algorithm can be used to build a general selection algorithm, as in median of medians. The best-known selection algorithm is quickselect, which is related to quicksort; like quicksort, it has (asymptotically) optimal average performance, but poor worst-case performance, though it can be modified to give optimal worst-case performance as well.\n\n","alt_names":["Selection_algorithm"],"name":"Selection algorithm","categories":["All articles with unsourced statements","Articles with unsourced statements from April 2014","Selection algorithms"],"tag_line":"In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rsa-numbers","_score":0,"_source":{"description":"In mathematics, the RSA numbers are a set of large semiprimes (numbers with exactly two prime factors) that are part of the RSA Factoring Challenge. The challenge was to find the prime factors but it was declared inactive in 2007. It was created by RSA Laboratories in March 1991 to encourage research into computational number theory and the practical difficulty of factoring large integers.\nRSA Laboratories published a number of semiprimes with 100 to 617 decimal digits. Cash prizes of varying size were offered for factorization of some of them. The smallest RSA number was factored in a few days. Most of the numbers have still not been factored and many of them are expected to remain unfactored for many years to come. As of September 2013, 18 of the 54 listed numbers have been factored: the 17 smallest from RSA-100 to RSA-704, plus RSA-768.\nThe RSA challenge officially ended in 2007 but people are still attempting to find the factorizations. According to RSA Laboratories, \"Now that the industry has a considerably more advanced understanding of the cryptanalytic strength of common symmetric-key and public-key algorithms, these challenges are no longer active.\" Some of the smaller prizes had been awarded at the time. The remaining prizes were retracted.\nThe first RSA numbers generated, from RSA-100 to RSA-500, were labeled according to their number of decimal digits. Later, beginning with RSA-576, binary digits are counted instead. An exception to this is RSA-617, which was created before the change in the numbering scheme. The numbers are listed in increasing order below.","name":"RSA numbers","categories":["All articles containing potentially dated statements","Articles containing potentially dated statements from September 2013","Integer factorization algorithms","Large integers","RSA Factoring Challenge"],"tag_line":"In mathematics, the RSA numbers are a set of large semiprimes (numbers with exactly two prime factors) that are part of the RSA Factoring Challenge."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cobweb-(clustering)","_score":0,"_source":{"description":"COBWEB is an incremental system for hierarchical conceptual clustering. COBWEB was invented by Professor Douglas H. Fisher, currently at Vanderbilt University.\nCOBWEB incrementally organizes observations into a classification tree. Each node in a classification tree represents a class (concept) and is labeled by a probabilistic concept that summarizes the attribute-value distributions of objects classified under the node. This classification tree can be used to predict missing attributes or the class of a new object.\nThere are four basic operations COBWEB employs in building the classification tree. Which operation is selected depends on the category utility of the classification achieved by applying it. The operations are:\nMerging Two Nodes\nMerging two nodes means replacing them by a node whose children is the union of the original nodes' sets of children and which summarizes the attribute-value distributions of all objects classified under them.\nSplitting a node\nA node is split by replacing it with its children.\nInserting a new node\nA node is created corresponding to the object being inserted into the tree.\nPassing an object down the hierarchy\nEffectively calling the COBWEB algorithm on the object and the subtree rooted in the node.","name":"Cobweb (clustering)","categories":["Artificial intelligence","Data clustering algorithms"],"tag_line":"COBWEB is an incremental system for hierarchical conceptual clustering."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rainbow-table","_score":0,"_source":{"description":"A rainbow table is a precomputed table for reversing cryptographic hash functions, usually for cracking password hashes. Tables are usually used in recovering a plaintext password up to a certain length consisting of a limited set of characters. It is a practical example of a space/time trade-off, using less computer processing time and more storage than a brute-force attack which calculates a hash on every attempt, but more processing time and less storage than a simple lookup table with one entry per hash. Use of a key derivation function that employs a salt makes this attack infeasible.\nRainbow tables are an application of an earlier, simpler algorithm by Martin Hellman.","name":"Rainbow table","categories":["All articles lacking in-text citations","All articles with unsourced statements","Articles lacking in-text citations from March 2009","Articles with DMOZ links","Articles with unsourced statements from January 2011","Articles with unsourced statements from January 2014","Articles with unsourced statements from July 2013","Cryptographic attacks","Cryptographic hash functions","Search algorithms"],"tag_line":"A rainbow table is a precomputed table for reversing cryptographic hash functions, usually for cracking password hashes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"berlekamp–massey-algorithm","_score":0,"_source":{"description":"The Berlekamp–Massey algorithm is an algorithm that will find the shortest linear feedback shift register (LFSR) for a given binary output sequence. The algorithm will also find the minimal polynomial of a linearly recurrent sequence in an arbitrary field. The field requirement means that the Berlekamp–Massey algorithm requires all non-zero elements to have a multiplicative inverse. Reeds and Sloane offer an extension to handle a ring.\nElwyn Berlekamp invented an algorithm for decoding Bose–Chaudhuri–Hocquenghem (BCH) codes. James Massey recognized its application to linear feedback shift registers and simplified the algorithm. Massey termed the algorithm the LFSR Synthesis Algorithm (Berlekamp Iterative Algorithm), but it is now known as the Berlekamp–Massey algorithm.","name":"Berlekamp–Massey algorithm","categories":["All articles with dead external links","Articles with German-language external links","Articles with dead external links from June 2015","Cryptanalytic algorithms","Error detection and correction"],"tag_line":"The Berlekamp–Massey algorithm is an algorithm that will find the shortest linear feedback shift register (LFSR) for a given binary output sequence."}}
,{"_index":"throwtable","_type":"algorithm","_id":"factor-base","_score":0,"_source":{"description":"In computational number theory, a factor base is a small set of prime numbers commonly used as a mathematical tool in algorithms involving extensive sieving for potential factors of a given integer.","name":"Factor base","categories":["All stub articles","Integer factorization algorithms","Number theory stubs"],"tag_line":"In computational number theory, a factor base is a small set of prime numbers commonly used as a mathematical tool in algorithms involving extensive sieving for potential factors of a given integer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pollard's-rho-algorithm","_score":0,"_source":{"description":"Pollard's rho algorithm is a special-purpose integer factorization algorithm. It was invented by John Pollard in 1975. It is particularly effective for a composite number having a small prime factor.","name":"Pollard's rho algorithm","categories":["Integer factorization algorithms"],"tag_line":"Pollard's rho algorithm is a special-purpose integer factorization algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"k-nearest-neighbors-algorithm","_score":0,"_source":{"description":"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\nIn k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\n\nk-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.\nBoth for classification and regression, it can be useful to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.\nThe neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\nA shortcoming of the k-NN algorithm is that it is sensitive to the local structure of the data. The algorithm has nothing to do with and is not to be confused with k-means, another popular machine learning technique.","name":"K-nearest neighbors algorithm","categories":["All articles with unsourced statements","Articles with unsourced statements from December 2008","Articles with unsourced statements from March 2013","Classification algorithms","Machine learning algorithms","Search algorithms"],"tag_line":"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression."}}
,{"_index":"throwtable","_type":"algorithm","_id":"constrained-clustering","_score":0,"_source":{"description":"In computer science, constrained clustering is a class of semi-supervised learning algorithms. Typically, constrained clustering incorporates either a set of must-link constraints, cannot-link constraints, or both, with a Data clustering algorithm. Both a must-link and a cannot-link constraint define a relationship between two data instances. A must-link constraint is used to specify that the two instances in the must-link relation should be associated with the same cluster. A cannot-link constraint is used to specify that the two instances in the cannot-link relation should not be associated with the same cluster. These sets of constraints acts as a guide for which a constrained clustering algorithm will attempt to find clusters in a data set which satisfy the specified must-link and cannot-link constraints. Some constrained clustering algorithms will abort if no such clustering exists which satisfies the specified constraints. Others will try to minimize the amount of constraint violation should it be impossible to find a clustering which satisfies the constraints. Constraints could also be used to guide the selection of a clustering model among several possible solutions. \nExamples of constrained clustering algorithms include:\nCOP K-means \nPCKmeans\nCMWK-Means","name":"Constrained clustering","categories":["All stub articles","Cluster analysis","Computer science stubs","Data clustering algorithms"],"tag_line":"In computer science, constrained clustering is a class of semi-supervised learning algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"decision-boundary","_score":0,"_source":{"description":"In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.\nA decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.\nIf the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable.\nDecision boundaries are not always clear cut. That is, the transition from one class in the feature space to another is not discontinuous, but gradual. This effect is common in fuzzy logic based classification algorithms, where membership in one class or another is ambiguous.","name":"Decision boundary","categories":["All articles needing additional references","All stub articles","Articles needing additional references from September 2014","Artificial intelligence stubs","Classification algorithms","Pattern recognition","Statistical classification"],"tag_line":"In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class."}}
,{"_index":"throwtable","_type":"algorithm","_id":"nearest-neighbor-chain-algorithm","_score":0,"_source":{"description":"In the theory of cluster analysis, the nearest-neighbor chain algorithm is a method that can be used to perform several types of agglomerative hierarchical clustering, using an amount of memory that is linear in the number of points to be clustered and an amount of time linear in the number of distinct distances between pairs of points. The main idea of the algorithm is to find pairs of clusters to merge by following paths in the nearest neighbor graph of the clusters until the paths terminate in pairs of mutual nearest neighbors. The algorithm was developed and implemented in 1982 by J. P. Benzécri and J. Juan, based on earlier methods that constructed hierarchical clusterings using mutual nearest neighbor pairs without taking advantage of nearest neighbor chains.","name":"Nearest-neighbor chain algorithm","categories":["Data clustering algorithms"],"tag_line":"In the theory of cluster analysis, the nearest-neighbor chain algorithm is a method that can be used to perform several types of agglomerative hierarchical clustering, using an amount of memory that is linear in the number of points to be clustered and an amount of time linear in the number of distinct distances between pairs of points."}}
,{"_index":"throwtable","_type":"algorithm","_id":"k-q-flats","_score":0,"_source":{"description":"In data mining and machine learning,  -flats algorithm   is an iterative method which aims to partition  observations into  clusters where each cluster is close to a -flat, where  is a given integer.\nIt is a generalization of the -means algorithm. In -means algorithm, clusters are formed in the way that each cluster is close to one point, which is a -flat.  -flats algorithm gives better clustering result than -means algorithm for some data set.\n\n","name":"K q-flats","categories":["All articles needing expert attention","All articles that are too technical","Articles needing expert attention from December 2011","Data clustering algorithms","Wikipedia articles that are too technical from December 2011"],"tag_line":"In data mining and machine learning,  -flats algorithm   is an iterative method which aims to partition  observations into  clusters where each cluster is close to a -flat, where  is a given integer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"logitboost","_score":0,"_source":{"description":"In machine learning and computational learning theory, LogitBoost is a boosting algorithm formulated by Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The original paper casts the AdaBoost algorithm into a statistical framework. Specifically, if one considers AdaBoost as a generalized additive model and then applies the cost functional of logistic regression, one can derive the LogitBoost algorithm.","name":"LogitBoost","categories":["All stub articles","Artificial intelligence stubs","Classification algorithms","Ensemble learning","Machine learning algorithms"],"tag_line":"In machine learning and computational learning theory, LogitBoost is a boosting algorithm formulated by Jerome Friedman, Trevor Hastie, and Robert Tibshirani."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sukhotin's-algorithm","_score":0,"_source":{"description":"Sukhotin's algorithm (introduced by Boris V. Sukhotin) is a statistical classification algorithm for classifying characters in a text as vowels or consonants. It may also be of use in some of substitution ciphers and has been considered in deciphering the Voynich manuscript, though one problem is to agree on the set of symbols the manuscript is written in.","name":"Sukhotin's algorithm","categories":["Classification algorithms","Natural language processing"],"tag_line":"Sukhotin's algorithm (introduced by Boris V. Sukhotin) is a statistical classification algorithm for classifying characters in a text as vowels or consonants."}}
,{"_index":"throwtable","_type":"algorithm","_id":"learning-vector-quantization","_score":0,"_source":{"description":"In computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems.","name":"Learning vector quantization","categories":["Artificial neural networks","Classification algorithms"],"tag_line":"In computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fftw","_score":0,"_source":{"description":"The Fastest Fourier Transform in the West (FFTW) is a software library for computing discrete Fourier transforms (DFTs) developed by Matteo Frigo and Steven G. Johnson at the Massachusetts Institute of Technology.\nFFTW is known as the fastest free software implementation of the Fast Fourier transform (FFT) algorithm (upheld by regular benchmarks). It can compute transforms of real and complex-valued arrays of arbitrary size and dimension in O(n log n) time.\nIt does this by supporting a variety of algorithms and choosing the one (a particular decomposition of the transform into smaller transforms) it estimates or measures to be preferable in the particular circumstances. It works best on arrays of sizes with small prime factors, with powers of two being optimal and large primes being worst case (but still O(n log n)). To decompose transforms of composite sizes into smaller transforms, it chooses among several variants of the Cooley–Tukey FFT algorithm (corresponding to different factorizations and/or different memory-access patterns), while for prime sizes it uses either Rader's or Bluestein's FFT algorithm. Once the transform has been broken up into subtransforms of sufficiently small sizes, FFTW uses hard-coded unrolled FFTs for these small sizes that were produced (at compile time, not at run time) by code generation; these routines use a variety of algorithms including Cooley–Tukey variants, Rader's algorithm, and prime-factor FFT algorithms.\nFor a sufficiently large number of repeated transforms it is advantageous to measure the performance of some or all of the supported algorithms on the given array size and platform. These measurements, which the authors refer to as \"wisdom\", can be stored in a file or string for later use.\nFFTW has a \"guru interface\" that intends \"to expose as much as possible of the flexibility in the underlying FFTW architecture\". This allows, among other things, multi-dimensional transforms and multiple transforms in a single call (e.g., where the data is interleaved in memory).\nFFTW has limited support for out-of-order transforms (using the MPI version). The data reordering incurs an overhead, which for in-place transforms of arbitrary size and dimension is non-trivial to avoid. It is undocumented for which transforms this overhead is significant.\nFFTW is licensed under the GNU General Public License. It is also licensed commercially by MIT and is used in the commercial MATLAB matrix package for calculating FFTs. FFTW is written in the C language, but Fortran and Ada interfaces exist, as well as interfaces for a few other languages. The Julia base library includes an interface to FFTW by default.  While the library itself is C, the code is actually generated from a program called 'genfft', which is written in OCaml.\nIn 1999, FFTW won the J. H. Wilkinson Prize for Numerical Software.","name":"FFTW","categories":["FFT algorithms","Free mathematics software","Numerical libraries","OCaml software"],"tag_line":"The Fastest Fourier Transform in the West (FFTW) is a software library for computing discrete Fourier transforms (DFTs) developed by Matteo Frigo and Steven G. Johnson at the Massachusetts Institute of Technology."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cuckoo-search","_score":0,"_source":{"description":"Cuckoo search (CS) is an optimization algorithm developed by Xin-she Yang and Suash Deb in 2009. It was inspired by the obligate brood parasitism of some cuckoo species by laying their eggs in the nests of other host birds (of other species). Some host birds can engage direct conflict with the intruding cuckoos. For example, if a host bird discovers the eggs are not their own, it will either throw these alien eggs away or simply abandon its nest and build a new nest elsewhere. Some cuckoo species such as the New World brood-parasitic Tapera have evolved in such a way that female parasitic cuckoos are often very specialized in the mimicry in colors and pattern of the eggs of a few chosen host species \nCuckoo search idealized such breeding behavior, and thus can be applied for various optimization problems. It seems that it can outperform other metaheuristic algorithms in applications.\nCuckoo search (CS) uses the following representations:\nEach egg in a nest represents a solution, and a cuckoo egg represents a new solution. The aim is to use the new and potentially better solutions (cuckoos) to replace a not-so-good solution in the nests. In the simplest form, each nest has one egg. The algorithm can be extended to more complicated cases in which each nest has multiple eggs representing a set of solutions.\nCS is based on three idealized rules:\nEach cuckoo lays one egg at a time, and dumps its egg in a randomly chosen nest;\nThe best nests with high quality of eggs will carry over to the next generation;\nThe number of available hosts nests is fixed, and the egg laid by a cuckoo is discovered by the host bird with a probability . Discovering operate on some set of worst nests, and discovered solutions dumped from farther calculations.\nIn addition, Yang and Deb discovered that the random-walk style search is better performed by Lévy flights rather than simple random walk.\nThe pseudo-code can be summarized as:\n\nObjective function: \nGenerate an initial population of  host nests; \nWhile (t<MaxGeneration) or (stop criterion)\n   Get a cuckoo randomly (say, i) and replace its solution by performing Lévy flights;\n   Evaluate its quality/fitness \n         [For maximization,  ];\n   Choose a nest among n (say, j) randomly;\n   if (),\n          Replace j by the new solution;\n   end if\n   A fraction () of the worse nests are abandoned and new ones are built;\n   Keep the best solutions/nests;\n   Rank the solutions/nests and find the current best;\n   Pass the current best solutions to the next generation;\nend while\n\nAn important advantage of this algorithm is its simplicity. In fact, comparing with other population- or agent-based metaheuristic algorithms such as particle swarm optimization and harmony search, there is essentially only a single parameter  in CS (apart from the population size ). Therefore, it is very easy to implement.","name":"Cuckoo search","categories":["All articles needing additional references","Articles needing additional references from June 2015","Evolutionary algorithms","Optimization algorithms and methods","Pages using citations with accessdate and no URL"],"tag_line":"Cuckoo search (CS) is an optimization algorithm developed by Xin-she Yang and Suash Deb in 2009."}}
,{"_index":"throwtable","_type":"algorithm","_id":"eagle-strategy","_score":0,"_source":{"description":"Eagle strategy is a search strategy for solving nonlinear optimization problems, and this strategy was developed by Xin-she Yang and Suash Deb, based on the foraging behaviour of eagle species such as golden eagles.\nIn optimization, a common strategy is to search for the optimal solution starting from a set of initial guess solutions (either random and educated guess). In the case when the cost functions are multimodal with multiple local best solutions, the final solutions may heavily depend on the initial starting solutions. In order to minimize such dependence on initial random solutions, most modern algorithms, especially metaheuristic algorithms, are able to escape local optima by using some sophisticated random techniques. However, most of these algorithms are one-stage type; that is, once initialization is done, the search process continues until an algorithm stops. Running an algorithm many times from different initial solutions may occasionally improve the overall performance on average.\nEagle strategy improves this by using an iterative, interacting two-stage strategy to enhance the search efficiency by escaping the local optima and use initial solutions in different regions. It uses a slow search stage and a fast stage to simulate an eagle searching for prey tends to search on a large area and then quickly switches to a rapid chasing phase once a prey is in sight. In optimization, it uses a coarse search stage on a larger area in a search space in combination with an intensive faster search algorithm in the neighbourhood of promising solutions. Two stages interchanges and proceed iteratively.\nAs there are two stages in the strategy, each stage can employ different algorithms. For example, differential evolution can be used within eagle strategy. Studies show that such a combination is better than any of its components.\nIn the simplest case, when the first stage does not use any algorithm (just initialization), it essentially degenerates into a hill-climbing with random restart. However, this strategy could be potentially much more powerful if a good combination of different algorithms is used.","name":"Eagle strategy","categories":["Artificial intelligence","Evolutionary algorithms","Optimization algorithms and methods"],"tag_line":"Eagle strategy is a search strategy for solving nonlinear optimization problems, and this strategy was developed by Xin-she Yang and Suash Deb, based on the foraging behaviour of eagle species such as golden eagles."}}
,{"_index":"throwtable","_type":"algorithm","_id":"algorithms-for-recovery-and-isolation-exploiting-semantics","_score":0,"_source":{"description":"In computer science, Algorithms for Recovery and Isolation Exploiting Semantics, or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; it is used by IBM DB2, Microsoft SQL Server and many other database systems.\nThree main principles lie behind ARIES\nWrite ahead logging: Any change to an object is first recorded in the log, and the log must be written to stable storage before changes to the object are written to disk.\nRepeating history during Redo: On restart after a crash, ARIES retraces the actions of a database before the crash and brings the system back to the exact state that it was in before the crash. Then it undoes the transactions still active at crash time.\nLogging changes during Undo: Changes made to the database while undoing transactions are logged to ensure such an action isn't repeated in the event of repeated restarts.","name":"Algorithms for Recovery and Isolation Exploiting Semantics","categories":["All articles lacking in-text citations","Articles lacking in-text citations from March 2013","Database algorithms"],"tag_line":"In computer science, Algorithms for Recovery and Isolation Exploiting Semantics, or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; it is used by IBM DB2, Microsoft SQL Server and many other database systems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"multifactor-dimensionality-reduction","_score":0,"_source":{"description":"Multifactor dimensionality reduction (MDR) is a data mining approach for detecting and characterizing combinations of attributes or independent variables that interact to influence a dependent or class variable. MDR was designed specifically to identify interactions among discrete variables that influence a binary outcome and is considered a nonparametric alternative to traditional statistical methods such as logistic regression.\nThe basis of the MDR method is a constructive induction algorithm that converts two or more variables or attributes to a single attribute. This process of constructing a new attribute changes the representation space of the data. The end goal is to create or discover a representation that facilitates the detection of nonlinear or nonadditive interactions among the attributes such that prediction of the class variable is improved over that of the original representation of the data.","name":"Multifactor dimensionality reduction","categories":["All articles lacking in-text citations","All articles with unsourced statements","Articles lacking in-text citations from November 2010","Articles with unsourced statements from December 2010","Classification algorithms","Data mining","Dimension reduction","Use dmy dates from December 2010"],"tag_line":"Multifactor dimensionality reduction (MDR) is a data mining approach for detecting and characterizing combinations of attributes or independent variables that interact to influence a dependent or class variable."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rules-extraction-system-family","_score":0,"_source":{"description":"Rules Extraction System (RULES) is one family of inductive learning that include several covering algorithms. This family is used to build a predictive model based on given observation. It works based on the concept of separate-and-conquer to directly induce rules from a given training set and build its knowledge repository.\nAlgorithms under RULES family are usually available in data mining tools, such as KEEL and WEKA, known for knowledge extraction and decision making.","name":"Rules Extraction System Family","categories":["All orphaned articles","Classification algorithms","Knowledge engineering","Orphaned articles from February 2015"],"tag_line":"Rules Extraction System (RULES) is one family of inductive learning that include several covering algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shepp–logan-phantom","_score":0,"_source":{"description":"The Shepp–Logan phantom is a standard test image created by Larry Shepp and Benjamin F. Logan for their 1974 paper The Fourier Reconstruction of a Head Section. It serves as the model of a human head in the development and testing of image reconstruction algorithms.","name":"Shepp–Logan phantom","categories":["1974 works","Algorithms and data structures stubs","All stub articles","Computer science stubs","Image processing","Test items"],"tag_line":"The Shepp–Logan phantom is a standard test image created by Larry Shepp and Benjamin F. Logan for their 1974 paper The Fourier Reconstruction of a Head Section."}}
,{"_index":"throwtable","_type":"algorithm","_id":"evolutionary-algorithm-for-landmark-detection","_score":0,"_source":{"description":"there are several algorithms for locating landmarks in images such as satellite maps, medical images etc.\nnowadays evolutionary algorithms such as particle swarm optimization are so useful to perform this task. evolutionary algorithms generally have two phase, training and test.\nin the training phase, we try to learn the algorithm to locate landmark correctly. this phase performs in some iterations and finally in the last iteration we hope to obtain a system that can locate the landmark, correctly. in the particle swarm optimization there are some particles that search for the landmark. each particle uses a specific formula in each iteration to optimizes the landmark detecting.\nThe fundamental particle swarm optimization algorithm used in training phase generally as follows:\nRandomly initialise 100 individuals in the search space in the range [-1,1]\nLOOP UNTIL 100 iterations performed OR detection error of gbest is 0%\nFOR each particle p\nDetection errors at x = 0\nFOR each image i in training set\nFOR each pixel coordinate c in i\nEvaluate x of p on visual features at c\nIF evaluation is highest so far for i THEN\nDetected position in i = c\nIF distance between detected position and marked-up position > 2mm THEN\nDetection errors at x = Detection errors at x + 1\nFitness of p at x = 1- ( Detection errors at x /Total no. of images in training set)\nIF new _tness of p at x > previous _tness of p at pbest THEN\npbest _tness of p = new _tness of p at x\npbest position of p = x of p\nIF new _tness of p at x > previous gbest _tness THEN\ngbest _tness = new _tness of p at x\ngbest position of p = x of p\nFOR each particle p\nCalculate v of p\nIF magnitude of v > v max THEN\nMagnitude of v = v max\nMove x of p to next position using v\nIF x of p outside [-1,1] range THEN\nx of p = -1 or 1 as appropriate\nREPEAT\nOutput gbest of last iteration as trained detector d","name":"Evolutionary Algorithm for Landmark Detection","categories":["All articles needing cleanup","All articles needing expert attention","All orphaned articles","Articles needing cleanup from December 2010","Articles needing expert attention from December 2010","Articles needing expert attention with no reason or talk parameter","Articles needing unspecified expert attention","Cleanup tagged articles without a reason field from December 2010","Evolutionary algorithms","Orphaned articles from November 2010","Wikipedia pages needing cleanup from December 2010"],"tag_line":"there are several algorithms for locating landmarks in images such as satellite maps, medical images etc."}}
,{"_index":"throwtable","_type":"algorithm","_id":"geometric-modeling","_score":0,"_source":{"description":"Geometric modeling is a branch of applied mathematics and computational geometry that studies methods and algorithms for the mathematical description of shapes.\nThe shapes studied in geometric modeling are mostly two- or three-dimensional, although many of its tools and principles can be applied to sets of any finite dimension. Today most geometric modeling is done with computers and for computer-based applications. Two-dimensional models are important in computer typography and technical drawing. Three-dimensional models are central to computer-aided design and manufacturing (CAD/CAM), and widely used in many applied technical fields such as civil and mechanical engineering, architecture, geology and medical image processing.\nGeometric models are usually distinguished from procedural and object-oriented models, which define the shape implicitly by an opaque algorithm that generates its appearance. They are also contrasted with digital images and volumetric models which represent the shape as a subset of a fine regular partition of space; and with fractal models that give an infinitely recursive definition of the shape. However, these distinctions are often blurred: for instance, a digital image can be interpreted as a collection of colored squares; and geometric shapes such as circles are defined by implicit mathematical equations. Also, a fractal model yields a parametric or implicit model when its recursive definition is truncated to a finite depth.\nNotable awards of the area are the John A. Gregory Memorial Award and the Bezier award.","name":"Geometric modeling","categories":["All articles needing additional references","All articles with unsourced statements","All stub articles","Applied mathematics stubs","Articles needing additional references from August 2014","Articles with unsourced statements from August 2014","Computer-aided design","Geometric algorithms"],"tag_line":"Geometric modeling is a branch of applied mathematics and computational geometry that studies methods and algorithms for the mathematical description of shapes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"centroidal-voronoi-tessellation","_score":0,"_source":{"description":"In geometry, a centroidal Voronoi tessellation (CVT) is a special type of Voronoi tessellation or Voronoi diagrams. A Voronoi tessellation is called centroidal when the generating point of each Voronoi cell is also its mean (center of mass). It can be viewed as an optimal partition corresponding to an optimal distribution of generators. A number of algorithms can be used to generate centroidal Voronoi tessellations, including Lloyd's algorithm for K-means clustering.\nGersho's conjecture, proven for one and two dimensions, says that \"asymptotically speaking, all cells of the optimal CVT, while forming a tessellation, are congruent to a basic cell which depends on the dimension.\" In two dimensions, the basic cell for the optimal CVT is a regular hexagon.\nCentroidal Voronoi tessellations are useful in data compression, optimal quadrature, optimal quantization, clustering, and optimal mesh generation. Many patterns seen in nature are closely approximated by a Centroidal Voronoi tessellation. Examples of this include the Giant's Causeway, the cells of the cornea, and the breeding pits of the male tilapia.\nA weighted centroidal Voronoi diagrams is a CVT in which each centroid is weighted according to a certain function. For example, a grayscale image can be used as a density function to weight the points of a CVT, as a way to create digital stippling.","name":"Centroidal Voronoi tessellation","categories":["CS1 maint: Explicit use of et al.","Diagrams","Discrete geometry","Geometric algorithms"],"tag_line":"In geometry, a centroidal Voronoi tessellation (CVT) is a special type of Voronoi tessellation or Voronoi diagrams."}}
,{"_index":"throwtable","_type":"algorithm","_id":"closest-pair-of-points-problem","_score":0,"_source":{"description":"The closest pair of points problem or closest pair problem is a problem of computational geometry: given n points in metric space, find a pair of points with the smallest distance between them. The closest pair problem for points in the Euclidean plane was among the first geometric problems which were treated at the origins of the systematic study of the computational complexity of geometric algorithms.\nA naive algorithm of finding distances between all pairs of points and selecting the minimum requires O(dn2) time. It turns out that the problem may be solved in O(n log n) time in a Euclidean space or Lp space of fixed dimension d. In the algebraic decision tree model of computation, the O(n log n) algorithm is optimal. The optimality follows from the observation that the element uniqueness problem (with the lower bound of Ω(n log n) for time complexity) is reducible to the closest pair problem: checking whether the minimal distance is 0 after the solving of the closest pair problem answers the question whether there are two coinciding points.\nIn the computational model which assumes that the floor function is computable in constant time the problem can be solved in O(n log log n) time. If we allow randomization to be used together with the floor function, the problem can be solved in O(n) time.","name":"Closest pair of points problem","categories":["All articles with unsourced statements","Articles with example pseudocode","Articles with unsourced statements from October 2015","Geometric algorithms"],"tag_line":"The closest pair of points problem or closest pair problem is a problem of computational geometry: given n points in metric space, find a pair of points with the smallest distance between them."}}
,{"_index":"throwtable","_type":"algorithm","_id":"evolutionary-programming","_score":0,"_source":{"description":"Evolutionary programming is one of the four major evolutionary algorithm paradigms. It is similar to genetic programming, but the structure of the program to be optimized is fixed, while its numerical parameters are allowed to evolve.\nIt was first used by Lawrence J. Fogel in the US in 1960 in order to use simulated evolution as a learning process aiming to generate artificial intelligence. Fogel used finite state machines as predictors and evolved them. Currently evolutionary programming is a wide evolutionary computing dialect with no fixed structure or (representation), in contrast with some of the other dialects. It is becoming harder to distinguish from evolutionary strategies.\nIts main variation operator is mutation; members of the population are viewed as part of a specific species rather than members of the same species therefore each parent generates an offspring, using a (μ + μ) survivor selection.","name":"Evolutionary programming","categories":["All stub articles","Computer science stubs","Evolutionary algorithms","Optimization algorithms and methods"],"tag_line":"Evolutionary programming is one of the four major evolutionary algorithm paradigms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"neuroevolution","_score":0,"_source":{"description":"Neuroevolution, or neuro-evolution, is a form of machine learning that uses evolutionary algorithms to train artificial neural networks. It is most commonly applied in artificial life, computer games, and evolutionary robotics. A main benefit is that neuroevolution can be applied more widely than supervised learning algorithms, which require a syllabus of correct input-output pairs. In contrast, neuroevolution requires only a measure of a network's performance at a task. For example, the outcome of a game (i.e. whether one player won or lost) can be easily measured without providing labeled examples of desired strategies.","name":"Neuroevolution","categories":["Evolutionary algorithms"],"tag_line":"Neuroevolution, or neuro-evolution, is a form of machine learning that uses evolutionary algorithms to train artificial neural networks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"natural-evolution-strategy","_score":0,"_source":{"description":"Natural evolution strategies (NES) are a family of numerical optimization algorithms for black-box problems. Similar in spirit to evolution strategies, they iteratively update the (continuous) parameters of a search distribution by following the natural gradient towards higher expected fitness.\n\n","name":"Natural evolution strategy","categories":["All articles lacking in-text citations","Articles lacking in-text citations from March 2015","Articles with example pseudocode","Evolutionary algorithms","Optimization algorithms and methods","Stochastic optimization"],"tag_line":"Natural evolution strategies (NES) are a family of numerical optimization algorithms for black-box problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cache-oblivious-matrix-multiplication","_score":0,"_source":{"description":"Rectangular matrices can be multiplied when the size of the cache and cache line is not known to the algorithm, or cache-obliviously. Cache-oblivious matrix multiplication was first formalized by Harald Prokop.","name":"Cache-oblivious matrix multiplication","categories":["Analysis of algorithms","Cache (computing)","External memory algorithms","Matrix multiplication algorithms","Models of computation"],"tag_line":"Rectangular matrices can be multiplied when the size of the cache and cache line is not known to the algorithm, or cache-obliviously."}}
,{"_index":"throwtable","_type":"algorithm","_id":"leonidas-j.-guibas","_score":0,"_source":{"description":"Leonidas John Guibas (Greek: Λεωνίδας Γκίμπας) is a professor of computer science at Stanford University, where he heads the geometric computation group and is a member of the computer graphics and artificial intelligence laboratories. Guibas was a student of Donald Knuth at Stanford, where he received his Ph.D. in 1976. He has worked for several industrial research laboratories, and joined the Stanford faculty in 1984. He was program chair for the ACM Symposium on Computational Geometry in 1996, is a Fellow of the ACM and the IEEE, and was awarded the ACM - AAAI Allen Newell Award for 2007 “for his pioneering contributions in applying algorithms to a wide range of computer science disciplines.“ He has Erdős number 2 due to his collaborations with Boris Aronov, Andrew Odlyzko, János Pach, Richard M. Pollack, Endre Szemerédi, and Frances Yao. The research contributions he is known for include finger trees, red-black trees, fractional cascading, the Guibas–Stolfi algorithm for Delaunay triangulation, an optimal data structure for point location, the quad-edge data structure for representing planar subdivisions, Metropolis light transport, and kinetic data structures for keeping track of objects in motion.","name":"Leonidas J. Guibas","categories":["American computer scientists","Articles containing Greek-language text","Fellow Members of the IEEE","Fellows of the Association for Computing Machinery","Greek computer scientists","Living people","Researchers in geometric algorithms","Stanford University Department of Computer Science faculty","Stanford University School of Engineering faculty","Stanford University alumni","Year of birth missing (living people)"],"tag_line":"Leonidas John Guibas (Greek: Λεωνίδας Γκίμπας) is a professor of computer science at Stanford University, where he heads the geometric computation group and is a member of the computer graphics and artificial intelligence laboratories."}}
,{"_index":"throwtable","_type":"algorithm","_id":"minkowski-portal-refinement","_score":0,"_source":{"description":"The Minkowski Portal Refinement collision detection algorithm is a technique for determining whether two convex shapes overlap.\nThe algorithm was created by Gary Snethen in 2006 and was first published in Game Programming Gems 7. The algorithm was used in Tomb Raider: Underworld and other games created by Crystal Dynamics and its sister studios within Eidos Interactive.\nMPR, like its cousin GJK, relies on shapes that are defined using support mappings. This allows the algorithm to support a limitless variety of shapes that are problematic for other algorithms. Support mappings require only a single mathematical function to represent a point, line segment, disc, cylinder, cone, ellipsoid, football, bullet, frustum or most any other common convex shape. Once a set of basic primitives have been created, they can easily be combined with one another using operations such as sweep, shrink-wrap and affine transformation.\nUnlike GJK, MPR does not provide the shortest distance between separated shapes. However, according to its author, MPR is simpler, more numerically robust and handles translational sweeping with very little modification. This makes it well-suited for games and other real-time applications.","name":"Minkowski Portal Refinement","categories":["All stub articles","Convex geometry","Geometric algorithms","Geometry stubs"],"tag_line":"The Minkowski Portal Refinement collision detection algorithm is a technique for determining whether two convex shapes overlap."}}
,{"_index":"throwtable","_type":"algorithm","_id":"jts-topology-suite","_score":0,"_source":{"description":"The Java Topology Suite (JTS) is an open source Java software library that provides an object model for Euclidean planar linear geometry together with a set of fundamental geometric functions. JTS is primarily intended to be used as a core component of vector-based geomatics software such as geographical information systems. It can also be used as a general-purpose library providing algorithms in computational geometry.\nJTS implements the geometry model and API defined in the OpenGIS Consortium Simple Features Specification for SQL.\nJTS defines a standards-compliant geometry system for building spatial applications; examples include viewers, spatial query processors, and tools for performing data validation, cleaning and integration. In addition to the Java library, the foundations of JTS and selected functions are maintained in a C++ port, for use in C-style linking on all major operating systems, in the form of the GEOS software library.\nJTS, and the GEOS port, are published under the GNU Lesser General Public License (LGPL).\n^ \"The 2012 Free and Open Source GIS Software Map – A Guide to facilitate Research, Development and Adoption\", S. Steiniger and A.J.S. Hunter\n^ \"Secrets of the JTS Topology Suite - M. Davis\" (PDF). Retrieved 2013-05-27.","name":"JTS Topology Suite","categories":["Application programming interfaces","Free software programmed in Java (programming language)","Geometric algorithms"],"tag_line":"The Java Topology Suite (JTS) is an open source Java software library that provides an object model for Euclidean planar linear geometry together with a set of fundamental geometric functions."}}
,{"_index":"throwtable","_type":"algorithm","_id":"roam","_score":0,"_source":{"description":"Real-time optimally adapting mesh (ROAM), is a continuous level of detail algorithm that optimizes terrain meshes. On modern computers, sometimes it is more effective to send a small amount of unneeded polygons to the GPU, rather than burden the CPU with LOD (Level of Detail) calculations—making algorithms like geomipmapping more effective than ROAM. This technique is used by graphics programmers in order to produce high quality displays while being able to maintain real-time frame rates. Algorithms such as ROAM exist to provide a control over scene quality versus performance in order to provide HQ scenes while retaining real-time frame rates on hardware. ROAM largely aims toward terrain visualization, but various elements from ROAM are difficult to place within a game system.\nTo assist regional geological mapping, more abundant and visualized expression forms are highly needs. Thus, the 3D terrain model is adopted as the carrier for the demands in many correlative fields. Based on the regular grid DEM (Digital Elevation Model) in DRGS, ROAM algorithm is applied to create a more dynamic model, which will give consideration to the importance of different features and select correspondence level of detail.","name":"ROAM","categories":["All stub articles","Computer graphics algorithms","Computer graphics stubs","Computer science stubs"],"tag_line":"Real-time optimally adapting mesh (ROAM), is a continuous level of detail algorithm that optimizes terrain meshes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"xiaolin-wu's-line-algorithm","_score":0,"_source":{"description":"Xiaolin Wu's line algorithm is an algorithm for line antialiasing, which was presented in the article An Efficient Antialiasing Technique in the July 1991 issue of Computer Graphics, as well as in the article Fast Antialiasing in the June 1992 issue of Dr. Dobb's Journal.\nBresenham's algorithm draws lines extremely quickly, but it does not perform anti-aliasing. In addition, it cannot handle any cases where the line endpoints do not lie exactly on integer points of the pixel grid. A naive approach to anti-aliasing the line would take an extremely long time. Wu's algorithm is comparatively fast, but is still slower than Bresenham's algorithm. The algorithm consists of drawing pairs of pixels straddling the line, each coloured according to its distance from the line. Pixels at the line ends are handled separately. Lines less than one pixel long are handled as a special case.\nAn extension to the algorithm for circle drawing was presented by Xiaolin Wu in the book Graphics Gems II. Just like the line drawing algorithm is a replacement for Bresenham's line drawing algorithm, the circle drawing algorithm is a replacement for Bresenham's circle drawing algorithm.","name":"Xiaolin Wu's line algorithm","categories":["All articles lacking in-text citations","Articles lacking in-text citations from January 2013","Articles with example pseudocode","Computer graphics algorithms"],"tag_line":"Xiaolin Wu's line algorithm is an algorithm for line antialiasing, which was presented in the article An Efficient Antialiasing Technique in the July 1991 issue of Computer Graphics, as well as in the article Fast Antialiasing in the June 1992 issue of Dr. Dobb's Journal."}}
,{"_index":"throwtable","_type":"algorithm","_id":"warnock-algorithm","_score":0,"_source":{"description":"The Warnock algorithm is a hidden surface algorithm invented by John Warnock that is typically used in the field of computer graphics. It solves the problem of rendering a complicated image by recursive subdivision of a scene until areas are obtained that are trivial to compute. In other words, if the scene is simple enough to compute efficiently then it is rendered; otherwise it is divided into smaller parts which are likewise tested for simplicity.\nThis is a divide and conquer algorithm with run-time of , where n is the number of polygons and p is the number of pixels in the viewport.\nThe inputs are a list of polygons and a viewport. The best case is that if the list of polygons is simple, then draw the polygons in the viewport. Simple is defined as one polygon (then the polygon or its part is drawn in appropriate part of a viewport) or a viewport that is one pixel in size (then that pixel gets a color of the polygon closest to the observer). The continuous step is to split the viewport into 4 equally sized quadrants and to recursively call the algorithm for each quadrant, with a polygon list modified such that it only contains polygons that are visible in that quadrant.","name":"Warnock algorithm","categories":["All stub articles","Computer graphics algorithms","Computer programming stubs"],"tag_line":"The Warnock algorithm is a hidden surface algorithm invented by John Warnock that is typically used in the field of computer graphics."}}
,{"_index":"throwtable","_type":"algorithm","_id":"digital-differential-analyzer-(graphics-algorithm)","_score":0,"_source":{"description":"In computer graphics, a digital differential analyzer (DDA) is hardware or software used for linear interpolation of variables over an interval between start and end point. DDAs are used for rasterization of lines, triangles and polygons. In its simplest implementation, the DDA algorithm interpolates values in interval by computing for each xi the equations xi = xi−1+1/m, yi = yi−1 + m, where Δx = xend − xstart and Δy = yend − ystart and m = Δy/Δx","name":"Digital differential analyzer (graphics algorithm)","categories":["All articles lacking in-text citations","Articles lacking in-text citations from June 2011","Articles with example C code","Computer graphics algorithms","Digital geometry"],"tag_line":"In computer graphics, a digital differential analyzer (DDA) is hardware or software used for linear interpolation of variables over an interval between start and end point."}}
,{"_index":"throwtable","_type":"algorithm","_id":"list-of-numerical-computational-geometry-topics","_score":0,"_source":{"description":"List of numerical computational geometry topics enumerates the topics of computational geometry that deals with geometric objects as continuous entities and applies methods and algorithms of nature characteristic to numerical analysis. This area is also called \"machine geometry\", computer-aided geometric design, and geometric modelling.\nSee List of combinatorial computational geometry topics for another flavor of computational geometry that states problems in terms of geometric objects as discrete entities and hence the methods of their solution are mostly theories and algorithms of combinatorial character.","name":"List of numerical computational geometry topics","categories":["Geometric algorithms","Mathematics-related lists"],"tag_line":"List of numerical computational geometry topics enumerates the topics of computational geometry that deals with geometric objects as continuous entities and applies methods and algorithms of nature characteristic to numerical analysis."}}
,{"_index":"throwtable","_type":"algorithm","_id":"simulated-annealing","_score":0,"_source":{"description":"Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic for approximate global optimization in a large search space. It is often used when the search space is discrete (e.g., all tours that visit a given set of cities). For problems where finding the precise global optimum is less important than finding an acceptable global optimum in a fixed amount of time, simulated annealing may be preferable to alternatives such as brute-force search or gradient descent.\nThe name and inspiration come from annealing in metallurgy, a technique involving heating and controlled cooling of a material to increase the size of its crystals and reduce defects. Both are attributes of the material that depend on its thermodynamic free energy. Heating and cooling the material affects both the temperature and the thermodynamic free energy. While the same amount of cooling brings the same decrease in temperature, the rate of cooling dictates the magnitude of decrease in the thermodynamic free energy, with slower cooling producing a bigger decrease. Simulated annealing interprets slow cooling as a slow decrease in the probability of accepting worse solutions as it explores the solution space. Accepting worse solutions is a fundamental property of metaheuristics because it allows for a more extensive search for the optimal solution.\nThe method was independently described by Scott Kirkpatrick, C. Daniel Gelatt and Mario P. Vecchi in 1983, and by Vlado Černý in 1985. The method is an adaptation of the Metropolis–Hastings algorithm, a Monte Carlo method to generate sample states of a thermodynamic system, invented by M.N. Rosenbluth and published by N. Metropolis et al. in 1953.","name":"Simulated annealing","categories":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from December 2009","Articles with inconsistent citation formats","Articles with unsourced statements from June 2011","Heuristic algorithms","Monte Carlo methods","Optimization algorithms and methods"],"tag_line":"Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dynamic-time-warping","_score":0,"_source":{"description":"In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences which may vary in time or speed. For instance, similarities in walking patterns could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data — indeed, any data which can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. Also it is seen that it can be used in partial shape matching application.\nIn general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restrictions. The sequences are \"warped\" non-linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. This sequence alignment method is often used in time series classification. Although DTW measures a distance-like quantity between two given sequences, it doesn't guarantee the triangle inequality to hold.\n\n","name":"Dynamic time warping","categories":["Articles with example pseudocode","CS1 maint: Explicit use of et al.","Dynamic programming","Machine learning algorithms","Time series analysis"],"tag_line":"In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences which may vary in time or speed."}}
,{"_index":"throwtable","_type":"algorithm","_id":"t-distributed-stochastic-neighbor-embedding","_score":0,"_source":{"description":"t-distributed stochastic neighbor embedding (t-SNE) is a machine learning algorithm for dimensionality reduction developed by Laurens van der Maaten and Geoffrey Hinton. It is a nonlinear dimensionality reduction technique that is particularly well suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points.\nThe t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an infinitesimal probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence between the two distributions with respect to the locations of the points in the map. Note that whilst the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate.\nt-SNE has been used in a wide range of applications, including computer security research, music analysis, cancer research, and bioinformatics.","name":"T-distributed stochastic neighbor embedding","categories":["Machine learning algorithms"],"tag_line":"t-distributed stochastic neighbor embedding (t-SNE) is a machine learning algorithm for dimensionality reduction developed by Laurens van der Maaten and Geoffrey Hinton."}}
,{"_index":"throwtable","_type":"algorithm","_id":"kernel-methods-for-vector-output","_score":0,"_source":{"description":"Kernel methods are a well-established tool to analyze the relationship between input data and the corresponding output of a function. Kernels encapsulate the properties of functions in a computationally efficient way and allow algorithms to easily swap functions of varying complexity.\nIn typical machine learning algorithms, these functions produce a scalar output. Recent development of kernel methods for functions with vector-valued output is due, at least in part, to interest in simultaneously solving related problems. Kernels which capture the relationship between the problems allow them to borrow strength from each other. Algorithms of this type include multi-task learning (also called multi-output learning or vector-valued learning), transfer learning, and co-kriging. Multi-label classification can be interpreted as mapping inputs to (binary) coding vectors with length equal to the number of classes.\nIn Gaussian processes, kernels are called covariance functions. Multiple-output functions correspond to considering multiple processes. See Bayesian interpretation of regularization for the connection between the two perspectives.","name":"Kernel methods for vector output","categories":["Kernel methods for machine learning","Machine learning algorithms"],"tag_line":"Kernel methods are a well-established tool to analyze the relationship between input data and the corresponding output of a function."}}
,{"_index":"throwtable","_type":"algorithm","_id":"heuristiclab","_score":0,"_source":{"description":"HeuristicLab   is a software environment for heuristic and evolutionary algorithms, developed by members of the Heuristic and Evolutionary Algorithm Laboratory (HEAL) at the University of Applied Sciences Upper Austria, Campus Hagenberg. HeuristicLab has a strong focus on providing a graphical user interface so that users are not required to have comprehensive programming skills to adjust and extend the algorithms for a particular problem. In HeuristicLab algorithms are represented as operator graphs and changing or rearranging operators can be done by drag-and-drop without actually writing code. The software thereby tries to shift algorithm development capability from the software engineer to the user and practitioner. Developers can still extend the functionality on code level and can use HeuristicLab's plug-in mechanism that allows them to integrate custom algorithms, solution representations or optimization problems.","name":"HeuristicLab","categories":["Heuristic algorithms"],"tag_line":"HeuristicLab   is a software environment for heuristic and evolutionary algorithms, developed by members of the Heuristic and Evolutionary Algorithm Laboratory (HEAL) at the University of Applied Sciences Upper Austria, Campus Hagenberg."}}
,{"_index":"throwtable","_type":"algorithm","_id":"roberto-tamassia","_score":0,"_source":{"description":"Roberto Tamassia is a computer scientist, the Plastech Professor of Computer Science at Brown University, and since 2007 has been chair of the Brown Computer Science department. His research specialty is in the design and analysis of algorithms for graph drawing, computational geometry, and computer security; he is also the author of several textbooks.","name":"Roberto Tamassia","categories":["American computer scientists","Brown University faculty","Fellow Members of the IEEE","Fellows of the American Association for the Advancement of Science","Fellows of the Association for Computing Machinery","Graph drawing people","Italian computer scientists","Living people","People associated with computer security","Researchers in geometric algorithms","University of Illinois at Urbana–Champaign alumni","Year of birth missing (living people)"],"tag_line":"Roberto Tamassia is a computer scientist, the Plastech Professor of Computer Science at Brown University, and since 2007 has been chair of the Brown Computer Science department."}}
,{"_index":"throwtable","_type":"algorithm","_id":"john-reif","_score":0,"_source":{"description":"John H. Reif (born 1951) is an American academic, and Professor of Computer Science at Duke University, who has made contributions to large number of fields in computer science: ranging from algorithms and computational complexity theory to robotics and to game theory.","name":"John Reif","categories":["1951 births","American academics","DNA nanotechnology","Duke University faculty","Fellow Members of the IEEE","Fellows of the American Association for the Advancement of Science","Fellows of the Association for Computing Machinery","Harvard University alumni","Harvard University faculty","Living people","Researchers in geometric algorithms","Theoretical computer scientists","Tufts University alumni"],"tag_line":"John H. Reif (born 1951) is an American academic, and Professor of Computer Science at Duke University, who has made contributions to large number of fields in computer science: ranging from algorithms and computational complexity theory to robotics and to game theory."}}
,{"_index":"throwtable","_type":"algorithm","_id":"manifold-alignment","_score":0,"_source":{"description":"Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. The concept was first introduced as such by Ham, Lee, and Saul in 2003, adding a manifold constraint to the general problem of correlating sets of high-dimensional vectors.","name":"Manifold alignment","categories":["Artificial intelligence","Machine learning algorithms"],"tag_line":"Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold."}}
,{"_index":"throwtable","_type":"algorithm","_id":"weighted-majority-algorithm","_score":0,"_source":{"description":"In machine learning, Weighted Majority Algorithm (WMA) is a meta-learning algorithm used to construct a compound algorithm from a pool of prediction algorithms, which could be any type of learning algorithms, classifiers, or even real human experts. The algorithm assumes that we have no prior knowledge about the accuracy of the algorithms in the pool, but there are sufficient reasons to believe that one or more will perform well.\nAssume that the problem is a binary decision problem. To construct the compound algorithm, a positive weight is given to each of the algorithms in the pool. The compound algorithm then collects weighted votes from all the algorithms in the pool, and gives the prediction that has a higher vote. If the compound algorithm makes a mistake, the algorithms in the pool that contributed to the wrong predicting will be discounted by a certain ratio β where 0<β<1.\nIt can be shown that the upper bounds on the number of mistakes made in a given sequence of predictions from a pool of algorithms  is\n\nif one algorithm in  makes at most  mistakes.\nThere are many variations of the Weighted Majority Algorithm to handle different situations, like shifting targets, infinite pools, or randomized predictions. The core mechanism remain similar, with the final performances of the compound algorithm bounded by a function of the performance of the specialist (best performing algorithm) in the pool.","name":"Weighted Majority Algorithm","categories":["Machine learning algorithms"],"tag_line":"In machine learning, Weighted Majority Algorithm (WMA) is a meta-learning algorithm used to construct a compound algorithm from a pool of prediction algorithms, which could be any type of learning algorithms, classifiers, or even real human experts."}}
,{"_index":"throwtable","_type":"algorithm","_id":"matrix-multiplication-algorithm","_score":0,"_source":{"description":"Because matrix multiplication is such a central operation in many numerical algorithms, much work has been invested in making matrix multiplication algorithms efficient. Applications of matrix multiplication in computational problems are found in many fields including scientific computing and pattern recognition and in seemingly unrelated problems such counting the paths through a graph. Many different algorithms have been designed for multiplying matrices on different types of hardware, including parallel and distributed systems, where the computational work is spread over multiple processors (perhaps over a network).\nDirectly applying the mathematical definition of matrix multiplication gives an algorithm that takes time on the order of n3 to multiply two n × n matrices (Θ(n3) in big O notation). Better asymptotic bounds on the time required to multiply matrices have been known since the work of Strassen in the 1960s, but it is still unknown what the optimal time is (i.e., what the complexity of the problem is).","name":"Matrix multiplication algorithm","categories":["All articles containing potentially dated statements","Articles containing potentially dated statements from 2010","Matrix multiplication algorithms","Unsolved problems in computer science"],"tag_line":"Because matrix multiplication is such a central operation in many numerical algorithms, much work has been invested in making matrix multiplication algorithms efficient."}}
,{"_index":"throwtable","_type":"algorithm","_id":"leaky-bucket","_score":0,"_source":{"description":"The leaky bucket is an algorithm used in packet switched computer networks and telecommunications networks. It can be used to check that data transmissions, in the form of packets, conform to defined limits on bandwidth and burstiness (a measure of the unevenness or variations in the traffic flow). It can also be used as a scheduling algorithm to determine the timing of transmissions that will comply with the limits set for the bandwidth and burstiness: see network scheduler. The leaky bucket algorithm is also used in leaky bucket counters, e.g. to detect when the average or peak rate of random or stochastic events or stochastic processes exceed defined limits.\nA version of the leaky bucket, the Generic Cell Rate Algorithm, is recommended for Asynchronous Transfer Mode (ATM) networks in Usage/Network Parameter Control at User–Network Interfaces or Inter-Network Interfaces or Network-Network Interfaces to protect a network from excessive traffic levels on connections routed through it. The Generic Cell Rate Algorithm, or an equivalent, may also be used to shape transmissions by a Network Interface Card onto an ATM network (i.e. on the user side of the User-Network Interface), e.g. to levels below the levels set for Usage/Network Parameter Control in the network to prevent it taking action to further limit that connection.","name":"Leaky bucket","categories":["Network scheduling algorithms"],"tag_line":"The leaky bucket is an algorithm used in packet switched computer networks and telecommunications networks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hierarchical-fair-service-curve","_score":0,"_source":{"description":"The hierarchical fair-service curve (HFSC) is a network scheduling algorithm for a network scheduler proposed by Ion Stoica, Hui Zhang and T. S. Eugene from Carnegie Mellon University at SIGCOMM 1997\nIt is based on a QoS and CBQ. An implementation of HFSC is available in all operating systems based on the Linux kernel, such as e.g. OpenWrt, and also in DD-WRT, NetBSD 5.0, FreeBSD 8.0 and OpenBSD 4.6.","name":"Hierarchical fair-service curve","categories":["All stub articles","Network performance","Network scheduling algorithms","Network software stubs"],"tag_line":"The hierarchical fair-service curve (HFSC) is a network scheduling algorithm for a network scheduler proposed by Ion Stoica, Hui Zhang and T. S. Eugene from Carnegie Mellon University at SIGCOMM 1997\nIt is based on a QoS and CBQ."}}
,{"_index":"throwtable","_type":"algorithm","_id":"delay-gradient-congestion-control","_score":0,"_source":{"description":"In computer networking, delay-gradient congestion control refers to a class of congestion control algorithms, which react to the differences in round-trip delay time (RTT), as opposed to classical congestion control methods, which react to packet loss or an RTT threshold being exceeded. Such algorithms include CAIA Delay-Gradient (CDG) and TIMELY.","name":"Delay-gradient congestion control","categories":["All stub articles","Computer network stubs","Network performance","Network scheduling algorithms"],"tag_line":"In computer networking, delay-gradient congestion control refers to a class of congestion control algorithms, which react to the differences in round-trip delay time (RTT), as opposed to classical congestion control methods, which react to packet loss or an RTT threshold being exceeded."}}
,{"_index":"throwtable","_type":"algorithm","_id":"linde–buzo–gray-algorithm","_score":0,"_source":{"description":"The Linde–Buzo–Gray algorithm (introduced by Yoseph Linde, Andrés Buzo and Robert M. Gray in 1980) is a vector quantization algorithm to derive a good codebook.\nIt is similar to the k-means method in data clustering.","name":"Linde–Buzo–Gray algorithm","categories":["Algorithms and data structures stubs","All articles lacking reliable references","All stub articles","Articles lacking reliable references from June 2012","Artificial neural networks","Computer science stubs","Data clustering algorithms","Machine learning algorithms"],"tag_line":"The Linde–Buzo–Gray algorithm (introduced by Yoseph Linde, Andrés Buzo and Robert M. Gray in 1980) is a vector quantization algorithm to derive a good codebook."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ancient-egyptian-multiplication","_score":0,"_source":{"description":"In mathematics, ancient Egyptian multiplication (also known as Egyptian multiplication, Ethiopian multiplication, Russian multiplication, or peasant multiplication), one of two multiplication methods used by scribes, was a systematic method for multiplying two numbers that does not require the multiplication table, only the ability to multiply and divide by 2, and to add. It decomposes one of the multiplicands (generally the larger) into a sum of powers of two and creates a table of doublings of the second multiplicand. This method may be called mediation and duplation, where mediation means halving one number and duplation means doubling the other number. It is still used in some areas.\nThe second Egyptian multiplication and division technique was known from the hieratic Moscow and Rhind Mathematical Papyri written in the seventeenth century B.C. by the scribe Ahmes.\nAlthough in ancient Egypt the concept of base 2 did not exist, the algorithm is essentially the same algorithm as long multiplication after the multiplier and multiplicand are converted to binary. The method as interpreted by conversion to binary is therefore still in wide use today as implemented by binary multiplier circuits in modern computer processors.","name":"Ancient Egyptian multiplication","categories":["All articles lacking in-text citations","All articles needing cleanup","Ancient Egyptian literature","Articles lacking in-text citations from February 2011","Articles needing cleanup from February 2011","Cleanup tagged articles without a reason field from February 2011","Egyptian fractions","Egyptian mathematics","Mathematics manuscripts","Multiplication","Number theoretic algorithms","Wikipedia pages needing cleanup from February 2011"],"tag_line":"In mathematics, ancient Egyptian multiplication (also known as Egyptian multiplication, Ethiopian multiplication, Russian multiplication, or peasant multiplication), one of two multiplication methods used by scribes, was a systematic method for multiplying two numbers that does not require the multiplication table, only the ability to multiply and divide by 2, and to add."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hindley–milner-type-system","_score":0,"_source":{"description":"In type theory and functional programming, Hindley–Milner (HM) (also known as Damas–Milner or Damas–Hindley–Milner) is a classical type system for the lambda calculus with parametric polymorphism, first described by J. Roger Hindley and later rediscovered by Robin Milner. Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.\nAmong HM's more notable properties is completeness and its ability to deduce the most general type of a given program without the need of any type annotations or other hints supplied by the programmer. Algorithm W is a fast algorithm, performing type inference in almost linear time with respect to the size of the source, making it practically usable to type large programs. HM is preferably used for functional languages. It was first implemented as part of the type system of the programming language ML. Since then, HM has been extended in various ways, most notably by constrained types as used in Haskell.","name":"Hindley–Milner type system","categories":["1969 in computer science","1978 in computer science","1985 in computer science","Algorithms","All accuracy disputes","Articles with disputed statements from October 2013","Formal methods","Lambda calculus","Theoretical computer science","Type inference","Type systems","Type theory"],"tag_line":"In type theory and functional programming, Hindley–Milner (HM) (also known as Damas–Milner or Damas–Hindley–Milner) is a classical type system for the lambda calculus with parametric polymorphism, first described by J. Roger Hindley and later rediscovered by Robin Milner."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pollard's-rho-algorithm-for-logarithms","_score":0,"_source":{"description":"Pollard's rho algorithm for logarithms is an algorithm introduced by John Pollard in 1978 to solve the discrete logarithm problem, analogous to Pollard's rho algorithm to solve the integer factorization problem.\nThe goal is to compute  such that , where  belongs to a cyclic group  generated by . The algorithm computes integers , , , and  such that . Assuming, for simplicity, that the underlying group is cyclic of order , we can calculate  as a solution of the equation .\nTo find the needed , , , and  the algorithm uses Floyd's cycle-finding algorithm to find a cycle in the sequence , where the function  is assumed to be random-looking and thus is likely to enter into a loop after approximately  steps. One way to define such a function is to use the following rules: Divide  into three disjoint subsets of approximately equal size: , , and . If  is in  then double both  and ; if  then increment , if  then increment .","name":"Pollard's rho algorithm for logarithms","categories":["Logarithms","Number theoretic algorithms"],"tag_line":"Pollard's rho algorithm for logarithms is an algorithm introduced by John Pollard in 1978 to solve the discrete logarithm problem, analogous to Pollard's rho algorithm to solve the integer factorization problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"round-robin-scheduling","_score":0,"_source":{"description":"Round-robin (RR) is one of the algorithms employed by process and network schedulers in computing. As the term is generally used, time slices are assigned to each process in equal portions and in circular order, handling all processes without priority (also known as cyclic executive). Round-robin scheduling is simple, easy to implement, and starvation-free. Round-robin scheduling can also be applied to other scheduling problems, such as data packet scheduling in computer networks. It is an Operating System concept.\nThe name of the algorithm comes from the round-robin principle known from other fields, where each person takes an equal share of something in turn.","name":"Round-robin scheduling","categories":["All articles needing additional references","Articles needing additional references from April 2015","CS1 errors: external links","Network scheduling algorithms","Processor scheduling algorithms"],"tag_line":"Round-robin (RR) is one of the algorithms employed by process and network schedulers in computing."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sardinas–patterson-algorithm","_score":0,"_source":{"description":"In coding theory, the Sardinas–Patterson algorithm is a classical algorithm for determining in polynomial time whether a given variable-length code is uniquely decodable, named after August Albert Sardinas and George W. Patterson, who published it in 1953. The algorithm carries out a systematic search for a string which admits two different decompositions into codewords. As Knuth reports, the algorithm was rediscovered about ten years later in 1963 by Floyd, despite the fact that it was at the time already well known in coding theory.","name":"Sardinas–Patterson algorithm","categories":["Algorithms","Coding theory","Data compression"],"tag_line":"In coding theory, the Sardinas–Patterson algorithm is a classical algorithm for determining in polynomial time whether a given variable-length code is uniquely decodable, named after August Albert Sardinas and George W. Patterson, who published it in 1953."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cipolla's-algorithm","_score":0,"_source":{"description":"In computational number theory, Cipolla's algorithm is a technique for solving a congruence of the form\n\nwhere , so n is the square of x, and where  is an odd prime. Here  denotes the finite field with  elements; . The algorithm is named after Michele Cipolla, an Italian mathematician who discovered it in 1907.","name":"Cipolla's algorithm","categories":["Articles containing proofs","Modular arithmetic","Number theoretic algorithms"],"tag_line":"In computational number theory, Cipolla's algorithm is a technique for solving a congruence of the form\n\nwhere , so n is the square of x, and where  is an odd prime."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gradient-method","_score":0,"_source":{"description":"In optimization, gradient method is an algorithm to solve problems of the form\n\nwith the search directions defined by the gradient of the function at the current point. Examples of gradient method are the gradient descent and the conjugate gradient.","name":"Gradient method","categories":["First order methods","Gradient methods","Numerical linear algebra","Optimization algorithms and methods"],"tag_line":"In optimization, gradient method is an algorithm to solve problems of the form\n\nwith the search directions defined by the gradient of the function at the current point."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sidi's-generalized-secant-method","_score":0,"_source":{"description":"Sidi's generalized secant method is a root-finding algorithm, that is, a numerical method for solving equations of the form  . The method was published by Avram Sidi.\nThe method is a generalization of the secant method. Like the secant method, it is an iterative method which requires one evaluation of  in each iteration and no derivatives of . The method can converge much faster though, with an order which approaches 2 provided that  satisfies the regularity conditions described below.\n^ Sidi, Avram, \"Generalization Of The Secant Method For Nonlinear Equations\", Applied Mathematics E-notes 8 (2008), 115–123, http://www.math.nthu.edu.tw/~amen/2008/070227-1.pdf","name":"Sidi's generalized secant method","categories":["Root-finding algorithms"],"tag_line":"Sidi's generalized secant method is a root-finding algorithm, that is, a numerical method for solving equations of the form  ."}}
,{"_index":"throwtable","_type":"algorithm","_id":"splitting-circle-method","_score":0,"_source":{"description":"In mathematics, the splitting circle method is a numerical algorithm for the numerical factorization of a polynomial and, ultimately, for finding its complex roots. It was introduced by Arnold Schönhage in his 1982 paper The fundamental theorem of algebra in terms of computational complexity (Technical report, Mathematisches Institut der Universität Tübingen). A revised algorithm was presented by Victor Pan in 1998. An implementation was provided by Xavier Gourdon in 1996 for the Magma and PARI/GP computer algebra systems.","name":"Splitting circle method","categories":["Root-finding algorithms"],"tag_line":"In mathematics, the splitting circle method is a numerical algorithm for the numerical factorization of a polynomial and, ultimately, for finding its complex roots."}}
,{"_index":"throwtable","_type":"algorithm","_id":"durand–kerner-method","_score":0,"_source":{"description":"In numerical analysis, the Durand–Kerner method, established 1960–66 and named after E. Durand and Immo Kerner, also called the method of Weierstrass, established 1859–91 and named after Karl Weierstrass, is a root-finding algorithm for solving polynomial equations. In other words, the method can be used to solve numerically the equation\nƒ(x) = 0\nwhere ƒ is a given polynomial, which can be taken to be scaled so that the leading coefficient is 1.","name":"Durand–Kerner method","categories":["CS1 maint: Explicit use of et al.","Root-finding algorithms"],"tag_line":"In numerical analysis, the Durand–Kerner method, established 1960–66 and named after E. Durand and Immo Kerner, also called the method of Weierstrass, established 1859–91 and named after Karl Weierstrass, is a root-finding algorithm for solving polynomial equations."}}
,{"_index":"throwtable","_type":"algorithm","_id":"inverse-quadratic-interpolation","_score":0,"_source":{"description":"In numerical analysis, inverse quadratic interpolation is a root-finding algorithm, meaning that it is an algorithm for solving equations of the form f(x) = 0. The idea is to use quadratic interpolation to approximate the inverse of f. This algorithm is rarely used on its own, but it is important because it forms part of the popular Brent's method.","name":"Inverse quadratic interpolation","categories":["Root-finding algorithms"],"tag_line":"In numerical analysis, inverse quadratic interpolation is a root-finding algorithm, meaning that it is an algorithm for solving equations of the form f(x) = 0."}}
,{"_index":"throwtable","_type":"algorithm","_id":"simultaneous-perturbation-stochastic-approximation","_score":0,"_source":{"description":"Simultaneous perturbation stochastic approximation (SPSA) is an algorithmic method for optimizing systems with multiple unknown parameters. It is a type of stochastic approximation algorithm. As an optimization method, it is appropriately suited to large-scale population models, adaptive modeling, simulation optimization, and atmospheric modeling. Many examples are presented at the SPSA website http://www.jhuapl.edu/SPSA. A comprehensive recent book on the subject is Bhatnagar et al. (2013). An early paper on the subject is Spall (1987) and the foundational paper providing the key theory and justification is Spall (1992).\nSPSA is a descent method capable of finding global minima, sharing this property with other methods as simulated annealing. Its main feature is the gradient approximation that requires only two measurements of the objective function, regardless of the dimension of the optimization problem. Recall that we want to find the optimal control  with loss function :\n\nBoth Finite Differences Stochastic Approximation (FDSA) and SPSA use the same iterative process:\n\nwhere  represents the  iterate,  is the estimate of the gradient of the objective function  evaluated at , and  is a positive number sequence converging to 0. If  is a p-dimensional vector, the  component of the symmetric finite difference gradient estimator is:\nFD: \n1 ≤i ≤p, where  is the unit vector with a 1 in the  place, and is a small positive number that decreases with n. With this method, 2p evaluations of J for each  are needed. Clearly, when p is large, this estimator loses efficiency.\nLet now  be a random perturbation vector. The  component of the stochastic perturbation gradient estimator is:\nSP: \nRemark that FD perturbs only one direction at a time, while the SP estimator disturbs all directions at the same time (the numerator is identical in all p components). The number of loss function measurements needed in the SPSA method for each  is always 2, independent of the dimension p. Thus, SPSA uses p times fewer function evaluations than FDSA, which makes it a lot more efficient.\nSimple experiments with p=2 showed that SPSA converges in the same number of iterations as FDSA. The latter follows approximately the steepest descent direction, behaving like the gradient method. On the other hand, SPSA, with the random search direction, does not follow exactly the gradient path. In average though, it tracks it nearly because the gradient approximation is an almost unbiased estimator of the gradient, as shown in the following lemma.","name":"Simultaneous perturbation stochastic approximation","categories":["Numerical climate and weather models","Optimization algorithms and methods","Stochastic algorithms"],"tag_line":"Simultaneous perturbation stochastic approximation (SPSA) is an algorithmic method for optimizing systems with multiple unknown parameters."}}
,{"_index":"throwtable","_type":"algorithm","_id":"k-server-problem","_score":0,"_source":{"description":"The k-server problem is a problem of theoretical computer science in the category of online algorithms, one of two abstract problems on metric spaces that are central to the theory of competitive analysis (the other being metrical task systems). In this problem, an online algorithm must control the movement of a set of k servers, represented as points in a metric space, and handle requests that are also in the form of points in the space. As each request arrives, the algorithm must determine which server to move to the requested point. The goal of the algorithm is to keep the total distance all servers move small, relative to the total distance the servers could have moved by an optimal adversary who knows in advance the entire sequence of requests.\nThe problem was first posed by Mark Manasse, Lyle A. McGeoch and Daniel Sleator (1990). The most prominent open question concerning the k-server problem is the so-called k-server conjecture, also posed by Manasse et al. This conjecture states that there is an algorithm for solving the k-server problem in an arbitrary metric space and for any number k of servers that has competitive ratio at least k. Manasse et al. were able to prove their conjecture when k = 2, and for more general values of k when the metric space is restricted to have exactly k+1 points. Chrobak and Larmore (1991) proved the conjecture for tree metrics. The special case of metrics in which all distances are equal is called the paging problem because it models the problem of page replacement algorithms in memory caches, and was also already known to have a k-competitive algorithm (Sleator and Tarjan 1985). Fiat et al. (1990) first proved that there exists an algorithm with finite competitive ratio for any constant k and any metric space, and finally Koutsoupias and Papadimitriou (1995) proved that Work Function Algorithm (WFA) has competitive ratio 2k - 1. However, despite the efforts of many other researchers, reducing the competitive ratio to k or providing an improved lower bound remains open as of 2014. The most common believed scenario is that the Work Function Algorithm is k-competitive. To this direction, in 2000 Bartal and Koutsoupias showed that this is true for some special cases (if the metric space is a line, a weighted star or any metric of k+2 points).\nIn 2011, a randomized algorithm with competitive bound Õ(log2k log3n) was found.","name":"K-server problem","categories":["All articles containing potentially dated statements","Articles containing potentially dated statements from 2014","Online algorithms","Unsolved problems in computer science"],"tag_line":"The k-server problem is a problem of theoretical computer science in the category of online algorithms, one of two abstract problems on metric spaces that are central to the theory of competitive analysis (the other being metrical task systems)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"randomized-algorithm","_score":0,"_source":{"description":"A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the \"average case\" over all possible choices of random bits. Formally, the algorithm's performance will be a random variable determined by the random bits; thus either the running time, or the output (or both) are random variables.\nOne has to distinguish between algorithms that use the random input to reduce the expected running time or memory usage, but always terminate with a correct result (Las Vegas algorithms) in a bounded amount of time, and probabilistic algorithms, which, depending on the random input, have a chance of producing an incorrect result (Monte Carlo algorithms) or fail to produce a result either by signalling a failure or failing to terminate.\nIn the second case, random performance and random output, the term \"algorithm\" for a procedure is somewhat questionable. In the case of random output, it is no longer formally effective. However, in some cases, probabilistic algorithms are the only practical means of solving a problem.\nIn common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior.","name":"Randomized algorithm","categories":["Analysis of algorithms","Probabilistic complexity theory","Randomized algorithms","Stochastic algorithms"],"tag_line":"A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic."}}
,{"_index":"throwtable","_type":"algorithm","_id":"las-vegas-algorithm","_score":0,"_source":{"description":"In computing, a Las Vegas algorithm is a randomized algorithm that always gives correct results; that is, it always produces the correct result or it informs about the failure. In other words, a Las Vegas algorithm does not gamble with the correctness of the result; it gambles only with the resources used for the computation. A simple example is randomized quicksort, where the pivot is chosen randomly, but the result is always sorted. The usual definition of a Las Vegas algorithm includes the restriction that the expected run time always be finite, when the expectation is carried out over the space of random information, or entropy, used in the algorithm. An alternative definition requires that a Las Vegas algorithm always terminate (be effective), but it may output a symbol not part of the solution space to indicate failure in finding a solution.\nLas Vegas algorithms were introduced by László Babai in 1979, in the context of the graph isomorphism problem, as a stronger version of Monte Carlo algorithms. Las Vegas algorithms can be used in situations where the number of possible solutions is relatively limited, and where verifying the correctness of a candidate solution is relatively easy while actually calculating the solution is complex.\nThe name refers to the city of Las Vegas, Nevada, which is well known within the United States as an icon of gambling.","name":"Las Vegas algorithm","categories":["Randomized algorithms"],"tag_line":"In computing, a Las Vegas algorithm is a randomized algorithm that always gives correct results; that is, it always produces the correct result or it informs about the failure."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rabin–karp-algorithm","_score":0,"_source":{"description":"In computer science, the Rabin–Karp algorithm or Karp–Rabin algorithm is a string searching algorithm created by Richard M. Karp and Michael O. Rabin (1987) that uses hashing to find any one of a set of pattern strings in a text. For text of length n and p patterns of combined length m, its average and best case running time is O(n+m) in space O(p), but its worst-case time is O(nm). In contrast, the Aho–Corasick string matching algorithm has asymptotic worst-time complexity O(n+m) in space O(m).\nA practical application of the algorithm is detecting plagiarism. Given source material, the algorithm can rapidly search through a paper for instances of sentences from the source material, ignoring details such as case and punctuation. Because of the abundance of the sought strings, single-string searching algorithms are impractical.","name":"Rabin–Karp algorithm","categories":["Hashing","String matching algorithms"],"tag_line":"In computer science, the Rabin–Karp algorithm or Karp–Rabin algorithm is a string searching algorithm created by Richard M. Karp and Michael O. Rabin (1987) that uses hashing to find any one of a set of pattern strings in a text."}}
,{"_index":"throwtable","_type":"algorithm","_id":"commentz-walter-algorithm","_score":0,"_source":{"description":"In computer science, the Commentz-Walter algorithm is a string searching algorithm invented by Beate Commentz-Walter. Like the Aho–Corasick string matching algorithm, it can search for multiple patterns at once. It combines ideas from Aho–Corasick with the fast matching of the Boyer–Moore string search algorithm. For a text of length n and maximum pattern length of m, its worst-case running time is O(mn), though the average case is often much better.\nGNU grep implements a string matching algorithm very similar to Commentz-Walter.","name":"Commentz-Walter algorithm","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","String matching algorithms"],"tag_line":"In computer science, the Commentz-Walter algorithm is a string searching algorithm invented by Beate Commentz-Walter."}}
,{"_index":"throwtable","_type":"algorithm","_id":"new-york-state-identification-and-intelligence-system","_score":0,"_source":{"description":"The New York State Identification and Intelligence System Phonetic Code, commonly known as NYSIIS, is a phonetic algorithm devised in 1970 as part of the New York State Identification and Intelligence System (now a part of the New York State Division of Criminal Justice Services). It features an accuracy increase of 2.7% over the traditional Soundex algorithm.","name":"New York State Identification and Intelligence System","categories":["Phonetic algorithms"],"tag_line":"The New York State Identification and Intelligence System Phonetic Code, commonly known as NYSIIS, is a phonetic algorithm devised in 1970 as part of the New York State Identification and Intelligence System (now a part of the New York State Division of Criminal Justice Services)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fifo-(computing-and-electronics)","_score":0,"_source":{"description":"FIFO is an acronym for first in, first out, a method for organizing and manipulating a data buffer, where the oldest (first) entry, or 'head' of the queue, is processed first. It is analogous to processing a queue with first-come, first-served (FCFS) behaviour: where the people leave the queue in the order in which they arrive.\nFCFS is also the jargon term for the FIFO operating system scheduling algorithm, which gives every process central processing unit (CPU) time in the order in which it is demanded.\nFIFO's opposite is LIFO, last-in-first-out, where the youngest entry or 'top of the stack' is processed first.\nA priority queue is neither FIFO or LIFO but may adopt similar behaviour temporarily or by default.\nQueueing theory encompasses these methods for processing data structures, as well as interactions between strict-FIFO queues.","name":"FIFO (computing and electronics)","categories":["All articles needing additional references","Articles needing additional references from March 2015","Cybernetics","Inter-process communication","Queue management","Scheduling algorithms","Wikipedia articles with GND identifiers"],"tag_line":"FIFO is an acronym for first in, first out, a method for organizing and manipulating a data buffer, where the oldest (first) entry, or 'head' of the queue, is processed first."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fair-share-scheduling","_score":0,"_source":{"description":"Fair-share scheduling is a scheduling algorithm for computer operating systems in which the CPU usage is equally distributed among system users or groups, as opposed to equal distribution among processes.\nOne common method of logically implementing the fair-share scheduling strategy is to recursively apply the round-robin scheduling strategy at each level of abstraction (processes, users, groups, etc.) The time quantum required by round-robin is arbitrary, as any equal division of time will produce the same results.\nThis was first developed by Judy Kay and Piers Lauder through their research at Sydney University in the 1980s.","name":"Fair-share scheduling","categories":["All articles needing additional references","Articles needing additional references from June 2012","Processor scheduling algorithms"],"tag_line":"Fair-share scheduling is a scheduling algorithm for computer operating systems in which the CPU usage is equally distributed among system users or groups, as opposed to equal distribution among processes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"multilevel-feedback-queue","_score":0,"_source":{"description":"In computer science, a multilevel feedback queue is a scheduling algorithm. Solaris 2.6 Time-Sharing (TS) scheduler implements this algorithm. The Mac OS X and Microsoft Windows schedulers can both be regarded as examples of the broader class of multilevel feedback queue schedulers. This scheduling algorithm is intended to meet the following design requirements for multimode systems:\nGive preference to short jobs.\nGive preference to I/O bound processes.\nSeparate processes into categories based on their need for the processor.\nThe Multi-level Feedback Queue scheduler was first developed by Fernando J. Corbató et al. in 1962, and this work, along with other work on Multics, led the ACM to award Corbató the Turing Award.","name":"Multilevel feedback queue","categories":["Processor scheduling algorithms"],"tag_line":"In computer science, a multilevel feedback queue is a scheduling algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lottery-scheduling","_score":0,"_source":{"description":"Lottery Scheduling is a probabilistic scheduling algorithm for processes in an operating system. Processes are each assigned some number of lottery tickets, and the scheduler draws a random ticket to select the next process. The distribution of tickets need not be uniform; granting a process more tickets provides it a relative higher chance of selection. This technique can be used to approximate other scheduling algorithms, such as Shortest job next and Fair-share scheduling.\nLottery scheduling solves the problem of starvation. Giving each process at least one lottery ticket guarantees that it has non-zero probability of being selected at each scheduling operation.","name":"Lottery scheduling","categories":["Processor scheduling algorithms"],"tag_line":"Lottery Scheduling is a probabilistic scheduling algorithm for processes in an operating system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rate-monotonic-scheduling","_score":0,"_source":{"description":"In computer science, rate-monotonic scheduling (RMS) is a scheduling algorithm used in real-time operating systems (RTOS) with a static-priority scheduling class. The static priorities are assigned on the basis of the cycle duration of the job: the shorter the cycle duration is, the higher is the job's priority.\nThese operating systems are generally preemptive and have deterministic guarantees with regard to response times. Rate monotonic analysis is used in conjunction with those systems to provide scheduling guarantees for a particular application.","name":"Rate-monotonic scheduling","categories":["All articles with unsourced statements","Articles with unsourced statements from October 2007","Processor scheduling algorithms","Real-time computing"],"tag_line":"In computer science, rate-monotonic scheduling (RMS) is a scheduling algorithm used in real-time operating systems (RTOS) with a static-priority scheduling class."}}
,{"_index":"throwtable","_type":"algorithm","_id":"yds-algorithm","_score":0,"_source":{"description":"YDS is a scheduling algorithm for dynamic speed scaling processors which minimizes the total energy consumption. It was named after and developed by Yao et al. There is both an online and an offline version of the algorithm.","name":"YDS algorithm","categories":["All articles needing expert attention","All articles that are too technical","Articles needing expert attention from July 2013","Processor scheduling algorithms","Real-time computing","Wikipedia articles that are too technical from July 2013"],"tag_line":"YDS is a scheduling algorithm for dynamic speed scaling processors which minimizes the total energy consumption."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shortest-seek-first","_score":0,"_source":{"description":"Shortest seek first (or shortest seek time first) is a secondary storage scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.","name":"Shortest seek first","categories":["All articles lacking sources","Articles lacking sources from December 2009","Disk scheduling algorithms"],"tag_line":"Shortest seek first (or shortest seek time first) is a secondary storage scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hopscotch-hashing","_score":0,"_source":{"description":"Hopscotch hashing is a scheme in computer programming for resolving hash collisions of values of hash functions in a table using open addressing. It is also well suited for implementing a concurrent hash table. Hopscotch hashing was introduced by Maurice Herlihy, Nir Shavit and Moran Tzafrir in 2008. The name is derived from the sequence of hops that characterize the table's insertion algorithm.\n\nThe algorithm uses a single array of n buckets. For each bucket, its neighborhood is a small collection of nearby consecutive buckets (i.e. one with close indexes to the original hashed bucket). The desired property of the neighborhood is that the cost of finding an item in the buckets of the neighborhood is close to the cost of finding it in the bucket itself (for example, by having buckets in the neighborhood fall within the same cache line). The size of the neighborhood must be sufficient to accommodate a logarithmic number of items in the worst case (i.e. it must accommodate log(n) items), but only a constant number on average. If some bucket's neighborhood is filled, the table is resized.\nIn hopscotch hashing, as in cuckoo hashing, and unlike in linear probing, a given item will always be inserted-into and found-in the neighborhood of its hashed bucket. In other words, it will always be found either in its original hashed array entry, or in one of the next H-1 neighboring entries. H could, for example, be 32, the standard machine word size. The neighborhood is thus a \"virtual\" bucket that has fixed size and overlaps with the next H-1 buckets. To speed the search, each bucket (array entry) includes a \"hop-information\" word, an H-bit bitmap that indicates which of the next H-1 entries contain items that hashed to the current entry's virtual bucket. In this way, an item can be found quickly by looking at the word to see which entries belong to the bucket, and then scanning through the constant number of entries (most modern processors support special bit manipulation operations that make the lookup in the \"hop-information\" bitmap very fast).\nHere is how to add item x which was hashed to bucket i:\nIf the entry i is empty, add x to i and return.\nStarting at entry i, use a linear probe to find an empty entry at index j.\nIf the empty entry's index j is within H-1 of entry i, place x there and return. Otherwise, entry j is too far from i. To create an empty entry closer to i, find an item y whose hash value lies between i and j, but within H-1 of j. Displacing y to j creates a new empty slot closer to i. Repeat until the empty entry is within H-1 of entry i, place x there and return. If no such item y exists, or if the bucket i already contains H items, resize and rehash the table.\nThe idea is that hopscotch hashing \"moves the empty slot towards the desired bucket\". This distinguishes it from linear probing which leaves the empty slot where it was found, possibly far away from the original bucket, or from cuckoo hashing that, in order to create a free bucket, moves an item out of one of the desired buckets in the target arrays, and only then tries to find the displaced item a new place.\nTo remove an item from the table, one simply removes it from the table entry. If the neighborhood buckets are cache aligned, then one could apply a reorganization operation in which items are moved into the now vacant location in order to improve alignment.\nOne advantage of hopscotch hashing is that it provides good performance at very high table load factors, even ones exceeding 0.9. Part of this efficiency is due to using a linear probe only to find an empty slot during insertion, not for every lookup as in the original linear probing hash table algorithm. Another advantage is that one can use any hash function, in particular simple ones that are close-to-universal.","name":"Hopscotch hashing","categories":["Hashing","Search algorithms"],"tag_line":"Hopscotch hashing is a scheme in computer programming for resolving hash collisions of values of hash functions in a table using open addressing."}}
,{"_index":"throwtable","_type":"algorithm","_id":"search-game","_score":0,"_source":{"description":"A search game is a two-person zero-sum game which takes place in a set called the search space. The searcher can choose any continuous trajectory subject to a maximal velocity constraint. It is always assumed that neither the searcher nor the hider has any knowledge about the movement of the other player until their distance apart is less than or equal to the discovery radius and at this very moment capture occurs. As mathematical models, search games can be applied to areas such as hide-and-seek games that children play or representations of some tactical military situations. The area of search games was introduced in the last chapter of Rufus Isaacs' classic book \"Differential Games\" and has been developed further by Shmuel Gal  and Steve Alpern.\nWhat is the best way to search a stationary target in a graph? A natural strategy is to find a minimal closed curve L that covers all the arcs of the graph. (L is called a Chinese postman tour). Then, traverse L with probability 1/2 for each direction. This strategy seems to work well if the graph is Eulerian. In general, this random Chinese postman tour is indeed an optimal search strategy if and only if the graph consists of a set of Eulerian graphs connected in a tree-like structure. A misleadingly simple example of a graph not in this family consists of two nodes connected by three arcs. The random Chinese postman tour (equivalent to traversing the three arcs in a random order) is not optimal. The optimal way to search these three arcs is surprisingly complicated [2] .\nThe princess and monster game deals with a moving target.\nSearching unbounded domains is also interesting. In general, the reasonable framework, as in the case of an online algorithm, is to use a normalized cost function (called the competitive ratio in Computer Science literature). The minimax trajectory for problems of these types is always a geometric sequence (or exponential function for continuous problems). This result yields an easy method to find the minimax trajectory by minimizing over a single parameter (the generator of this sequence) instead of searching over the whole trajectory space. This tool has been used for the linear search problem, i.e., finding a target on the infinite line, which has attracted much attention over several decades and has been analyzed as a search game. It has also been used to find a minimax trajectory for searching a set of concurrent rays. Optimal searching in the plane is performed by using exponential spirals. Searching a set of concurrent rays was later re-discovered in Computer Science literature as the 'cow-path problem'.","name":"Search game","categories":["Game theory","Search algorithms"],"tag_line":"A search game is a two-person zero-sum game which takes place in a set called the search space."}}
,{"_index":"throwtable","_type":"algorithm","_id":"perceptual-hashing","_score":0,"_source":{"description":"Perceptual hashing is the use of an algorithm that produces a snippet or fingerprint of various forms of multimedia. Perceptual hash functions are analogous if features are similar, whereas cryptographic rely on the avalanche effect of a small change in input value creating a drastic change in output value. Perceptual hash functions are widely used to protect against copyright infringement and digital forensics because of the ability to have a correlation between hashes so you can compare and map source data. For example, Wikipedia could maintain a database of text hashes of popular online books or articles for which the authors hold copyrights to, anytime a Wikipedia user uploads an online book or article that has a copyright, the hashes will be almost exactly the same and could be flagged as plagiarism. This same flagging system can be used for any multimedia or text file.","name":"Perceptual hashing","categories":["Algorithms and data structures stubs","All articles covered by WikiProject Wikify","All articles with too few wikilinks","All stub articles","Articles covered by WikiProject Wikify from November 2014","Articles with too few wikilinks from November 2014","Computer science stubs","Hashing"],"tag_line":"Perceptual hashing is the use of an algorithm that produces a snippet or fingerprint of various forms of multimedia."}}
,{"_index":"throwtable","_type":"algorithm","_id":"exponential-search","_score":0,"_source":{"description":"In computer science, an exponential search (also called doubling search or galloping search) is an algorithm, created by Jon Bentley and Andrew Chi-Chih Yao in 1976, for searching sorted, unbounded/infinite lists. There are numerous ways to implement this with the most common being to determine a range that the search key resides in and performing a binary search within that range. This takes O(log i) where i is the position of the search key in the list, if the search key is in the list, or the position where the search key should be, if the search key is not in the list.\nExponential search can also be used to search in bounded lists. Exponential search can even out-perform more traditional searches for bounded lists, such as binary search, when the element being searched for is near the beginning of the array. This is because exponential search will run in O(log i) time, where i is the index of the element being searched for in the list, whereas binary search would run in O(log n) time, where n is the number of elements in the list.","name":"Exponential search","categories":["Search algorithms"],"tag_line":"In computer science, an exponential search (also called doubling search or galloping search) is an algorithm, created by Jon Bentley and Andrew Chi-Chih Yao in 1976, for searching sorted, unbounded/infinite lists."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dancing-links","_score":0,"_source":{"description":"In computer science, dancing links, also known as DLX, is the technique suggested by Donald Knuth to efficiently implement his Algorithm X. Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm that finds all solutions to the exact cover problem. Some of the better-known exact cover problems include tiling, the n queens problem, and Sudoku.\nThe name dancing links stems from the way the algorithm works, as iterations of the algorithm cause the links to \"dance\" with partner links so as to resemble an \"exquisitely choreographed dance.\" Knuth credits Hiroshi Hitotsumatsu and Kōhei Noshita with having invented the idea in 1979, but it is his paper which has popularized it.","name":"Dancing Links","categories":["Articles containing video clips","Donald Knuth","Linked lists","Search algorithms","Sudoku"],"tag_line":"In computer science, dancing links, also known as DLX, is the technique suggested by Donald Knuth to efficiently implement his Algorithm X. Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm that finds all solutions to the exact cover problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"stack-search","_score":0,"_source":{"description":"Stack search (also known as Stack decoding algorithm) is a search algorithm similar to beam search. It can be used to explore tree-structured search spaces and is often employed in Natural language processing applications, such as parsing of natural languages, or for decoding of error correcting codes where the technique goes under the name of sequential decoding.\nStack search keeps a list of the best n candidates seen so far. These candidates are incomplete solutions to the search problems, e.g. partial parse trees. It then iteratively expands the best partial solution, putting all resulting partial solutions onto the stack and then trimming the resulting list of partial solutions to the top n candidates, until a real solution (i.e. complete parse tree) has been found.\nStack search is not guaranteed to find the optimal solution to the search problem. The quality of the result depends on the quality of the search heuristic.","name":"Stack search","categories":["All stub articles","Computing stubs","Search algorithms"],"tag_line":"Stack search (also known as Stack decoding algorithm) is a search algorithm similar to beam search."}}
,{"_index":"throwtable","_type":"algorithm","_id":"search-algorithm","_score":0,"_source":{"description":"In computer science, a search algorithm is an algorithm for finding an item with specified properties among a collection of items which are coded into a computer program, that look for clues to return what is wanted. The items may be stored individually as records in a database; or may be elements of a search space defined by a mathematical formula or procedure, such as the roots of an equation with integer variables; or a combination of the two, such as the Hamiltonian circuits of a graph.","name":"Search algorithm","categories":["All Wikipedia articles needing context","All articles lacking sources","All articles needing expert attention","All pages needing cleanup","Articles lacking sources from December 2014","Articles needing expert attention from December 2014","Computer science articles needing expert attention","Search algorithms","Wikipedia articles needing context from December 2014","Wikipedia introduction cleanup from December 2014"],"tag_line":"In computer science, a search algorithm is an algorithm for finding an item with specified properties among a collection of items which are coded into a computer program, that look for clues to return what is wanted."}}
,{"_index":"throwtable","_type":"algorithm","_id":"best-bin-first","_score":0,"_source":{"description":"Best bin first is a search algorithm that is designed to efficiently find an approximate solution to the nearest neighbor search problem in very-high-dimensional spaces. The algorithm is based on a variant of the kd-tree search algorithm which makes indexing higher-dimensional spaces possible. Best bin first is an approximate algorithm which returns the nearest neighbor for a large fraction of queries and a very close neighbor otherwise.","name":"Best bin first","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Search algorithms"],"tag_line":"Best bin first is a search algorithm that is designed to efficiently find an approximate solution to the nearest neighbor search problem in very-high-dimensional spaces."}}
,{"_index":"throwtable","_type":"algorithm","_id":"uuhash","_score":0,"_source":{"description":"UUHash is a hash algorithm employed by clients on the FastTrack network. It is employed for its ability to hash very large files in a very short period of time, even on older computers. However, this is achieved by only hashing a fraction of the file. This weakness makes it trivial to create a hash collision, allowing large sections to be completely altered without altering the checksum.\nThis method is used by Kazaa. The weakness of UUHash is exploited by anti-p2p agencies to corrupt downloads.","name":"UUHash","categories":["Search algorithms"],"tag_line":"UUHash is a hash algorithm employed by clients on the FastTrack network."}}
,{"_index":"throwtable","_type":"algorithm","_id":"iterative-proportional-fitting","_score":0,"_source":{"description":"The iterative proportional fitting procedure (IPFP, also known as biproportional fitting in statistics, RAS algorithm in economics and matrix raking or matrix scaling in computer science) is an iterative algorithm for estimating cell values of a contingency table such that the marginal totals remain fixed and the estimated table decomposes into an outer product.\nFirst introduced by Deming and Stephan in 1940 (they proposed IPFP as an algorithm leading to a minimizer of the Pearson X-squared statistic, which it does not, and even failed to prove convergence), it has seen various extensions and related research. A rigorous proof of convergence by means of differential geometry is due to Fienberg (1970). He interpreted the family of contingency tables of constant crossproduct ratios as a particular (IJ − 1)-dimensional manifold of constant interaction and showed that the IPFP is a fixed-point iteration on that manifold. Nevertheless, he assumed strictly positive observations. Generalization to tables with zero entries is still considered a hard and only partly solved problem.\nAn exhaustive treatment of the algorithm and its mathematical foundations can be found in the book of Bishop et al. (1975). The first general proof of convergence, built on non-trivial measure theoretic theorems and entropy minimization, is due to Csiszár (1975). Relatively new results on convergence and error behavior have been published by Pukelsheim and Simeone (2009) . They proved simple necessary and sufficient conditions for the convergence of the IPFP for arbitrary two-way tables (i.e. tables with zero entries) by analysing an -error function.\nOther general algorithms can be modified to yield the same limit as the IPFP, for instance the Newton–Raphson method and the EM algorithm. In most cases, IPFP is preferred due to its computational speed, numerical stability and algebraic simplicity.","name":"Iterative proportional fitting","categories":["Categorical data","Statistical algorithms"],"tag_line":"The iterative proportional fitting procedure (IPFP, also known as biproportional fitting in statistics, RAS algorithm in economics and matrix raking or matrix scaling in computer science) is an iterative algorithm for estimating cell values of a contingency table such that the marginal totals remain fixed and the estimated table decomposes into an outer product."}}
,{"_index":"throwtable","_type":"algorithm","_id":"levenberg–marquardt-algorithm","_score":0,"_source":{"description":"In mathematics and computing, the Levenberg–Marquardt algorithm (LMA), also known as the damped least-squares (DLS) method, is used to solve non-linear least squares problems. These minimization problems arise especially in least squares curve fitting.\nThe LMA is used in many software applications for solving generic curve-fitting problems. However, as for many fitting algorithms, the LMA finds only a local minimum, which is not necessarily the global minimum. The LMA interpolates between the Gauss–Newton algorithm (GNA) and the method of gradient descent. The LMA is more robust than the GNA, which means that in many cases it finds a solution even if it starts very far off the final minimum. For well-behaved functions and reasonable starting parameters, the LMA tends to be a bit slower than the GNA. LMA can also be viewed as Gauss–Newton using a trust region approach.\nThe algorithm was first published in 1944 by Kenneth Levenberg, while working at the Frankford Army Arsenal. It was rediscovered in 1963 by Donald Marquardt who worked as a statistician at DuPont and independently by Girard, Wynn and Morrison.","name":"Levenberg–Marquardt algorithm","categories":["All articles with specifically marked weasel-worded phrases","Articles with specifically marked weasel-worded phrases from July 2015","Least squares","Optimization algorithms and methods","Pages with citations lacking titles","Statistical algorithms"],"tag_line":"In mathematics and computing, the Levenberg–Marquardt algorithm (LMA), also known as the damped least-squares (DLS) method, is used to solve non-linear least squares problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pagerank","_score":0,"_source":{"description":"PageRank is an algorithm used by Google Search to rank websites in their search engine results. PageRank was named after Larry Page, one of the founders of Google. PageRank is a way of measuring the importance of website pages. According to Google:\n\nPageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites.\n\nIt is not the only algorithm used by Google to order search engine results, but it is the first algorithm that was used by the company, and it is the best-known.","name":"PageRank","categories":["All Wikipedia articles in need of updating","All articles lacking reliable references","All articles with unsourced statements","American inventions","Articles lacking reliable references from October 2012","Articles with example MATLAB/Octave code","Articles with inconsistent citation formats","Articles with unsourced statements from June 2013","Articles with unsourced statements from November 2015","Articles with unsourced statements from October 2015","Crowdsourcing","Google Search","Internet search algorithms","Link analysis","Markov models","Pages containing cite templates with deprecated parameters","Reputation management","Search engine optimization","Wikipedia articles in need of updating from February 2014"],"tag_line":"PageRank is an algorithm used by Google Search to rank websites in their search engine results."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hunt–mcilroy-algorithm","_score":0,"_source":{"description":"In computer science, the Hunt–McIlroy algorithm is a solution to the longest common subsequence problem. It was one of the first non-heuristic algorithms used in diff. To this day, variations of this algorithm are found in incremental version control systems, wiki engines, and molecular phylogenetics research software.\nThe research accompanying the final version of Unix diff, written by Douglas McIlroy, was published in the 1976 paper \"An Algorithm for Differential File Comparison\", co-written with James W. Hunt, who developed an initial prototype of diff.","name":"Hunt–McIlroy algorithm","categories":["Algorithms on strings","Combinatorics","Dynamic programming"],"tag_line":"In computer science, the Hunt–McIlroy algorithm is a solution to the longest common subsequence problem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lander–green-algorithm","_score":0,"_source":{"description":"The Lander–Green algorithm is an algorithm, due to Eric Lander and Philip Green for computing the likelihood of observed genotype data given a pedigree. It is appropriate for relatively small pedigrees and a large number of markers. It is used in the analysis of genetic linkage.","name":"Lander–Green algorithm","categories":["All stub articles","Genetic epidemiology","Genetic linkage analysis","Genetics stubs","Statistical algorithms","Statistical genetics","Statistics stubs"],"tag_line":"The Lander–Green algorithm is an algorithm, due to Eric Lander and Philip Green for computing the likelihood of observed genotype data given a pedigree."}}
,{"_index":"throwtable","_type":"algorithm","_id":"identity-matrix","_score":0,"_source":{"description":"In linear algebra, the identity matrix or unit matrix of size n is the n × n square matrix with ones on the main diagonal and zeros elsewhere. It is denoted by In, or simply by I if the size is immaterial or can be trivially determined by the context. (In some fields, such as quantum mechanics, the identity matrix is denoted by a boldface one, 1; otherwise it is identical to I.) Less frequently, some mathematics books use U or E to represent the identity matrix, meaning \"unit matrix\" and the German word \"Einheitsmatrix\", respectively.\n\nWhen A is m×n, it is a property of matrix multiplication that\n\nIn particular, the identity matrix serves as the unit of the ring of all n×n matrices, and as the identity element of the general linear group GL(n) consisting of all invertible n×n matrices. (The identity matrix itself is invertible, being its own inverse.)\nWhere n×n matrices are used to represent linear transformations from an n-dimensional vector space to itself, In represents the identity function, regardless of the basis.\nThe ith column of an identity matrix is the unit vector ei. It follows that the determinant of the identity matrix is 1 and the trace is n.\nUsing the notation that is sometimes used to concisely describe diagonal matrices, we can write:\n\nIt can also be written using the Kronecker delta notation:\n\nThe identity matrix also has the property that, when it is the product of two square matrices, the matrices can be said to be the inverse of one another.\nThe identity matrix of a given size is the only idempotent matrix of that size having full rank. That is, it is the only matrix such that (a) when multiplied by itself the result is itself, and (b) all of its rows, and all of its columns, are linearly independent.\nThe principal square root of an identity matrix is itself, and this is its only positive definite square root. However, every identity matrix with at least two rows and columns has an infinitude of symmetric square roots.","alt_names":["identity_matrix"],"name":"Identity matrix","categories":["1 (number)","Matrices","Sparse matrices"],"tag_line":"In linear algebra, the identity matrix or unit matrix of size n is the n × n square matrix with ones on the main diagonal and zeros elsewhere."}}
,{"_index":"throwtable","_type":"algorithm","_id":"function-composition-(computer-science)","_score":0,"_source":{"description":"In computer science, function composition (not to be confused with object composition) is an act or mechanism to combine simple functions to build more complicated ones. Like the usual composition of functions in mathematics, the result of each function is passed as the argument of the next, and the result of the last one is the result of the whole.\nProgrammers frequently apply functions to results of other functions, and almost all programming languages allow it. In some cases, the composition of functions is interesting as a function in its own right, to be used later. Such a function can always be defined but languages with first-class functions make it easier.\nThe ability to easily compose functions encourages factoring (breaking apart) functions for maintainability and code reuse. More generally, big systems might be built by composing whole programs.\nNarrowly speaking, function composition applies to functions that operate on a finite amount of data, each step sequentially processing it before handing it to the next. Functions that operate on potentially infinite data (a stream or other codata) are known as filters, and are instead connected in a pipeline, which is analogous to function composition and can execute concurrently.\n\n","alt_names":["Function_composition_(computer_science)"],"name":"Function composition (computer science)","categories":["Higher-order functions","Programming language topics"],"tag_line":"In computer science, function composition (not to be confused with object composition) is an act or mechanism to combine simple functions to build more complicated ones."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pythagorean-means","_score":0,"_source":{"description":"In mathematics, the three classical Pythagorean means are the arithmetic mean (A), the geometric mean (G), and the harmonic mean (H). They are defined by:\n\nEach mean has the following properties:\nValue preservation: \nFirst order homogeneity: \nInvariance under exchange:  for any  and .\nAveraging: \nThese means were studied with proportions by Pythagoreans and later generations of Greek mathematicians because of their importance in geometry and music. The harmonic and arithmetic means are reciprocal duals of each other for positive arguments () while the geometric mean is its own reciprocal dual.","alt_names":["Pythagorean_means"],"name":"Pythagorean means","categories":["Means"],"tag_line":"In mathematics, the three classical Pythagorean means are the arithmetic mean (A), the geometric mean (G), and the harmonic mean (H)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"markov-algorithm","_score":0,"_source":{"description":"In theoretical computer science, a Markov algorithm is a string rewriting system that uses grammar-like rules to operate on strings of symbols. Markov algorithms have been shown to be Turing-complete, which means that they are suitable as a general model of computation and can represent any mathematical expression from its simple notation. Markov algorithms are named after the Soviet mathematician Andrey Markov, Jr.\nRefal is a programming language based on Markov algorithms.","alt_names":["Markov_algorithm"],"name":"Markov algorithm","categories":["Models of computation","Rewriting systems","Theory of computation"],"tag_line":"In theoretical computer science, a Markov algorithm is a string rewriting system that uses grammar-like rules to operate on strings of symbols."}}
,{"_index":"throwtable","_type":"algorithm","_id":"almost-prime","_score":0,"_source":{"description":"In number theory, a natural number is called almost prime if there exists an absolute constant K such that the number has at most K prime factors. An almost prime n is denoted by Pr if and only if the number of prime factors of n, counted according to multiplicity, is at most r. A natural number is called k-almost prime if it has exactly k prime factors, counted with multiplicity. More formally, a number n is k-almost prime if and only if Ω(n) = k, where Ω(n) is the total number of primes in the prime factorization of n:\n\nA natural number is thus prime if and only if it is 1-almost prime, and semiprime if and only if it is 2-almost prime. The set of k-almost primes is usually denoted by Pk. The smallest k-almost prime is 2k. The first few k-almost primes are:\nThe number πk(n) of positive integers less than or equal to n with at most k prime divisors (not necessarily distinct) is asymptotic to:\n\na result of Landau. See also the Hardy–Ramanujan theorem.\n^ Sándor, József; Dragoslav, Mitrinović S.; Crstici, Borislav (2006). Handbook of Number Theory I. Springer. p. 316. ISBN 978-1-4020-4215-7. \n^ Rényi, Alfréd A. (1948). \"On the representation of an even number as the sum of a single prime and single almost-prime number\". Izvestiya Rossiiskoi Akademii Nauk. Seriya Matematicheskaya (in Russian) 12 (1): 57–78. \n^ Heath-Brown, D. R. (May 1978). \"Almost-primes in arithmetic progressions and short intervals\". Mathematical Proceedings of the Cambridge Philosophical Society 83 (3): 357–375. doi:10.1017/S0305004100054657. \n^ Tenenbaum, Gerald (1995). Introduction to Analytic and Probabilistic Number Theory. Cambridge University Press. ISBN 0-521-41261-7.","alt_names":["Almost_prime"],"name":"Almost prime","categories":["CS1 Russian-language sources (ru)","Integer sequences","Prime numbers"],"tag_line":"In number theory, a natural number is called almost prime if there exists an absolute constant K such that the number has at most K prime factors."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pascal's-triangle","_score":0,"_source":{"description":"In mathematics, Pascal's triangle is a triangular array of the binomial coefficients. In much of the Western world, it is named after French mathematician Blaise Pascal, although other mathematicians studied it centuries before him in India, Persia (Iran), China, Germany, and Italy.\nThe rows of Pascal's triangle (sequence A007318 in OEIS) are conventionally enumerated starting with row n = 0 at the top (the 0th row). The entries in each row are numbered from the left beginning with k = 0 and are usually staggered relative to the numbers in the adjacent rows. Having the indices of both rows and columns start at zero makes it possible to state that the binomial coefficient  appears in the nth row and kth column of Pascal's triangle. A simple construction of the triangle proceeds in the following manner: In row 0, the topmost row, the entry is  (the entry is in the zeroth row and zeroth column). Then, to construct the elements of the following rows, add the number above and to the left with the number above and to the right of a given position to find the new value to place in that position. If either the number to the right or left is not present, substitute a zero in its place. For example, the initial number in the first (or any other) row is 1 (the sum of 0 and 1), whereas the numbers 1 and 3 in the third row are added to produce the number 4 in the fourth row.\nThis construction is related to the binomial coefficients by Pascal's rule, which says that if\n\nthen\n\nfor any non-negative integer n and any integer k between 0 and n.\nPascal's triangle has higher dimensional generalizations. The three-dimensional version is called Pascal's pyramid or Pascal's tetrahedron, while the general versions are called Pascal's simplices.","alt_names":["Pascal's_triangle"],"name":"Pascal's triangle","categories":["Blaise Pascal","Chinese mathematical discoveries","Commons category with page title same as on Wikidata","Factorial and binomial topics","Indian inventions","Triangles of numbers"],"tag_line":"In mathematics, Pascal's triangle is a triangular array of the binomial coefficients."}}
,{"_index":"throwtable","_type":"algorithm","_id":"factorial","_score":0,"_source":{"description":"In mathematics, the factorial of a non-negative integer n, denoted by n!, is the product of all positive integers less than or equal to n. For example,\n\nThe value of 0! is 1, according to the convention for an empty product.\nThe factorial operation is encountered in many areas of mathematics, notably in combinatorics, algebra, and mathematical analysis. Its most basic occurrence is the fact that there are n! ways to arrange n distinct objects into a sequence (i.e., permutations of the set of objects). This fact was known at least as early as the 12th century, to Indian scholars. Fabian Stedman in 1677 described factorials as applied to change ringing. After describing a recursive approach, Stedman gives a statement of a factorial (using the language of the original):\n\nNow the nature of these methods is such, that the changes on one number comprehends [includes] the changes on all lesser numbers, ... insomuch that a compleat Peal of changes on one number seemeth to be formed by uniting of the compleat Peals on all lesser numbers into one entire body;\n\nThe notation n! was introduced by Christian Kramp in 1808.\nThe definition of the factorial function can also be extended to non-integer arguments, while retaining its most important properties; this involves more advanced mathematics, notably techniques from mathematical analysis.","name":"Factorial","categories":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from May 2013","Articles needing additional references from September 2013","Articles with unsourced statements from February 2015","CS1 French-language sources (fr)","Combinatorics","Commons category with local link same as on Wikidata","Factorial and binomial topics","Gamma and related functions"],"tag_line":"In mathematics, the factorial of a non-negative integer n, denoted by n!, is the product of all positive integers less than or equal to n. For example,\n\nThe value of 0!"}}
,{"_index":"throwtable","_type":"algorithm","_id":"line-printer","_score":0,"_source":{"description":"The line printer is an impact computer printer that prints one entire line of text at a time. It is mostly associated with unit record equipment and the early days of digital computing, but the technology is still in use. Print speeds of 600 lines-per-minute (approximately 10 pages per minute) were achieved in the 1950s, later increasing to as much as 1200 lpm. Line printers print a complete line at a time and have speeds in the range of 150 to 2500 lines per minute. The different types of line printers are drum printers and chain printers.","alt_names":["line_printer"],"name":"Line printer","categories":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from February 2010","Articles with unsourced statements from December 2011","Articles with unsourced statements from November 2011","Commons category with local link same as on Wikidata","Computer printers","Impact printers"],"tag_line":"The line printer is an impact computer printer that prints one entire line of text at a time."}}
,{"_index":"throwtable","_type":"algorithm","_id":"floyd's-triangle","_score":0,"_source":{"description":"Floyd's triangle is a right-angled triangular array of natural numbers, used in computer science education. It is named after Robert Floyd. It is defined by filling the rows of the triangle with consecutive numbers, starting with a 1 in the top left corner:\nBeginning programmers are often assigned the task of writing a program to print out the table in the format shown.\nThe numbers along the left edge of the triangle are the lazy caterer's sequence and the numbers along the right edge are the triangular numbers. The nth row sums to n(n2 + 1)/2, the constant of an n × n magic square (sequence A006003 in OEIS).\nSumming up the row sums in Floyd's triangle reveals the doubly triangular numbers (triangular numbers with an index that is triangular)(sequence A002817 in OEIS)\n1            = 1 = T(T(1))\n1            = 6 = T(T(3))\n2 + 3\n1\n2 + 3     = 21 = T((T6))\n4 + 5 + 6","alt_names":["Floyd's_triangle"],"name":"Floyd's triangle","categories":["Computer programming","Computer science education","Triangles of numbers"],"tag_line":"Floyd's triangle is a right-angled triangular array of natural numbers, used in computer science education."}}
,{"_index":"throwtable","_type":"algorithm","_id":"leap-year","_score":0,"_source":{"description":"A leap year (also known as an intercalary year or a bissextile year) is a year containing one additional day (or, in the case of lunisolar calendars, a month) added to keep the calendar year synchronized with the astronomical or seasonal year. Because seasons and astronomical events do not repeat in a whole number of days, calendars that have the same number of days in each year, drift over time with respect to the event that the year is supposed to track. By inserting (also called intercalating) an additional day or month into the year, the drift can be corrected. A year that is not a leap year is called a common year.\nFor example, in the Gregorian calendar, each leap year has 366 days instead of the usual 365, by extending February to 29 days rather than the common 28. Similarly, in the lunisolar Hebrew calendar, Adar Aleph, a 13th lunar month, is added seven times every 19 years to the twelve lunar months in its common years to keep its calendar year from drifting through the seasons.\nThe name \"leap year\" comes from the fact that while a fixed date in the Gregorian calendar normally advances one day of the week from one year to the next, the day of the week in a leap year will advance two days (from March onwards) due to the extra day added at the end of February (thus \"leaping over\" one of the days in the week). For example, Christmas fell on Tuesday in 2001, Wednesday in 2002, and Thursday in 2003 but then \"leapt\" over Friday to fall on a Saturday in 2004.","alt_names":["Leap_year"],"name":"Leap year","categories":["All articles with unsourced statements","Articles containing simplified Chinese-language text","Articles containing traditional Chinese-language text","Articles with example pseudocode","Articles with unsourced statements from June 2011","Calendars","Pages using citations with accessdate and no URL","Units of time"],"tag_line":"A leap year (also known as an intercalary year or a bissextile year) is a year containing one additional day (or, in the case of lunisolar calendars, a month) added to keep the calendar year synchronized with the astronomical or seasonal year."}}
,{"_index":"throwtable","_type":"algorithm","_id":"scrypt","_score":0,"_source":{"description":"In cryptography, scrypt is a password-based key derivation function created by Colin Percival, originally for the Tarsnap online backup service. The algorithm was specifically designed to make it costly to perform large-scale custom hardware attacks by requiring large amounts of memory. In 2012, the scrypt algorithm was published by IETF as an Internet Draft, intended to become an informational RFC. A simplified version of scrypt is used as a proof-of-work scheme by a number of cryptocurrencies first implemented by an anonymous programmer called ArtForz in Tenebrix followed by Fairbrix and Litecoin soon.","name":"Scrypt","categories":["Cryptographic algorithms","Key derivation functions"],"tag_line":"In cryptography, scrypt is a password-based key derivation function created by Colin Percival, originally for the Tarsnap online backup service."}}
,{"_index":"throwtable","_type":"algorithm","_id":"merkle–hellman-knapsack-cryptosystem","_score":0,"_source":{"description":"The Merkle–Hellman knapsack cryptosystem was one of the earliest public key cryptosystems invented by Ralph Merkle and Martin Hellman in 1978. The ideas behind it are simpler than those involving RSA, and it has been broken.\n^ Merkle, Ralph; Hellman, Martin (1978). \"Hiding information and signatures in trapdoor knapsacks\". Information Theory, IEEE Transactions on 24 (5): 525–530. doi:10.1109/TIT.1978.1055927. \n^ Shamir, Adi (1984). \"A polynomial-time algorithm for breaking the basic Merkle - Hellman cryptosystem\". Information Theory, IEEE Transactions on 30 (5): 699–704. doi:10.1109/SFCS.1982.5.","name":"Merkle–Hellman knapsack cryptosystem","categories":["Broken cryptography algorithms","Public-key encryption schemes"],"tag_line":"The Merkle–Hellman knapsack cryptosystem was one of the earliest public key cryptosystems invented by Ralph Merkle and Martin Hellman in 1978."}}
,{"_index":"throwtable","_type":"algorithm","_id":"key-schedule","_score":0,"_source":{"description":"In cryptography, the so-called product ciphers are a certain kind of ciphers, where the (de-)ciphering of data is done in \"rounds\". The general setup of each round is the same, except for some hard-coded parameters and a part of the cipher key, called a subkey. A key schedule is an algorithm that, given the key, calculates the subkeys for these rounds.","name":"Key schedule","categories":["All articles needing additional references","Articles needing additional references from July 2008","Cryptographic algorithms"],"tag_line":"In cryptography, the so-called product ciphers are a certain kind of ciphers, where the (de-)ciphering of data is done in \"rounds\"."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shor's-algorithm","_score":0,"_source":{"description":"Shor's algorithm, named after mathematician Peter Shor, is a quantum algorithm (an algorithm that runs on a quantum computer) for integer factorization formulated in 1994. Informally it solves the following problem: given an integer N, find its prime factors.\nOn a quantum computer, to factor an integer N, Shor's algorithm runs in polynomial time (the time taken is polynomial in log N, which is the size of the input). Specifically it takes quantum gates of order O((log N)2(log log N)(log log log N)) using fast multiplication, demonstrating that the integer factorization problem can be efficiently solved on a quantum computer and is thus in the complexity class BQP. This is substantially faster than the most efficient known classical factoring algorithm, the general number field sieve, which works in sub-exponential time — about O(e1.9 (log N)1/3 (log log N)2/3). The efficiency of Shor's algorithm is due to the efficiency of the quantum Fourier transform, and modular exponentiation by repeated squarings.\nIf a quantum computer with a sufficient number of qubits could operate without succumbing to noise and other quantum decoherence phenomena, Shor's algorithm could be used to break public-key cryptography schemes such as the widely used RSA scheme. RSA is based on the assumption that factoring large numbers is computationally intractable. So far as is known, this assumption is valid for classical (non-quantum) computers; no classical algorithm is known that can factor in polynomial time. However, Shor's algorithm shows that factoring is efficient on an ideal quantum computer, so it may be feasible to defeat RSA by constructing a large quantum computer. It was also a powerful motivator for the design and construction of quantum computers and for the study of new quantum computer algorithms. It has also facilitated research on new cryptosystems that are secure from quantum computers, collectively called post-quantum cryptography.\nIn 2001, Shor's algorithm was demonstrated by a group at IBM, who factored 15 into 3 × 5, using an NMR implementation of a quantum computer with 7 qubits. After IBM's implementation, two independent groups, one at the University of Science and Technology of China, and the other one at the University of Queensland, have implemented Shor's algorithm using photonic qubits, emphasizing that multi-qubit entanglement was observed when running the Shor's algorithm circuits. In 2012, the factorization of 15 was repeated. Also in 2012, the factorization of 21 was achieved, setting the record for the largest number factored with a quantum computer. In April 2012, the factorization of 143 was achieved, although this used adiabatic quantum computation rather than Shor's algorithm. It was discovered in November 2014 that this adiabatic quantum computation in 2012 had in fact also factored larger numbers, the largest being 56153, which is currently the record for the largest integer factored on a quantum device.","name":"Shor's algorithm","categories":["All articles lacking in-text citations","All articles needing expert attention","All articles that are too technical","Articles containing proofs","Articles lacking in-text citations from September 2010","Articles needing expert attention from February 2014","Integer factorization algorithms","Post-quantum cryptography","Quantum algorithms","Quantum information science","Wikipedia articles that are too technical from February 2014"],"tag_line":"Shor's algorithm, named after mathematician Peter Shor, is a quantum algorithm (an algorithm that runs on a quantum computer) for integer factorization formulated in 1994."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rsa-factoring-challenge","_score":0,"_source":{"description":"The RSA Factoring Challenge was a challenge put forward by RSA Laboratories on March 18, 1991 to encourage research into computational number theory and the practical difficulty of factoring large integers and cracking RSA keys used in cryptography. They published a list of semiprimes (numbers with exactly two prime factors) known as the RSA numbers, with a cash prize for the successful factorization of some of them. The smallest of them, a 100 decimal digit number called RSA-100 was factored by April 1, 1991, but many of the bigger numbers have still not been factored and are expected to remain unfactored for quite some time, however advances in quantum computers make this prediction uncertain due to Shor's algorithm.\nThe RSA challenges ended in 2007. RSA Laboratories stated: \"Now that the industry has a considerably more advanced understanding of the cryptanalytic strength of common symmetric-key and public-key algorithms, these challenges are no longer active.\"\nThe factoring challenge was intended to track the cutting edge in integer factorization. A primary application is for choosing the key length of the RSA public-key encryption scheme. Progress in this challenge should give an insight into which key sizes are still safe and for how long. As RSA Laboratories is a provider of RSA-based products, the challenge was used by them as an incentive for the academic community to attack the core of their solutions — in order to prove its strength.\nThe RSA numbers were generated on a computer with no network connection of any kind. The computer's hard drive was subsequently destroyed so that no record would exist, anywhere, of the solution to the factoring challenge.\nThe first RSA numbers generated, RSA-100 to RSA-500 and RSA-617, were labeled according to their number of decimal digits; the other RSA numbers (beginning with RSA-576) were generated later and labelled according to their number of binary digits.","name":"RSA Factoring Challenge","categories":["1991 establishments in the United States","2007 disestablishments","Cryptography contests","Integer factorization algorithms","RSA Factoring Challenge"],"tag_line":"The RSA Factoring Challenge was a challenge put forward by RSA Laboratories on March 18, 1991 to encourage research into computational number theory and the practical difficulty of factoring large integers and cracking RSA keys used in cryptography."}}
,{"_index":"throwtable","_type":"algorithm","_id":"general-number-field-sieve","_score":0,"_source":{"description":"In number theory, the general number field sieve (GNFS) is the most efficient classical algorithm known for factoring integers larger than 100 digits. Heuristically, its complexity for factoring an integer n (consisting of  bits) is of the form\n\n(in L-notation), where ln is the natural logarithm. It is a generalization of the special number field sieve: while the latter can only factor numbers of a certain special form, the general number field sieve can factor any number apart from prime powers (which are trivial to factor by taking roots). When the term number field sieve (NFS) is used without qualification, it refers to the general number field sieve.\nThe principle of the number field sieve (both special and general) can be understood as an improvement to the simpler rational sieve or quadratic sieve. When using such algorithms to factor a large number n, it is necessary to search for smooth numbers (i.e. numbers with small prime factors) of order n1/2. The size of these values is exponential in the size of n (see below). The general number field sieve, on the other hand, manages to search for smooth numbers that are subexponential in the size of n. Since these numbers are smaller, they are more likely to be smooth than the numbers inspected in previous algorithms. This is the key to the efficiency of the number field sieve. In order to achieve this speed-up, the number field sieve has to perform computations and factorizations in number fields. This results in many rather complicated aspects of the algorithm, as compared to the simpler rational sieve.\nNote that log2 n is the number of bits in the binary representation of n, that is the size of the input to the algorithm, so any element of the order nc for a constant c is exponential in log n. The running time of the number field sieve is super-polynomial but sub-exponential in the size of the input.","name":"General number field sieve","categories":["Integer factorization algorithms"],"tag_line":"In number theory, the general number field sieve (GNFS) is the most efficient classical algorithm known for factoring integers larger than 100 digits."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lenstra-elliptic-curve-factorization","_score":0,"_source":{"description":"The Lenstra elliptic curve factorization or the elliptic curve factorization method (ECM) is a fast, sub-exponential running time algorithm for integer factorization which employs elliptic curves. For general purpose factoring, ECM is the third-fastest known factoring method. The second fastest is the multiple polynomial quadratic sieve and the fastest is the general number field sieve. The Lenstra elliptic curve factorization is named after Hendrik Lenstra.\nPractically speaking, ECM is considered a special purpose factoring algorithm as it is most suitable for finding small factors. Currently, it is still the best algorithm for divisors not greatly exceeding 20 to 25 digits (64 to 83 bits or so), as its running time is dominated by the size of the smallest factor p rather than by the size of the number n to be factored. Frequently, ECM is used to remove small factors from a very large integer with many factors; if the remaining integer is still composite, then it has only large factors and is factored using general purpose techniques. The largest factor found using ECM so far has 83 digits and was discovered on 7 September 2013 by R. Propper. Increasing the number of curves tested improves the chances of finding a factor, but they are not linear with the increase in the number of digits.","name":"Lenstra elliptic curve factorization","categories":["All articles containing potentially dated statements","Articles containing potentially dated statements from 2006","Finite fields","Integer factorization algorithms"],"tag_line":"The Lenstra elliptic curve factorization or the elliptic curve factorization method (ECM) is a fast, sub-exponential running time algorithm for integer factorization which employs elliptic curves."}}
,{"_index":"throwtable","_type":"algorithm","_id":"baton","_score":0,"_source":{"description":"BATON is a Type 1 block cipher in use since at least 1995 by the United States government to secure classified information.\nWhile the BATON algorithm itself is secret (as is the case with all algorithms in the NSA's Suite A), the public PKCS#11 standard includes some general information about how it is used. It has a 320-bit key and uses a 128-bit block in most modes, and also supports a 96-bit electronic codebook mode. 160 bits of the key are checksum material. It supports a \"shuffle\" mode of operation, like the NSA cipher JUNIPER. It may use up to 192 bits as an initialization vector, regardless of the block size.\nIn response to a Senate question about encrypted video links, NSA said that BATON could be used for encryption at speeds higher than those possible with Skipjack.","name":"BATON","categories":["All stub articles","Block ciphers","Cryptography stubs","Type 1 encryption algorithms"],"tag_line":"BATON is a Type 1 block cipher in use since at least 1995 by the United States government to secure classified information."}}
,{"_index":"throwtable","_type":"algorithm","_id":"nsa-cryptography","_score":0,"_source":{"description":"The vast majority of the National Security Agency's work on encryption is classified, but from time to time NSA participates in standards processes or otherwise publishes information about its cryptographic algorithms. The NSA has categorized encryption items into four product types, and algorithms into two suites. The following is a brief and incomplete summary of public knowledge about NSA algorithms and protocols.","name":"NSA cryptography","categories":["All articles needing additional references","Articles needing additional references from February 2008","National Security Agency","National Security Agency cryptography","National Security Agency encryption devices","Type 1 encryption algorithms","Type 2 encryption algorithms","Type 3 encryption algorithms"],"tag_line":"The vast majority of the National Security Agency's work on encryption is classified, but from time to time NSA participates in standards processes or otherwise publishes information about its cryptographic algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"congruence-of-squares","_score":0,"_source":{"description":"In number theory, a congruence of squares is a congruence commonly used in integer factorization algorithms.","name":"Congruence of squares","categories":["All articles lacking sources","Articles lacking sources from December 2009","Integer factorization algorithms","Modular arithmetic"],"tag_line":"In number theory, a congruence of squares is a congruence commonly used in integer factorization algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fuzzy-clustering","_score":0,"_source":{"description":"Data clustering is the process of dividing data elements into classes or clusters so that items in the same class are as similar as possible, and items in different classes are as dissimilar as possible. Depending on the nature of the data and the purpose for which clustering is being used, different measures of similarity may be used to place items into classes, where the similarity measure controls how the clusters are formed. Some examples of measures that can be used as in clustering include distance, connectivity, and intensity.\nIn hard clustering, data is divided into distinct clusters, where each data element belongs to exactly one cluster. In fuzzy clustering (also referred to as soft clustering), data elements can belong to more than one cluster, and associated with each element is a set of membership levels. These indicate the strength of the association between that data element and a particular cluster. Fuzzy clustering is a process of assigning these membership levels, and then using them to assign data elements to one or more clusters.\nOne of the most widely used fuzzy clustering algorithms is the Fuzzy C-Means (FCM) Algorithm (Bezdek 1981). The FCM algorithm attempts to partition a finite collection of  elements  into a collection of c fuzzy clusters with respect to some given criterion. Given a finite set of data, the algorithm returns a list of  cluster centres  and a partition matrix , where each element  tells the degree to which element  belongs to cluster . Like the K-means clustering, the FCM aims to minimize an objective function:\n\nwhere:\n\nThis differs from the k-means objective function by the addition of the membership values  and the fuzzifier , with . The fuzzifier  determines the level of cluster fuzziness. A large  results in smaller memberships  and hence, fuzzier clusters. In the limit , the memberships  converge to 0 or 1, which implies a crisp partitioning. In the absence of experimentation or domain knowledge,  is commonly set to 2.\n\n","name":"Fuzzy clustering","categories":["All articles lacking in-text citations","All articles needing cleanup","Articles lacking in-text citations from August 2009","Articles needing cleanup from October 2011","Articles with inconsistent citation formats","Cleanup tagged articles with a reason field from October 2011","Data clustering algorithms","Wikipedia pages needing cleanup from October 2011"],"tag_line":"Data clustering is the process of dividing data elements into classes or clusters so that items in the same class are as similar as possible, and items in different classes are as dissimilar as possible."}}
,{"_index":"throwtable","_type":"algorithm","_id":"k-means-clustering","_score":0,"_source":{"description":"k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.\nThe problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.\nThe algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means because of the k in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.","name":"K-means clustering","categories":["All articles with dead external links","All articles with unsourced statements","Articles with dead external links from January 2013","Articles with unsourced statements from March 2014","CS1 French-language sources (fr)","Data clustering algorithms","Statistical algorithms"],"tag_line":"k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gsp-algorithm","_score":0,"_source":{"description":"GSP Algorithm (Generalized Sequential Pattern algorithm) is an algorithm used for sequence mining. The algorithms for solving sequence mining problems are mostly based on the a priori (level-wise) algorithm. One way to use the level-wise paradigm is to first discover all the frequent items in a level-wise fashion. It simply means counting the occurrences of all singleton elements in the database. Then, the transactions are filtered by removing the non-frequent items. At the end of this step, each transaction consists of only the frequent elements it originally contained. This modified database becomes an input to the GSP algorithm. This process requires one pass over the whole database.\nGSP Algorithm makes multiple database passes. In the first pass, all single items (1-sequences) are counted. From the frequent items, a set of candidate 2-sequences are formed, and another pass is made to identify their frequency. The frequent 2-sequences are used to generate the candidate 3-sequences, and this process is repeated until no more frequent sequences are found. There are two main steps in the algorithm.\nCandidate Generation. Given the set of frequent (k-1)-frequent sequences F(k-1), the candidates for the next pass are generated by joining F(k-1) with itself. A pruning phase eliminates any sequence, at least one of whose subsequences is not frequent.\nSupport Counting. Normally, a hash tree–based search is employed for efficient support counting. Finally non-maximal frequent sequences are removed.","name":"GSP Algorithm","categories":["All articles lacking sources","Articles lacking sources from May 2007","Articles with example pseudocode","Data mining algorithms","Pages with citations lacking titles"],"tag_line":"GSP Algorithm (Generalized Sequential Pattern algorithm) is an algorithm used for sequence mining."}}
,{"_index":"throwtable","_type":"algorithm","_id":"alpha-algorithm","_score":0,"_source":{"description":"The α-algorithm is an algorithm used in process mining, aimed at reconstructing causality from a set of sequences of events. It was first put forward by van der Aalst, Weijters and Măruşter. Several extensions or modifications of it have since been presented, which will be listed below.\nIt constructs P/T nets with special properties (workflow nets) from event logs (as might be collected by an ERP system). Each transition in the net corresponds to an observed task.","name":"Alpha algorithm","categories":["All articles to be expanded","Articles to be expanded from May 2010","Data mining algorithms","Process mining"],"tag_line":"The α-algorithm is an algorithm used in process mining, aimed at reconstructing causality from a set of sequences of events."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cluster-weighted-modeling","_score":0,"_source":{"description":"In data mining, cluster-weighted modeling (CWM) is an algorithm-based approach to non-linear prediction of outputs (dependent variables) from inputs (independent variables) based on density estimation using a set of models (clusters) that are each notionally appropriate in a sub-region of the input space. The overall approach works in jointly input-output space and an initial version was proposed by Neil Gershenfeld.","name":"Cluster-weighted modeling","categories":["Data clustering algorithms","Estimation of densities","Multivariate statistics"],"tag_line":"In data mining, cluster-weighted modeling (CWM) is an algorithm-based approach to non-linear prediction of outputs (dependent variables) from inputs (independent variables) based on density estimation using a set of models (clusters) that are each notionally appropriate in a sub-region of the input space."}}
,{"_index":"throwtable","_type":"algorithm","_id":"data-stream-clustering","_score":0,"_source":{"description":"In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. Data stream clustering is usually studied as a streaming algorithm and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time.","name":"Data stream clustering","categories":["Data clustering algorithms"],"tag_line":"In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mean-shift","_score":0,"_source":{"description":"Mean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. Application domains include cluster analysis in computer vision and image processing.","name":"Mean shift","categories":["Computer vision","Data clustering algorithms","Pages using citations with accessdate and no URL"],"tag_line":"Mean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"alopex","_score":0,"_source":{"description":"ALOPEX (an acronym from \"ALgorithms Of Pattern EXtraction\") is a correlation based machine learning algorithm first proposed by Tzanakou and Harth in 1974.","name":"ALOPEX","categories":["All stub articles","Artificial intelligence stubs","Artificial neural networks","Classification algorithms"],"tag_line":"ALOPEX (an acronym from \"ALgorithms Of Pattern EXtraction\") is a correlation based machine learning algorithm first proposed by Tzanakou and Harth in 1974."}}
,{"_index":"throwtable","_type":"algorithm","_id":"boruta-(algorithm)","_score":0,"_source":{"description":"Boruta is an algorithm in the field of machine-learning, and more specifically, a feature-selection algorithm. The aim of the algorithm as presented in the original paper describing it is to find all relevant features (compare with minimal-optimal features set). The Boruta algorithm is not a stand-alone algorithm, but is implemented as a wrapper algorithm around the random-forest classification algorithm. In its essence, Boruta works in an iterative manner, and in each iteration the aim is to remove features which according to a statistical test, are less relevant than what is defined by the authors as a random probe. One of the fundamental components of Boruta is the use of shadow attributes. Shadow attributes are pseudo-features that are added to the information system, and produced by taking existing features from the original data-set and shuffling the values of those features between the original samples (data points). After generating the shadow attributes the procedure proceeds with building random-forest trees and comparing the Z-scores obtained by original features to Z-scores obtained by the shadow attributes. This comparison is the foundation for Boruta to decide whether a feature is important or not.\n\n High level pseudo-code:\n\n1.  Copy all variables (features)\n2.  Shuffle values in each feature\n3.  Run random-forest on the extended system (shuffled features), gather Z scores\n4.  Find maximum MSZA (max Z-score among shadow attributes)\n5.  Run random-forest on original features\n6.  Assign each original feature a hit if feature Z-score > MSZA\n7.  If Z-score <= MSZA, perform two-side equality test against MSZA\n8.  If Z-score < MSZA significantly, drop feature as unimportant\n9.  If Z-score > MSZA significantly, keep feature as important\n10. Repeat from step 5 until all importance is determined for all features or max RF runs have been reached","name":"Boruta (algorithm)","categories":["All articles needing additional references","All articles with topics of unclear notability","Articles needing additional references from March 2015","Articles needing additional references from September 2014","Articles with topics of unclear notability from March 2015","Classification algorithms","Decision trees","Ensemble learning"],"tag_line":"Boruta is an algorithm in the field of machine-learning, and more specifically, a feature-selection algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"compositional-pattern-producing-network","_score":0,"_source":{"description":"Compositional pattern-producing networks (CPPNs), are a variation of artificial neural networks (ANNs) which differ in their set of activation functions and how they are applied.\nWhile ANNs often contain only sigmoid functions and sometimes Gaussian functions, CPPNs can include both types of functions and many others. The choice of functions for the canonical set can be biased toward specific types of patterns and regularities. For example, periodic functions such as sine produce segmented patterns with repetitions, while symmetric functions such as Gaussian produce symmetric patterns. Linear functions can be employed to produce linear or fractal-like patterns. Thus, the architect of a CPPN-based genetic art system can bias the types of patterns it generates by deciding the set of canonical functions to include.\nFurthermore, unlike typical ANNs, CPPNs are applied across the entire space of possible inputs so that they can represent a complete image. Since they are compositions of functions, CPPNs in effect encode images at infinite resolution and can be sampled for a particular display at whatever resolution is optimal.\nCPPNs can be evolved through neuroevolution techniques such as NeuroEvolution of Augmenting Topologies (called CPPN-NEAT).\nCPPNs have been shown to be a very powerful encoding when evolving the following:\nNeural Networks, via the HyperNEAT algorithm\n2D images, on \"PicBreeder.org\"\n3D objects, on \"EndlessForms.com\"\nRobot Morphologies Rigid Robots Soft Robots","name":"Compositional pattern-producing network","categories":["Artificial neural networks","Classification algorithms"],"tag_line":"Compositional pattern-producing networks (CPPNs), are a variation of artificial neural networks (ANNs) which differ in their set of activation functions and how they are applied."}}
,{"_index":"throwtable","_type":"algorithm","_id":"multispectral-pattern-recognition","_score":0,"_source":{"description":"Multispectral remote sensing is the collection and analysis of reflected, emitted, or back-scattered energy from an object or an area of interest in multiple bands of regions of the electromagnetic spectrum (Jensen, 2005). Subcategories of multispectral remote sensing include hyperspectral, in which hundreds of bands are collected and analyzed, and ultraspectral remote sensing where many hundreds of bands are used (Logicon, 1997). The main purpose of multispectral imaging is the potential to classify the image using multispectral classification. This is a much faster method of image analysis than is possible by human interpretation.\nThe Iterative Self-Organizing Data Analysis Technique (ISODATA) algorithm used for Multispectral pattern recognition was developed by Geoffrey H. Ball and David J. Hall, working in the Stanford Research Institute in Menlo Park, CA. They published their findings in a technical report entitled: ISODATA, a novel method of data analysis and pattern classification (Stanford Research Institute, 1965). ISODATA is defined in the abstract as: 'a novel method of data analysis and pattern classification, is described in verbal and pictorial terms, in terms of a two-dimensional example, and by giving the mathematical calculations that the method uses. The technique clusters many-variable data around points in the data's original high- dimensional space and by doing so provides a useful description of the data.' (1965, pp v.)ISODATA was developed to facilitate the modelling and tracking of weather patterns.","name":"Multispectral pattern recognition","categories":["Classification algorithms","Imaging"],"tag_line":"Multispectral remote sensing is the collection and analysis of reflected, emitted, or back-scattered energy from an object or an area of interest in multiple bands of regions of the electromagnetic spectrum (Jensen, 2005)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"butterfly-diagram","_score":0,"_source":{"description":"This article is about butterfly diagrams in FFT algorithms; for the sunspot diagrams of the same name, see Solar cycle.\n\nIn the context of fast Fourier transform algorithms, a butterfly is a portion of the computation that combines the results of smaller discrete Fourier transforms (DFTs) into a larger DFT, or vice versa (breaking a larger DFT up into subtransforms). The name \"butterfly\" comes from the shape of the data-flow diagram in the radix-2 case, as described below. The earliest occurrence in print of the term is thought to be in a 1969 MIT technical report. The same structure can also be found in the Viterbi algorithm, used for finding the most likely sequence of hidden states.\nMost commonly, the term \"butterfly\" appears in the context of the Cooley–Tukey FFT algorithm, which recursively breaks down a DFT of composite size n = rm into r smaller transforms of size m where r is the \"radix\" of the transform. These smaller DFTs are then combined via size-r butterflies, which themselves are DFTs of size r (performed m times on corresponding outputs of the sub-transforms) pre-multiplied by roots of unity (known as twiddle factors). (This is the \"decimation in time\" case; one can also perform the steps in reverse, known as \"decimation in frequency\", where the butterflies come first and are post-multiplied by twiddle factors. See also the Cooley–Tukey FFT article.)","name":"Butterfly diagram","categories":["Diagrams","FFT algorithms"],"tag_line":"This article is about butterfly diagrams in FFT algorithms; for the sunspot diagrams of the same name, see Solar cycle."}}
,{"_index":"throwtable","_type":"algorithm","_id":"relevance-vector-machine","_score":0,"_source":{"description":"In mathematics, a relevance vector machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification. The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.\nIt is actually equivalent to a Gaussian process model with covariance function:\n\nwhere  is the kernel function (usually Gaussian),'s as the variances of the prior on the weight vector  ,and  are the input vectors of the training set.\nCompared to that of support vector machines (SVM), the Bayesian formulation of the RVM avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, which are guaranteed to find a global optimum (of the convex problem).\nThe relevance vector machine is patented in the United States by Microsoft.","name":"Relevance vector machine","categories":["All articles with unsourced statements","Articles with unsourced statements from February 2010","Classification algorithms","Kernel methods for machine learning","Nonparametric Bayesian statistics"],"tag_line":"In mathematics, a relevance vector machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification."}}
,{"_index":"throwtable","_type":"algorithm","_id":"margin-infused-relaxed-algorithm","_score":0,"_source":{"description":"Margin-infused relaxed algorithm (MIRA) is a machine learning algorithm, an online algorithm for multiclass classification problems. It is designed to learn a set of parameters (vector or matrix) by processing all the given training examples one-by-one and updating the parameters according to each training example, so that the current training example is classified correctly with a margin against incorrect classifications at least as large as their loss. The change of the parameters is kept as small as possible.\nA two-class version called binary MIRA simplifies the algorithm by not requiring the solution of a quadratic programming problem (see below). When used in a one-vs.-all configuration, binary MIRA can be extended to a multiclass learner that approximates full MIRA, but may be faster to train.\nThe flow of the algorithm looks as follows:\n\nThe update step is then formalized as a quadratic programming problem: Find , so that , i.e. the score of the current correct training  must be greater than the score of any other possible  by at least the loss (number of errors) of that  in comparison to .\n\n","name":"Margin Infused Relaxed Algorithm","categories":["Classification algorithms"],"tag_line":"Margin-infused relaxed algorithm (MIRA) is a machine learning algorithm, an online algorithm for multiclass classification problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"c4.5-algorithm","_score":0,"_source":{"description":"C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier.\nIt became quite popular after ranking #1 in the Top 10 Algorithms in Data Mining pre-eminent paper published by Springer LNCS in 2008.\n\n","name":"C4.5 algorithm","categories":["All NPOV disputes","All articles lacking in-text citations","Articles lacking in-text citations from July 2008","Classification algorithms","Decision trees","NPOV disputes from August 2011"],"tag_line":"C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan."}}
,{"_index":"throwtable","_type":"algorithm","_id":"id3-algorithm","_score":0,"_source":{"description":"In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domains.","name":"ID3 algorithm","categories":["Articles with example pseudocode","Classification algorithms","Decision trees"],"tag_line":"In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset."}}
,{"_index":"throwtable","_type":"algorithm","_id":"co-training","_score":0,"_source":{"description":"Co-training is a machine learning algorithm used when there are only small amounts of labeled data and large amounts of unlabeled data. One of its uses is in text mining for search engines. It was introduced by Avrim Blum and Tom Mitchell in 1998.","name":"Co-training","categories":["Classification algorithms"],"tag_line":"Co-training is a machine learning algorithm used when there are only small amounts of labeled data and large amounts of unlabeled data."}}
,{"_index":"throwtable","_type":"algorithm","_id":"level-set-method","_score":0,"_source":{"description":"Level set methods (LSM) are a conceptual framework for using level sets as a tool for numerical analysis of surfaces and shapes. The advantage of the level set model is that one can perform numerical computations involving curves and surfaces on a fixed Cartesian grid without having to parameterize these objects (this is called the Eulerian approach). Also, the level set method makes it very easy to follow shapes that change topology, for example when a shape splits in two, develops holes, or the reverse of these operations. All these make the level set method a great tool for modeling time-varying objects, like inflation of an airbag, or a drop of oil floating in water.\n\nThe figure on the right illustrates several important ideas about the level set method. In the upper-left corner we see a shape; that is, a bounded region with a well-behaved boundary. Below it, the red surface is the graph of a level set function  determining this shape, and the flat blue region represents the xy-plane. The boundary of the shape is then the zero level set of , while the shape itself is the set of points in the plane for which  is positive (interior of the shape) or zero (at the boundary).\nIn the top row we see the shape changing its topology by splitting in two. It would be quite hard to describe this transformation numerically by parameterizing the boundary of the shape and following its evolution. One would need an algorithm able to detect the moment the shape splits in two, and then construct parameterizations for the two newly obtained curves. On the other hand, if we look at the bottom row, we see that the level set function merely translated downward. This is an example of when it can be much easier to work with a shape through its level set function than with the shape directly, where using the shape directly would need to consider and handle all the possible deformations the shape might undergo.\nThus, in two dimensions, the level set method amounts to representing a closed curve  (such as the shape boundary in our example) using an auxiliary function , called the level set function.  is represented as the zero level set of  by\n\nand the level set method manipulates  implicitly, through the function . This function  is assumed to take positive values inside the region delimited by the curve  and negative values outside.","name":"Level set method","categories":["Articles containing video clips","Computational fluid dynamics","Computer graphics algorithms","Image processing","Mathematical optimization","Numerical analysis"],"tag_line":"Level set methods (LSM) are a conceptual framework for using level sets as a tool for numerical analysis of surfaces and shapes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"false-radiosity","_score":0,"_source":{"description":"False Radiosity is a 3D computer graphics technique used to create texture mapping for objects that emulates patch interaction algorithms in radiosity rendering. Though practiced in some form since the late 90s, this term was coined only around 2002 by architect Andrew Hartness, then head of 3D and real-time design at Ateliers Jean Nouvel.\nDuring the period of nascent commercial enthusiasm for radiosity-enhanced imagery, but prior to the democratization of powerful computational hardware, architects and graphic artists experimented with time-saving 3D rendering techniques. By darkening areas of texture maps corresponding to corners, joints and recesses, and applying maps via self-illumination or diffuse mapping in a 3D program, a radiosity-like effect of patch interaction could be created with a standard scan-line renderer. Successful emulation of radiosity required a theoretical understanding and graphic application of patch view factors, path tracing and global illumination algorithms. Texture maps were usually produced with image editing software, such as Adobe Photoshop. The advantage of this method is decreased rendering time and easily modifiable overall lighting strategies.\nAnother common approach similar to false radiosity is the manual placement of standard omni-type lights with limited attenuation in places in the 3D scene where the artist would expect radiosity reflections to occur. This method uses many lights and can require an advanced light-grouping system, depending on what assigned materials/objects are illuminated, how many surfaces require false radiosity treatment, and to what extent it is anticipated that lighting strategies be set up for frequent changes.","name":"False radiosity","categories":["3D computer graphics","All stub articles","Computer graphics algorithms","Graphics software stubs","Image processing","Rendering systems"],"tag_line":"False Radiosity is a 3D computer graphics technique used to create texture mapping for objects that emulates patch interaction algorithms in radiosity rendering."}}
,{"_index":"throwtable","_type":"algorithm","_id":"prime-factor-fft-algorithm","_score":0,"_source":{"description":"The prime-factor algorithm (PFA), also called the Good–Thomas algorithm (1958/1963), is a fast Fourier transform (FFT) algorithm that re-expresses the discrete Fourier transform (DFT) of a size N = N1N2 as a two-dimensional N1×N2 DFT, but only for the case where N1 and N2 are relatively prime. These smaller transforms of size N1 and N2 can then be evaluated by applying PFA recursively or by using some other FFT algorithm.\nPFA should not be confused with the mixed-radix generalization of the popular Cooley–Tukey algorithm, which also subdivides a DFT of size N = N1N2 into smaller transforms of size N1 and N2. The latter algorithm can use any factors (not necessarily relatively prime), but it has the disadvantage that it also requires extra multiplications by roots of unity called twiddle factors, in addition to the smaller transforms. On the other hand, PFA has the disadvantages that it only works for relatively prime factors (e.g. it is useless for power-of-two sizes) and that it requires a more complicated re-indexing of the data based on the Chinese remainder theorem (CRT). Note, however, that PFA can be combined with mixed-radix Cooley–Tukey, with the former factorizing N into relatively prime components and the latter handling repeated factors.\nPFA is also closely related to the nested Winograd FFT algorithm, where the latter performs the decomposed N1 by N2 transform via more sophisticated two-dimensional convolution techniques. Some older papers therefore also call Winograd's algorithm a PFA FFT.\n(Although the PFA is distinct from the Cooley–Tukey algorithm, Good's 1958 work on the PFA was cited as inspiration by Cooley and Tukey in their famous 1965 paper, and there was initially some confusion about whether the two algorithms were different. In fact, it was the only prior FFT work cited by them, as they were not then aware of the earlier research by Gauss and others.)","name":"Prime-factor FFT algorithm","categories":["FFT algorithms"],"tag_line":"The prime-factor algorithm (PFA), also called the Good–Thomas algorithm (1958/1963), is a fast Fourier transform (FFT) algorithm that re-expresses the discrete Fourier transform (DFT) of a size N = N1N2 as a two-dimensional N1×N2 DFT, but only for the case where N1 and N2 are relatively prime."}}
,{"_index":"throwtable","_type":"algorithm","_id":"split-radix-fft-algorithm","_score":0,"_source":{"description":"The split-radix FFT is a fast Fourier transform (FFT) algorithm for computing the discrete Fourier transform (DFT), and was first described in an initially little-appreciated paper by R. Yavne (1968) and subsequently rediscovered simultaneously by various authors in 1984. (The name \"split radix\" was coined by two of these reinventors, P. Duhamel and H. Hollmann.) In particular, split radix is a variant of the Cooley-Tukey FFT algorithm that uses a blend of radices 2 and 4: it recursively expresses a DFT of length N in terms of one smaller DFT of length N/2 and two smaller DFTs of length N/4.\nThe split-radix FFT, along with its variations, long had the distinction of achieving the lowest published arithmetic operation count (total exact number of required real additions and multiplications) to compute a DFT of power-of-two sizes N. The arithmetic count of the original split-radix algorithm was improved upon in 2004 (with the initial gains made in unpublished work by J. Van Buskirk via hand optimization for N=64 [1] [2]), but it turns out that one can still achieve the new lowest count by a modification of split radix (Johnson and Frigo, 2007). Although the number of arithmetic operations is not the sole factor (or even necessarily the dominant factor) in determining the time required to compute a DFT on a computer, the question of the minimum possible count is of longstanding theoretical interest. (No tight lower bound on the operation count has currently been proven.)\nThe split-radix algorithm can only be applied when N is a multiple of 4, but since it breaks a DFT into smaller DFTs it can be combined with any other FFT algorithm as desired.","name":"Split-radix FFT algorithm","categories":["FFT algorithms"],"tag_line":"The split-radix FFT is a fast Fourier transform (FFT) algorithm for computing the discrete Fourier transform (DFT), and was first described in an initially little-appreciated paper by R. Yavne (1968) and subsequently rediscovered simultaneously by various authors in 1984."}}
,{"_index":"throwtable","_type":"algorithm","_id":"twiddle-factor","_score":0,"_source":{"description":"A twiddle factor, in fast Fourier transform (FFT) algorithms, is any of the trigonometric constant coefficients that are multiplied by the data in the course of the algorithm. This term was apparently coined by Gentleman & Sande in 1966, and has since become widespread in thousands of papers of the FFT literature.\nMore specifically, \"twiddle factors\" originally referred to the root-of-unity complex multiplicative constants in the butterfly operations of the Cooley-Tukey FFT algorithm, used to recursively combine smaller discrete Fourier transforms. This remains the term's most common meaning, but it may also be used for any data-independent multiplicative constant in an FFT.\nThe Prime-factor FFT algorithm is one unusual case in which an FFT can be performed without twiddle factors, albeit only for restricted factorizations of the transform size.","name":"Twiddle factor","categories":["FFT algorithms"],"tag_line":"A twiddle factor, in fast Fourier transform (FFT) algorithms, is any of the trigonometric constant coefficients that are multiplied by the data in the course of the algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"evolutionary-algorithm","_score":0,"_source":{"description":"In artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators. Artificial evolution (AE) describes a process involving individual evolutionary algorithms; EAs are individual components that participate in an AE.\nEvolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape; this generality is shown by successes in fields as diverse as engineering, art, biology, economics, marketing, genetics, operations research, robotics, social sciences, physics, politics and chemistry.\nTechniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. The computer simulations Tierra and Avida attempt to model macroevolutionary dynamics.\nIn most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.\nA possible limitation  of many evolutionary algorithms is their lack of a clear genotype-phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism. Such indirect (aka generative or developmental) encodings also enable evolution to exploit the regularity in the environment. Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype-phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes.","name":"Evolutionary algorithm","categories":["All articles with specifically marked weasel-worded phrases","All articles with unsourced statements","Articles that may contain original research from May 2013","Articles with specifically marked weasel-worded phrases from May 2013","Articles with unsourced statements from May 2015","Articles with unsourced statements from September 2008","Cybernetics","Evolution","Evolutionary algorithms","Optimization algorithms and methods"],"tag_line":"In artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bregman-divergence","_score":0,"_source":{"description":"In mathematics, a Bregman divergence or Bregman distance is similar to a metric, but does not satisfy the triangle inequality nor symmetry. There are two ways in which Bregman divergences are important. Firstly, they generalize squared Euclidean distance to a class of distances that all share similar properties. Secondly, they bear a strong connection to exponential families of distributions; as has been shown by (Banerjee et al. 2005), there is a bijection between regular exponential families and regular Bregman divergences.\nBregman divergences are named after Lev M. Bregman, who introduced the concept in 1967. More recently researchers in geometric algorithms have shown that many important algorithms can be generalized from Euclidean metrics to distances defined by Bregman divergence (Banerjee et al. 2005; Nielsen and Nock 2006; Boissonnat et al. 2010).","name":"Bregman divergence","categories":["Geometric algorithms","Statistical distance measures"],"tag_line":"In mathematics, a Bregman divergence or Bregman distance is similar to a metric, but does not satisfy the triangle inequality nor symmetry."}}
,{"_index":"throwtable","_type":"algorithm","_id":"prune-and-search","_score":0,"_source":{"description":"Prune and search is a method of solving optimization problems suggested by Nimrod Megiddo in 1983. \nThe basic idea of the method is a recursive procedure in which at each step the input size is reduced (\"pruned\") by a constant factor 0 < p < 1. As such, it is a form of decrease and conquer algorithm, where at each step the decrease is by a constant factor. Let n be the input size, T(n) be the time complexity of the whole prune-and-search algorithm, S(n) is the time complexity of the pruning step, then T(n) obeys the following recurrence relation:\n\nwhich has the solution T(n) = O(S(n)), since summing a geometric series only multiplies by a constant factor, namely \nIn particular, Megiddo himself used this approach in his linear time algorithm for the linear programming problem when the dimension is fixed and for the minimal enclosing sphere problem for a set of points in space.\n\n","name":"Prune and search","categories":["Geometric algorithms","Linear programming"],"tag_line":"Prune and search is a method of solving optimization problems suggested by Nimrod Megiddo in 1983."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cgal","_score":0,"_source":{"description":"The Computational Geometry Algorithms Library (CGAL) is a software library of computational geometry algorithms. While primarily written in C++, Scilab bindings and bindings generated with SWIG (supporting Python and Java for now) are also available.\nThe software is available under dual licensing scheme. When used for other open source software, it is available under open source licenses (LGPL or GPL depending on the component). In other cases commercial license may be purchased, under different options for academic/research and industrial customers.","name":"CGAL","categories":["All articles containing potentially dated statements","Articles containing potentially dated statements from 2010","Articles containing potentially dated statements from 2013","C++ libraries","Free computer libraries","Geometric algorithms","Max Planck Institute for Informatics","Python libraries"],"tag_line":"The Computational Geometry Algorithms Library (CGAL) is a software library of computational geometry algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"möller–trumbore-intersection-algorithm","_score":0,"_source":{"description":"The Möller–Trumbore ray-triangle intersection algorithm, named after its inventors Tomas Möller and Ben Trumbore, is a fast method for calculating the intersection of a ray and a triangle in three dimensions without needing precomputation of the plane equation of the plane containing the triangle. Among other uses, it can be used in computer graphics to implement ray tracing computations involving triangle meshes.","name":"Möller–Trumbore intersection algorithm","categories":["All stub articles","Computer science stubs","Geometric algorithms","Geometry","Geometry stubs"],"tag_line":"The Möller–Trumbore ray-triangle intersection algorithm, named after its inventors Tomas Möller and Ben Trumbore, is a fast method for calculating the intersection of a ray and a triangle in three dimensions without needing precomputation of the plane equation of the plane containing the triangle."}}
,{"_index":"throwtable","_type":"algorithm","_id":"learning-classifier-system","_score":0,"_source":{"description":"A learning classifier system, or LCS, is a machine learning system with close links to reinforcement learning and genetic algorithms. First described by John Holland, his LCS consisted of a population of binary rules on which a genetic algorithm altered and selected the best rules. Rule fitness was based on a reinforcement learning technique.","name":"Learning classifier system","categories":["All articles with unsourced statements","All stub articles","Articles with unsourced statements from February 2014","Articles with unsourced statements from July 2015","Evolutionary algorithms","Technology stubs"],"tag_line":"A learning classifier system, or LCS, is a machine learning system with close links to reinforcement learning and genetic algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"midpoint-circle-algorithm","_score":0,"_source":{"description":"In computer graphics, the midpoint circle algorithm is an algorithm used to determine the points needed for drawing a circle. Bresenham's circle algorithm is derived from the midpoint circle algorithm. The algorithm can be generalized to conic sections.\n\nThe algorithm is related to work by Pitteway and Van Aken.","name":"Midpoint circle algorithm","categories":["All Wikipedia articles needing clarification","Articles with example C code","Articles with example JavaScript code","Digital geometry","Geometric algorithms","Wikipedia articles needing clarification from February 2009"],"tag_line":"In computer graphics, the midpoint circle algorithm is an algorithm used to determine the points needed for drawing a circle."}}
,{"_index":"throwtable","_type":"algorithm","_id":"artificial-immune-system","_score":0,"_source":{"description":"In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent systems inspired by the principles and processes of the vertebrate immune system. The algorithms typically exploit the immune system's characteristics of learning and memory to solve a problem.","name":"Artificial immune system","categories":["Artificial immune systems","Evolutionary algorithms","Pages using citations with accessdate and no URL","Pages with citations lacking titles"],"tag_line":"In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent systems inspired by the principles and processes of the vertebrate immune system."}}
,{"_index":"throwtable","_type":"algorithm","_id":"godfried-toussaint","_score":0,"_source":{"description":"Godfried T. Toussaint is a Professor of Computer Science and the Head of the Computer Science Program at New York University Abu Dhabi (NYUAD) in Abu Dhabi, United Arab Emirates. He does research on various aspects of computational geometry, discrete geometry, and their applications: pattern recognition (k-nearest neighbor algorithm, cluster analysis), motion planning, visualization (computer graphics), knot theory (stuck unknot problem), linkage (mechanical) reconfiguration, the art gallery problem, polygon triangulation, the largest empty circle problem, unimodality (unimodal function), and others. Other interests include meander (art), compass and straightedge constructions, instance-based learning, music information retrieval, and computational music theory.\nHe is a co-founder of the Annual ACM Symposium on Computational Geometry, and the Annual Canadian Conference on Computational Geometry.\nAlong with Selim Akl, he is an author and namesake of the efficient \"Akl–Toussaint algorithm\" for the construction of the convex hull of a planar point set. This algorithm exhibits a computational complexity with expected value linear in the size of the input. In 1980 he introduced the relative neighborhood graph (RNG) to the fields of pattern recognition and machine learning, and showed that it contained the minimum spanning tree, and was a subgraph of the Delaunay triangulation. Three other well known proximity graphs are the nearest neighbor graph, the Urquhart graph, and the Gabriel graph. The first is contained in the minimum spanning tree, and the Urquhart graph contains the RNG, and is contained in the Delaunay triangulation. Since all these graphs are nested together they are referred to as the Toussaint hierarchy.","name":"Godfried Toussaint","categories":["Canadian computer scientists","Living people","McGill University faculty","New York University Abu Dhabi faculty","Researchers in geometric algorithms","Wikipedia articles with ISNI identifiers","Wikipedia articles with VIAF identifiers","Wikipedia articles with possible conflicts of interest from August 2010"],"tag_line":"Godfried T. Toussaint is a Professor of Computer Science and the Head of the Computer Science Program at New York University Abu Dhabi (NYUAD) in Abu Dhabi, United Arab Emirates."}}
,{"_index":"throwtable","_type":"algorithm","_id":"jit-bose","_score":0,"_source":{"description":"Prosenjit K. \"Jit\" Bose is a Canadian mathematician and computer scientist who works at Carleton University as a professor in the School of Computer Science and associate dean of research and graduate studies for the Faculty of Science. His research concerns graph algorithms and computational geometry, including work on geometric spanners and geographic routing in wireless ad hoc networks.\nBose did his undergraduate studies in mathematics at the University of Waterloo, graduating in 1990, and earned a master's degree from Waterloo in 1991. He earned his Ph.D. in computer science from McGill University in 1994 under the supervision of Godfried Toussaint. After postdoctoral studies at the University of British Columbia, he became an assistant professor at the Université du Québec à Trois-Rivières in 1995, and moved to Carleton in 1997.","name":"Jit Bose","categories":["20th-century mathematicians","21st-century mathematicians","Canadian computer scientists","Canadian mathematicians","Carleton University faculty","Graph drawing people","Living people","McGill University alumni","Researchers in geometric algorithms","University of Waterloo alumni","Université du Québec à Trois-Rivières faculty","Year of birth missing (living people)"],"tag_line":"Prosenjit K. \"Jit\" Bose is a Canadian mathematician and computer scientist who works at Carleton University as a professor in the School of Computer Science and associate dean of research and graduate studies for the Faculty of Science."}}
,{"_index":"throwtable","_type":"algorithm","_id":"marching-cubes","_score":0,"_source":{"description":"Marching cubes is a computer graphics algorithm, published in the 1987 SIGGRAPH proceedings by Lorensen and Cline, for extracting a polygonal mesh of an isosurface from a three-dimensional discrete scalar field (sometimes called voxels). This paper is one of the most cited papers in the computer graphics field. The applications of this algorithm are mainly concerned with medical visualizations such as CT and MRI scan data images, and special effects or 3-D modelling with what is usually called metaballs or other metasurfaces. An analogous two-dimensional method is called the marching squares algorithm.\n\n","name":"Marching cubes","categories":["3D computer graphics","All articles with unsourced statements","Articles with unsourced statements from August 2015","Commons category template with no category set","Commons category without a link on Wikidata","Computer graphics algorithms","Mesh generation"],"tag_line":"Marching cubes is a computer graphics algorithm, published in the 1987 SIGGRAPH proceedings by Lorensen and Cline, for extracting a polygonal mesh of an isosurface from a three-dimensional discrete scalar field (sometimes called voxels)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"beier–neely-morphing-algorithm","_score":0,"_source":{"description":"Image morphing is a technique to synthesize a fluid transformation from one image (source image) to another (destination image). Source image can be one or more than one images. There are two parts in the image morphing implementation. The first part is warping and the second part is cross-dissolving.\nThe algorithm of Beier and Neely is a method to compute a mapping of coordinates between 2 images from a set of lines; i.e., the warp is specified by a set of line pairs where the start-points and end-points are given for both images. The algorithm is widely used within morphing software.\nAlso noteworthy, this algorithm only discussed about the situation with at most 2 source images as there are other algorithms introducing multiple source images.\n\n","name":"Beier–Neely morphing algorithm","categories":["Algorithms and data structures stubs","All stub articles","Computer graphics algorithms","Computer science stubs"],"tag_line":"Image morphing is a technique to synthesize a fluid transformation from one image (source image) to another (destination image)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sgi-algorithm","_score":0,"_source":{"description":"The SGI algorithm creates triangle strips from a set of triangles. It was published by K. Akeley, P. Haeberli, and D. Burns as a C program named \"tomesh.c\" for use with Silicon Graphics' IRIS GL API.\nThe algorithm operates on the set of triangles that have not yet been added to a triangle strip, starting with the entire set of input triangles. Triangles are greedily added to a strip until no triangle is available that can be appended to the strip; a new strip will be started in this case. When choosing a triangle for starting or continuing a triangle strip, the selection is based on a triangle's degree (i.e. the number of triangles adjacent to it), with smaller degrees being preferred.\nIf implemented using a priority queue to quickly identify triangles that can start a new strip, the algorithm runs in linear time.","name":"SGI algorithm","categories":["Computer graphics algorithms"],"tag_line":"The SGI algorithm creates triangle strips from a set of triangles."}}
,{"_index":"throwtable","_type":"algorithm","_id":"simulated-fluorescence-process-algorithm","_score":0,"_source":{"description":"The Simulated Fluorescence Process (SFP) is a computing algorithm used for scientific visualization of 3D data from, for example, fluorescence microscopes. By modeling a physical light/matter interaction process an image is computed showing the data as it would have appeared in reality when viewed under these conditions.","name":"Simulated fluorescence process algorithm","categories":["All articles covered by WikiProject Wikify","All articles lacking in-text citations","All articles with too few wikilinks","All stub articles","Articles covered by WikiProject Wikify from March 2015","Articles lacking in-text citations from December 2010","Articles with too few wikilinks from March 2015","Computational science","Computer graphics algorithms","Fluorescence","Microscopes","Microscopy","Simulation software stubs","Visualization (graphic)"],"tag_line":"The Simulated Fluorescence Process (SFP) is a computing algorithm used for scientific visualization of 3D data from, for example, fluorescence microscopes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"recursive-xy-cut","_score":0,"_source":{"description":"The recursive X-Y cut is a top-down page segmentation technique that decomposes a document image recursively into a set of rectangular blocks. The algorithm works by projecting the document bitmap (i.e. summing up all the pixels in a line) to the sides of the document page. By this method, a white space density graph is produced, with peaks for vertical or horizontal whitespace lines. These peaks define the cuts of the document and are used top-down to segment the document into smaller pieces.","name":"Recursive XY-cut","categories":["All orphaned articles","All stub articles","Computer graphics algorithms","Computer science stubs","Orphaned articles from February 2009"],"tag_line":"The recursive X-Y cut is a top-down page segmentation technique that decomposes a document image recursively into a set of rectangular blocks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"john-hershberger","_score":0,"_source":{"description":"John E. Hershberger (born 1959) is an American computer scientist and software professional, a principal engineer at Mentor Graphics Corporation since 1993. He is known for his research in computational geometry and algorithm engineering.","name":"John Hershberger","categories":["1959 births","American computer scientists","BLP articles lacking sources from January 2013","CS1 errors: chapter ignored","California Institute of Technology alumni","Fellows of the Association for Computing Machinery","Living people","People from Tigard, Oregon","Researchers in geometric algorithms"],"tag_line":"John E. Hershberger (born 1959) is an American computer scientist and software professional, a principal engineer at Mentor Graphics Corporation since 1993."}}
,{"_index":"throwtable","_type":"algorithm","_id":"online-machine-learning","_score":0,"_source":{"description":"Online machine learning is used when data becomes available in a sequential order to determine a mapping from data set corresponding labels. The difference between online learning and batch learning (or \"offline\" learning) techniques, is that in online learning the mapping is updated after the arrival of every new data point in a scale fashion, whereas batch techniques are used when one has access to the entire training data set at once. Online learning could be used in the case of a process occurring in time, for example the value of a stock given its history and other external factors, in which case the mapping updates as time goes on and we get more and more samples.\nIdeally in online learning, the memory needed to store the function remains constant even with added data points, since the solution computed at one step is updated when a new data point becomes available, after which that data point can then be discarded. For many formulations, for example nonlinear kernel methods, true online learning is not possible, though a form of hybrid online learning with recursive algorithms can be used. In this case, the space requirements are no longer guaranteed to be constant since it requires storing all previous data points, but the solution may take less time to compute with the addition of a new data point, as compared to batch learning techniques.\nAs in all machine learning problems, the goal of the algorithm is to minimize some performance criteria using a loss function. For example, with stock market prediction the algorithm may attempt to minimize the mean squared error between the predicted and true value of a stock. Another popular performance criterion is to minimize the number of mistakes when dealing with classification problems. In addition to applications of a sequential nature, online learning algorithms are also relevant in applications with huge amounts of data such that traditional learning approaches that use the entire data set in aggregate are computationally infeasible.","name":"Online machine learning","categories":["All articles covered by WikiProject Wikify","All articles needing additional references","All pages needing cleanup","Articles covered by WikiProject Wikify from March 2014","Articles needing additional references from November 2008","Articles with inconsistent citation formats","Articles with weasel words from November 2012","Machine learning algorithms","Wikipedia introduction cleanup from March 2014"],"tag_line":"Online machine learning is used when data becomes available in a sequential order to determine a mapping from data set corresponding labels."}}
,{"_index":"throwtable","_type":"algorithm","_id":"diffusion-map","_score":0,"_source":{"description":"Diffusion maps is a dimensionality reduction or feature extraction algorithm introduced by R. R. Coifman and S. Lafon. It computes a family of embeddings of a data set into Euclidean space (often low-dimensional) whose coordinates can be computed from the eigenvectors and eigenvalues of a diffusion operator on the data. The Euclidean distance between points in the embedded space is equal to the \"diffusion distance\" between probability distributions centered at those points. Different from linear dimensionality reduction methods such as principal component analysis (PCA) and multi-dimensional scaling (MDS), diffusion maps is part of the family of nonlinear dimensionality reduction methods which focus on discovering the underlying manifold that the data has been sampled from. By integrating local similarities at different scales, diffusion maps gives a global description of the data-set. Compared with other methods, the diffusion maps algorithm is robust to noise perturbation and is computationally inexpensive.","name":"Diffusion map","categories":["Machine learning algorithms","Pages containing cite templates with deprecated parameters"],"tag_line":"Diffusion maps is a dimensionality reduction or feature extraction algorithm introduced by R. R. Coifman and S. Lafon."}}
,{"_index":"throwtable","_type":"algorithm","_id":"non-negative-matrix-factorization","_score":0,"_source":{"description":"Non-negative matrix factorization (NMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.\nNMF finds applications in such fields as computer vision, document clustering, chemometrics, audio signal processing and recommender systems.","name":"Non-negative matrix factorization","categories":["All articles with unsourced statements","Articles with unsourced statements from April 2015","Linear algebra","Machine learning algorithms","Matrix theory","Multivariate statistics","Vague or ambiguous time from April 2011"],"tag_line":"Non-negative matrix factorization (NMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fastica","_score":0,"_source":{"description":"FastICA is an efficient and popular algorithm for independent component analysis invented by Aapo Hyvärinen at Helsinki University of Technology. Like most ICA algorithms, FastICA seeks an orthogonal rotation of prewhitened data, through a fixed-point iteration scheme, that maximizes a measure of non-Gaussianity of the rotated components. Non-gaussianity serves as a proxy for statistical independence, which is a very strong condition and requires infinite data to verify. FastICA can also be alternatively derived as an approximative Newton iteration.","name":"FastICA","categories":["All articles needing additional references","Articles needing additional references from April 2013","Computational statistics","Machine learning algorithms","Multivariate statistics"],"tag_line":"FastICA is an efficient and popular algorithm for independent component analysis invented by Aapo Hyvärinen at Helsinki University of Technology."}}
,{"_index":"throwtable","_type":"algorithm","_id":"prefrontal-cortex-basal-ganglia-working-memory","_score":0,"_source":{"description":"Prefrontal cortex basal ganglia working memory (PBWM) is an algorithm that models working memory in the prefrontal cortex and the basal ganglia. It can be compared to long short-term memory (LSTM) in functionality, but is more biologically explainable.\nIt uses the primary value learned value model to train prefrontal cortex working-memory updating system, based on the biology of the prefrontal cortex and basal ganglia.\nIt is used as part of the Leabra framework and was implemented in Emergent.","name":"Prefrontal cortex basal ganglia working memory","categories":["All articles lacking reliable references","All articles needing additional references","All articles with unsourced statements","Articles lacking reliable references from April 2015","Articles needing additional references from September 2015","Articles with unsourced statements from September 2015","Machine learning algorithms","Neuroscience","Vague or ambiguous time from September 2015"],"tag_line":"Prefrontal cortex basal ganglia working memory (PBWM) is an algorithm that models working memory in the prefrontal cortex and the basal ganglia."}}
,{"_index":"throwtable","_type":"algorithm","_id":"randomized-weighted-majority-algorithm","_score":0,"_source":{"description":"The randomized weighted majority algorithm is an algorithm in machine learning theory. It improves the mistake bound of the weighted majority algorithm.\nImagine that every morning before the stock market opens, we get a prediction from each of our \"experts\" about whether the stock market will go up or down. Our goal is to somehow combine this set of predictions into a single prediction that we then use to make a buy or sell decision for the day. The RWMA gives us a way to do this combination such that our prediction record will be nearly as good as that of the single best expert in hindsight.","name":"Randomized weighted majority algorithm","categories":["Machine learning algorithms"],"tag_line":"The randomized weighted majority algorithm is an algorithm in machine learning theory."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bootstrap-aggregating","_score":0,"_source":{"description":"Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.","name":"Bootstrap aggregating","categories":["Computational statistics","Ensemble learning","Machine learning algorithms"],"tag_line":"Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cn2-algorithm","_score":0,"_source":{"description":"The CN2 induction algorithm is a learning algorithm for rule induction. It is designed to work even when the training data is imperfect. It is based on ideas from the AQ algorithm and the ID3 algorithm. As a consequence it creates a rule set like that created by AQ but is able to handle noisy data like ID3.","name":"CN2 algorithm","categories":["All articles needing additional references","All stub articles","Articles needing additional references from September 2012","Artificial intelligence stubs","Machine learning algorithms"],"tag_line":"The CN2 induction algorithm is a learning algorithm for rule induction."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hexq","_score":0,"_source":{"description":"HEXQ is a reinforcement learning algorithm created by Bernhard Hengst, which attempts to solve a Markov Decision Process by decomposing it hierarchically.\nBernhard Hengst (2002). \"Discovering Hierarchy in Reinforcement Learning with HEXQ\".","name":"HEXQ","categories":["All orphaned articles","All stub articles","Computer science stubs","Machine learning algorithms","Orphaned articles from February 2009"],"tag_line":"HEXQ is a reinforcement learning algorithm created by Bernhard Hengst, which attempts to solve a Markov Decision Process by decomposing it hierarchically."}}
,{"_index":"throwtable","_type":"algorithm","_id":"class-based-queueing","_score":0,"_source":{"description":"Class-based queuing (CBQ) is a queuing discipline for the network scheduler that allows traffic to share bandwidth equally, after being grouped by classes. The classes can be based upon a variety of parameters, such as priority, interface, or originating program.\nCBQ is a traffic management algorithm developed by the Network Research Group at Lawrence Berkeley National Laboratory as an alternative to traditional router-based technology. Now in the public domain as an open technology, CBQ is deployed by companies at the boundary of their WANs.\nCBQ divides user traffic into a hierarchy of classes based on any combination of IP addresses, protocols and application types. A company's accounting department, for example, may not need the same Internet access privileges as the engineering department. Because every company is organized differently and has different policies and business requirements, it is vital for traffic management technology to provide flexibility and granularity in classifying traffic flows.\nCBQ lets network managers classify traffic in a multilevel hierarchy. For instance, some companies may first identify the overall needs of each department or business group, and then define the requirements of each application or group of applications within each department. For performance and architectural reasons, traditional router-based queuing schemes are limited to a small number of classes and only allow one-dimensional classification.\nBecause it operates at the IP network layer, CBQ provides the same benefits across any Layer 2 technology and is equally effective with any IP protocol, such as Transmission Control Protocol (TCP) and User Datagram Protocol (UDP). It also operates with any client or server TCP/IP stack variation, since it takes advantage of standard TCP/IP flow control mechanisms to control end-to-end traffic.\nAn implementation is available under the GNU General Public License for the Linux kernel.","name":"Class-based queueing","categories":["All stub articles","Computer network stubs","Computer networking","Network scheduling algorithms"],"tag_line":"Class-based queuing (CBQ) is a queuing discipline for the network scheduler that allows traffic to share bandwidth equally, after being grouped by classes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"nagle's-algorithm","_score":0,"_source":{"description":"Nagle's algorithm, named after John Nagle, is a means of improving the efficiency of TCP/IP networks by reducing the number of packets that need to be sent over the network.\nNagle's document, Congestion Control in IP/TCP Internetworks (RFC 896) describes what he called the \"small packet problem\", where an application repeatedly emits data in small chunks, frequently only 1 byte in size. Since TCP packets have a 40 byte header (20 bytes for TCP, 20 bytes for IPv4), this results in a 41 byte packet for 1 byte of useful information, a huge overhead. This situation often occurs in Telnet sessions, where most keypresses generate a single byte of data that is transmitted immediately. Worse, over slow links, many such packets can be in transit at the same time, potentially leading to congestion collapse.\nNagle's algorithm works by combining a number of small outgoing messages, and sending them all at once. Specifically, as long as there is a sent packet for which the sender has received no acknowledgment, the sender should keep buffering its output until it has a full packet's worth of output, so that output can be sent all at once.","name":"Nagle's algorithm","categories":["All articles needing additional references","Articles needing additional references from June 2014","Networking algorithms","Transmission Control Protocol"],"tag_line":"Nagle's algorithm, named after John Nagle, is a means of improving the efficiency of TCP/IP networks by reducing the number of packets that need to be sent over the network."}}
,{"_index":"throwtable","_type":"algorithm","_id":"forward–backward-algorithm","_score":0,"_source":{"description":"The forward–backward algorithm is an inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables given a sequence of observations/emissions , i.e. it computes, for all hidden state variables , the distribution . This inference task is usually called smoothing. The algorithm makes use of the principle of dynamic programming to compute efficiently the values that are required to obtain the posterior marginal distributions in two passes. The first pass goes forward in time while the second goes backward in time; hence the name forward–backward algorithm.\nThe term forward–backward algorithm is also used to refer to any algorithm belonging to the general class of algorithms that operate on sequence models in a forward–backward manner. In this sense, the descriptions in the remainder of this article refer but to one specific instance of this class.","name":"Forward–backward algorithm","categories":["Dynamic programming","Error detection and correction","Machine learning algorithms","Markov models"],"tag_line":"The forward–backward algorithm is an inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables given a sequence of observations/emissions , i.e."}}
,{"_index":"throwtable","_type":"algorithm","_id":"buddy-memory-allocation","_score":0,"_source":{"description":"The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as possible. This system makes use of splitting memory into halves to try to give a best-fit. According to Donald Knuth, the buddy system was invented in 1963 by Harry Markowitz, who won the 1990 Nobel Memorial Prize in Economics, and was first described by Kenneth C. Knowlton (published 1965). Buddy memory allocation is relatively easy to implement. It supports limited but efficient splitting and coalescing of memory blocks.\n^ Kenneth C. Knowlton. A Fast storage allocator. Communications of the ACM 8(10):623-625, Oct 1965. also Kenneth C Knowlton. A programmer's description of L6. Communications of the ACM, 9(8):616-625, Aug. 1966 [see also : Google books [1] page 85]","name":"Buddy memory allocation","categories":["Memory management algorithms","Use dmy dates from August 2012"],"tag_line":"The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as possible."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fair-queuing","_score":0,"_source":{"description":"Fair queuing is a family of scheduling algorithms used in some process and network schedulers. The concept implies a separate data packet queue (or job queue) for each traffic flow (or for each program process) as opposed to the traditional approach with one FIFO queue for all packet flows (or for all process jobs). The purpose is to achieve fairness when a limited resource is shared, for example to avoid that flows with large packets (or processes that generate small jobs) achieve more throughput (or CPU time) than other flows (or processes).\nFair queuing is implemented in some advanced packet switches and routers.","name":"Fair queuing","categories":["Network scheduling algorithms"],"tag_line":"Fair queuing is a family of scheduling algorithms used in some process and network schedulers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"backpressure-routing","_score":0,"_source":{"description":"In queueing theory, a discipline within the mathematical theory of probability, the backpressure routing algorithm is a method for directing traffic around a queueing network that achieves maximum network throughput, which is established using concepts of Lyapunov drift. Backpressure routing considers the situation where each job can visit multiple service nodes in the network. It is an extension of max-weight scheduling where rather each job visits only a single service node.","name":"Backpressure routing","categories":["Networking algorithms","Queueing theory","Routing algorithms"],"tag_line":"In queueing theory, a discipline within the mathematical theory of probability, the backpressure routing algorithm is a method for directing traffic around a queueing network that achieves maximum network throughput, which is established using concepts of Lyapunov drift."}}
,{"_index":"throwtable","_type":"algorithm","_id":"wake-sleep-algorithm","_score":0,"_source":{"description":"The wake-sleep algorithm is an unsupervised learning algorithm for a stochastic multilayer neural network. The algorithm adjusts the parameters so as to produce a good density estimator. There are two learning phases, the “wake” phase and the sleep “sleep” phase, which are performed alternately. It was first designed as a model for brain functioning using Variational Bayesian Learning. After that, the algorithm was adapted to machine learning. It can be viewed as a way to train a Helmholtz Machine","name":"Wake-sleep algorithm","categories":["Machine learning algorithms"],"tag_line":"The wake-sleep algorithm is an unsupervised learning algorithm for a stochastic multilayer neural network."}}
,{"_index":"throwtable","_type":"algorithm","_id":"weighted-fair-queueing","_score":0,"_source":{"description":"Weighted fair queueing (WFQ) is a data packet scheduling algorithm used by network schedulers. WFQ is both a packet based implementation of the generalized processor sharing policy (GPS), and a natural generalization of fair queuing (FQ): whereas FQ shares the link's capacity in equal subparts, WFQ allows to specify, for each flow, which fraction of the capacity will be given.\nWeighted fair queuing (WFQ) is also known as Packet-by-Packet GPS (PGPS or P-GPS) since it approximates generalized processor sharing \"to within one packet transmission time, regardless of the arrival patterns.\"\nIn WFQ, a scheduler handling N flows is configured with one weight  for each flow. Then, the flow of number i will achieve an average data rate of . A WFQ scheduler where all weights are equals is a FQ scheduler.\nLike all fair-queuing schedulers, each flow is protected from the others, and it can be proven that if a data flow is leaky bucket constrained, an end-to-end delay bound can be guaranteed.","name":"Weighted fair queueing","categories":["Network scheduling algorithms","Routing algorithms"],"tag_line":"Weighted fair queueing (WFQ) is a data packet scheduling algorithm used by network schedulers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"random-early-detection","_score":0,"_source":{"description":"Random early detection (RED), also known as random early discard or random early drop is a queueing discipline for a network scheduler suited for congestion avoidance.\nIn the conventional tail drop algorithm, a router or other network component buffers as many packets as it can, and simply drops the ones it cannot buffer. If buffers are constantly full, the network is congested. Tail drop distributes buffer space unfairly among traffic flows. Tail drop can also lead to TCP global synchronization as all TCP connections \"hold back\" simultaneously, and then step forward simultaneously. Networks become under-utilized and flooded by turns. RED addresses these issues.","name":"Random early detection","categories":["Network performance","Network scheduling algorithms"],"tag_line":"Random early detection (RED), also known as random early discard or random early drop is a queueing discipline for a network scheduler suited for congestion avoidance."}}
,{"_index":"throwtable","_type":"algorithm","_id":"floyd–warshall-algorithm","_score":0,"_source":{"description":"In computer science, the Floyd–Warshall algorithm is an algorithm for finding shortest paths in a weighted graph with positive or negative edge weights (but with no negative cycles). A single execution of the algorithm will find the lengths (summed weights) of the shortest paths between all pairs of vertices, though it does not return details of the paths themselves. Versions of the algorithm can also be used for finding the transitive closure of a relation , or (in connection with the Schulze voting system) widest paths between all pairs of vertices in a weighted graph.","name":"Floyd–Warshall algorithm","categories":["Articles with example pseudocode","Commons category without a link on Wikidata","Dynamic programming","Graph algorithms","Polynomial-time problems","Routing algorithms"],"tag_line":"In computer science, the Floyd–Warshall algorithm is an algorithm for finding shortest paths in a weighted graph with positive or negative edge weights (but with no negative cycles)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"optimization-mechanism","_score":0,"_source":{"description":"In network science started in, the optimization mechanism is a network growth algorithm, which randomly places new nodes in the system, and connects them to the existing nodes based on a cost-benefit analysis. Depending on the parameters used in the optimization mechanism, the algorithm can build three types of networks: a star network, a random network, and a scale-free network. Optimization mechanism is thought to be the underlying mechanism in several real networks, such as transportation networks, power grid, router networks, the network of highways, etc.","name":"Optimization mechanism","categories":["All articles needing additional references","All orphaned articles","Articles needing additional references from August 2014","Networks","Orphaned articles from August 2014","Routing algorithms"],"tag_line":"In network science started in, the optimization mechanism is a network growth algorithm, which randomly places new nodes in the system, and connects them to the existing nodes based on a cost-benefit analysis."}}
,{"_index":"throwtable","_type":"algorithm","_id":"baby-step-giant-step","_score":0,"_source":{"description":"In group theory, a branch of mathematics, the baby-step giant-step is a meet-in-the-middle algorithm computing the discrete logarithm. The discrete log problem is of fundamental importance to the area of public key cryptography. Many of the most commonly used cryptography systems are based on the assumption that the discrete log is extremely difficult to compute; the more difficult it is, the more security it provides a data transfer. One way to increase the difficulty of the discrete log problem is to base the cryptosystem on a larger group.","name":"Baby-step giant-step","categories":["Articles with example C code","Group theory","Number theoretic algorithms"],"tag_line":"In group theory, a branch of mathematics, the baby-step giant-step is a meet-in-the-middle algorithm computing the discrete logarithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"token-bucket","_score":0,"_source":{"description":"The token bucket is an algorithm used in packet switched computer networks and telecommunications networks. It can be used to check that data transmissions, in the form of packets, conform to defined limits on bandwidth and burstiness (a measure of the unevenness or variations in the traffic flow). It can also be used as a scheduling algorithm to determine the timing of transmissions that will comply with the limits set for the bandwidth and burstiness: see network scheduler.","name":"Token bucket","categories":["All articles to be expanded","Articles to be expanded from June 2008","Network performance","Network scheduling algorithms"],"tag_line":"The token bucket is an algorithm used in packet switched computer networks and telecommunications networks."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mentor-routing-algorithm","_score":0,"_source":{"description":"The MENTOR routing algorithm is an algorithm for use in routing of mesh networks, specifically pertaining to their initial topology. It was developed in 1991 by Aaron Kershenbaum, Parviz Kermani, and George A. Grove and was published by the IEEE.","name":"MENTOR routing algorithm","categories":["All orphaned articles","Orphaned articles from February 2009","Routing algorithms"],"tag_line":"The MENTOR routing algorithm is an algorithm for use in routing of mesh networks, specifically pertaining to their initial topology."}}
,{"_index":"throwtable","_type":"algorithm","_id":"expected-transmission-count","_score":0,"_source":{"description":"The ETX metric, or expected transmission count, is a measure of the quality of a path between two nodes in a wireless packet data network. It is used extensively in mesh networking algorithms.","name":"Expected transmission count","categories":["Routing algorithms","Wireless networking"],"tag_line":"The ETX metric, or expected transmission count, is a measure of the quality of a path between two nodes in a wireless packet data network."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pohlig–hellman-algorithm","_score":0,"_source":{"description":"In number theory, the Pohlig–Hellman algorithm sometimes credited as the Silver–Pohlig–Hellman algorithm is a special-purpose algorithm for computing discrete logarithms in a multiplicative group whose order is a smooth integer.\nThe algorithm was discovered by Roland Silver, but first published by Stephen Pohlig and Martin Hellman (independent of Silver).\nWe will explain the algorithm as it applies to the group Z*p consisting of all the elements of Zp which are coprime to p, and leave it to the advanced reader to extend the algorithm to other groups by using Lagrange's theorem.\nInput Integers p, g, e.\nOutput An Integer x, such that e ≡ gx (mod p) (if one exists).\n\nDetermine the prime factorization of the order of the group  :\n(All the pi are considered small since the group order is smooth.)\nFrom the Chinese remainder theorem it will be sufficient to determine the values of x modulo each prime power dividing the group order. Suppose for illustration that p1 divides this order but p12 does not. Then we need to determine x mod p1, that is, we need to know the ending coefficient b1 in the base-p1 expansion of x, i.e. in the expansion x = a1 p1 + b1. We can find the value of b1 by examining all the possible values between 0 and p1-1. (We may also use a faster algorithm such as baby-step giant-step when the order of the group is prime.) The key behind the examination is that:\n\n(using Euler's theorem). With everything else now known, we may try each value of b1 to see which makes the equation be true. If , then there is exactly one b1, and that b1 is the value of x modulo p1. (An exception arises if  since then the order of g is less than φ(p). The conclusion in this case depends on the value of  on the left: if this quantity is not 1, then no solution x exists; if instead this quantity is also equal to 1, there will be more than one solution for x less than φ(p), but since we are attempting to return only one solution x, we may use b1=0.)\nThe same operation is now performed for p2 through pn.\nA minor modification is needed where a prime number is repeated. Suppose we are seeing pi for the (k + 1)st time. Then we already know ci in the equation x = ai pik+1 + bi pik + ci, and we find either bi or ci the same way as before, depending on whether .\nWith all the bi known, we have enough simultaneous congruences to determine x using the Chinese remainder theorem.","name":"Pohlig–Hellman algorithm","categories":["Number theoretic algorithms"],"tag_line":"In number theory, the Pohlig–Hellman algorithm sometimes credited as the Silver–Pohlig–Hellman algorithm is a special-purpose algorithm for computing discrete logarithms in a multiplicative group whose order is a smooth integer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"tonelli–shanks-algorithm","_score":0,"_source":{"description":"The Tonelli–Shanks algorithm (referred to by Shanks as the RESSOL algorithm) is used within modular arithmetic to solve a congruence of the form\n\nwhere n is a quadratic residue (mod p), and p is an odd prime.\nTonelli–Shanks cannot be used for composite moduli; finding square roots modulo composite numbers is a computational problem equivalent to integer factorization.\nAn equivalent, but slightly more redundant version of this algorithm was developed by Alberto Tonelli in 1891. The version discussed here was developed independently by Daniel Shanks in 1973, who explained:\n\n\"My tardiness in learning of these historical references was because I had lent Volume 1 of Dickson's History to a friend and it was never returned.\"","name":"Tonelli–Shanks algorithm","categories":["Articles containing proofs","Modular arithmetic","Number theoretic algorithms","Pages containing cite templates with deprecated parameters"],"tag_line":"The Tonelli–Shanks algorithm (referred to by Shanks as the RESSOL algorithm) is used within modular arithmetic to solve a congruence of the form\n\nwhere n is a quadratic residue (mod p), and p is an odd prime."}}
,{"_index":"throwtable","_type":"algorithm","_id":"interval-contractor","_score":0,"_source":{"description":"In mathematics, an interval contractor (or contractor for short)  associated to a set X is an operator C which associates to a box [x] in Rn another box C([x]) of Rn such that the two following properties are always satisfied\n (contractance property)\n (completeness property)\nA contractor associated to a constraint (such as an equation or an inequality) is a contractor associated to the set X of all x which satisfy the constraint. Contractors make it possible to improve the efficiency of branch-and-bound algorithms classically used in interval analysis.","name":"Interval contractor","categories":["Arithmetic","Computer arithmetic","Mathematical optimization","Numerical analysis","Optimization algorithms and methods"],"tag_line":"In mathematics, an interval contractor (or contractor for short)  associated to a set X is an operator C which associates to a box [x] in Rn another box C([x]) of Rn such that the two following properties are always satisfied\n (contractance property)\n (completeness property)\nA contractor associated to a constraint (such as an equation or an inequality) is a contractor associated to the set X of all x which satisfy the constraint."}}
,{"_index":"throwtable","_type":"algorithm","_id":"binary-gcd-algorithm","_score":0,"_source":{"description":"The binary GCD algorithm, also known as Stein's algorithm, is an algorithm that computes the greatest common divisor of two nonnegative integers. Stein's algorithm uses simpler arithmetic operations than the conventional Euclidean algorithm; it replaces division with arithmetic shifts, comparisons, and subtraction. Although the algorithm was first published by the Israeli physicist and programmer Josef Stein in 1967, it may have been known in 1st-century China.","name":"Binary GCD algorithm","categories":["All articles with unsourced statements","Articles with example C code","Articles with unsourced statements from March 2014","Number theoretic algorithms"],"tag_line":"The binary GCD algorithm, also known as Stein's algorithm, is an algorithm that computes the greatest common divisor of two nonnegative integers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gradient-descent","_score":0,"_source":{"description":"Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.\nGradient descent is also known as steepest descent, or the method of steepest descent. However, gradient descent should not be confused with the method of steepest descent for approximating integrals.","name":"Gradient descent","categories":["Articles with example Python code","First order methods","Gradient methods","Optimization algorithms and methods"],"tag_line":"Gradient descent is a first-order optimization algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"minimax-approximation-algorithm","_score":0,"_source":{"description":"A minimax approximation algorithm (or L∞ approximation or uniform approximation) is a method to find an approximation of a mathematical function that minimizes maximum error.\nFor example, given a function  defined on the interval  and a degree bound , a minimax polynomial approximation algorithm will find a polynomial  of degree at most  to minimize","name":"Minimax approximation algorithm","categories":["Algorithms and data structures stubs","All stub articles","Computer science stubs","Numerical analysis"],"tag_line":"A minimax approximation algorithm (or L∞ approximation or uniform approximation) is a method to find an approximation of a mathematical function that minimizes maximum error."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pocklington's-algorithm","_score":0,"_source":{"description":"Pocklington's algorithm is a technique for solving a congruence of the form\n\nwhere x and a are integers and a is a quadratic residue.\nThe algorithm is one of the first efficient methods to solve such a congruence. It was described by H.C. Pocklington in 1917.\n^ H.C. Pocklington, Proceedings of the Cambridge Philosophical Society, Volume 19, pages 57–58","name":"Pocklington's algorithm","categories":["Modular arithmetic","Number theoretic algorithms"],"tag_line":"Pocklington's algorithm is a technique for solving a congruence of the form\n\nwhere x and a are integers and a is a quadratic residue."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dynamic-programming","_score":0,"_source":{"description":"In mathematics, management science, economics, computer science, and bioinformatics, dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions - ideally, using a memory-based data structure. The next time the same subproblem occurs, instead of recomputing its solution, one simply looks up the previously computed solution, thereby saving computation time at the expense of a (hopefully) modest expenditure in storage space. (Each of the subproblem solutions is indexed in some way, typically based on the values of its input parameters, so as to facilitate its lookup.) The act of storing solutions to subproblems is called \"memoization\". In contrast, a more naive method would not recognize that a particular subproblem has already been solved previously, and would repeatedly solve the same subproblem many times.\nThis approach is especially useful when the number of repeating subproblems grows exponentially as a function of the size of the input.\nDynamic programming is applicable to problems exhibiting the properties of overlapping subproblems and optimal substructure (described below). When applicable, the method takes far less time than other methods that don't take advantage of the subproblem overlap (like depth-first search).\nDynamic programming algorithms are used for optimization (for example, finding the shortest path between two points, or the fastest way to multiply many matrices). A dynamic programming algorithm will examine the previously solved subproblems and will combine their solutions to give the best solution for the given problem. The alternatives are many, such as using a greedy algorithm, which picks the locally optimal choice at each branch in the road. The locally optimal choice may be a poor choice for the overall solution. While a greedy algorithm does not guarantee an optimal solution, it is often faster to calculate. Fortunately, some greedy algorithms (such as minimum spanning trees) are proven to lead to the optimal solution.\nFor example, let's say that you have to get from point A to point B as fast as possible, in a given city, during rush hour. A dynamic programming algorithm will look at finding the shortest paths to points close to A, and use those solutions to eventually find the shortest path to B. On the other hand, a greedy algorithm will start you driving immediately and will pick the road that looks the fastest at every intersection. As you can imagine, this strategy might not lead to the fastest arrival time, since you might take some \"easy\" streets and then find yourself hopelessly stuck in a traffic jam.\nSometimes, applying memoization to a naive basic recursive solution already results in a dynamic programming solution with asymptotically optimal time complexity; however, the optimal solution to some problems requires more sophisticated dynamic programming algorithms. Some of these may be recursive as well but parametrized differently from the naive solution. Others can be more complicated and cannot be implemented as a recursive function with memoization. Examples of these are the two solutions to the Egg Dropping puzzle below.","name":"Dynamic programming","categories":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from April 2014","Articles needing additional references from May 2013","Articles with unsourced statements from May 2009","Dynamic programming","Equations","Mathematical optimization","Operations research","Optimal control","Optimization algorithms and methods","Systems engineering","Wikipedia articles with GND identifiers"],"tag_line":"In mathematics, management science, economics, computer science, and bioinformatics, dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions - ideally, using a memory-based data structure."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gaussian-elimination","_score":0,"_source":{"description":"In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the associated matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 CE (see History section).\nTo perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and in every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.\n\nUsing row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.","name":"Gaussian elimination","categories":["All articles to be merged","All articles with specifically marked weasel-worded phrases","Articles to be merged from March 2013","Articles with example pseudocode","Articles with specifically marked weasel-worded phrases from January 2014","Exchange algorithms","Numerical linear algebra","Pages using citations with accessdate and no URL","Pages using web citations with no URL"],"tag_line":"In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations."}}
,{"_index":"throwtable","_type":"algorithm","_id":"coppersmith–winograd-algorithm","_score":0,"_source":{"description":"In linear algebra, the Coppersmith–Winograd algorithm, named after Don Coppersmith and Shmuel Winograd, was the asymptotically fastest known algorithm for square matrix multiplication until 2010. It can multiply two  matrices in  time  (see Big O notation). This is an improvement over the naïve  time algorithm and the  time Strassen algorithm. Algorithms with better asymptotic running time than the Strassen algorithm are rarely used in practice, because the large constant factors in their running times make them impractical. It is possible to improve the exponent further; however, the exponent must be at least 2 (because an  matrix has  values, and all of them have to be read at least once to calculate the exact result).\nIn 2010, Andrew Stothers gave an improvement to the algorithm,  In 2011, Virginia Williams combined a mathematical short-cut from Stothers' paper with her own insights and automated optimization on computers, improving the bound to  In 2014, François Le Gall simplified the methods of Williams and obtained an improved bound of \nThe Coppersmith–Winograd algorithm is frequently used as a building block in other algorithms to prove theoretical time bounds. However, unlike the Strassen algorithm, it is not used in practice because it only provides an advantage for matrices so large that they cannot be processed by modern hardware.\nHenry Cohn, Robert Kleinberg, Balázs Szegedy and Chris Umans have re-derived the Coppersmith–Winograd algorithm using a group-theoretic construction. They also showed that either of two different conjectures would imply that the optimal exponent of matrix multiplication is 2, as has long been suspected. However, they were not able to formulate a specific solution leading to a better running-time than Coppersmith-Winograd at the time.","name":"Coppersmith–Winograd algorithm","categories":["Matrix multiplication algorithms","Matrix theory","Numerical linear algebra","Use dmy dates from July 2013"],"tag_line":"In linear algebra, the Coppersmith–Winograd algorithm, named after Don Coppersmith and Shmuel Winograd, was the asymptotically fastest known algorithm for square matrix multiplication until 2010."}}
,{"_index":"throwtable","_type":"algorithm","_id":"flajolet–martin-algorithm","_score":0,"_source":{"description":"The Flajolet–Martin algorithm is an algorithm for approximating the number of distinct elements in a stream with a single pass and space-consumption which is logarithmic in the maximum number of possible distinct elements in the stream. The algorithm was introduced by Philippe Flajolet and G. Nigel Martin in their 1984 paper \"Probabilistic Counting Algorithms for Data Base Applications\". Later it has been refined in the papers \"LogLog counting of large cardinalities\" by Marianne Durand and Philippe Flajolet, and \"HyperLogLog: The analysis of a near-optimal cardinality estimation algorithm\" by Philippe Flajolet et al.\nIn their 2010 paper \"An optimal algorithm for the distinct elements problem\", Daniel M. Kane, Jelani Nelson and David P. Woodruff gives an improved algorithm which uses nearly optimal space, and has optimal O(1) update and reporting times.\n^ Flajolet, P.; Nigel Martin, G. (1985). \"Probabilistic counting algorithms for data base applications\". Journal of Computer and System Sciences 31 (2): 182. doi:10.1016/0022-0000(85)90041-8. \n^ Durand, M.; Flajolet, P. (2003). \"Loglog Counting of Large Cardinalities\". Algorithms - ESA 2003. Lecture Notes in Computer Science 2832. p. 605. doi:10.1007/978-3-540-39658-1_55. ISBN 978-3-540-20064-2. \n^ Philippe Flajolet, Éric Fusy, Olivier Gandouet, Frédéric Meunier (2007). \"Hyperloglog: The analysis of a near-optimal cardinality estimation algorithm\" (PDF). Discrete Mathematics and Theoretical Computer Science: 127–146. CiteSeerX.psu:10.1.1.76.4286. \n^ Kane, D. M.; Nelson, J.; Woodruff, D. P. (2010). \"An optimal algorithm for the distinct elements problem\". Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems of data - PODS '10. p. 41. doi:10.1145/1807085.1807094. ISBN 9781450300339.","name":"Flajolet–Martin algorithm","categories":["Algorithms","All orphaned articles","Orphaned articles from November 2014"],"tag_line":"The Flajolet–Martin algorithm is an algorithm for approximating the number of distinct elements in a stream with a single pass and space-consumption which is logarithmic in the maximum number of possible distinct elements in the stream."}}
,{"_index":"throwtable","_type":"algorithm","_id":"iterated-filtering","_score":0,"_source":{"description":"Iterated filtering algorithms are a tool for maximum likelihood inference on partially observed dynamical systems. Stochastic perturbations to the unknown parameters are used to explore the parameter space. Applying sequential Monte Carlo (the particle filter) to this extended model results in the selection of the parameter values that are more consistent with the data. Appropriately constructed procedures, iterating with successively diminished perturbations, converge to the maximum likelihood estimate. Iterated filtering methods have so far been used most extensively to study infectious disease transmission dynamics. Case studies include cholera, Ebola virus, influenza, malaria, HIV, pertussis, poliovirus and measles. Other areas which have been proposed to be suitable for these methods include ecological dynamics and finance.\nThe perturbations to the parameter space play several different roles. Firstly, they smooth out the likelihood surface, enabling the algorithm to overcome small-scale features of the likelihood during early stages of the global search. Secondly, Monte Carlo variation allows the search to escape from local minima. Thirdly, the iterated filtering update uses the perturbed parameter values to construct an approximation to the derivative of the log likelihood even though this quantity is not typically available in closed form. Fourthly, the parameter perturbations help to overcome numerical difficulties that can arise during sequential Monte Carlo.","name":"Iterated filtering","categories":["Dynamical systems","Monte Carlo methods","Nonlinear filters","Pages containing cite templates with deprecated parameters","Statistical algorithms"],"tag_line":"Iterated filtering algorithms are a tool for maximum likelihood inference on partially observed dynamical systems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"jenkins–traub-algorithm","_score":0,"_source":{"description":"The Jenkins–Traub algorithm for polynomial zeros is a fast globally convergent iterative method published in 1970 by Michael A. Jenkins and Joseph F. Traub. They gave two variants, one for general polynomials with complex coefficients, commonly known as the \"CPOLY\" algorithm, and a more complicated variant for the special case of polynomials with real coefficients, commonly known as the \"RPOLY\" algorithm. The latter is \"practically a standard in black-box polynomial root-finders\".\nThis article describes the complex variant. Given a polynomial P,\n\nwith complex coefficients it computes approximations to the n zeros  of P(z), one at a time in roughly increasing order of magnitude. After each root is computed, its linear factor is removed from the polynomial. Using this deflation guarantees that each root is computed only once and that all roots are found.\nThe real variant follows the same pattern, but computes two roots at a time, either two real roots or a pair of conjugate complex roots. By avoiding complex arithmetic, the real variant can be faster (by a factor of 4) than the complex variant. The Jenkins–Traub algorithm has stimulated considerable research on theory and software for methods of this type.","name":"Jenkins–Traub algorithm","categories":["Numerical analysis","Root-finding algorithms"],"tag_line":"The Jenkins–Traub algorithm for polynomial zeros is a fast globally convergent iterative method published in 1970 by Michael A. Jenkins and Joseph F. Traub."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pattern-search-(optimization)","_score":0,"_source":{"description":"Pattern search (PS) is a family of numerical optimization methods that do not require the gradient of the problem to be optimized. Hence PS can be used on functions that are not continuous or differentiable. Such optimization methods are also known as direct-search, derivative-free, or black-box methods.\nThe name, pattern search, was coined by Hooke and Jeeves. An early and simple PS variant is attributed to Fermi and Metropolis when they worked at the Los Alamos National Laboratory as described by Davidon  who summarized the algorithm as follows:\n\nThey varied one theoretical parameter at a time by steps of the same magnitude, and when no such increase or decrease in any one parameter further improved the fit to the experimental data, they halved the step size and repeated the process until the steps were deemed sufficiently small.","name":"Pattern search (optimization)","categories":["Mathematical optimization","Optimization algorithms and methods"],"tag_line":"Pattern search (PS) is a family of numerical optimization methods that do not require the gradient of the problem to be optimized."}}
,{"_index":"throwtable","_type":"algorithm","_id":"metropolis–hastings-algorithm","_score":0,"_source":{"description":"In statistics and in statistical physics, the Metropolis–Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult. This sequence can be used to approximate the distribution (i.e., to generate a histogram), or to compute an integral (such as an expected value). Metropolis–Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. For single-dimensional distributions, other methods are usually available (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and are free from the problem of auto-correlated samples that is inherent in MCMC methods.","name":"Metropolis–Hastings algorithm","categories":["Markov chain Monte Carlo","Monte Carlo methods","Statistical algorithms"],"tag_line":"In statistics and in statistical physics, the Metropolis–Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lu-reduction","_score":0,"_source":{"description":"LU reduction is an algorithm related to LU decomposition. This term is usually used in the context of super computing and highly parallel computing. In this context it is used as a benchmarking algorithm, i.e. to provide a comparative measurement of speed for different computers. LU reduction is a special parallelized version of an LU decomposition algorithm, an example can be found in (Guitart 2001). The parallelized version usually distributes the work for a matrix row to a single processor and synchronizes the result with the whole matrix (Escribano 2000).","name":"LU reduction","categories":["Algorithms and data structures stubs","All stub articles","Applied mathematics stubs","Computer science stubs","Numerical linear algebra","Supercomputers"],"tag_line":"LU reduction is an algorithm related to LU decomposition."}}
,{"_index":"throwtable","_type":"algorithm","_id":"backtracking","_score":0,"_source":{"description":"Backtracking is a general algorithm for finding all (or some) solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons each partial candidate c (\"backtracks\") as soon as it determines that c cannot possibly be completed to a valid solution.\nThe classic textbook example of the use of backtracking is the eight queens puzzle, that asks for all arrangements of eight chess queens on a standard chessboard so that no queen attacks any other. In the common backtracking approach, the partial candidates are arrangements of k queens in the first k rows of the board, all in different rows and columns. Any partial solution that contains two mutually attacking queens can be abandoned.\nBacktracking can be applied only for problems which admit the concept of a \"partial candidate solution\" and a relatively quick test of whether it can possibly be completed to a valid solution. It is useless, for example, for locating a given value in an unordered table. When it is applicable, however, backtracking is often much faster than brute force enumeration of all complete candidates, since it can eliminate a large number of candidates with a single test.\nBacktracking is an important tool for solving constraint satisfaction problems, such as crosswords, verbal arithmetic, Sudoku, and many other puzzles. It is often the most convenient (if not the most efficient) technique for parsing, for the knapsack problem and other combinatorial optimization problems. It is also the basis of the so-called logic programming languages such as Icon, Planner and Prolog.\nBacktracking depends on user-given \"black box procedures\" that define the problem to be solved, the nature of the partial candidates, and how they are extended into complete candidates. It is therefore a metaheuristic rather than a specific algorithm – although, unlike many other meta-heuristics, it is guaranteed to find all solutions to a finite problem in a bounded amount of time.\nThe term \"backtrack\" was coined by American mathematician D. H. Lehmer in the 1950s. The pioneer string-processing language SNOBOL (1962) may have been the first to provide a built-in general backtracking facility.","name":"Backtracking","categories":["All articles with unsourced statements","Articles with unsourced statements from January 2011","Operations research","Pages with URL errors","Pattern matching","Search algorithms"],"tag_line":"Backtracking is a general algorithm for finding all (or some) solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons each partial candidate c (\"backtracks\") as soon as it determines that c cannot possibly be completed to a valid solution."}}
,{"_index":"throwtable","_type":"algorithm","_id":"phonetic-algorithm","_score":0,"_source":{"description":"A phonetic algorithm is an algorithm for indexing of words by their pronunciation. Most phonetic algorithms were developed for use with the English language; consequently, applying the rules to words in other languages might not give a meaningful result.\nThey are necessarily complex algorithms with many rules and exceptions, because English spelling and pronunciation is complicated by historical changes in pronunciation and words borrowed from many languages.\nAmong the best-known phonetic algorithms are:\nSoundex, which was developed to encode surnames for use in censuses. Soundex codes are four-character strings composed of a single letter followed by three numbers.\nDaitch–Mokotoff Soundex, which is a refinement of Soundex designed to better match surnames of Slavic and Germanic origin. Daitch–Mokotoff Soundex codes are strings composed of six numeric digits.\nKölner Phonetik: This is similar to Soundex, but more suitable for German words.\nMetaphone, Double Metaphone, and Metaphone 3 which are suitable for use with most English words, not just names. Metaphone algorithms are the basis for many popular spell checkers.\nNew York State Identification and Intelligence System (NYSIIS), which maps similar phonemes to the same letter. The result is a string that can be pronounced by the reader without decoding.\nMatch Rating Approach developed by Western Airlines in 1977 - this algorithm has an encoding and range comparison technique.\nCaverphone, created to assist in data matching between late 19th century and early 20th century electoral rolls, optimized for accents present in parts of New Zealand.","name":"Phonetic algorithm","categories":["Algorithms and data structures stubs","All articles needing additional references","All stub articles","Articles needing additional references from August 2009","Computer science stubs","Phonetic algorithms","Phonetics stubs","Phonology"],"tag_line":"A phonetic algorithm is an algorithm for indexing of words by their pronunciation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pivot-element","_score":0,"_source":{"description":"The pivot or pivot element is the element of a matrix, or an array, which is selected first by an algorithm (e.g. Gaussian elimination, simplex algorithm, etc.), to do certain calculations. In the case of matrix algorithms, a pivot entry is usually required to be at least distinct from zero, and often distant from it; in this case finding this element is called pivoting. Pivoting may be followed by an interchange of rows or columns to bring the pivot to a fixed position and allow the algorithm to proceed successfully, and possibly to reduce round-off error. It is often used for verifying row echelon form\nPivoting might be thought of as swapping or sorting rows or columns in a matrix, and thus it can be represented as multiplication by permutation matrices. However, algorithms rarely move the matrix elements because this would cost too much time; instead, they just keep track of the permutations.\nOverall, pivoting adds more operations to the computational cost of an algorithm. These additional operations are sometimes necessary for the algorithm to work at all. Other times these additional operations are worthwhile because they add numerical stability to the final result.","name":"Pivot element","categories":["Exchange algorithms","Numerical linear algebra","Pages using duplicate arguments in template calls","Wikipedia articles incorporating text from PlanetMath"],"tag_line":"The pivot or pivot element is the element of a matrix, or an array, which is selected first by an algorithm (e.g."}}
,{"_index":"throwtable","_type":"algorithm","_id":"online-algorithm","_score":0,"_source":{"description":"In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i.e., in the order that the input is fed to the algorithm, without having the entire input available from the start.\nIn contrast, an offline algorithm is given the whole problem data from the beginning and is required to output an answer which solves the problem at hand.\nAs an example, consider the sorting algorithms selection sort and insertion sort: Selection sort repeatedly selects the minimum element from the unsorted remainder and places it at the front, which requires access to the entire input; it is thus an offline algorithm. On the other hand, insertion sort considers one input element per iteration and produces a partial solution without considering future elements. Thus insertion sort is an online algorithm.\nNote that insertion sort produces the optimum result, i.e., a correctly sorted list. For many problems, online algorithms cannot match the performance of offline algorithms. If the ratio between the performance of an online algorithm and an optimal offline algorithm is bounded, the online algorithm is called competitive.\nNot every online algorithm has an offline counterpart.","name":"Online algorithm","categories":["All articles needing additional references","Articles needing additional references from June 2013","Online algorithms"],"tag_line":"In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i.e., in the order that the input is fed to the algorithm, without having the entire input available from the start."}}
,{"_index":"throwtable","_type":"algorithm","_id":"caverphone","_score":0,"_source":{"description":"The Caverphone phonetic matching algorithm was created by David Hood in the Caversham Project at the University of Otago in New Zealand in 2002, revised in 2004. It was created to assist in data matching between late 19th century and early 20th century electoral rolls, where the name only needed to be in a \"commonly recognisable form\". The algorithm was intended to apply to those names that could not easily be matched between electoral rolls, after the exact matches were removed from the pool of potential matches. The algorithm is optimised for accents present in the study area (southern part of the city of Dunedin, New Zealand).","name":"Caverphone","categories":["All Wikipedia articles needing context","All articles with topics of unclear notability","All pages needing cleanup","Articles with topics of unclear notability from September 2008","Phonetic algorithms","Wikipedia articles needing context from October 2009","Wikipedia introduction cleanup from October 2009"],"tag_line":"The Caverphone phonetic matching algorithm was created by David Hood in the Caversham Project at the University of Otago in New Zealand in 2002, revised in 2004."}}
,{"_index":"throwtable","_type":"algorithm","_id":"metrical-task-system","_score":0,"_source":{"description":"Task systems are mathematical objects used to model the set of possible configuration of online algorithms. They were introduced by Borodin, Linial and Saks (1992) to model a variety of online problems. A task system determines a set of states and costs to change states. Task systems obtain as input a sequence of requests such that each request assigns processing times to the states. The objective of an online algorithm for task systems is to create a schedule that minimizes the overall cost incurred due to processing the tasks with respect to the states and due to the cost to change states.\nIf the cost function to change states is a metric, the task system is a metrical task system (MTS). This is the most common type of task systems. Metrical task systems generalize online problems such as paging, list accessing, and the k-server problem (in finite spaces).","name":"Metrical task system","categories":["Online algorithms"],"tag_line":"Task systems are mathematical objects used to model the set of possible configuration of online algorithms."}}
,{"_index":"throwtable","_type":"algorithm","_id":"brent's-method","_score":0,"_source":{"description":"In numerical analysis, Brent's method is a complicated but popular root-finding algorithm combining the bisection method, the secant method and inverse quadratic interpolation. It has the reliability of bisection but it can be as quick as some of the less reliable methods. The algorithm tries to use the potentially fast-converging secant method or inverse quadratic interpolation if possible, but it falls back to the more robust bisection method if necessary. Brent's method is due to Richard Brent and builds on an earlier algorithm by Theodorus Dekker. Consequently, the method is also known as Brent-Dekker.","name":"Brent's method","categories":["Root-finding algorithms"],"tag_line":"In numerical analysis, Brent's method is a complicated but popular root-finding algorithm combining the bisection method, the secant method and inverse quadratic interpolation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"secant-method","_score":0,"_source":{"description":"In numerical analysis, the secant method is a root-finding algorithm that uses a succession of roots of secant lines to better approximate a root of a function f. The secant method can be thought of as a finite difference approximation of Newton's method. However, the method was developed independently of Newton's method, and predated the latter by over 3,000 years.","name":"Secant method","categories":["Root-finding algorithms"],"tag_line":"In numerical analysis, the secant method is a root-finding algorithm that uses a succession of roots of secant lines to better approximate a root of a function f. The secant method can be thought of as a finite difference approximation of Newton's method."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adversary-model","_score":0,"_source":{"description":"In computer science, an online algorithm measures its competitiveness against different adversary models. For deterministic algorithms, the adversary is the same as the adaptive offline adversary. For randomized online algorithms competitiveness can depend upon the adversary model used.","name":"Adversary model","categories":["Algorithms and data structures stubs","All stub articles","Analysis of algorithms","Computer science stubs","Online algorithms"],"tag_line":"In computer science, an online algorithm measures its competitiveness against different adversary models."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ziggurat-algorithm","_score":0,"_source":{"description":"The ziggurat algorithm is an algorithm for pseudo-random number sampling. Belonging to the class of rejection sampling algorithms, it relies on an underlying source of uniformly-distributed random numbers, typically from a pseudo-random number generator, as well as precomputed tables. The algorithm is used to generate values from a monotone decreasing probability distribution. It can also be applied to symmetric unimodal distributions, such as the normal distribution, by choosing a value from one half of the distribution and then randomly choosing which half the value is considered to have been drawn from. It was developed by George Marsaglia and others in the 1960s.\nA typical value produced by the algorithm only requires the generation of one random floating-point value and one random table index, followed by one table lookup, one multiply operation and one comparison. Sometimes (2.5% of the time, in the case of a normal or exponential distribution when using typical table sizes) more computations are required. Nevertheless, the algorithm is computationally much faster than the two most commonly used methods of generating normally distributed random numbers, the Marsaglia polar method and the Box–Muller transform, which require at least one logarithm and one square root calculation for each pair of generated values. However, since the ziggurat algorithm is more complex to implement it is best used when large quantities of random numbers are required.\nThe term ziggurat algorithm dates from Marsaglia's paper with Wai Wan Tsang in 2000; it is so named because it is conceptually based on covering the probability distribution with rectangular segments stacked in decreasing order of size, resulting in a figure that resembles a ziggurat.","name":"Ziggurat algorithm","categories":["All articles with unsourced statements","Articles with unsourced statements from September 2011","Non-uniform random numbers","Pages using citations with format and no URL","Pseudorandom number generators","Statistical algorithms"],"tag_line":"The ziggurat algorithm is an algorithm for pseudo-random number sampling."}}
,{"_index":"throwtable","_type":"algorithm","_id":"grover's-algorithm","_score":0,"_source":{"description":"Grover's algorithm is a quantum algorithm that finds with high probability the unique input to a black box function that produces a particular output value, using just O(N1/2) evaluations of the function, where N is the size of the function's domain.\nThe analogous problem in classical computation cannot be solved in fewer than O(N) evaluations (because, in the worst case, the correct input might be the last one that is tried). At roughly the same time that Grover published his algorithm, Bennett, Bernstein, Brassard, and Vazirani published a proof that no quantum solution to the problem can evaluate the function fewer than O(N1/2) times, so Grover's algorithm is asymptotically optimal.\nUnlike other quantum algorithms, which may provide exponential speedup over their classical counterparts, Grover's algorithm provides only a quadratic speedup. However, even quadratic speedup is considerable when N is large. Grover's algorithm could brute force a 128-bit symmetric cryptographic key in roughly 264 iterations, or a 256-bit key in roughly 2128 iterations. As a result, it is sometimes suggested that symmetric key lengths be doubled to protect against future quantum attacks.\nLike many quantum algorithms, Grover's algorithm is probabilistic in the sense that it gives the correct answer with a probability of less than 1. Though there is technically no upper bound on the number of repetitions that might be needed before the correct answer is obtained, the expected number of repetitions is a constant factor that does not grow with N.\nGrover's original paper described the algorithm as a database search algorithm, and this description is still common. The database in this analogy is a table of all of the function's outputs, indexed by the corresponding input.","name":"Grover's algorithm","categories":["Quantum algorithms","Search algorithms"],"tag_line":"Grover's algorithm is a quantum algorithm that finds with high probability the unique input to a black box function that produces a particular output value, using just O(N1/2) evaluations of the function, where N is the size of the function's domain."}}
,{"_index":"throwtable","_type":"algorithm","_id":"aho–corasick-algorithm","_score":0,"_source":{"description":"In computer science, the Aho–Corasick algorithm is a string searching algorithm invented by Alfred V. Aho and Margaret J. Corasick. It is a kind of dictionary-matching algorithm that locates elements of a finite set of strings (the \"dictionary\") within an input text. It matches all patterns simultaneously. The complexity of the algorithm is linear in the length of the patterns plus the length of the searched text plus the number of output matches. Note that because all matches are found, there can be a quadratic number of matches if every substring matches (e.g. dictionary = a, aa, aaa, aaaa and input string is aaaa).\nInformally, the algorithm constructs a finite state machine that resembles a trie with additional links between the various internal nodes. These extra internal links allow fast transitions between failed pattern matches (e.g. a search for cat in a trie that does not contain cat, but contains cart, and thus would fail at the node prefixed by ca), to other branches of the trie that share a common prefix (e.g., in the previous case, a branch for attribute might be the best lateral transition). This allows the automaton to transition between pattern matches without the need for backtracking.\nWhen the pattern dictionary is known in advance (e.g. a computer virus database), the construction of the automaton can be performed once off-line and the compiled automaton stored for later use. In this case, its run time is linear in the length of the input plus the number of matched entries.\nThe Aho–Corasick string matching algorithm formed the basis of the original Unix command fgrep.","name":"Aho–Corasick algorithm","categories":["All articles lacking in-text citations","Articles lacking in-text citations from February 2013","String matching algorithms"],"tag_line":"In computer science, the Aho–Corasick algorithm is a string searching algorithm invented by Alfred V. Aho and Margaret J. Corasick."}}
,{"_index":"throwtable","_type":"algorithm","_id":"iso-14651","_score":0,"_source":{"description":"ISO/IEC 14651:2007, Information technology -- International string ordering and comparison -- Method for comparing character strings and description of the common template tailorable ordering, is an ISO Standard specifying an algorithm that can be used when comparing two strings. This comparison can be used when collating a set of strings. The standard also specifies a datafile specifying the comparison order, the Common Tailorable Template, CTT. The comparison order is supposed to be tailored for different languages (hence the CTT is regarded as a template and not a default, though the empty tailoring, not changing any weighting, is appropriate in many cases), since different languages have incompatible ordering requirements. One such tailoring is European ordering rules (EOR), which in turn is supposed to be tailored for different European languages.\nThe Common Tailorable Template (CTT) datafile of this ISO Standard is aligned with the Default Unicode Collation Entity Table (DUCET) datafile of the Unicode Collation Algorithm (UCA) specified in Unicode Technical Standard #10.","name":"ISO 14651","categories":["All stub articles","Collation","Computing stubs","ISO standards","Standards and measurement stubs","String collation algorithms","Unicode algorithms"],"tag_line":"ISO/IEC 14651:2007, Information technology -- International string ordering and comparison -- Method for comparing character strings and description of the common template tailorable ordering, is an ISO Standard specifying an algorithm that can be used when comparing two strings."}}
,{"_index":"throwtable","_type":"algorithm","_id":"unicode-collation-algorithm","_score":0,"_source":{"description":"The Unicode collation algorithm (UCA) is an algorithm defined in Unicode Technical Report #10, which defines a customizable method to compare two strings. These comparisons can then be used to collate or sort text in any writing system and language that can be represented with Unicode.\nUnicode Technical Report #10 also specifies the Default Unicode Collation Element Table (DUCET). This datafile specifies the default collation ordering. The DUCET is customizable for different languages. Some such customisations can be found in Common Locale Data Repository (CLDR).\nAn important open source implementation of UCA is included with the International Components for Unicode, ICU. ICU also supports tailoring and the collation tailorings from CLDR are included in ICU. You can see the effects of tailoring and a large number of language specific tailorings in the on-line ICU Locale Explorer.","name":"Unicode collation algorithm","categories":["Algorithms and data structures stubs","All stub articles","Collation","Computer science stubs","Standards and measurement stubs","String collation algorithms","Unicode algorithms"],"tag_line":"The Unicode collation algorithm (UCA) is an algorithm defined in Unicode Technical Report #10, which defines a customizable method to compare two strings."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cyk-algorithm","_score":0,"_source":{"description":"In computer science, the Cocke–Younger–Kasami algorithm (alternatively called CYK, or CKY) is a parsing algorithm for context-free grammars, named after its inventors, John Cocke, Daniel Younger and Tadao Kasami. It employs bottom-up parsing and dynamic programming.\nThe standard version of CYK operates only on context-free grammars given in Chomsky normal form (CNF). However any context-free grammar may be transformed to a CNF grammar expressing the same language (Sipser 1997).\nThe importance of the CYK algorithm stems from its high efficiency in certain situations. Using Landau symbols, the worst case running time of CYK is , where n is the length of the parsed string and |G| is the size of the CNF grammar G. This makes it one of the most efficient parsing algorithms in terms of worst-case asymptotic complexity, although other algorithms exist with better average running time in many practical scenarios.","name":"CYK algorithm","categories":["Parsing algorithms"],"tag_line":"In computer science, the Cocke–Younger–Kasami algorithm (alternatively called CYK, or CKY) is a parsing algorithm for context-free grammars, named after its inventors, John Cocke, Daniel Younger and Tadao Kasami."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gauss–newton-algorithm","_score":0,"_source":{"description":"The Gauss–Newton algorithm is used to solve non-linear least squares problems. It is a modification of Newton's method for finding a minimum of a function. Unlike Newton's method, the Gauss–Newton algorithm can only be used to minimize a sum of squared function values, but it has the advantage that second derivatives, which can be challenging to compute, are not required.\nNon-linear least squares problems arise for instance in non-linear regression, where parameters in a model are sought such that the model is in good agreement with available observations.\nThe method is named after the mathematicians Carl Friedrich Gauss and Isaac Newton.","name":"Gauss–Newton algorithm","categories":["Least squares","Optimization algorithms and methods","Statistical algorithms"],"tag_line":"The Gauss–Newton algorithm is used to solve non-linear least squares problems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"elston–stewart-algorithm","_score":0,"_source":{"description":"The Elston–Stewart algorithm is an algorithm for computing the likelihood of observed genotype data given a pedigree. It is due to Robert Elston and John Stewart. It can handle relatively large pedigrees providing they are (almost) outbred. Its computation time is exponential in the number of markers. It is used in the analysis of genetic linkage.","name":"Elston–Stewart algorithm","categories":["All stub articles","Genetic epidemiology","Genetic linkage analysis","Genetics stubs","Statistical algorithms","Statistical genetics","Statistics stubs"],"tag_line":"The Elston–Stewart algorithm is an algorithm for computing the likelihood of observed genotype data given a pedigree."}}
,{"_index":"throwtable","_type":"algorithm","_id":"nested-sampling-algorithm","_score":0,"_source":{"description":"The nested sampling algorithm is a computational approach to the problem of comparing models in Bayesian statistics, developed in 2004 by physicist John Skilling.","name":"Nested sampling algorithm","categories":["All Wikipedia articles needing context","All pages needing cleanup","Bayesian statistics","Model selection","Statistical algorithms","Wikipedia articles needing context from October 2009","Wikipedia introduction cleanup from October 2009"],"tag_line":"The nested sampling algorithm is a computational approach to the problem of comparing models in Bayesian statistics, developed in 2004 by physicist John Skilling."}}
,{"_index":"throwtable","_type":"algorithm","_id":"linear-programming","_score":0,"_source":{"description":"Linear programming (LP; also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (mathematical optimization).\nMore formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists.\nLinear programs are problems that can be expressed in canonical form as\n\nwhere x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and  is the matrix transpose. The expression to be maximized or minimized is called the objective function (cTx in this case). The inequalities Ax ≤ b and x ≥ 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second then we can say the first vector is less-than or equal-to the second vector.\nLinear programming can be applied to various fields of study. It is widely used in business and economics, and is also utilized for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proved useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.\n\n","alt_names":["Mixed_integer_programming"],"name":"Linear programming","categories":["All articles needing additional references","Articles containing Russian-language text","Articles needing additional references from October 2015","Convex optimization","Geometric algorithms","Linear programming","Mathematical and quantitative methods (economics)","Operations research","P-complete problems","Unsolved problems in computer science","Wikipedia external links cleanup from August 2010","Wikipedia spam cleanup from August 2010"],"tag_line":"Linear programming (LP; also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quickselect","_score":0,"_source":{"description":"In computer science, quickselect is a selection algorithm to find the kth smallest element in an unordered list. It is related to the quicksort sorting algorithm. Like quicksort, it was developed by Tony Hoare, and thus is also known as Hoare's selection algorithm. Like quicksort, it is efficient in practice and has good average-case performance, but has poor worst-case performance. Quickselect and variants is the selection algorithm most often used in efficient real-world implementations.\nQuickselect uses the same overall approach as quicksort, choosing one element as a pivot and partitioning the data in two based on the pivot, accordingly as less than or greater than the pivot. However, instead of recursing into both sides, as in quicksort, quickselect only recurses into one side – the side with the element it is searching for. This reduces the average complexity from O(n log n) to O(n).\nAs with quicksort, quickselect is generally implemented as an in-place algorithm, and beyond selecting the k'th element, it also partially sorts the data. See selection algorithm for further discussion of the connection with sorting.","name":"Quickselect","categories":["All articles needing additional references","Articles needing additional references from August 2013","Selection algorithms"],"tag_line":"In computer science, quickselect is a selection algorithm to find the kth smallest element in an unordered list."}}
,{"_index":"throwtable","_type":"algorithm","_id":"longest-increasing-subsequence","_score":0,"_source":{"description":"In computer science, the longest increasing subsequence problem is to find a subsequence of a given sequence in which the subsequence's elements are in sorted order, lowest to highest, and in which the subsequence is as long as possible. This subsequence is not necessarily contiguous, or unique. Longest increasing subsequences are studied in the context of various disciplines related to mathematics, including algorithmics, random matrix theory, representation theory, and physics. The longest increasing subsequence problem is solvable in time O(n log n), where n denotes the length of the input sequence.\n\n","alt_names":["Longest_increasing_subsequence"],"name":"Longest increasing subsequence","categories":["Combinatorics","Dynamic programming","Formal languages","Pages using citations with old-style implicit et al. in editors","Problems on strings"],"tag_line":"In computer science, the longest increasing subsequence problem is to find a subsequence of a given sequence in which the subsequence's elements are in sorted order, lowest to highest, and in which the subsequence is as long as possible."}}
,{"_index":"throwtable","_type":"algorithm","_id":"transpose","_score":0,"_source":{"description":"This article is about the transpose of a matrix. For other uses, see Transposition\nNote that this article assumes that matrices are taken over a commutative ring. These results may not hold in the non-commutative case.\n\nIn linear algebra, the transpose of a matrix A is another matrix AT (also written A′, Atr, tA or At) created by any one of the following equivalent actions:\nreflect A over its main diagonal (which runs from top-left to bottom-right) to obtain AT\nwrite the rows of A as the columns of AT\nwrite the columns of A as the rows of AT\nFormally, the i th row, j th column element of AT is the j th row, i th column element of A:\n\nIf A is an m × n matrix then AT is an n × m matrix.\nThe transpose of a matrix was introduced in 1858 by the British mathematician Arthur Cayley.","name":"Transpose","categories":["Abstract algebra","Linear algebra","Matrices"],"tag_line":"This article is about the transpose of a matrix."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hofstadter-sequence","_score":0,"_source":{"description":"In mathematics, a Hofstadter sequence is a member of a family of related integer sequences defined by non-linear recurrence relations.","alt_names":["Hofstadter_sequence"],"name":"Hofstadter sequence","categories":["CS1 errors: chapter ignored","Integer sequences","Pages with citations lacking titles"],"tag_line":"In mathematics, a Hofstadter sequence is a member of a family of related integer sequences defined by non-linear recurrence relations."}}
,{"_index":"throwtable","_type":"algorithm","_id":"langton's-ant","_score":0,"_source":{"description":"Langton's ant is a two-dimensional Turing machine with a very simple set of rules but complex emergent behavior. It was invented by Chris Langton in 1986 and runs on a square lattice of black and white cells. The universality of Langton's ant was proven in 2000. The idea has been generalized in several different ways, such as turmites which add more colors and more states.","alt_names":["Langton's_ant"],"name":"Langton's ant","categories":["Artificial life","Cellular automaton rules","Turing machine"],"tag_line":"Langton's ant is a two-dimensional Turing machine with a very simple set of rules but complex emergent behavior."}}
,{"_index":"throwtable","_type":"algorithm","_id":"json","_score":0,"_source":{"description":"JSON, (canonically pronounced /ˈdʒeɪsən/ JAY-sən; sometimes JavaScript Object Notation), is an open standard format that uses human-readable text to transmit data objects consisting of attribute–value pairs. It is the primary data format used for asynchronous browser/server communication (AJAJ), largely replacing XML (used by AJAX).\nAlthough originally derived from the JavaScript scripting language, JSON is a language-independent data format. Code for parsing and generating JSON data is readily available in many programming languages.\nThe JSON format was originally specified by Douglas Crockford. It is currently described by two competing standards, RFC 7159 and ECMA-404. The ECMA standard is minimal, describing only the allowed grammar syntax, whereas the RFC also provides some semantic and security considerations. The official Internet media type for JSON is application/json. The JSON filename extension is .json.","name":"JSON","categories":["2001 introductions","Ajax (programming)","All articles containing potentially dated statements","All articles with dead external links","Articles containing potentially dated statements from 2011","Articles with dead external links from April 2012","Data serialization formats","JSON","JavaScript","Markup languages","Wikipedia articles with LCCN identifiers"],"tag_line":"JSON, (canonically pronounced /ˈdʒeɪsən/ JAY-sən; sometimes JavaScript Object Notation), is an open standard format that uses human-readable text to transmit data objects consisting of attribute–value pairs."}}
,{"_index":"throwtable","_type":"algorithm","_id":"penney's-game","_score":0,"_source":{"description":"Penney's game, named after its inventor Walter Penney, is a binary (head/tail) sequence generating game between two players. At the start of the game, the two players agree on the length of the sequences to be generated. This length is usually taken to be three, but can be any larger number. Player A then selects a sequence of heads and tails of the required length, and shows this sequence to player B. Player B then selects another sequence of heads and tails of the same length. Subsequently, a fair coin is tossed until either player A's or player B's sequence appears as a consecutive subsequence of the coin toss outcomes. The player whose sequence appears first wins.\nProvided sequences of at least length three are used, the second player (B) has an edge over the starting player (A). This is because the game is nontransitive such that for any given sequence of length three or longer one can find another sequence that has higher probability of occurring first.","alt_names":["Penney's_game"],"name":"Penney's game","categories":["Mathematical games","Use dmy dates from September 2010"],"tag_line":"Penney's game, named after its inventor Walter Penney, is a binary (head/tail) sequence generating game between two players."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cholesky-decomposition","_score":0,"_source":{"description":"In linear algebra, the Cholesky decomposition or Cholesky factorization is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose, which is useful e.g. for efficient numerical solutions and Monte Carlo simulations. It was discovered by André-Louis Cholesky for real matrices. When it is applicable, the Cholesky decomposition is roughly twice as efficient as the LU decomposition for solving systems of linear equations.","alt_names":["Cholesky_decomposition"],"name":"Cholesky decomposition","categories":["All articles with unsourced statements","Articles with French-language external links","Articles with inconsistent citation formats","Articles with unsourced statements from February 2011","Articles with unsourced statements from June 2011","CS1 errors: chapter ignored","Matrix decompositions","Numerical linear algebra","Operator theory"],"tag_line":"In linear algebra, the Cholesky decomposition or Cholesky factorization is a decomposition of a Hermitian, positive-definite matrix into the product of a lower triangular matrix and its conjugate transpose, which is useful e.g."}}
,{"_index":"throwtable","_type":"algorithm","_id":"permutation","_score":0,"_source":{"description":"In mathematics, the notion of permutation relates to the act of arranging all the members of a set into some sequence or order, or if the set is already ordered, rearranging (reordering) its elements, a process called permuting. These differ from combinations, which are selections of some members of a set where order is disregarded. For example, written as tuples, there are six permutations of the set {1,2,3}, namely: (1,2,3), (1,3,2), (2,1,3), (2,3,1), (3,1,2), and (3,2,1). These are all the possible orderings of this three element set. As another example, an anagram of a word, all of whose letters are different, is a permutation of its letters. In this example, the letters are already ordered in the original word and the anagram is a reordering of the letters. The study of permutations of finite sets is a topic in the field of combinatorics.\nPermutations occur, in more or less prominent ways, in almost every area of mathematics. They often arise when different orderings on certain finite sets are considered, possibly only because one wants to ignore such orderings and needs to know how many configurations are thus identified. For similar reasons permutations arise in the study of sorting algorithms in computer science.\nThe number of permutations of n distinct objects is n factorial usually written as n!, which means the product of all positive integers less than or equal to n.\nIn algebra and particularly in group theory, a permutation of a set S is defined as a bijection from S to itself. That is, it is a function from S to S for which every element occurs exactly once as an image value. This is related to the rearrangement of the elements of S in which each element s is replaced by the corresponding f(s). The collection of such permutations form a group called the symmetric group of S. The key to this group's structure is the fact that the composition of two permutations (performing two given rearrangements in succession) results in another rearrangement. Permutations may act on structured objects by rearranging their components, or by certain replacements (substitutions) of symbols.\nIn elementary combinatorics, the k-permutations, or partial permutations, are the ordered arrangements of k distinct elements selected from a set. When k is equal to the size of the set, these are the permutations of the set.\n\n","name":"Permutation","categories":["All articles covered by WikiProject Wikify","All articles needing references cleanup","Articles covered by WikiProject Wikify from April 2015","Commons category with local link same as on Wikidata","Factorial and binomial topics","Permutations","Wikipedia references cleanup from April 2015"],"tag_line":"In mathematics, the notion of permutation relates to the act of arranging all the members of a set into some sequence or order, or if the set is already ordered, rearranging (reordering) its elements, a process called permuting."}}
,{"_index":"throwtable","_type":"algorithm","_id":"median-filter","_score":0,"_source":{"description":"In signal processing, it is often desirable to be able to perform some kind of noise reduction on an image or signal. The median filter is a nonlinear digital filtering technique, often used to remove noise. Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image). Median filtering is very widely used in digital image processing because, under certain conditions, it preserves edges while removing noise (but see discussion below).","alt_names":["Median_filter"],"name":"Median filter","categories":["Image noise reduction techniques","Nonlinear filters","Signal processing"],"tag_line":"In signal processing, it is often desirable to be able to perform some kind of noise reduction on an image or signal."}}
,{"_index":"throwtable","_type":"algorithm","_id":"monty-hall-problem","_score":0,"_source":{"description":"The Monty Hall problem is a brain teaser, in the form of a probability puzzle (Gruber, Krauss and others), loosely based on the American television game show Let's Make a Deal and named after its original host, Monty Hall. The problem was originally posed in a letter by Steve Selvin to the American Statistician in 1975 (Selvin 1975a), (Selvin 1975b). It became famous as a question from a reader's letter quoted in Marilyn vos Savant's \"Ask Marilyn\" column in Parade magazine in 1990 (vos Savant 1990a):\n\nSuppose you're on a game show, and you're given the choice of three doors: Behind one door is a car; behind the others, goats. You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door, say No. 3, which has a goat. He then says to you, \"Do you want to pick door No. 2?\" Is it to your advantage to switch your choice?\n\nVos Savant's response was that the contestant should switch to the other door (vos Savant 1990a). Under the standard assumptions, contestants who switch have a 2/3 chance of winning the car, while contestants who stick to their choice have only a 1/3 chance.\nThe given probabilities depend on specific assumptions about how the host and contestant choose their doors. A key insight is that, under these standard conditions, there is more information about doors 2 and 3 that was not available at the beginning of the game, when the door 1 was chosen by the player: the host's deliberate action adds value to the door he did not choose to eliminate, but not to the one chosen by the contestant originally. Other possible behaviors than the one described can reveal different additional information, or none at all, and yield different probabilities.\nMany readers of vos Savant's column refused to believe switching is beneficial despite her explanation. After the problem appeared in Parade, approximately 10,000 readers, including nearly 1,000 with PhDs, wrote to the magazine, most of them claiming vos Savant was wrong (Tierney 1991). Even when given explanations, simulations, and formal mathematical proofs, many people still do not accept that switching is the best strategy (vos Savant 1991a). Paul Erdős, one of the most prolific mathematicians in history, remained unconvinced until he was shown a computer simulation confirming the predicted result (Vazsonyi 1999).\nThe problem is a paradox of the veridical type, because the correct result (you should switch doors) is so counterintuitive it can seem absurd, but is nevertheless demonstrably true. The Monty Hall problem is mathematically closely related to the earlier Three Prisoners problem and to the much older Bertrand's box paradox.\n\n","alt_names":["Monty_Hall_problem"],"name":"Monty Hall problem","categories":["Articles with DMOZ links","Decision theory paradoxes","Game theory","Let's Make a Deal","Mathematical problems","Microeconomics","Named probability problems","Probability theory paradoxes","Use Harvard referencing from August 2015","Wikipedia external links cleanup from October 2014","Wikipedia spam cleanup from October 2014"],"tag_line":"The Monty Hall problem is a brain teaser, in the form of a probability puzzle (Gruber, Krauss and others), loosely based on the American television game show Let's Make a Deal and named after its original host, Monty Hall."}}
,{"_index":"throwtable","_type":"algorithm","_id":"gray-code","_score":0,"_source":{"description":"The reflected binary code, also known as Gray code after Frank Gray, is a binary numeral system where two successive values differ in only one bit (binary digit). The reflected binary code was originally designed to prevent spurious output from electromechanical switches. Today, Gray codes are widely used to facilitate error correction in digital communications such as digital terrestrial television and some cable TV systems.\n\n","alt_names":["Gray_code"],"name":"Gray code","categories":["All articles with failed verification","Articles with failed verification from July 2015","Articles with failed verification from November 2015","Data transmission","Numeral systems"],"tag_line":"The reflected binary code, also known as Gray code after Frank Gray, is a binary numeral system where two successive values differ in only one bit (binary digit)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bulls-and-cows","_score":0,"_source":{"description":"Bulls and Cows (also known as Cows and Bulls or Pigs and Bulls or Bulls and Cleots) is an old code-breaking mind or paper and pencil game for two or more players, predating the similar commercially marketed board game Mastermind.\nIt is a game with numbers or words that may date back a century or more. It is played by two opponents.","alt_names":["Bulls_and_Cows"],"name":"Bulls and Cows","categories":["Games of mental skill","Logic puzzles","Paper-and-pencil games"],"tag_line":"Bulls and Cows (also known as Cows and Bulls or Pigs and Bulls or Bulls and Cleots) is an old code-breaking mind or paper and pencil game for two or more players, predating the similar commercially marketed board game Mastermind."}}
,{"_index":"throwtable","_type":"algorithm","_id":"24-game","_score":0,"_source":{"description":"The 24 Game is an arithmetical card game in which the objective is to find a way to manipulate four integers so that the end result is 24. Addition, subtraction, multiplication, or division, and sometimes other operations, may be used to make four digits from one to nine equal 24. For example, card with the numbers 4,7,8,8, a possible solution is the following: (7-(8/8))*4=24.\nThe game has been played in Shanghai since the 1960s, using ordinary playing cards. It is similar to the card game Maths24.","alt_names":["24_Game"],"name":"24 Game","categories":["Brain fitness video games","Card games introduced in the 1960s","Mathematical games","Year of introduction missing"],"tag_line":"The 24 Game is an arithmetical card game in which the objective is to find a way to manipulate four integers so that the end result is 24."}}
,{"_index":"throwtable","_type":"algorithm","_id":"first-class-function","_score":0,"_source":{"description":"In computer science, a programming language is said to have first-class functions if it treats functions as first-class citizens. Specifically, this means the language supports passing functions as arguments to other functions, returning them as the values from other functions, and assigning them to variables or storing them in data structures. Some programming language theorists require support for anonymous functions (function literals) as well. In languages with first-class functions, the names of functions do not have any special status; they are treated like ordinary variables with a function type. The term was coined by Christopher Strachey in the context of \"functions as first-class citizens\" in the mid-1960s.\nFirst-class functions are a necessity for the functional programming style, in which the use of higher-order functions is a standard practice. A simple example of a higher-ordered function is the map function, which takes, as its arguments, a function and a list, and returns the list formed by applying the function to each member of the list. For a language to support map, it must support passing a function as an argument.\nThere are certain implementation difficulties in passing functions as arguments and returning them as results, especially in the presence of non-local variables introduced in nested and anonymous functions. Historically, these were termed the funarg problems, the name coming from \"function argument\". In early imperative languages these problems were avoided by either not supporting functions as result types (e.g. ALGOL 60, Pascal) or omitting nested functions and thus non-local variables (e.g. C). The early functional language Lisp took the approach of dynamic scoping, where non-local variables refer to the closest definition of that variable at the point where the function is executed, instead of where it was defined. Proper support for lexically scoped first-class functions was introduced in Scheme and requires handling references to functions as closures instead of bare function pointers, which in turn makes garbage collection a necessity.","alt_names":["First-class_function"],"name":"First-class function","categories":["Articles with example C code","Articles with example Haskell code","Compiler construction","Data types","Functional programming","Primitive types","Programming language theory","Subroutines"],"tag_line":"In computer science, a programming language is said to have first-class functions if it treats functions as first-class citizens."}}
,{"_index":"throwtable","_type":"algorithm","_id":"naming-convention-(programming)","_score":0,"_source":{"description":"In computer programming, a naming convention is a set of rules for choosing the character sequence to be used for identifiers which denote variables, types, functions, and other entities in source code and documentation.\nReasons for using a naming convention (as opposed to allowing programmers to choose any character sequence) include the following:\nto reduce the effort needed to read and understand source code;\nto enable code reviews to focus on more important issues than arguing over syntax and naming standards.\nto enable code quality review tools to focus their reporting mainly on significant issues other than syntax and style preferences.\nto enhance source code appearance (for example, by disallowing overly long names or unclear abbreviations).\nThe choice of naming conventions can be an enormously controversial issue, with partisans of each holding theirs to be the best and others to be inferior. Colloquially, this is said to be a matter of dogma. Many companies have also established their own set of conventions to best meet their interests.","alt_names":["Naming_convention_(programming)"],"name":"Naming convention (programming)","categories":["All articles needing additional references","All articles to be merged","All articles with unsourced statements","Articles needing additional references from September 2010","Articles to be merged from December 2013","Articles with unsourced statements from February 2007","Articles with unsourced statements from November 2011","CS1 errors: dates","Naming conventions","Source code","Use dmy dates from January 2012"],"tag_line":"In computer programming, a naming convention is a set of rules for choosing the character sequence to be used for identifiers which denote variables, types, functions, and other entities in source code and documentation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lu-decomposition","_score":0,"_source":{"description":"In numerical analysis, LU decomposition (where 'LU' stands for 'lower upper', and also called LU factorization) factors a matrix as the product of a lower triangular matrix and an upper triangular matrix. The product sometimes includes a permutation matrix as well. The LU decomposition can be viewed as the matrix form of Gaussian elimination. Computers usually solve square systems of linear equations using the LU decomposition, and it is also a key step when inverting a matrix, or computing the determinant of a matrix. The LU decomposition was introduced by mathematician Alan Turing in 1948.\n\n","alt_names":["LU_decomposition"],"name":"LU decomposition","categories":["Matrix decompositions","Numerical linear algebra"],"tag_line":"In numerical analysis, LU decomposition (where 'LU' stands for 'lower upper', and also called LU factorization) factors a matrix as the product of a lower triangular matrix and an upper triangular matrix."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sundial","_score":0,"_source":{"description":"A sundial is a device that tells the time of day by the apparent position of the Sun in the sky. In the narrowest sense of the word it will consists of a flat plate, the dial and a gnomon which will throw a shadow onto the surface; lines marked onto the surface will indicate the time of day. The style is the time-telling edge of the gnomon, though a single point or nodus may be used. As the sun appears to move across the sky, the shadow-edge aligns with different hour-lines. The gnomon may be a thin rod, a wire or an elaborately decorated metal casting. The style must be parallel to the axis of the Earth's rotation to remain accurate throughout the year. The style's angle from the horizontal is equal the sundial's geographical latitude.\nIn a broader sense a sundial is any device that uses the sun's altitude or azimuth to show the time. It is common for inexpensive mass-produced decorative sundials to have incorrect hour angles, which cannot be adjusted to tell correct time.Sundials are valued as decorative objects, as literary metaphors and as object of mathematical study.","alt_names":["sundial"],"name":"Sundial","categories":["All articles with dead external links","All articles with unsourced statements","Ancient inventions","Architectural elements","Articles with dead external links from June 2014","Articles with unsourced statements from August 2012","Articles with unsourced statements from June 2013","CS1 German-language sources (de)","Clocks","Commons category with local link same as on Wikidata","Garden features","Garden ornaments","Horology","Outdoor sculptures","Sundials","Wikipedia articles with GND identifiers"],"tag_line":"A sundial is a device that tells the time of day by the apparent position of the Sun in the sky."}}
,{"_index":"throwtable","_type":"algorithm","_id":"short-circuit-evaluation","_score":0,"_source":{"description":"Short-circuit evaluation, minimal evaluation, or McCarthy evaluation denotes the semantics of some Boolean operators in some programming languages in which the second argument is executed or evaluated only if the first argument does not suffice to determine the value of the expression: when the first argument of the AND function evaluates to false, the overall value must be false; and when the first argument of the OR function evaluates to true, the overall value must be true. In some programming languages (Lisp), the usual Boolean operators are short-circuit. In others (Java, Ada), both short-circuit and standard Boolean operators are available. For some Boolean operations, like XOR, it is not possible to short-circuit, because both operands are always required to determine the result.\nThe short-circuit expression x Sand y (using Sand to denote the short-circuit variety) is equivalent to the conditional expression if x then y else false; the expression x Sor y is equivalent to if x then true else y.\nShort-circuit operators are, in effect, control structures rather than simple arithmetic operators, as they are not strict. In imperative language terms (notably C and C++), where side effects are important, short-circuit operators introduce a sequence point – they completely evaluate the first argument, including any side effects, before (optionally) processing the second argument. ALGOL 68 used \"proceduring\" to achieve user defined short-circuit operators & procedures.\nIn loosely typed languages that have more than the two truth-values True and False, short-circuit operators may return the last evaluated subexpression, so that x Sor y and x Sand y are actually equivalent to if x then x else y and if x then y else x respectively (without actually evaluating x twice). This is called \"Last value\" in the table below.\nIn languages that use lazy evaluation by default (like Haskell), all functions are effectively \"short-circuit\", and special short-circuit operators are unnecessary.","alt_names":["Short-circuit_evaluation"],"name":"Short-circuit evaluation","categories":["All articles needing additional references","All articles with specifically marked weasel-worded phrases","All articles with unsourced statements","Articles needing additional references from August 2013","Articles with example C code","Articles with example Perl code","Articles with specifically marked weasel-worded phrases from July 2010","Articles with unsourced statements from July 2010","Evaluation strategy","Wikipedia articles needing clarification from November 2010"],"tag_line":"Short-circuit evaluation, minimal evaluation, or McCarthy evaluation denotes the semantics of some Boolean operators in some programming languages in which the second argument is executed or evaluated only if the first argument does not suffice to determine the value of the expression: when the first argument of the AND function evaluates to false, the overall value must be false; and when the first argument of the OR function evaluates to true, the overall value must be true."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sudoku","_score":0,"_source":{"description":"Sudoku (数独, sūdoku, digit-single) (/suːˈdoʊkuː/, /-ˈdɒ-/, /sə-/; originally called Number Place), is a logic-based, combinatorial number-placement puzzle. The objective is to fill a 9×9 grid with digits so that each column, each row, and each of the nine 3×3 sub-grids that compose the grid (also called \"boxes\", \"blocks\", \"regions\", or \"sub-squares\") contains all of the digits from 1 to 9. The puzzle setter provides a partially completed grid, which for a well-posed puzzle has a unique solution.\nCompleted puzzles are always a type of Latin square with an additional constraint on the contents of individual regions. For example, the same single integer may not appear twice in the same row, column or in any of the nine 3×3 subregions of the 9x9 playing board.\nFrench newspapers featured variations of the puzzles in the 19th century, and the puzzle has appeared since 1979 in puzzle books under the name Number Place. However, the modern sudoku only started to become mainstream in 1986 by the Japanese puzzle company Nikoli, under the name Sudoku, meaning single number. It first appeared in a US newspaper and then The Times (UK) in 2004, from the efforts of Wayne Gould, who devised a computer program to rapidly produce distinct puzzles.","name":"Sudoku","categories":["Articles containing Japanese-language text","Articles including recorded pronunciations","Articles with DMOZ links","CS1 Japanese-language sources (ja)","CS1 uses Japanese-language script (ja)","Logic puzzles","NP-complete problems","Pages containing cite templates with deprecated parameters","Pages containing links to subscription-only content","Pages with duplicate reference names","Pages with login required references or sources","Pages with reference errors","Puzzle video games","Recreational mathematics","Sudoku","Use mdy dates from August 2013"],"tag_line":"Sudoku (数独, sūdoku, digit-single) (/suːˈdoʊkuː/, /-ˈdɒ-/, /sə-/; originally called Number Place), is a logic-based, combinatorial number-placement puzzle."}}
,{"_index":"throwtable","_type":"algorithm","_id":"power-set","_score":0,"_source":{"description":"In mathematics, the power set (or powerset) of any set S, written P(S), ℘(S), P(S), ℙ(S) or 2S, is the set of all subsets of S, including the empty set and S itself. In axiomatic set theory (as developed, for example, in the ZFC axioms), the existence of the power set of any set is postulated by the axiom of power set.\nAny subset of P(S) is called a family of sets over S.","alt_names":["Power_set"],"name":"Power set","categories":["Abstract algebra","Algebra","Basic concepts in set theory"],"tag_line":"In mathematics, the power set (or powerset) of any set S, written P(S), ℘(S), P(S), ℙ(S) or 2S, is the set of all subsets of S, including the empty set and S itself."}}
,{"_index":"throwtable","_type":"algorithm","_id":"van-der-corput-sequence","_score":0,"_source":{"description":"A van der Corput sequence is an example of the simplest one-dimensional low-discrepancy sequence over the unit interval; it was first described in 1935 by the Dutch mathematician J. G. van der Corput. It is constructed by reversing the base n representation of the sequence of natural numbers (1, 2, 3, …).\nThe b-ary representation of the positive integer n (≥ 1) is\n\nwhere b is the base of in which number n is represented, and 0 ≤ dk(n) < b, i.e. the kth digit in the b-ary expansion of n. The nth number in the van der Corput sequence is","alt_names":["Van_der_Corput_sequence"],"name":"Van der Corput sequence","categories":["CS1 German-language sources (de)","Diophantine approximation","Quasirandomness","Sequences and series"],"tag_line":"A van der Corput sequence is an example of the simplest one-dimensional low-discrepancy sequence over the unit interval; it was first described in 1935 by the Dutch mathematician J. G. van der Corput."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hidato","_score":0,"_source":{"description":"Hidato (Hebrew: חידאתו‎, originating from the Hebrew word Hida = Riddle) is a logic puzzle game invented by Dr. Gyora Benedek, an Israeli mathematician. The goal of Hidato is to fill the grid with consecutive numbers that connect horizontally, vertically, or diagonally. Numbrix puzzles, created by Marilyn vos Savant, are similar to Hidato except that diagonal moves are not allowed. Jadium puzzles (formerly Snakepit puzzles), created by Jeff Marchant, are a more difficult version of Numbrix with fewer given numbers and have appeared on the Parade magazine web site regularly since 2014. The names Numbrix and Hidato are registered trademarks. Some publishers use different names for this puzzle such as Number Snake.","name":"Hidato","categories":["All articles lacking sources","Articles containing Hebrew-language text","Articles lacking sources from December 2008","Israeli inventions","Logic puzzles","Puzzle video games"],"tag_line":"Hidato (Hebrew: חידאתו‎, originating from the Hebrew word Hida = Riddle) is a logic puzzle game invented by Dr. Gyora Benedek, an Israeli mathematician."}}
,{"_index":"throwtable","_type":"algorithm","_id":"problem-of-apollonius","_score":0,"_source":{"description":"In Euclidean plane geometry, Apollonius's problem is to construct circles that are tangent to three given circles in a plane (Figure 1). Apollonius of Perga (ca. 262 BC – ca. 190 BC) posed and solved this famous problem in his work Ἐπαφαί (Epaphaí, \"Tangencies\"); this work has been lost, but a 4th-century report of his results by Pappus of Alexandria has survived. Three given circles generically have eight different circles that are tangent to them (Figure 2) and each solution circle encloses or excludes the three given circles in a different way: in each solution, a different subset of the three circles is enclosed (its complement is excluded) and there are 8 subsets of a set whose cardinality is 3, since 8 = 23.\nIn the 16th century, Adriaan van Roomen solved the problem using intersecting hyperbolas, but this solution does not use only straightedge and compass constructions. François Viète found such a solution by exploiting limiting cases: any of the three given circles can be shrunk to zero radius (a point) or expanded to infinite radius (a line). Viète's approach, which uses simpler limiting cases to solve more complicated ones, is considered a plausible reconstruction of Apollonius' method. The method of van Roomen was simplified by Isaac Newton, who showed that Apollonius' problem is equivalent to finding a position from the differences of its distances to three known points. This has applications in navigation and positioning systems such as LORAN.\nLater mathematicians introduced algebraic methods, which transform a geometric problem into algebraic equations. These methods were simplified by exploiting symmetries inherent in the problem of Apollonius: for instance solution circles generically occur in pairs, with one solution enclosing the given circles that the other excludes (Figure 2). Joseph Diaz Gergonne used this symmetry to provide an elegant straightedge and compass solution, while other mathematicians used geometrical transformations such as reflection in a circle to simplify the configuration of the given circles. These developments provide a geometrical setting for algebraic methods (using Lie sphere geometry) and a classification of solutions according to 33 essentially different configurations of the given circles.\nApollonius' problem has stimulated much further work. Generalizations to three dimensions—constructing a sphere tangent to four given spheres—and beyond have been studied. The configuration of three mutually tangent circles has received particular attention. René Descartes gave a formula relating the radii of the solution circles and the given circles, now known as Descartes' theorem. Solving Apollonius' problem iteratively in this case leads to the Apollonian gasket, which is one of the earliest fractals to be described in print, and is important in number theory via Ford circles and the Hardy–Littlewood circle method.","alt_names":["Problem_of_Apollonius"],"name":"Problem of Apollonius","categories":["Articles containing Greek-language text","Articles with French-language external links","Articles with Latin-language external links","CS1 French-language sources (fr)","CS1 German-language sources (de)","CS1 Latin-language sources (la)","Conformal geometry","Euclidean plane geometry","Featured articles","History of geometry","Incidence geometry"],"tag_line":"In Euclidean plane geometry, Apollonius's problem is to construct circles that are tangent to three given circles in a plane (Figure 1)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sokoban","_score":0,"_source":{"description":"Sokoban (倉庫番, sōkoban, warehouse keeper) is a type of transport puzzle, in which the player pushes boxes or crates around in a warehouse, trying to get them to storage locations. The puzzle is usually implemented as a video game.\nSokoban was created in 1981 by Hiroyuki Imabayashi, and published in 1982 by Thinking Rabbit, a software house based in Takarazuka, Japan.","name":"Sokoban","categories":["1982 video games","All articles lacking in-text citations","Articles containing Japanese-language text","Articles lacking in-text citations from July 2012","DOS games","FM-7 games","GP2X games","Japanese inventions","Linux games","Logic puzzles","NEC PC-6001 games","NEC PC-8801 games","NEC PC-9801 games","OS X games","Open-source puzzle video games","PSPACE-complete problems","Puzzle video games","Video games developed in Japan","Windows Mobile Professional games","Windows games","ZX Spectrum games"],"tag_line":"Sokoban (倉庫番, sōkoban, warehouse keeper) is a type of transport puzzle, in which the player pushes boxes or crates around in a warehouse, trying to get them to storage locations."}}
,{"_index":"throwtable","_type":"algorithm","_id":"derangement","_score":0,"_source":{"description":"In combinatorial mathematics, a derangement is a permutation of the elements of a set, such that no element appears in its original position.\nThe number of derangements of a set of size n, usually written Dn, dn, or !n, is called the \"derangement number\" or \"de Montmort number\". (These numbers are generalized to rencontres numbers.) The subfactorial function (not to be confused with the factorial n!) maps n to !n. No standard notation for subfactorials is agreed upon; n¡ is sometimes used instead of !n.\nThe problem of counting derangements was first considered by Pierre Raymond de Montmort in 1708; he solved it in 1713, as did Nicholas Bernoulli at about the same time.","name":"Derangement","categories":["CS1 errors: external links","Fixed points (mathematics)","Integer sequences","Permutations"],"tag_line":"In combinatorial mathematics, a derangement is a permutation of the elements of a set, such that no element appears in its original position."}}
,{"_index":"throwtable","_type":"algorithm","_id":"median","_score":0,"_source":{"description":"In statistics and probability theory, a median is the number separating the higher half of a data sample, a population, or a probability distribution, from the lower half. The median of a finite list of numbers can be found by arranging all the observations from lowest value to highest value and picking the middle one (e.g., the median of {3, 3, 5, 9, 11} is 5). If there is an even number of observations, then there is no single middle value; the median is then usually defined to be the mean of the two middle values   (the median of {3, 5, 7, 9} is (5 + 7) / 2 = 6), which corresponds to interpreting the median as the fully trimmed mid-range. The median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of 50%: so long as no more than half the data are contaminated, the median will not give an arbitrarily large result. A median is only defined on ordered one-dimensional data, and is independent of any distance metric. A geometric median, on the other hand, is defined in any number of dimensions.\nIn a sample of data, or a finite population, there may be no member of the sample whose value is identical to the median (in the case of an even sample size); if there is such a member, there may be more than one so that the median may not uniquely identify a sample member. Nonetheless, the value of the median is uniquely determined with the usual definition. A related concept, in which the outcome is forced to correspond to a member of the sample, is the medoid. At most, half the population have values strictly less than the median, and, at most, half have values strictly greater than the median. If each group contains less than half the population, then some of the population is exactly equal to the median. For example, if a < b < c, then the median of the list {a, b, c} is b, and, if a < b < c < d, then the median of the list {a, b, c, d} is the mean of b and c; i.e., it is (b + c)/2.\nThe median can be used as a measure of location when a distribution is skewed, when end-values are not known, or when one requires reduced importance to be attached to outliers, e.g., because they may be measurement errors.\nIn terms of notation, some authors represent the median of a variable x either as x͂ or as μ1/2 sometimes also M. There is no widely accepted standard notation for the median, so the use of these or other symbols for the median needs to be explicitly defined when they are introduced.\nThe median is the 2nd quartile, 5th decile, and 50th percentile.","name":"Median","categories":["All articles with unsourced statements","Articles with unsourced statements from July 2012","Articles with unsourced statements from May 2012","Articles with unsourced statements from October 2015","Means","Robust statistics","Statistical terminology","Wikipedia articles incorporating text from PlanetMath"],"tag_line":"In statistics and probability theory, a median is the number separating the higher half of a data sample, a population, or a probability distribution, from the lower half."}}
,{"_index":"throwtable","_type":"algorithm","_id":"continuous-knapsack-problem","_score":0,"_source":{"description":"In theoretical computer science, the continuous knapsack problem (also known as the fractional knapsack problem) is an algorithmic problem in combinatorial optimization in which the goal is to fill a container (the \"knapsack\") with fractional amounts of different materials chosen to maximize the value of the selected materials. It resembles the classic knapsack problem, in which the items to be placed in the container are indivisible; however, the continuous knapsack problem may be solved in polynomial time whereas the classic knapsack problem is NP-hard. It is a classic example of how a seemingly small change in the formulation of a problem can have a large impact on its computational complexity.","alt_names":["Continuous_knapsack_problem"],"name":"Continuous knapsack problem","categories":["Combinatorial optimization"],"tag_line":"In theoretical computer science, the continuous knapsack problem (also known as the fractional knapsack problem) is an algorithmic problem in combinatorial optimization in which the goal is to fill a container (the \"knapsack\") with fractional amounts of different materials chosen to maximize the value of the selected materials."}}
,{"_index":"throwtable","_type":"algorithm","_id":"object-copying","_score":0,"_source":{"description":"In object-oriented programming, object copying is creating a copy of an existing object, a unit of data in object-oriented programming. The resulting object is called an object copy or simply copy of the original object. Copying is basic but has subtleties and can have significant overhead. There are several ways to copy an object, most commonly by a copy constructor or cloning. Copying is primarily done so the copy can be modified or moved, or the current value preserved; if these are not necessary, a reference to the original data is sufficient and more efficient, as it does not require copying.\nObjects in general store composite data; while in simple cases copying can be done by allocating a new, uninitialized object and copying all fields (attributes) from the original object, in more complex cases this does not result in desired behavior.","alt_names":["Deep_copy"],"name":"Object copying","categories":["Object (computer science)","Pages with duplicate reference names","Pages with reference errors"],"tag_line":"In object-oriented programming, object copying is creating a copy of an existing object, a unit of data in object-oriented programming."}}
,{"_index":"throwtable","_type":"algorithm","_id":"501(c)-organization","_score":0,"_source":{"description":"A 501(c) organization, also known colloquially as a 501(c), is a tax-exempt nonprofit organization in the United States. Section 501(c) of the United States Internal Revenue Code (26 U.S.C. § 501(c)) provides that 29 types of nonprofit organizations are exempt from some federal income taxes. Sections 503 through 505 set out the requirements for attaining such exemptions. Many states refer to Section 501(c) for definitions of organizations exempt from state taxation as well. 501(c) organizations can receive unlimited contributions from individuals, corporations, and unions.\nThe most common type of tax-exempt nonprofit organization falls under category 501(c)(3), whereby a nonprofit organization is exempt from federal income tax if its activities have the following purposes: charitable, religious, educational, scientific, literary, testing for public safety, fostering amateur sports competition, or preventing cruelty to children or animals. The 501(c)(4) and 501(c)(6) categories are for politically active nonprofits, which have become increasingly important since the 2004 presidential election.\n\n","alt_names":["501(c)"],"name":"501(c) organization","categories":["All articles containing potentially dated statements","All articles lacking reliable references","Articles containing potentially dated statements from 2006","Articles lacking reliable references from February 2014","Charity law","Internal Revenue Code","Non-profit organizations based in the United States","Taxation in the United States","Types of organization","Use dmy dates from May 2014","Wikipedia indefinitely move-protected pages"],"tag_line":"A 501(c) organization, also known colloquially as a 501(c), is a tax-exempt nonprofit organization in the United States."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fractal","_score":0,"_source":{"description":"A fractal is a natural phenomenon or a mathematical set that exhibits a repeating pattern that displays at every scale. It is also known as expanding symmetry or evolving symmetry. If the replication is exactly the same at every scale, it is called a self-similar pattern. An example of this is the Menger Sponge. Fractals can also be nearly the same at different levels. This latter pattern is illustrated in the magnifications of the Mandelbrot set. Fractals also include the idea of a detailed pattern that repeats itself.\nFractals are different from other geometric figures because of the way in which they scale. Doubling the edge lengths of a polygon multiplies its area by four, which is two (the ratio of the new to the old side length) raised to the power of two (the dimension of the space the polygon resides in). Likewise, if the radius of a sphere is doubled, its volume scales by eight, which is two (the ratio of the new to the old radius) to the power of three (the dimension that the sphere resides in). But if a fractal's one-dimensional lengths are all doubled, the spatial content of the fractal scales by a power that is not necessarily an integer. This power is called the fractal dimension of the fractal, and it usually exceeds the fractal's topological dimension.\nAs mathematical equations, fractals are usually nowhere differentiable. An infinite fractal curve can be conceived of as winding through space differently from an ordinary line, still being a 1-dimensional line yet having a fractal dimension indicating it also resembles a surface.\n\nThe mathematical roots of the idea of fractals have been traced throughout the years as a formal path of published works, starting in the 17th century with notions of recursion, then moving through increasingly rigorous mathematical treatment of the concept to the study of continuous but not differentiable functions in the 19th century, and on to the coining of the word fractal in the 20th century with a subsequent burgeoning of interest in fractals and computer-based modelling in the 21st century. The term \"fractal\" was first used by mathematician Benoît Mandelbrot in 1975. Mandelbrot based it on the Latin frāctus meaning \"broken\" or \"fractured\", and used it to extend the concept of theoretical fractional dimensions to geometric patterns in nature.\nThere is some disagreement amongst authorities about how the concept of a fractal should be formally defined. Mandelbrot himself summarized it as \"beautiful, damn hard, increasingly useful. That's fractals.\" The general consensus is that theoretical fractals are infinitely self-similar, iterated, and detailed mathematical constructs having fractal dimensions, of which many examples have been formulated and studied in great depth. Fractals are not limited to geometric patterns, but can also describe processes in time. Fractal patterns with various degrees of self-similarity have been rendered or studied in images, structures and sounds and found in nature, technology, art, and law.","name":"Fractal","categories":["Articles with DMOZ links","CS1 French-language sources (fr)","CS1 errors: chapter ignored","Digital art","Fractals","Mathematical structures","Pages using web citations with no URL","Pages with URL errors","Topology"],"tag_line":"A fractal is a natural phenomenon or a mathematical set that exhibits a repeating pattern that displays at every scale."}}
,{"_index":"throwtable","_type":"algorithm","_id":"antenna-(radio)","_score":0,"_source":{"description":"An antenna (plural antennae or antennas), or aerial, is an electrical device which converts electric power into radio waves, and vice versa. It is usually used with a radio transmitter or radio receiver. In transmission, a radio transmitter supplies an electric current oscillating at radio frequency (i.e. a high frequency alternating current (AC)) to the antenna's terminals, and the antenna radiates the energy from the current as electromagnetic waves (radio waves). In reception, an antenna intercepts some of the power of an electromagnetic wave in order to produce a tiny voltage at its terminals, that is applied to a receiver to be amplified.\nAntennas are essential components of all equipment that uses radio. They are used in systems such as radio broadcasting, broadcast television, two-way radio, communications receivers, radar, cell phones, and satellite communications, as well as other devices such as garage door openers, wireless microphones, Bluetooth-enabled devices, wireless computer networks, baby monitors, and RFID tags on merchandise.\nTypically an antenna consists of an arrangement of metallic conductors (elements), electrically connected (often through a transmission line) to the receiver or transmitter. An oscillating current of electrons forced through the antenna by a transmitter will create an oscillating magnetic field around the antenna elements, while the charge of the electrons also creates an oscillating electric field along the elements. These time-varying fields radiate away from the antenna into space as a moving transverse electromagnetic field wave. Conversely, during reception, the oscillating electric and magnetic fields of an incoming radio wave exert force on the electrons in the antenna elements, causing them to move back and forth, creating oscillating currents in the antenna.\nAntennas can be designed to transmit and receive radio waves in all horizontal directions equally (omnidirectional antennas), or preferentially in a particular direction (directional or high gain antennas). In the latter case, an antenna may also include additional elements or surfaces with no electrical connection to the transmitter or receiver, such as parasitic elements, parabolic reflectors or horns, which serve to direct the radio waves into a beam or other desired radiation pattern.\nThe first antennas were built in 1888 by German physicist Heinrich Hertz in his pioneering experiments to prove the existence of electromagnetic waves predicted by the theory of James Clerk Maxwell. Hertz placed dipole antennas at the focal point of parabolic reflectors for both transmitting and receiving. He published his work in Annalen der Physik und Chemie (vol. 36, 1889).","alt_names":["Antenna_(radio)"],"name":"Antenna (radio)","categories":["All articles needing additional references","All articles to be merged","All articles with unsourced statements","Antennas (radio)","Articles needing additional references from January 2014","Articles to be merged from November 2014","Articles with unsourced statements from June 2011","CS1 Russian-language sources (ru)","Commons category with local link same as on Wikidata","Radio electronics","Wikipedia articles with GND identifiers"],"tag_line":"An antenna (plural antennae or antennas), or aerial, is an electrical device which converts electric power into radio waves, and vice versa."}}
,{"_index":"throwtable","_type":"algorithm","_id":"munching-square","_score":0,"_source":{"description":"The Munching Square is a display hack dating back to the PDP-1 (ca. 1962, reportedly discovered by Jackson Wright), which employs a trivial computation (repeatedly plotting the graph Y = X XOR T for successive values of T) to produce an impressive display of moving and growing squares that devour the screen. The initial value of T is treated as a parameter, which, when well-chosen, can produce amazing effects. Some of these, later (re)discovered on the LISP machine, have been christened munching triangles (try AND for XOR and toggling points instead of plotting them), munching w's, and munching mazes. More generally, suppose a graphics program produces an impressive and ever-changing display of some basic form, foo, on a display terminal, and does it using a relatively simple program; then the program (or the resulting display) is likely to be referred to as munching foos.\n^ \"munching squares\". http://www.catb.org. Retrieved 28 Feb 2013.","alt_names":["Munching_square"],"name":"Munching square","categories":["All stub articles","CS1 errors: external links","History of software","Novelty software","Screensavers","Software stubs","Wikipedia articles incorporating text from the Jargon File"],"tag_line":"The Munching Square is a display hack dating back to the PDP-1 (ca."}}
,{"_index":"throwtable","_type":"algorithm","_id":"multiplicative-order","_score":0,"_source":{"description":"In number theory, given an integer a and a positive integer n with gcd(a,n) = 1, the multiplicative order of a modulo n is the smallest positive integer k with\nak ≡ 1 (mod n).\nIn other words, the multiplicative order of a modulo n is the order of a in the multiplicative group of the units in the ring of the integers modulo n.\nThe order of a modulo n is usually written ordn(a), or On(a).","alt_names":["Multiplicative_order"],"name":"Multiplicative order","categories":["Modular arithmetic"],"tag_line":"In number theory, given an integer a and a positive integer n with gcd(a,n) = 1, the multiplicative order of a modulo n is the smallest positive integer k with\nak ≡ 1 (mod n)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"discordian-calendar","_score":0,"_source":{"description":"The Discordian or Erisian calendar is an alternative calendar used by some adherents of Discordianism. It is specified on page 00034 of the Principia Discordia.\nThe Discordian year 1 YOLD is 1166 BC. (Elsewhere in the Principia Discordia, it is mentioned that the Curse of Greyface occurred in 1166 BC, so this is presumably related to the start-date of the calendar.) As a reference, 2015 AD is 3181 YOLD (Year of Our Lady of Discord). The abbreviation \"YOLD\" is not used in the Principia, though the phrase \"Year of Our Lady of Discord\" is mentioned once.\n^ Malaclypse the Younger, Principia Discordia, Page 00034\n^ Malaclypse the Younger, Principia Discordia, Page 00042\n^ Malaclypse the Younger, Principia Discordia, Page 00053","alt_names":["Discordian_calendar"],"name":"Discordian calendar","categories":["Calendar eras","Discordian holidays","Discordianism","Specific calendars"],"tag_line":"The Discordian or Erisian calendar is an alternative calendar used by some adherents of Discordianism."}}
,{"_index":"throwtable","_type":"algorithm","_id":"heronian-triangle","_score":0,"_source":{"description":"In geometry, a Heronian triangle is a triangle that has side lengths and area that are all integers. Heronian triangles are named after Hero of Alexandria. The term is sometimes applied more widely to triangles whose sides and area are all rational numbers.","alt_names":["Heronian_triangle"],"name":"Heronian triangle","categories":["Arithmetic problems of plane geometry","Articles containing proofs","Discrete geometry","Triangles"],"tag_line":"In geometry, a Heronian triangle is a triangle that has side lengths and area that are all integers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fibonacci","_score":0,"_source":{"description":"Leonardo Bonacci (c. 1170 – c. 1250)—known as Fibonacci (Italian: [fiboˈnattʃi]), and also Leonardo of Pisa, Leonardo Pisano Bigollo, Leonardo Fibonacci—was an Italian mathematician, considered to be \"the most talented Western mathematician of the Middle Ages\".\nFibonacci popularized the Hindu–Arabic numeral system to the Western World primarily through his composition in 1202 of Liber Abaci (Book of Calculation). He also introduced to Europe the sequence of Fibonacci numbers which he used as an example in Liber Abaci.","name":"Fibonacci","categories":["1170 births","1250 deaths","13th-century Italian mathematicians","13th-century Latin writers","Articles with hCards","Fibonacci numbers","Italian Roman Catholics","Medieval European mathematics","Number theorists","People from Pisa","Wikipedia articles with BNF identifiers","Wikipedia articles with GND identifiers","Wikipedia articles with ISNI identifiers","Wikipedia articles with LCCN identifiers","Wikipedia articles with NLA identifiers","Wikipedia articles with SBN identifiers","Wikipedia articles with SELIBR identifiers","Wikipedia articles with ULAN identifiers","Wikipedia articles with VIAF identifiers","Wikipedia pending changes protected pages (level 1)"],"tag_line":"Leonardo Bonacci (c. 1170 – c. 1250)—known as Fibonacci (Italian: [fiboˈnattʃi]), and also Leonardo of Pisa, Leonardo Pisano Bigollo, Leonardo Fibonacci—was an Italian mathematician, considered to be \"the most talented Western mathematician of the Middle Ages\"."}}
,{"_index":"throwtable","_type":"algorithm","_id":"man-or-boy-test","_score":0,"_source":{"description":"The man or boy test was proposed by computer scientist Donald Knuth as a means of evaluating implementations of the ALGOL 60 programming language. The aim of the test was to distinguish compilers that correctly implemented \"recursion and non-local references\" from those that did not.\n\nThere are quite a few ALGOL60 translators in existence which have been designed to handle recursion and non-local references properly, and I thought perhaps a little test-program may be of value. Hence I have written the following simple routine, which may separate the man-compilers from the boy-compilers.","alt_names":["Man_or_boy_test"],"name":"Man or boy test","categories":["Articles with example ALGOL 60 code","Compiler construction","Donald Knuth","Programming language design","Programming language implementation"],"tag_line":"The man or boy test was proposed by computer scientist Donald Knuth as a means of evaluating implementations of the ALGOL 60 programming language."}}
,{"_index":"throwtable","_type":"algorithm","_id":"k-d-tree","_score":0,"_source":{"description":"In computer science, a k-d tree (short for k-dimensional tree) is a space-partitioning data structure for organizing points in a k-dimensional space. k-d trees are a useful data structure for several applications, such as searches involving a multidimensional search key (e.g. range searches and nearest neighbor searches). k-d trees are a special case of binary space partitioning trees.","alt_names":["Kd-trie"],"name":"K-d tree","categories":["All articles to be expanded","Articles to be expanded from February 2011","Articles to be expanded from November 2008","Computer graphics data structures","Data types","Database index techniques","Geometric data structures","Trees (data structures)"],"tag_line":"In computer science, a k-d tree (short for k-dimensional tree) is a space-partitioning data structure for organizing points in a k-dimensional space."}}
,{"_index":"throwtable","_type":"algorithm","_id":"abstract-type","_score":0,"_source":{"description":"In programming languages, an abstract type is a type in a nominative type system which cannot be instantiated directly. Abstract types are also known as existential types. An abstract type may provide no implementation, or an incomplete implementation. Often, abstract types will have one or more implementations provided separately, for example, in the form of concrete subclasses which can be instantiated. It may include abstract methods or abstract properties that are shared by its subtypes.\nThe object oriented form of abstract types are known as abstract base classes or simply abstract classes. In some languages, abstract types with no implementation are known as protocols, interfaces, signatures, class types. Other names for language features that are (or may be) used to implement abstract types include traits, mixins, flavors, roles, or type classes.\nA type that is not abstract is called a concrete type (or concrete class).","alt_names":["Abstract_type"],"name":"Abstract type","categories":["All articles with unsourced statements","Articles with unsourced statements from September 2014","Type theory"],"tag_line":"In programming languages, an abstract type is a type in a nominative type system which cannot be instantiated directly."}}
,{"_index":"throwtable","_type":"algorithm","_id":"12riven:-the-psi-climinal-of-integral","_score":0,"_source":{"description":"12Riven: The Psi-Climinal of Integral is a visual novel video game developed by KID, CyberFront, and SDR Project, and released on March 13, 2008 for the PlayStation 2 and April 16, 2009 for the PlayStation Portable. There is a regular edition, and a special edition which included the game's original soundtrack. KID are known for producing the Memories Off series, and the Infinity series which includes Never 7: The End of Infinity, Ever 17: The Out of Infinity, and Remember 11: The Age of Infinity. When KID declared bankruptcy in November 2006, production on the game was halted until CyberFront took over and resumed production.\nThe game had two different PC releases on April 4, 2008. It was released as a standalone product and as a part of the Infinity Plus pack (which includes PC versions of Never 7: The End of Infinity, Ever 17: The Out of Infinity, Remember 11: The Age of Infinity and 12Riven). A port for Android and iOS is also planned.\n\n","alt_names":["12Riven"],"name":"12Riven: The Psi-Climinal of Integral","categories":["2008 video games","Android (operating system) games","Articles containing Japanese-language text","Articles with Chinese-language external links","Articles with Japanese-language external links","Bishōjo games","CS1 Japanese-language sources (ja)","Fiction with alternate endings","IOS games","Infinity (series)","Japan-exclusive video games","KID games","PlayStation 2 games","PlayStation Portable games","Romance video games","Visual novels","Windows games"],"tag_line":"12Riven: The Psi-Climinal of Integral is a visual novel video game developed by KID, CyberFront, and SDR Project, and released on March 13, 2008 for the PlayStation 2 and April 16, 2009 for the PlayStation Portable."}}
,{"_index":"throwtable","_type":"algorithm","_id":"linear-regression","_score":0,"_source":{"description":"In statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X. The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. (This term should be distinguished from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.)\nIn linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of y given the value of X is assumed to be an affine function of X; less commonly, the median or some other quantile of the conditional distribution of y given X is expressed as a linear function of X. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of y given X, rather than on the joint probability distribution of y and X, which is the domain of multivariate analysis.\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\nIf the goal is prediction, or forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of y and X values. After developing such a model, if an additional value of X is then given without its accompanying value of y, the fitted model can be used to make a prediction of the value of y.\nGiven a variable y and a number of variables X1, ..., Xp that may be related to y, linear regression analysis can be applied to quantify the strength of the relationship between y and the Xj, to assess which Xj may have no relationship with y at all, and to identify which subsets of the Xj contain redundant information about y.\nLinear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares loss function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely linked, they are not synonymous.","alt_names":["Linear_regression"],"name":"Linear regression","categories":["All articles to be expanded","Articles to be expanded from January 2010","Articles with inconsistent citation formats","Econometrics","Estimation theory","Parametric statistics","Regression analysis","Wikipedia articles needing clarification from March 2012"],"tag_line":"In statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable y and one or more explanatory variables (or independent variables) denoted X."}}
,{"_index":"throwtable","_type":"algorithm","_id":"table-of-keyboard-shortcuts","_score":0,"_source":{"description":"In computing, a keyboard shortcut is a sequence or combination of keystrokes on a computer keyboard which invokes commands in software.\nSome keyboard shortcuts require the user to press a single key or a sequence of keys one after the other. Other keyboard shortcuts require pressing and holding several keys simultaneously (indicated in the tables below by this sign: +). Keyboard shortcuts may depend on the keyboard layout (localization).\n\n","alt_names":["Table_of_keyboard_shortcuts"],"name":"Table of keyboard shortcuts","categories":["CS1 errors: external links","User interface techniques"],"tag_line":"In computing, a keyboard shortcut is a sequence or combination of keystrokes on a computer keyboard which invokes commands in software."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rot13","_score":0,"_source":{"description":"ROT13 (\"rotate by 13 places\", sometimes hyphenated ROT-13) is a simple letter substitution cipher that replaces a letter with the letter 13 letters after it in the alphabet. ROT13 is a special case of the Caesar cipher, developed in ancient Rome.\nBecause there are 26 letters (2×13) in the basic Latin alphabet, ROT13 is its own inverse; that is, to undo ROT13, the same algorithm is applied, so the same action can be used for encoding and decoding. The algorithm provides virtually no cryptographic security, and is often cited as a canonical example of weak encryption.\nROT13 is used in online forums as a means of hiding spoilers, punchlines, puzzle solutions, and offensive materials from the casual glance. ROT13 has been described as the \"Usenet equivalent of a magazine printing the answer to a quiz upside down\". ROT13 has inspired a variety of letter and word games on-line, and is frequently mentioned in newsgroup conversations.","name":"ROT13","categories":["Classical ciphers","Featured articles","Use dmy dates from September 2010"],"tag_line":"ROT13 (\"rotate by 13 places\", sometimes hyphenated ROT-13) is a simple letter substitution cipher that replaces a letter with the letter 13 letters after it in the alphabet."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sorting-algorithm","_score":0,"_source":{"description":"A sorting algorithm is an algorithm that puts elements of a list in a certain order. The most-used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the use of other algorithms (such as search and merge algorithms) which require input data to be in sorted lists; it is also often useful for canonicalizing data and for producing human-readable output. More formally, the output must satisfy two conditions:\nThe output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);\nThe output is a permutation (reordering) of the input.\nFurther, the data is often taken to be in an array, which allows random access, rather than a list, which only allows sequential access, though often algorithms can be applied with suitable modification to either type of data.\nSince the dawn of computing, the sorting problem has attracted a great deal of research, perhaps due to the complexity of solving it efficiently despite its simple, familiar statement. For example, bubble sort was analyzed as early as 1956. A fundamental limit of comparison sorting algorithms is that they require linearithmic time – O(n log n) – in the worst case, though better performance is possible on real-world data (such as almost-sorted data), and algorithms not based on comparisons, such as counting sort, can have better performance. Although many consider sorting a solved problem – asymptotically optimal algorithms have been known since the mid-20th century – useful new algorithms are still being invented, with the now widely used Timsort dating to 2002, and the library sort being first published in 2006.\nSorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, time-space tradeoffs, and upper and lower bounds.\n\n","alt_names":["Strand_sort"],"name":"Sorting algorithm","categories":["All accuracy disputes","All articles lacking in-text citations","All articles with specifically marked weasel-worded phrases","Articles lacking in-text citations from September 2009","Articles with disputed statements from November 2015","Articles with specifically marked weasel-worded phrases from September 2015","Commons category with local link same as on Wikidata","Data processing","Sorting algorithms"],"tag_line":"A sorting algorithm is an algorithm that puts elements of a list in a certain order."}}
,{"_index":"throwtable","_type":"algorithm","_id":"row-echelon-form","_score":0,"_source":{"description":"In linear algebra, a matrix is in echelon form if it has the shape resulting of a Gaussian elimination. Row echelon form means that Gaussian elimination has operated on the rows and column echelon form means that Gaussian elimination has operated on the columns. In other words, a matrix is in column echelon form if its transpose is in row echelon form. Therefore only row echelon forms are considered in the remainder of this article. The similar properties of column echelon form are easily deduced by transposing all the matrices.\nSpecifically, a matrix is in row echelon form if\nall nonzero rows (rows with at least one nonzero element) are above any rows of all zeroes (all zero rows, if any, belong at the bottom of the matrix), and\nthe leading coefficient (the first nonzero number from the left, also called the pivot) of a nonzero row is always strictly to the right of the leading coefficient of the row above it (some texts add the condition that the leading coefficient must be 1).\nThese two conditions imply that all entries in a column below a leading coefficient are zeros. \nThis is an example of a 3×5 matrix in row echelon form:\n\nMany properties of matrices may be easily deduced from their row echelon form, such that the rank and the kernel.\n\n","alt_names":["Row_echelon_form"],"name":"Row echelon form","categories":["All articles needing additional references","All articles to be merged","Articles needing additional references from July 2010","Articles to be merged from March 2013","Numerical linear algebra"],"tag_line":"In linear algebra, a matrix is in echelon form if it has the shape resulting of a Gaussian elimination."}}
,{"_index":"throwtable","_type":"algorithm","_id":"three-valued-logic","_score":0,"_source":{"description":"In logic, a three-valued logic (also trinary logic, trivalent, ternary, or trilean, sometimes abbreviated 3VL) is any of several many-valued logic systems in which there are three truth values indicating true, false and some indeterminate third value. This is contrasted with the more commonly known bivalent logics (such as classical sentential or Boolean logic) which provide only for true and false. Conceptual form and basic ideas were initially created by Jan Łukasiewicz and C. I. Lewis. These were then re-formulated by Grigore Moisil in an axiomatic algebraic form, and also extended to n-valued logics in 1945.","alt_names":["Ternary_logic"],"name":"Three-valued logic","categories":["All articles needing additional references","All articles to be expanded","All articles with empty sections","All articles with failed verification","All articles with unsourced statements","Articles needing additional references from January 2011","Articles to be expanded from August 2014","Articles with empty sections from August 2014","Articles with failed verification from October 2012","Articles with unsourced statements from February 2007","Articles with unsourced statements from September 2013","Many-valued logic","Ternary computers"],"tag_line":"In logic, a three-valued logic (also trinary logic, trivalent, ternary, or trilean, sometimes abbreviated 3VL) is any of several many-valued logic systems in which there are three truth values indicating true, false and some indeterminate third value."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cryptographically-secure-pseudorandom-number-generator","_score":0,"_source":{"description":"A cryptographically secure pseudo-random number generator (CSPRNG) or cryptographic pseudo-random number generator (CPRNG) is a pseudo-random number generator (PRNG) with properties that make it suitable for use in cryptography.\nMany aspects of cryptography require random numbers, for example:\nkey generation\nnonces\none-time pads\nsalts in certain signature schemes, including ECDSA, RSASSA-PSS\nThe \"quality\" of the randomness required for these applications varies. For example creating a nonce in some protocols needs only uniqueness. On the other hand, generation of a master key requires a higher quality, such as more entropy. And in the case of one-time pads, the information-theoretic guarantee of perfect secrecy only holds if the key material comes from a true random source with high entropy.\nIdeally, the generation of random numbers in CSPRNGs uses entropy obtained from a high-quality source, which might be a hardware random number generator or perhaps unpredictable system processes — though unexpected correlations have been found in several such ostensibly independent processes. From an information-theoretic point of view, the amount of randomness, the entropy that can be generated, is equal to the entropy provided by the system. But sometimes, in practical situations, more random numbers are needed than there is entropy available. Also the processes to extract randomness from a running system are slow in actual practice. In such instances, a CSPRNG can sometimes be used. A CSPRNG can \"stretch\" the available entropy over more bits.","alt_names":["Cryptographically_secure_pseudorandom_number_generator"],"name":"Cryptographically secure pseudorandom number generator","categories":["All Wikipedia articles needing clarification","All articles with unsourced statements","Articles with unsourced statements from January 2012","Cryptographic algorithms","Cryptographic primitives","Cryptographically secure pseudorandom number generators","Wikipedia articles needing clarification from January 2015"],"tag_line":"A cryptographically secure pseudo-random number generator (CSPRNG) or cryptographic pseudo-random number generator (CPRNG) is a pseudo-random number generator (PRNG) with properties that make it suitable for use in cryptography."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fixed-point-combinator","_score":0,"_source":{"description":"In computer science, a fixed-point combinator (or fixpoint combinator) is a higher-order function y that satisfies the equation,\n\nIt is so named because, by setting , it represents a solution to the fixed point equation,\n\nA fixed point of a function f is a value that doesn't change under the application of the function f. Consider the function . The values 0 and 1 are fixed points of this function, because  and . This function has no other fixed points.\nA fixed point combinator need not exist for all functions. Also if f is a function of more than 1 parameter, the fixed point of the function need not be a total function.\nFunctions that satisfy the equation for y expand as,\n\nA particular implementation of y is Curry's paradoxical combinator Y, represented in lambda calculus by,\n\nThis combinator may be used in implementing Curry's paradox. The heart of Curry's paradox is that untyped lambda calculus is unsound as a deductive system, and the Y combinator demonstrates that by allowing an anonymous expression to represent zero, or even many values. This is inconsistent in mathematical logic.\nApplied to a function with one variable the Y combinator usually does not terminate. More interesting results are obtained by applying the Y combinator to functions of two or more variables. The second variable may be used as a counter, or index. The resulting function behaves like a while or a for loop in an imperative language.\nUsed in this way the Y combinator implements simple recursion. In the lambda calculus it is not possible to refer to the definition of a function in a function body. Recursion may only be achieved by passing in a function as a parameter. The Y combinator demonstrates this style of programming.\n\n","alt_names":["Fixed_point_combinator"],"name":"Fixed-point combinator","categories":["Combinatory logic","Fixed points (mathematics)","Lambda calculus","Mathematics of computing","Recursion"],"tag_line":"In computer science, a fixed-point combinator (or fixpoint combinator) is a higher-order function y that satisfies the equation,\n\nIt is so named because, by setting , it represents a solution to the fixed point equation,\n\nA fixed point of a function f is a value that doesn't change under the application of the function f. Consider the function ."}}
,{"_index":"throwtable","_type":"algorithm","_id":"magic-square","_score":0,"_source":{"description":"In recreational mathematics, a magic square is an arrangement of distinct numbers (i.e. each number is used once), usually integers, in a square grid, where the numbers in each row, and in each column, and the numbers in the main and secondary diagonals, all add up to the same number. A magic square has the same number of rows as it has columns, and in conventional math notation, \"n\" stands for the number of rows (and columns) it has. Thus, a magic square always contains n2 numbers, and its size (the number of rows [and columns] it has) is described as being \"of order n\". A magic square that contains the integers from 1 to n2 is called a normal magic square. (The term \"magic square\" is also sometimes used to refer to any of various types of word squares.)\nNormal magic squares of all sizes except 2 × 2 (that is, where n = 2) can be constructed. The 1 × 1 magic square, with only one cell containing the number 1, is trivial. The smallest (and unique up to rotation and reflection) nontrivial case, 3 × 3, is shown below.\n\nAny magic square can be rotated and reflected to produce 8 trivially distinct squares. In magic square theory all of these are generally deemed equivalent and the eight such squares are said to comprise a single equivalence class.\nThe constant that is the sum of every row, column and diagonal is called the magic constant or magic sum, M. Every normal magic square has a constant dependent on n, calculated by the formula M = [n(n2 + 1)] / 2. For normal magic squares of order n = 3, 4, 5, 6, 7, and 8, the magic constants are, respectively: 15, 34, 65, 111, 175, and 260 (sequence A006003 in the OEIS).\nMagic squares have a long history, dating back to 650 BC in China. At various times they have acquired magical or mythical significance, and have appeared as symbols in works of art. In modern times they have been generalized a number of ways, including using extra or different constraints, multiplying instead of adding cells, using alternate shapes or more than two dimensions, and replacing numbers with shapes and addition with geometric operations.","alt_names":["Magic_square"],"name":"Magic square","categories":["Articles containing Arabic-language text","Articles containing Chinese-language text","Articles with DMOZ links","CS1 Latin-language sources (la)","Chinese mathematical discoveries","Magic squares","Magic symbols","Matrices","Unsolved problems in mathematics","Wikipedia articles with GND identifiers"],"tag_line":"In recreational mathematics, a magic square is an arrangement of distinct numbers (i.e."}}
,{"_index":"throwtable","_type":"algorithm","_id":"i-before-e-except-after-c","_score":0,"_source":{"description":"\"I before E, except after C\" is a mnemonic rule of thumb for English spelling. If one is unsure whether a word is spelled with the sequence ei or ie, the rhyme suggests that the correct order is ie unless the preceding letter is c, in which case it is ei. For example:\nie in believe, fierce, collie, die, friend\nei after c in deceive, ceiling, receipt, ceilidh\nThe rule is very well known; Edward Carney calls it \"this supreme, and for many people solitary, spelling rule\". However, the short form quoted above has many common exceptions; for example:\nie after c: species, science, sufficient\nei not preceded by c: seize, weird, vein, their, foreign, feisty, heist\nMany more exceptions are listed below.\nThe proportion of exceptions can be reduced by restricting application of the rule based on the sound represented by the spelling. Two common restrictions are:\nexcluding cases where the spelling represents the \"long a\" sound (the lexical sets of FACE /eɪ/ and perhaps SQUARE /ɛər/). This is commonly expressed by continuing the rhyme \"or when sounding like A, as in neighbor or weigh\"\nincluding only cases where the spelling represents the \"long e\" sound (the lexical sets of FLEECE /iː/ and perhaps NEAR /ɪər/ and happY /i/).\nSome authorities deprecate the rule as having too many exceptions to be worth learning.","alt_names":["I_before_E_except_after_C"],"name":"I before E except after C","categories":["Dynamic lists","English spelling","Mnemonics","Rules of thumb"],"tag_line":"\"I before E, except after C\" is a mnemonic rule of thumb for English spelling."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bernoulli-number","_score":0,"_source":{"description":"In mathematics, the Bernoulli numbers Bn are a sequence of rational numbers with deep connections to number theory. The values of the first few Bernoulli numbers are\nB0 = 1, B1 = ±1⁄2, B2 = 1⁄6, B3 = 0, B4 = −1⁄30, B5 = 0, B6 = 1⁄42, B7 = 0, B8 = −1⁄30.\nIf the convention B1 = −1⁄2 is used, this sequence is also known as the first Bernoulli numbers ( A027641 /  A027642 in OEIS); with the convention B1 = +1⁄2 is known as the second Bernoulli numbers ( A164555 /  A027642). Except for this one difference, the first and second Bernoulli numbers agree. Since Bn = 0 for all odd n > 1, and many formulas only involve even-index Bernoulli numbers, some authors write Bn instead of B2n.\nThe Bernoulli numbers appear in the Taylor series expansions of the tangent and hyperbolic tangent functions, in formulas for the sum of powers of the first positive integers, in the Euler–Maclaurin formula, and in expressions for certain values of the Riemann zeta function.\nThe Bernoulli numbers were discovered around the same time by the Swiss mathematician Jakob Bernoulli, after whom they are named, and independently by Japanese mathematician Seki Kōwa. Seki's discovery was posthumously published in 1712 in his work Katsuyo Sampo; Bernoulli's, also posthumously, in his Ars Conjectandi of 1713. Ada Lovelace's note G on the analytical engine from 1842 describes an algorithm for generating Bernoulli numbers with Babbage's machine. As a result, the Bernoulli numbers have the distinction of being the subject of one of the first computer programs.\n\n","alt_names":["Bernoulli_number"],"name":"Bernoulli number","categories":["CS1 errors: dates","Integer sequences","Number theory","Topology"],"tag_line":"In mathematics, the Bernoulli numbers Bn are a sequence of rational numbers with deep connections to number theory."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fractran","_score":0,"_source":{"description":"FRACTRAN is a Turing-complete esoteric programming language invented by the mathematician John Conway. A FRACTRAN program is an ordered list of positive fractions together with an initial positive integer input n. The program is run by updating the integer n as follows:\nfor the first fraction f in the list for which nf is an integer, replace n by nf\nrepeat this rule until no fraction in the list produces an integer when multiplied by n, then halt.\nIn The Book of Numbers, John Conway and Richard Guy gave a formula for primes in FRACTRAN:\n\nStarting with n=2, this FRACTRAN program generates the following sequence of integers:\n2, 15, 825, 725, 1925, 2275, 425, 390, 330, 290, 770, ... (sequence A007542 in OEIS)\nAfter 2, this sequence contains the following powers of 2:\n (sequence A034785 in OEIS)\nwhich are the prime powers of 2.","name":"FRACTRAN","categories":["Esoteric programming languages","Models of computation","Recreational mathematics"],"tag_line":"FRACTRAN is a Turing-complete esoteric programming language invented by the mathematician John Conway."}}
,{"_index":"throwtable","_type":"algorithm","_id":"go-fish","_score":0,"_source":{"description":"Go Fish or Fish is a card game usually played by two to five players, although it can be played with up to 10 players.","alt_names":["Go_Fish"],"name":"Go Fish","categories":["All articles with dead external links","All articles with unsourced statements","Articles with dead external links from September 2012","Articles with unsourced statements from October 2015","Matching card games","Year of introduction missing"],"tag_line":"Go Fish or Fish is a card game usually played by two to five players, although it can be played with up to 10 players."}}
,{"_index":"throwtable","_type":"algorithm","_id":"rock-paper-scissors","_score":0,"_source":{"description":"Rock-paper-scissors is a zero sum hand game usually played between two people, in which each player simultaneously forms one of three shapes with an outstretched hand. These shapes are \"rock\" (a simple fist), \"paper\" (a flat hand), and \"scissors\" (a fist with the index and middle fingers together forming a V). The game has only three possible outcomes other than a tie: a player who decides to play rock will beat another player who has chosen scissors (\"rock crushes scissors\") but will lose to one who has played paper (\"paper covers rock\"); a play of paper will lose to a play of scissors (\"scissors cut paper\"). If both players throw the same shape, the game is tied and is usually immediately replayed to break the tie. Other names for the game in the English-speaking world include roshambo and other orderings of the three items, sometimes with \"rock\" being called \"stone\".\nThe game is often used as a choosing method in a way similar to coin flipping, drawing straws, or throwing dice. Unlike truly random selection methods, however, rock-paper-scissors can be played with a degree of skill by recognizing and exploiting non-random behavior in opponents.","name":"Rock-paper-scissors","categories":["All articles that may contain original research","All articles with dead external links","All articles with unsourced statements","Articles containing simplified Chinese-language text","Articles containing traditional Chinese-language text","Articles that may contain original research from January 2014","Articles with dead external links from October 2010","Articles with hAudio microformats","Articles with unsourced statements from December 2015","Articles with unsourced statements from February 2008","Articles with unsourced statements from July 2013","Articles with unsourced statements from June 2013","CS1 Japanese-language sources (ja)","Children's games","Game theory","Games of chance","Hand games","Interlanguage link template link number","Japanese games","Pages containing links to subscription-only content","Paper","Pictograms","Random selection","Rock-paper-scissors","Spoken articles","Wikipedia articles needing clarification from July 2013"],"tag_line":"Rock-paper-scissors is a zero sum hand game usually played between two people, in which each player simultaneously forms one of three shapes with an outstretched hand."}}
,{"_index":"throwtable","_type":"algorithm","_id":"variadic-function","_score":0,"_source":{"description":"In computer programming, a variadic function is a function of indefinite arity, i.e., one which accepts a variable number of arguments. Support for variadic functions differs widely among programming languages.\nThere are many mathematical and logical operations that come across naturally as variadic functions. For instance, the summing of numbers or the concatenation of strings or other sequences are operations that can logically apply to any number of operands.\nAnother operation that has been implemented as a variadic function in many languages is output formatting. The C function printf and the Common Lisp function format are two such examples. Both take one argument that specifies the formatting of the output, and any number of arguments that provide the values to be formatted.\nVariadic functions can expose type-safety problems in some languages. For instance, C's printf, if used incautiously, can give rise to a class of security holes known as format string attacks. The attack is possible because the language support for variadic functions is not type-safe: it permits the function to attempt to pop more arguments off the stack than were placed there, corrupting the stack and leading to unexpected behavior. As a consequence of this, the CERT Coordination Center considers variadic functions in C to be a high-severity security risk.\nVariadic functionality can be considered complementary to the apply function, which takes a function and a list/sequence/array as arguments, and calls the function (once) with the arguments supplied in that list, thus passing a variable number of arguments to the function.","alt_names":["Variadic_function"],"name":"Variadic function","categories":["Subroutines"],"tag_line":"In computer programming, a variadic function is a function of indefinite arity, i.e., one which accepts a variable number of arguments."}}
,{"_index":"throwtable","_type":"algorithm","_id":"/dev/random","_score":0,"_source":{"description":"In Unix-like operating systems, /dev/random is a special file that serves as a blocking pseudorandom number generator. It allows access to environmental noise collected from device drivers and other sources. Not all operating systems implement the same semantics for /dev/random.","name":"/dev/random","categories":["All articles with specifically marked weasel-worded phrases","All articles with unsourced statements","Articles with specifically marked weasel-worded phrases from October 2015","Articles with unsourced statements from August 2014","Device file","Random number generation","Unix file system technology"],"tag_line":"In Unix-like operating systems, /dev/random is a special file that serves as a blocking pseudorandom number generator."}}
,{"_index":"throwtable","_type":"algorithm","_id":"polynomial-long-division","_score":0,"_source":{"description":"In algebra, polynomial long division is an algorithm for dividing a polynomial by another polynomial of the same or lower degree, a generalised version of the familiar arithmetic technique called long division. It can be done easily by hand, because it separates an otherwise complex division problem into smaller ones. Sometimes using a shorthand version called synthetic division is faster, with less writing and fewer calculations.\nPolynomial long division is an algorithm that implements the Euclidean division of polynomials, which starting from two polynomials A (the dividend) and B (the divisor) produces, if B is not zero, a quotient Q and a remainder R such that\nA = BQ + R,\nand either R = 0 or the degree of R is lower than the degree of B. These conditions define uniquely Q and R, which means that Q and R do not depend on the method used to compute them.","alt_names":["Polynomial_long_division"],"name":"Polynomial long division","categories":["Computer algebra","Division (mathematics)","Polynomials"],"tag_line":"In algebra, polynomial long division is an algorithm for dividing a polynomial by another polynomial of the same or lower degree, a generalised version of the familiar arithmetic technique called long division."}}
,{"_index":"throwtable","_type":"algorithm","_id":"happy-number","_score":0,"_source":{"description":"A happy number is a number defined by the following process: Starting with any positive integer, replace the number by the sum of the squares of its digits, and repeat the process until the number either equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1. Those numbers for which this process ends in 1 are happy numbers, while those that do not end in 1 are unhappy numbers (or sad numbers).","alt_names":["Happy_number"],"name":"Happy number","categories":["All articles containing potentially dated statements","All articles with failed verification","All articles with unsourced statements","Articles containing potentially dated statements from 2010","Articles with failed verification from December 2014","Articles with unsourced statements from June 2013","Base-dependent integer sequences","Recreational mathematics","Use dmy dates from June 2011"],"tag_line":"A happy number is a number defined by the following process: Starting with any positive integer, replace the number by the sum of the squares of its digits, and repeat the process until the number either equals 1 (where it will stay), or it loops endlessly in a cycle which does not include 1."}}
,{"_index":"throwtable","_type":"algorithm","_id":"arithmetic–geometric-mean","_score":0,"_source":{"description":"In mathematics, the arithmetic–geometric mean (AGM) of two positive real numbers x and y is defined as follows:\nFirst compute the arithmetic mean of x and y and call it a1. Next compute the geometric mean of x and y and call it g1; this is the square root of the product xy:\n\nThen iterate this operation with a1 taking the place of x and g1 taking the place of y. In this way, two sequences (an) and (gn) are defined:\n\nThese two sequences converge to the same number, which is the arithmetic–geometric mean of x and y; it is denoted by M(x, y), or sometimes by agm(x, y).\nThis can be used for algorithmic purposes as in the AGM method.\n\n","alt_names":["Arithmetic-geometric_mean"],"name":"Arithmetic–geometric mean","categories":["All articles lacking in-text citations","All articles to be merged","Articles containing proofs","Articles lacking in-text citations from October 2008","Articles to be merged from September 2012","Elliptic functions","Means","Special functions"],"tag_line":"In mathematics, the arithmetic–geometric mean (AGM) of two positive real numbers x and y is defined as follows:\nFirst compute the arithmetic mean of x and y and call it a1."}}
,{"_index":"throwtable","_type":"algorithm","_id":"regular-number","_score":0,"_source":{"description":"Regular numbers are numbers that evenly divide powers of 60 (or, equivalently powers of 30). As an example, 602 = 3600 = 48 × 75, so both 48 and 75 are divisors of a power of 60. Thus, they are regular numbers. Equivalently, they are the numbers whose only prime divisors are 2, 3, and 5.\nThe numbers that evenly divide the powers of 60 arise in several areas of mathematics and its applications, and have different names coming from these different areas of study.\nIn number theory, these numbers are called 5-smooth, because they can be characterized as having only 2, 3, or 5 as prime factors. This is a specific case of the more general k-smooth numbers, i.e., a set of numbers that have no prime factor greater than k.\nIn the study of Babylonian mathematics, the divisors of powers of 60 are called regular numbers or regular sexagesimal numbers, and are of great importance due to the sexagesimal number system used by the Babylonians.\nIn music theory, regular numbers occur in the ratios of tones in five-limit just intonation.\nIn computer science, regular numbers are often called Hamming numbers, after Richard Hamming, who proposed the problem of finding computer algorithms for generating these numbers in order.\n\n^ Inspired by similar diagrams by Erkki Kurenniemi in \"Chords, scales, and divisor lattices\".","alt_names":["Hamming_numbers"],"name":"Regular number","categories":["Babylonian mathematics","Functional programming","Integer sequences","Mathematics of music"],"tag_line":"Regular numbers are numbers that evenly divide powers of 60 (or, equivalently powers of 30)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"concurrent-computing","_score":0,"_source":{"description":"Concurrent computing is a form of computing in which several computations are executing during overlapping time periods—concurrently—instead of sequentially (one completing before the next starts). This is a property of a system—this may be an individual program, a computer, or a network—and there is a separate execution point or \"thread of control\" for each computation (\"process\"). A concurrent system is one where a computation can make progress without waiting for all other computations to complete—where more than one computation can make progress at \"the same time\".\nAs a programming paradigm, concurrent computing is a form of modular programming, namely factoring an overall computation into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.","alt_names":["Concurrent_computing"],"name":"Concurrent computing","categories":["All articles needing additional references","All articles to be expanded","All articles with unsourced statements","Articles needing additional references from December 2006","Articles needing additional references from February 2014","Articles to be expanded from February 2014","Articles with unsourced statements from August 2010","Articles with unsourced statements from May 2013","Concurrent computing","Operating system technology","Pages using citations with accessdate and no URL","Pages using web citations with no URL"],"tag_line":"Concurrent computing is a form of computing in which several computations are executing during overlapping time periods—concurrently—instead of sequentially (one completing before the next starts)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"17th-national-congress-of-the-communist-party-of-china","_score":0,"_source":{"description":"The 17th National Congress of the Communist Party of China was held in Beijing, China, at the Great Hall of the People from 15 to 21 October 2007. The Congress marked significant shift in the political direction of the country as CPC General Secretary Hu Jintao solidified his position of leadership. Hu's signature policy doctrine, the Scientific Development Concept, which aimed to create a \"Socialist Harmonious Society\" through egalitarian wealth distribution and concern for the country's less well-off, was enshrined into the Party Constitution.\nThe Congress also set up the political scene for a smooth transition to the fifth generation of party leadership, introducing rising political stars Xi Jinping and Li Keqiang to the Politburo Standing Committee (PSC), the country's de facto top decision-making body. Vice-President Zeng Qinghong, an important ally of former General secretary Jiang Zemin, retired from the PSC. Party anti-graft chief Wu Guanzheng, and Legal and Political Commission chief Luo Gan also retired due to age, replaced by He Guoqiang and Zhou Yongkang in their respective posts.","alt_names":["17th_National_Congress_of_the_Communist_Party_of_China"],"name":"17th National Congress of the Communist Party of China","categories":["2007 in China","2007 in politics","All articles with dead external links","All articles with unsourced statements","Articles containing Chinese-language text","Articles containing simplified Chinese-language text","Articles containing traditional Chinese-language text","Articles with dead external links from October 2012","Articles with unsourced statements from December 2011","Articles with unsourced statements from September 2007","National Congress of the Communist Party of China","Use dmy dates from June 2013"],"tag_line":"The 17th National Congress of the Communist Party of China was held in Beijing, China, at the Great Hall of the People from 15 to 21 October 2007."}}
,{"_index":"throwtable","_type":"algorithm","_id":"boolean-algebra","_score":0,"_source":{"description":"In mathematics and mathematical logic, Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0 respectively. Instead of elementary algebra where the values of the variables are numbers, and the main operations are addition and multiplication, the main operations of Boolean algebra are the conjunction and, denoted ∧, the disjunction or, denoted ∨, and the negation not, denoted ¬. It is thus a formalism for describing logical relations in the same way that ordinary algebra describes numeric relations.\nBoolean algebra was introduced by George Boole in his first book The Mathematical Analysis of Logic (1847), and set forth more fully in his An Investigation of the Laws of Thought (1854). According to Huntington, the term \"Boolean algebra\" was first suggested by Sheffer in 1913.\nBoolean algebra has been fundamental in the development of digital electronics, and is provided for in all modern programming languages. It is also used in set theory and statistics.","alt_names":["Boolean_algebra"],"name":"Boolean algebra","categories":["Algebraic logic","Articles with example code","Boolean algebra","Wikipedia articles with GND identifiers"],"tag_line":"In mathematics and mathematical logic, Boolean algebra is the branch of algebra in which the values of the variables are the truth values true and false, usually denoted 1 and 0 respectively."}}
,{"_index":"throwtable","_type":"algorithm","_id":"binary-search-algorithm","_score":0,"_source":{"description":"In computer science, a binary search or half-interval search algorithm finds the position of a target value within a sorted array. The binary search algorithm can be classified as a dichotomic divide-and-conquer search algorithm and executes in logarithmic time.","alt_names":["Binary_search"],"name":"Binary search algorithm","categories":["All articles needing cleanup","All articles with unsourced statements","Articles needing cleanup from April 2011","Articles with unsourced statements from August 2009","Articles with unsourced statements from October 2011","Cleanup tagged articles without a reason field from April 2011","Search algorithms","Wikipedia articles needing clarification from January 2015","Wikipedia pages needing cleanup from April 2011"],"tag_line":"In computer science, a binary search or half-interval search algorithm finds the position of a target value within a sorted array."}}
,{"_index":"throwtable","_type":"algorithm","_id":"finite-difference","_score":0,"_source":{"description":"A finite difference is a mathematical expression of the form f(x + b) − f(x + a). If a finite difference is divided by b − a, one gets a difference quotient. The approximation of derivatives by finite differences plays a central role in finite difference methods for the numerical solution of differential equations, especially boundary value problems.\nCertain recurrence relations can be written as difference equations by replacing iteration notation with finite differences.\nToday, the term \"finite difference\" is often taken as synonymous with finite difference approximations of derivatives, especially in the context of numerical methods. Finite difference approximations are finite difference quotients in the terminology employed above.\nFinite differences have also been the topic of study as abstract self-standing mathematical objects, e.g. in works by George Boole (1860), L. M. Milne-Thomson (1933), and Károly Jordan (1939), tracing its origins back to Isaac Newton. In this viewpoint, the formal calculus of finite differences is an alternative to the calculus of infinitesimals.","alt_names":["Forward_difference"],"name":"Finite difference","categories":["Articles with inconsistent citation formats","Factorial and binomial topics","Finite differences","Linear operators in calculus","Mathematical analysis","Numerical differential equations","Use mdy dates from September 2011"],"tag_line":"A finite difference is a mathematical expression of the form f(x + b) − f(x + a)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"\"hello,-world!\"-program","_score":0,"_source":{"description":"A \"Hello, World!\" program is a computer program that outputs \"Hello, World!\" on a display device. Being a very simple program in most programming languages, it is often used to illustrate to beginning programmers the basic syntax for constructing a working program. It is also used to verify that a language or system is operating correctly.","alt_names":["Hello_world_program"],"name":"\"Hello, World!\" program","categories":["All articles lacking reliable references","All articles needing additional references","All articles that may contain original research","All articles with unsourced statements","Articles lacking reliable references from February 2015","Articles lacking reliable references from March 2015","Articles needing additional references from April 2015","Articles that may contain original research from March 2015","Articles with example code","Articles with specifically marked weasel-worded phrases from March 2015","Articles with unsourced statements from March 2015","Articles with unsourced statements from November 2015","Commons category with local link same as on Wikidata","Computer programming","Test items"],"tag_line":"A \"Hello, World!\""}}
,{"_index":"throwtable","_type":"algorithm","_id":"bit","_score":0,"_source":{"description":"A bit is the basic unit of information in computing and digital communications. A bit can have only one of two values, and may therefore be physically implemented with a two-state device. These values are most commonly represented as either a 0or1. The term bit is a portmanteau of binary digit.\nThe two values can also be interpreted as logical values (true/false, yes/no), algebraic signs (+/−), activation states (on/off), or any other two-valued attribute. The correspondence between these values and the physical states of the underlying storage or device is a matter of convention, and different assignments may be used even within the same device or program. The length of a binary number may be referred to as its bit-length.\nIn information theory, one bit is typically defined as the uncertainty of a binary random variable that is 0 or 1 with equal probability, or the information that is gained when the value of such a variable becomes known.\nIn quantum computing, a quantum bit or qubit is a quantum system that can exist in superposition of two classical (i.e., non-quantum) bit values.\nThe symbol for bit, as a unit of information, is either simply bit (recommended by the ISO/IEC standard 80000-13 (2008)) or lowercase b (recommended by the IEEE 1541 Standard (2002)). A group of eight bits is commonly called one byte, but historically the size of the byte is not strictly defined.","name":"Bit","categories":["All articles needing additional references","Articles needing additional references from August 2010","Binary arithmetic","Data types","Primitive types","Units of information","Use dmy dates from July 2012"],"tag_line":"A bit is the basic unit of information in computing and digital communications."}}
,{"_index":"throwtable","_type":"algorithm","_id":"amphotericin-b","_score":0,"_source":{"description":"Amphotericin B is an antifungal drug often used intravenously for serious systemic fungal infections and is the only effective treatment for some fungal infections.\nCommon side effects include a reaction of fever, shaking chills, headaches and low blood pressure soon after it is infused, as well as kidney and electrolyte problems. Allergic symptoms including anaphylaxis may occur.\nIt was originally extracted from Streptomyces nodosus, a filamentous bacterium, in 1955, at the Squibb Institute for Medical Research. Its name originates from the chemical's amphoteric properties. It is on the World Health Organization's List of Essential Medicines, the most important medications needed in a basic health system.\nIt is of the polyene class. Currently, the drug is available in many forms. Either \"conventionally\" complexed with sodium deoxycholate (ABD), as a cholesteryl sulfate complex (ABCD), as a lipid complex (ABLC), and as a liposomal formulation (LAMB). The latter formulations have been developed to improve tolerability and decrease toxicity, but may show considerably different pharmacokinetic characteristics compared to conventional amphotericin B.\n^ a b c \"Amphotericin B\". The American Society of Health-System Pharmacists. Retrieved Jan 1, 2015. \n^ \"WHO Model List of EssentialMedicines\" (PDF). World Health Organization. October 2013. Retrieved 22 April 2014. \n^","alt_names":["AMB"],"name":"Amphotericin B","categories":["Antifungals","Antifungals for dermatologic use","Antiprotozoal agents","Articles without UNII source","CS1 errors: dates","Chemical articles having calculated molecular weight overwritten","Drugboxes which contain changes to watched fields","Gilead Sciences","Pages using citations with accessdate and no URL","Polyketide antibiotics","Wikipedia articles needing page number citations from October 2012","World Health Organization essential medicines"],"tag_line":"Amphotericin B is an antifungal drug often used intravenously for serious systemic fungal infections and is the only effective treatment for some fungal infections."}}
,{"_index":"throwtable","_type":"algorithm","_id":"formal-power-series","_score":0,"_source":{"description":"In mathematics, a formal power series is a generalization of a polynomial, where the number of terms is allowed to be infinite; this implies giving up the possibility of replacing the variable in the polynomial with an arbitrary number. Thus a formal power series differs from a polynomial in that it may have infinitely many terms, and differs from a power series, whose variables can take on numerical values. One way to view a formal power series is as an infinite ordered sequence of numbers. In this case, the powers of the variable are used only to indicate the order of the coefficients. The coefficient of  is just the fifth term in the series, while the coefficient of  is the zeroth term. In combinatorics, formal power series provide representations of numerical sequences and of multisets, and for instance allow concise expressions for recursively defined sequences regardless of whether the recursion can be explicitly solved; this is known as the method of generating functions. More generally, formal power series can include series with any finite number of variables, and with coefficients in an arbitrary ring.","alt_names":["Formal_power_series"],"name":"Formal power series","categories":["Abstract algebra","All articles needing additional references","All articles to be expanded","Articles needing additional references from December 2009","Articles to be expanded from August 2014","Enumerative combinatorics","Mathematical series","Ring theory"],"tag_line":"In mathematics, a formal power series is a generalization of a polynomial, where the number of terms is allowed to be infinite; this implies giving up the possibility of replacing the variable in the polynomial with an arbitrary number."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mutual-recursion","_score":0,"_source":{"description":"In mathematics and computer science, mutual recursion is a form of recursion where two mathematical or computational objects, such as functions or data types, are defined in terms of each other. Mutual recursion is very common in functional programming and in some problem domains, such as recursive descent parsers, where the data types are naturally mutually recursive, but is uncommon in other domains.","alt_names":["Mutual_recursion"],"name":"Mutual recursion","categories":["Recursion","Theory of computation"],"tag_line":"In mathematics and computer science, mutual recursion is a form of recursion where two mathematical or computational objects, such as functions or data types, are defined in terms of each other."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pig-(dice-game)","_score":0,"_source":{"description":"Pig is a simple dice game first described in print by John Scarne in 1945. As with many games of folk origin, Pig is played with many rule variations. Commercial variants of Pig include Pass the Pigs, Pig Dice, and Skunk. Pig is commonly used by mathematics teachers to teach probability concepts.\nPig is one of a family of dice games described by Reiner Knizia as \"jeopardy dice games\". For jeopardy dice games, the dominant type of decision is whether or not to jeopardize previous gains by rolling for potential greater gains. Most jeopardy dice games can be further subdivided into two categories: jeopardy race games and jeopardy approach games. In jeopardy race games, the object is to be the first to meet or exceed a goal score (e.g. Pig, Pass the Pigs, Cosmic Wimpout, Can't Stop). In jeopardy approach games, the object is to most closely approach a goal score without exceeding it.","alt_names":["Pig_(dice)"],"name":"Pig (dice game)","categories":["Dice games"],"tag_line":"Pig is a simple dice game first described in print by John Scarne in 1945."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sha-2","_score":0,"_source":{"description":"SHA-2 (Secure Hash Algorithm 2) is a set of cryptographic hash functions designed by the NSA. SHA stands for Secure Hash Algorithm. Cryptographic hash functions are mathematical operations run on digital data; by comparing the computed \"hash\" (the output from execution of the algorithm) to a known and expected hash value, a person can determine the data's integrity. For example, computing the hash of a downloaded file and comparing the result to a previously published hash result can show whether the download has been modified or tampered with. A key aspect of cryptographic hash functions is their collision resistance: nobody should be able to find two different input values that result in the same hash output.\nSHA-2 includes significant changes from its predecessor, SHA-1. The SHA-2 family consists of six hash functions with digests (hash values) that are 224, 256, 384 or 512 bits: SHA-224, SHA-256, SHA-384, SHA-512, SHA-512/224, SHA-512/256.\nSHA-256 and SHA-512 are novel hash functions computed with 32-bit and 64-bit words, respectively. They use different shift amounts and additive constants, but their structures are otherwise virtually identical, differing only in the number of rounds. SHA-224 and SHA-384 are simply truncated versions of the first two, computed with different initial values. SHA-512/224 and SHA-512/256 are also truncated versions of SHA-512, but the initial values are generated using the method described in FIPS PUB 180-4. SHA-2 was published in 2001 by the NIST as a U.S. federal standard (FIPS). The SHA-2 family of algorithms are patented in US 6829355 . The United States has released the patent under a royalty-free license.\nIn 2005, an algorithm emerged for finding SHA-1 collisions in about 2000-times fewer steps than was previously thought possible. Although (as of 2015) no example of a SHA-1 collision has been published yet, the security margin left by SHA-1 is weaker than intended, and its use is therefore no longer recommended for applications that depend on collision resistance, such as digital signatures. Although SHA-2 bears some similarity to the SHA-1 algorithm, these attacks have not been successfully extended to SHA-2.\nCurrently, the best public attacks break preimage resistance for 52 rounds of SHA-256 or 57 rounds of SHA-512, and collision resistance for 46 rounds of SHA-256, as shown in the Cryptanalysis and validation section below.","alt_names":["SHA-256"],"name":"SHA-2","categories":["All articles containing potentially dated statements","All articles with dead external links","All articles with unsourced statements","Articles containing potentially dated statements from 2013","Articles with dead external links from November 2012","Articles with example pseudocode","Articles with unsourced statements from December 2015","Checksum algorithms","Cryptographic hash functions","National Security Agency cryptography"],"tag_line":"SHA-2 (Secure Hash Algorithm 2) is a set of cryptographic hash functions designed by the NSA."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pushdown-automaton","_score":0,"_source":{"description":"In computer science, a pushdown automaton (PDA) is a type of automaton that employs a stack.\nPushdown automata are used in theories about what can be computed by machines. They are more capable than finite-state machines but less capable than Turing machines. Deterministic pushdown automata can recognize all deterministic context-free languages while nondeterministic ones can recognize all context-free languages. Mainly the former are used in parser design.\nThe term \"pushdown\" refers to the fact that the stack can be regarded as being \"pushed down\" like a tray dispenser at a cafeteria, since the operations never work on elements other than the top element. A stack automaton, by contrast, does allow access to and operations on deeper elements. Stack automata can recognize a strictly larger set of languages than pushdown automata. A nested stack automaton allows full access, and also allows stacked values to be entire sub-stacks rather than just single finite symbols.\nThe remainder of this article describes the nondeterministic pushdown automaton.","alt_names":["Stack_automaton"],"name":"Pushdown automaton","categories":["Automata (computation)","Models of computation"],"tag_line":"In computer science, a pushdown automaton (PDA) is a type of automaton that employs a stack."}}
,{"_index":"throwtable","_type":"algorithm","_id":".ar","_score":0,"_source":{"description":".ar is the Internet country code top-level domain (ccTLD) for Argentina. It is administered by NIC Argentina. Registering a .AR domain directly is not allowed, only the 8 second-level domains below are open to everyone, although a local presence in Argentina is required.","name":".ar","categories":["All articles needing expert attention","All articles that are too technical","All stub articles","Articles needing expert attention from May 2013","Country code top-level domains","Domain name stubs","Internet in Argentina","Pages with URL errors","Use dmy dates from March 2013","Wikipedia articles that are too technical from May 2013"],"tag_line":".ar is the Internet country code top-level domain (ccTLD) for Argentina."}}
,{"_index":"throwtable","_type":"algorithm","_id":"alshermond-singleton","_score":0,"_source":{"description":"Alshermond Glendale (\"Al\") Singleton (born August 7, 1975) is a former American college and professional football player who was a linebacker for ten seasons in the National Football League during the 1990s and early 2000s. Glendale played college football at Temple University. He was drafted in the fourth round (128 overall) of the 1997 NFL Draft by the Tampa Bay Buccaneers, and played professionally for the Tampa Bay Buccaneers and Dallas Cowboys.","alt_names":["Al_Singleton"],"name":"Alshermond Singleton","categories":["1975 births","American football linebackers","American football outside linebackers","Dallas Cowboys players","Living people","Super Bowl champions","Tampa Bay Buccaneers players","Temple Owls football players"],"tag_line":"Alshermond Glendale (\"Al\") Singleton (born August 7, 1975) is a former American college and professional football player who was a linebacker for ten seasons in the National Football League during the 1990s and early 2000s."}}
,{"_index":"throwtable","_type":"algorithm","_id":"tower-of-hanoi","_score":0,"_source":{"description":"The Tower of Hanoi (also called the Tower of Brahma or Lucas' Tower, and sometimes pluralized) is a mathematical game or puzzle. It consists of three rods, and a number of disks of different sizes which can slide onto any rod. The puzzle starts with the disks in a neat stack in ascending order of size on one rod, the smallest at the top, thus making a conical shape.\nThe objective of the puzzle is to move the entire stack to another rod, obeying the following simple rules:\nOnly one disk can be moved at a time.\nEach move consists of taking the upper disk from one of the stacks and placing it on top of another stack i.e. a disk can only be moved if it is the uppermost disk on a stack.\nNo disk may be placed on top of a smaller disk.\nWith three disks, the puzzle can be solved in seven moves. The minimum number of moves required to solve a Tower of Hanoi puzzle is 2n - 1, where n is the number of disks.","alt_names":["Towers_of_Hanoi"],"name":"Tower of Hanoi","categories":["All articles needing additional references","Articles needing additional references from November 2013","CS1 Russian-language sources (ru)","Commons category with local link same as on Wikidata","Mechanical puzzles"],"tag_line":"The Tower of Hanoi (also called the Tower of Brahma or Lucas' Tower, and sometimes pluralized) is a mathematical game or puzzle."}}
,{"_index":"throwtable","_type":"algorithm","_id":"color-quantization","_score":0,"_source":{"description":"In computer graphics, color quantization or color image quantization is a process that reduces the number of distinct colors used in an image, usually with the intention that the new image should be as visually similar as possible to the original image. Computer algorithms to perform color quantization on bitmaps have been studied since the 1970s. Color quantization is critical for displaying images with many colors on devices that can only display a limited number of colors, usually due to memory limitations, and enables efficient compression of certain types of images.\nThe name \"color quantization\" is primarily used in computer graphics research literature; in applications, terms such as optimized palette generation, optimal palette generation, or decreasing color depth are used. Some of these are misleading, as the palettes generated by standard algorithms are not necessarily the best possible.","alt_names":["Color_quantization"],"name":"Color quantization","categories":["All articles with unsourced statements","Articles with unsourced statements from July 2011","Image processing"],"tag_line":"In computer graphics, color quantization or color image quantization is a process that reduces the number of distinct colors used in an image, usually with the intention that the new image should be as visually similar as possible to the original image."}}
,{"_index":"throwtable","_type":"algorithm","_id":"reciprocal-gamma-function","_score":0,"_source":{"description":"In mathematics, the reciprocal gamma function is the function\n\nwhere Γ(z) denotes the gamma function. Since the gamma function is meromorphic and nonzero everywhere in the complex plane, its reciprocal is an entire function. As an entire function, it is of order 1 (meaning that  grows no faster than ), but of infinite type (meaning that  grows faster than any multiple of |z|, since its growth is approximately proportional to  in the left-hand plane).\nThe reciprocal is sometimes used as a starting point for numerical computation of the gamma function, and a few software libraries provide it separately from the regular gamma function.\nKarl Weierstrass called the reciprocal gamma function the \"factorielle\" and used it in his development of the Weierstrass factorization theorem.\n\n","alt_names":["Reciprocal_gamma_function"],"name":"Reciprocal gamma function","categories":["Analytic functions","Gamma and related functions"],"tag_line":"In mathematics, the reciprocal gamma function is the function\n\nwhere Γ(z) denotes the gamma function."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pendulum","_score":0,"_source":{"description":"A pendulum is a weight suspended from a pivot so that it can swing freely. When a pendulum is displaced sideways from its resting, equilibrium position, it is subject to a restoring force due to gravity that will accelerate it back toward the equilibrium position. When released, the restoring force combined with the pendulum's mass causes it to oscillate about the equilibrium position, swinging back and forth. The time for one complete cycle, a left swing and a right swing, is called the period. The period depends on the length of the pendulum, and also to a slight degree on the amplitude, the width of the pendulum's swing.\nFrom its examination in around 1602 by Galileo Galilei, the regular motion of pendulums was used for timekeeping, and was the world's most accurate timekeeping technology until the 1930s. Pendulums are used to regulate pendulum clocks, and are used in scientific instruments such as accelerometers and seismometers. Historically they were used as gravimeters to measure the acceleration of gravity in geophysical surveys, and even as a standard of length. The word \"pendulum\" is new Latin, from the Latin pendulus, meaning 'hanging'.\nThe simple gravity pendulum is an idealized mathematical model of a pendulum. This is a weight (or bob) on the end of a massless cord suspended from a pivot, without friction. When given an initial push, it will swing back and forth at a constant amplitude. Real pendulums are subject to friction and air drag, so the amplitude of their swings declines.\n\n","name":"Pendulum","categories":["CS1 Italian-language sources (it)","CS1 errors: dates","CS1 errors: external links","Commons category with local link same as on Wikidata","Pendulums","Wikipedia articles with GND identifiers"],"tag_line":"A pendulum is a weight suspended from a pivot so that it can swing freely."}}
,{"_index":"throwtable","_type":"algorithm","_id":"entropy-(information-theory)","_score":0,"_source":{"description":"In information theory, systems are modeled by a transmitter, channel, and receiver. The transmitter produces messages that are sent through the channel. The channel modifies the message in some way. The receiver attempts to infer which message was sent. In this context, entropy (more specifically, Shannon entropy) is the expected value (average) of the information contained in each message. 'Messages' can be modeled by any flow of information.\nIn a more technical sense, there are reasons (explained below) to define information as the negative of the logarithm of the probability distribution. The probability distribution of the events, coupled with the information amount of every event, forms a random variable whose expected value is the average amount of information, or entropy, generated by this distribution. Units of entropy are the shannon, nat, or hartley, depending on the base of the logarithm used to define it, though the shannon is commonly referred to as a bit.\nThe logarithm of the probability distribution is useful as a measure of entropy because it is additive for independent sources. For instance, the entropy of a coin toss is 1 shannon, whereas of m tosses it is m shannons. Generally, you need log2(n) bits to represent a variable that can take one of n values if n is a power of 2. If these values are equally probable, the entropy (in shannons) is equal to the number of bits. Equality between number of bits and shannons holds only while all outcomes are equally probable. If one of the events is more probable than others, observation of that event is less informative. Conversely, rarer events provide more information when observed. Since observation of less probable events occurs more rarely, the net effect is that the entropy (thought of as average information) received from non-uniformly distributed data is less than log2(n). Entropy is zero when one outcome is certain. Shannon entropy quantifies all these considerations exactly when a probability distribution of the source is known. The meaning of the events observed (the meaning of messages) does not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves.\nGenerally, entropy refers to disorder or uncertainty. Shannon entropy was introduced by Claude E. Shannon in his 1948 paper \"A Mathematical Theory of Communication\". Shannon entropy provides an absolute limit on the best possible average length of lossless encoding or compression of an information source. Rényi entropy generalizes Shannon entropy.\n\n","alt_names":["Entropy_(information_theory)"],"name":"Entropy (information theory)","categories":["All articles needing additional references","All articles with unsourced statements","Articles needing additional references from April 2012","Articles with unsourced quotes","Articles with unsourced statements from April 2013","Entropy and information","Information theory","Randomness","Statistical theory","Use dmy dates from July 2013","Wikipedia articles incorporating text from PlanetMath","Wikipedia articles needing clarification from July 2014","Wikipedia articles with GND identifiers","Wikipedia external links cleanup from June 2015","Wikipedia spam cleanup from June 2015"],"tag_line":"In information theory, systems are modeled by a transmitter, channel, and receiver."}}
,{"_index":"throwtable","_type":"algorithm","_id":"aliquot-sequence","_score":0,"_source":{"description":"In mathematics, an aliquot sequence is a recursive sequence in which each term is the sum of the proper divisors of the previous term. The aliquot sequence starting with a positive integer k can be defined formally in terms of the sum-of-divisors function σ1 in the following way:\ns0 = k\nsn = σ1(sn−1) − sn−1.\nFor example, the aliquot sequence of 10 is 10, 8, 7, 1, 0 because:\nσ1(10) − 10 = 5 + 2 + 1 = 8\nσ1(8) − 8 = 4 + 2 + 1 = 7\nσ1(7) − 7 = 1\nσ1(1) − 1 = 0\nMany aliquot sequences terminate at zero (sequence A080907 in OEIS); all such sequences necessarily end with a prime number followed by 1 (since the only proper divisor of a prime is 1), followed by 0 (since 1 has no proper divisors). There are a variety of ways in which an aliquot sequence might not terminate:\nA perfect number has a repeating aliquot sequence of period 1. The aliquot sequence of 6, for example, is 6, 6, 6, 6, ...\nAn amicable number has a repeating aliquot sequence of period 2. For instance, the aliquot sequence of 220 is 220, 284, 220, 284, ...\nA sociable number has a repeating aliquot sequence of period 3 or greater. (Sometimes the term sociable number is used to encompass amicable numbers as well.) For instance, the aliquot sequence of 1264460 is 1264460, 1547860, 1727636, 1305184, 1264460, ...\nSome numbers have an aliquot sequence which is eventually periodic, but the number itself is not perfect, amicable, or sociable. For instance, the aliquot sequence of 95 is 95, 25, 6, 6, 6, 6, ... . Numbers like 95 that are not perfect, but have an eventually repeating aliquot sequence of period 1 are called aspiring numbers ( A063769).\nThe lengths of the Aliquot sequences that start at n are\n1, 2, 2, 3, 2, 1, 2, 3, 4, 4, 2, 7, 2, 5, 5, 6, 2, 4, 2, 7, 3, 6, 2, 5, 1, 7, 3, 1, 2, 15, 2, 3, 6, 8, 3, 4, 2, 7, 3, 4, 2, 14, 2, 5, 7, 8, 2, 6, 4, 3, ... (sequence A044050 in OEIS)\nThe final terms (excluding 1) of the Aliquot sequences that start at n are\n1, 2, 3, 3, 5, 6, 7, 7, 3, 7, 11, 3, 13, 7, 3, 3, 17, 11, 19, 7, 11, 7, 23, 17, 6, 3, 13, 28, 29, 3, 31, 31, 3, 7, 13, 17, 37, 7, 17, 43, 41, 3, 43, 43, 3, 3, 47, 41, 7, 43, ... (sequence A115350 in OEIS)\nNumbers whose Aliquot sequence terminates in 1 are\n1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, ... (sequence A080907 in OEIS)\nNumbers whose Aliquot sequence terminates in a perfect number are\n25, 95, 119, 143, 417, 445, 565, 608, 650, 652, 675, 685, 783, 790, 909, 913, ... (sequence A063769 in OEIS)\nNumbers whose Aliquot sequence terminates in a cycle with length at least 2 are\n220, 284, 562, 1064, 1184, 1188, 1210, 1308, 1336, 1380, 1420, 1490, 1604, 1690, 1692, 1772, 1816, 1898, 2008, 2122, 2152, 2172, 2362, ... (sequence A121507 in OEIS)\nNumbers whose Aliquot sequence is not known to be finite or eventually periodic are\n276, 306, 396, 552, 564, 660, 696, 780, 828, 888, 966, 996, 1074, 1086, 1098, 1104, 1134, 1218, 1302, 1314, 1320, 1338, 1350, 1356, 1392, 1398, 1410, 1464, 1476, 1488, ... (sequence A131884 in OEIS)\nAn important conjecture due to Catalan with respect to aliquot sequences is that every aliquot sequence ends in one of the above ways–with a prime number, a perfect number, or a set of amicable or sociable numbers. The alternative would be that a number exists whose aliquot sequence is infinite, yet aperiodic. Any one of the many numbers whose aliquot sequences have not been fully determined might be such a number. The first five candidate numbers are called the Lehmer five (named after Dick Lehmer): 276, 552, 564, 660, and 966.\nAs of April 2015, there were 898 positive integers less than 100,000 whose aliquot sequences have not been fully determined, and 9190 such integers less than 1,000,000.","alt_names":["Aliquot_sequence"],"name":"Aliquot sequence","categories":["All articles containing potentially dated statements","Arithmetic functions","Articles containing potentially dated statements from April 2015","Divisor function"],"tag_line":"In mathematics, an aliquot sequence is a recursive sequence in which each term is the sum of the proper divisors of the previous term."}}
,{"_index":"throwtable","_type":"algorithm","_id":"levenshtein-distance","_score":0,"_source":{"description":"In information theory and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. Informally, the Levenshtein distance between two words is the minimum number of single-character edits (i.e. insertions, deletions or substitutions) required to change one word into the other. It is named after Vladimir Levenshtein, who considered this distance in 1965.\nLevenshtein distance may also be referred to as edit distance, although that may also denote a larger family of distance metrics. It is closely related to pairwise string alignments.","alt_names":["Levenshtein_distance"],"name":"Levenshtein distance","categories":["Accuracy disputes from March 2015","All articles needing additional references","Articles needing additional references from February 2010","Articles with example pseudocode","CS1 Russian-language sources (ru)","CS1 uses Russian-language script (ru)","Dynamic programming","Quantitative linguistics","String similarity measures"],"tag_line":"In information theory and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences."}}
,{"_index":"throwtable","_type":"algorithm","_id":"grayscale","_score":0,"_source":{"description":"In photography and computing, a grayscale or greyscale digital image is an image in which the value of each pixel is a single sample, that is, it carries only intensity information. Images of this sort, also known as black-and-white, are composed exclusively of shades of gray, varying from black at the weakest intensity to white at the strongest.\nGrayscale images are distinct from one-bit bi-tonal black-and-white images, which in the context of computer imaging are images with only the two colors, black, and white (also called bilevel or binary images). Grayscale images have many shades of gray in between.\nGrayscale images are often the result of measuring the intensity of light at each pixel in a single band of the electromagnetic spectrum (e.g. infrared, visible light, ultraviolet, etc.), and in such cases they are monochromatic proper when only a given frequency is captured. But also they can be synthesized from a full color image; see the section about converting to grayscale.","name":"Grayscale","categories":["Color depths","Imaging"],"tag_line":"In photography and computing, a grayscale or greyscale digital image is an image in which the value of each pixel is a single sample, that is, it carries only intensity information."}}
,{"_index":"throwtable","_type":"algorithm","_id":"bitcoin","_score":0,"_source":{"description":"Bitcoin is a digital asset and a payment system invented by Satoshi Nakamoto, who published the invention in 2008 and released it as open-source software in 2009. The system is peer-to-peer; users can transact directly without needing an intermediary. Transactions are verified by network nodes and recorded in a public distributed ledger called the block chain. The ledger uses bitcoin as its unit of account. The system works without a central repository or single administrator, which has led the U.S. Treasury to categorize bitcoin as a decentralized virtual currency. Bitcoin is often called the first cryptocurrency, although prior systems existed. Bitcoin is more correctly described as the first decentralized digital currency. It is the largest of its kind in terms of total market value.\nBitcoins are created as a reward for payment processing work in which users offer their computing power to verify and record payments into a public ledger. This activity is called mining and the miners are rewarded with transaction fees and newly created bitcoins. Besides being obtained by mining, bitcoins can be obtained in exchange for different currencies, products, and services. Users can send and receive bitcoins for an optional transaction fee.\nBitcoin as a form of payment for products and services has grown, and merchants have an incentive to accept it because fees are lower than the 2–3% typically imposed by credit card processors. Unlike credit cards, any fees are paid by the purchaser, not the vendor. The European Banking Authority and other sources have warned that bitcoin users are not protected by refund rights or chargebacks. Despite a big increase in the number of merchants accepting bitcoin, the cryptocurrency does not have much momentum in retail transactions.\nThe use of bitcoin by criminals has attracted the attention of financial regulators, legislative bodies, law enforcement, and media. Criminal activities are primarily centered around black markets and theft, though officials in countries such as the United States also recognize that bitcoin can provide legitimate financial services.\nBitcoin has drawn the support of a few politicians, notably U.S. Presidential candidate Rand Paul, who accepts donations in bitcoin.\n\n","alt_names":["bitcoin"],"name":"Bitcoin","categories":["2009 introductions","2009 software","All accuracy disputes","All articles containing potentially dated statements","All articles lacking reliable references","All articles with unsourced statements","Alternative currencies","Application layer protocols","Articles containing potentially dated statements from 2014","Articles containing potentially dated statements from 2015","Articles containing potentially dated statements from August 2014","Articles containing potentially dated statements from December 2014","Articles containing potentially dated statements from February 2015","Articles containing potentially dated statements from November 2012","Articles lacking reliable references from April 2015","Articles lacking reliable references from September 2015","Articles with DMOZ links","Articles with disputed statements from November 2015","Articles with unsourced statements from September 2015","Bitcoin","Cryptocurrencies","Financial technology","Pages using citations with accessdate and no URL","Use dmy dates from December 2013","Wikipedia articles needing clarification from July 2015","Wikipedia articles with GND identifiers","Wikipedia articles with LCCN identifiers","Words coined in the 2000s"],"tag_line":"Bitcoin is a digital asset and a payment system invented by Satoshi Nakamoto, who published the invention in 2008 and released it as open-source software in 2009."}}
,{"_index":"throwtable","_type":"algorithm","_id":"inverted-index","_score":0,"_source":{"description":"In computer science, an inverted index (also referred to as postings file or inverted file) is an index data structure storing a mapping from content, such as words or numbers, to its locations in a database file, or in a document or a set of documents. The purpose of an inverted index is to allow fast full text searches, at a cost of increased processing when a document is added to the database. The inverted file may be the database file itself, rather than its index. It is the most popular data structure used in document retrieval systems, used on a large scale for example in search engines. Several significant general-purpose mainframe-based database management systems have used inverted list architectures, including ADABAS, DATACOM/DB, and Model 204. In more recent document-oriented databases such as Clusterpoint database inverted index is used in Big Data processing for fast data analytics and data visualization where relational database queries processing aggregate values may take an unacceptably long time.\nThere are two main variants of inverted indexes: A record level inverted index (or inverted file index or just inverted file) contains a list of references to documents for each word. A word level inverted index (or full inverted index or inverted list) additionally contains the positions of each word within a document. The latter form offers more functionality (like phrase searches), but needs more processing power and space to be created.","alt_names":["Inverted_index"],"name":"Inverted index","categories":["CS1 maint: Extra text","Data management","Database index techniques","Pages containing cite templates with deprecated parameters","Search algorithms","Substring indices"],"tag_line":"In computer science, an inverted index (also referred to as postings file or inverted file) is an index data structure storing a mapping from content, such as words or numbers, to its locations in a database file, or in a document or a set of documents."}}
,{"_index":"throwtable","_type":"algorithm","_id":"knight's-tour","_score":0,"_source":{"description":"A knight's tour is a sequence of moves of a knight on a chessboard such that the knight visits every square only once. If the knight ends on a square that is one knight's move from the beginning square (so that it could tour the board again immediately, following the same path), the tour is closed, otherwise it is open.\nThe knight's tour problem is the mathematical problem of finding a knight's tour. Creating a program to find a knight's tour is a common problem given to computer science students. Variations of the knight's tour problem involve chessboards of different sizes than the usual 8 × 8, as well as irregular (non-rectangular) boards.","alt_names":["Knight's_tour"],"name":"Knight's tour","categories":["Chess problems","Commons category with local link same as on Wikidata","Graph algorithms","Hamiltonian paths and cycles","Mathematical chess problems","Mathematical problems"],"tag_line":"A knight's tour is a sequence of moves of a knight on a chessboard such that the knight visits every square only once."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pythagorean-triple","_score":0,"_source":{"description":"A Pythagorean triple consists of three positive integers a, b, and c, such that a2 + b2 = c2. Such a triple is commonly written (a, b, c), and a well-known example is (3, 4, 5). If (a, b, c) is a Pythagorean triple, then so is (ka, kb, kc) for any positive integer k. A primitive Pythagorean triple is one in which a, b and c are coprime. A right triangle whose sides form a Pythagorean triple is called a Pythagorean triangle.\nThe name is derived from the Pythagorean theorem, stating that every right triangle has side lengths satisfying the formula a2 + b2 = c2; thus, Pythagorean triples describe the three integer side lengths of a right triangle. However, right triangles with non-integer sides do not form Pythagorean triples. For instance, the triangle with sides a = b = 1 and c = √2 is right, but (1, 1, √2) is not a Pythagorean triple because √2 is not an integer. Moreover, 1 and √2 do not have an integer common multiple because √2 is irrational.","alt_names":["Pythagorean_triple"],"name":"Pythagorean triple","categories":["Arithmetic problems of plane geometry","CS1 Dutch-language sources (nl)","CS1 Swedish-language sources (sv)","Diophantine equations","Triangle geometry","Wikipedia articles needing clarification from April 2015"],"tag_line":"A Pythagorean triple consists of three positive integers a, b, and c, such that a2 + b2 = c2."}}
,{"_index":"throwtable","_type":"algorithm","_id":"priority-queue","_score":0,"_source":{"description":"In computer science, a priority queue is an abstract data type which is like a regular queue or stack data structure, but where additionally each element has a \"priority\" associated with it. In a priority queue, an element with high priority is served before an element with low priority. If two elements have the same priority, they are served according to their order in the queue.\nWhile priority queues are often implemented with heaps, they are conceptually distinct from heaps. A priority queue is an abstract concept like \"a list\" or \"a map\"; just as a list can be implemented with a linked list or an array, a priority queue can be implemented with a heap or a variety of other methods such as an unordered array.","alt_names":["Priority_queue"],"name":"Priority queue","categories":["Abstract data types","All articles lacking in-text citations","Articles lacking in-text citations from October 2013","Priority queues"],"tag_line":"In computer science, a priority queue is an abstract data type which is like a regular queue or stack data structure, but where additionally each element has a \"priority\" associated with it."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sparkline","_score":0,"_source":{"description":"A sparkline is a very small line chart, typically drawn without axes or coordinates. It presents the general shape of the variation (typically over time) in some measurement, such as temperature or stock market price, in a simple and highly condensed way. Sparklines are small enough to be embedded in text, or several sparklines may be grouped together as elements of a small multiple.\nWhereas the typical chart is designed to show as much data as possible, and is set off from the flow of text, sparklines are intended to be succinct, memorable, and located where they are discussed.","name":"Sparkline","categories":["All stub articles","Infographics","Statistics stubs"],"tag_line":"A sparkline is a very small line chart, typically drawn without axes or coordinates."}}
,{"_index":"throwtable","_type":"algorithm","_id":"zebra-puzzle","_score":0,"_source":{"description":"The zebra puzzle is a well-known logic puzzle. It is often called Einstein's Puzzle or Einstein's Riddle because it is said to have been invented by Albert Einstein as a boy. The puzzle is also sometimes attributed to Lewis Carroll. However, there is no known evidence for Einstein's or Carroll's authorship and the Life International version of the puzzle mentions brands of cigarette, such as Kools, that did not exist during Carroll's lifetime or Einstein's boyhood.\nThere are several versions. The one below is from the first known publication in Life International magazine on December 17, 1962. The March 25, 1963, issue contained the solution below, and the names of several hundred solvers from around the world.\nIt is often claimed that only 2% of the population can solve the puzzle.","alt_names":["Zebra_puzzle"],"name":"Zebra Puzzle","categories":["Albert Einstein","Logic puzzles"],"tag_line":"The zebra puzzle is a well-known logic puzzle."}}
,{"_index":"throwtable","_type":"algorithm","_id":"symmetric-difference","_score":0,"_source":{"description":"In mathematics, the symmetric difference of two sets is the set of elements which are in either of the sets and not in their intersection. The symmetric difference of the sets A and B is commonly denoted by\n\nor\n\nor\n\nFor example, the symmetric difference of the sets  and  is . The symmetric difference of the set of all students and the set of all females consists of all non-female students together with all female non-students.\nThe power set of any set becomes an abelian group under the operation of symmetric difference, with the empty set as the neutral element of the group and every element in this group being its own inverse. The power set of any set becomes a Boolean ring with symmetric difference as the addition of the ring and intersection as the multiplication of the ring.","alt_names":["Symmetric_difference"],"name":"Symmetric difference","categories":["Algebra","All articles needing additional references","Articles needing additional references from April 2015","Basic concepts in set theory","Binary operations","Pages using citations with accessdate and no URL","Wikipedia articles needing clarification from April 2015"],"tag_line":"In mathematics, the symmetric difference of two sets is the set of elements which are in either of the sets and not in their intersection."}}
,{"_index":"throwtable","_type":"algorithm","_id":"s-expression","_score":0,"_source":{"description":"In computing, s-expressions, sexprs or sexps (for \"symbolic expression\") are a notation for nested list (tree-structured) data, invented for and popularized by the programming language Lisp, which uses them for source code as well as data. In the usual parenthesized syntax of Lisp, an s-expression is classically defined inductively as\nan atom, or\nan expression of the form (x . y) where x and y are s-expressions.\nThe second, recursive part of the definition represents an ordered pair so that s-exprs are effectively binary trees.\nThe definition of an atom varies per context; in the original definition by John McCarthy, it was assumed that there existed \"an infinite set of distinguishable atomic symbols\" represented as \"strings of capital Latin letters and digits with single embedded blanks\" (i.e., character string and numeric literals). Most modern sexpr notations in addition use an abbreviated notation to represent lists in s-expressions, so that\n(x y z)\nstands for\n(x . (y . (z . NIL)))\nwhere NIL is the special end-of-list symbol (written '() in Scheme).\nIn the Lisp family of programming languages, s-expressions are used to represent both source code and data. Other uses of S-expressions are in Lisp-derived languages such as DSSSL, and as mark-up in communications protocols like IMAP and John McCarthy's CBCL. The details of the syntax and supported data types vary in the different languages, but the most common feature among these languages is the use of S-expressions and prefix notation.","alt_names":["S-Expression"],"name":"S-expression","categories":["All articles lacking in-text citations","Articles lacking in-text citations from April 2010","Data serialization formats","Lisp (programming language)"],"tag_line":"In computing, s-expressions, sexprs or sexps (for \"symbolic expression\") are a notation for nested list (tree-structured) data, invented for and popularized by the programming language Lisp, which uses them for source code as well as data."}}
,{"_index":"throwtable","_type":"algorithm","_id":"standard-deviation","_score":0,"_source":{"description":"In statistics, the standard deviation (SD, also represented by the Greek letter sigma σ or s) is a measure that is used to quantify the amount of variation or dispersion of a set of data values. A standard deviation close to 0 indicates that the data points tend to be very close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the data points are spread out over a wider range of values.\nThe standard deviation of a random variable, statistical population, data set, or probability distribution is the square root of its variance. It is algebraically simpler, though in practice less robust, than the average absolute deviation. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same units as the data. There are also other measures of deviation from the norm, including mean absolute deviation, which provide different mathematical properties from standard deviation.\nIn addition to expressing the variability of a population, the standard deviation is commonly used to measure confidence in statistical conclusions. For example, the margin of error in polling data is determined by calculating the expected standard deviation in the results if the same poll were to be conducted multiple times. The reported margin of error is typically about twice the standard deviation—the half-width of a 95 percent confidence interval. In science, researchers commonly report the standard deviation of experimental data, and only effects that fall much farther than two standard deviations away from what would have been expected are considered statistically significant—normal random error or variation in the measurements is in this way distinguished from causal variation. The standard deviation is also important in finance, where the standard deviation on the rate of return on an investment is a measure of the volatility of the investment.\nWhen only a sample of data from a population is available, the term standard deviation of the sample or sample standard deviation can refer to either the above-mentioned quantity as applied to those data or to a modified quantity that is a better estimate of the population standard deviation (the standard deviation of the entire population).","alt_names":["Standard_Deviation"],"name":"Standard deviation","categories":["All articles needing additional references","All articles to be split","All articles with failed verification","All articles with unsourced statements","Articles needing additional references from June 2011","Articles to be split from April 2013","Articles to be split from December 2013","Articles with failed verification from May 2015","Articles with unsourced statements from January 2012","Articles with unsourced statements from July 2012","Articles with unsourced statements from November 2015","Data analysis","Statistical deviation and dispersion","Statistical terminology","Summary statistics","Use dmy dates from June 2011","Wikipedia articles with GND identifiers"],"tag_line":"In statistics, the standard deviation (SD, also represented by the Greek letter sigma σ or s) is a measure that is used to quantify the amount of variation or dispersion of a set of data values."}}
,{"_index":"throwtable","_type":"algorithm","_id":"thiele's-interpolation-formula","_score":0,"_source":{"description":"In mathematics, Thiele's interpolation formula is a formula that defines a rational function  from a finite set of inputs  and their function values . The problem of generating a function whose graph passes through a given set of function values is called interpolation. This interpolation formula is named after the Danish mathematician Thorvald N. Thiele. It is expressed as a continued fraction, where ρ represents the reciprocal difference:\n\n","alt_names":["Thiele's_interpolation_formula"],"name":"Thiele's interpolation formula","categories":["All stub articles","Articles with example ALGOL 68 code","Finite differences","Interpolation","Mathematical analysis stubs"],"tag_line":"In mathematics, Thiele's interpolation formula is a formula that defines a rational function  from a finite set of inputs  and their function values ."}}
,{"_index":"throwtable","_type":"algorithm","_id":"vampire-number","_score":0,"_source":{"description":"In mathematics, a vampire number (or true vampire number) is a composite natural number v, with an even number of digits n, that can be factored into two integers x and y each with n/2 digits and not both with trailing zeroes, where v contains precisely all the digits from x and from y, in any order, counting multiplicity. x and y are called the fangs.\nFor example: 1260 is a vampire number, with 21 and 60 as fangs, since 21 × 60 = 1260. However, 126000 (which can be expressed as 21 × 6000 or 210 × 600) is not, as 21 and 6000 do not have the correct length, and both 210 and 600 have trailing zeroes. Similarly, 1023 (which can be expressed as 31 × 33) is not, as although 1023 contains all the digits of 31 and 33, the list of digits of the factors does not coincide with the list of digits of the original number.\nVampire numbers first appeared in a 1994 post by Clifford A. Pickover to the Usenet group sci.math, and the article he later wrote was published in chapter 30 of his book Keys to Infinity.\nThe vampire numbers are:\n1260, 1395, 1435, 1530, 1827, 2187, 6880, 102510, 104260, 105210, 105264, 105750, 108135, 110758, 115672, 116725, 117067, 118440, 120600, 123354, 124483, 125248, 125433, 125460, 125500, ... (sequence A014575 in OEIS)\nThere are many known sequences of infinitely many vampire numbers following a pattern, such as:\n1530 = 30×51, 150300 = 300×501, 15003000 = 3000×5001, ...","alt_names":["Vampire_number"],"name":"Vampire number","categories":["All articles containing potentially dated statements","All articles lacking in-text citations","Articles containing potentially dated statements from 2006","Articles lacking in-text citations from September 2010","Base-dependent integer sequences"],"tag_line":"In mathematics, a vampire number (or true vampire number) is a composite natural number v, with an even number of digits n, that can be factored into two integers x and y each with n/2 digits and not both with trailing zeroes, where v contains precisely all the digits from x and from y, in any order, counting multiplicity."}}
,{"_index":"throwtable","_type":"algorithm","_id":"chinese-remainder-theorem","_score":0,"_source":{"description":"The Chinese remainder theorem is a result about congruences in number theory and its generalizations in abstract algebra. It was first published in the 3rd to 5th centuries by the Chinese mathematician Sun Tzu.\nIn its basic form, the Chinese remainder theorem will determine a number n that, when divided by some given divisors, leaves given remainders. For example, what is the lowest number n that when divided by 3 leaves a remainder of 2, when divided by 5 leaves a remainder of 3, and when divided by 7 leaves a remainder of 2?\n\n","alt_names":["Chinese_Remainder_Theorem"],"name":"Chinese remainder theorem","categories":["All articles lacking in-text citations","Articles containing Chinese-language text","Articles containing proofs","Articles lacking in-text citations from February 2015","Chinese mathematical discoveries","Commutative algebra","Modular arithmetic","Theorems in number theory"],"tag_line":"The Chinese remainder theorem is a result about congruences in number theory and its generalizations in abstract algebra."}}
,{"_index":"throwtable","_type":"algorithm","_id":"multiplication-algorithm","_score":0,"_source":{"description":"A multiplication algorithm is an algorithm (or method) to multiply two numbers. Depending on the size of the numbers, different algorithms are in use. Efficient multiplication algorithms have existed since the advent of the decimal system.","alt_names":["long_multiplication"],"name":"Multiplication algorithm","categories":["All articles needing additional references","All articles to be expanded","Articles needing additional references from January 2013","Articles needing additional references from May 2013","Articles needing additional references from September 2012","Articles to be expanded from October 2008","Computer arithmetic algorithms","Multiplication","Pages with citations having bare URLs","Pages with citations lacking titles","Unsolved problems in computer science"],"tag_line":"A multiplication algorithm is an algorithm (or method) to multiply two numbers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"1984-(advertisement)","_score":0,"_source":{"description":"\"1984\" is an American television commercial which introduced the Apple Macintosh personal computer. It was conceived by Steve Hayden, Brent Thomas and Lee Clow at Chiat\\Day, produced by New York production company Fairbanks Films, and directed by Ridley Scott. English athlete Anya Major performed as the unnamed heroine and David Graham as Big Brother. It was aired only twice on American television, first in 10 local outlets, including Twin Falls, Idaho, where Chiat\\Day ran the ad on December 31, 1983, at the last possible break before midnight on KMVT, so that the advertisement qualified for 1983 advertising awards. Its second televised airing, and only national airing, was on January 22, 1984, during a break in the third quarter of the telecast of Super Bowl XVIII by CBS.\nIn one interpretation of the commercial, \"1984\" used the unnamed heroine to represent the coming of the Macintosh (indicated by her white tank top with a stylized line drawing of Apple’s Macintosh computer on it) as a means of saving humanity from \"conformity\" (Big Brother). These images were an allusion to George Orwell's noted novel, Nineteen Eighty-Four, which described a dystopian future ruled by a televised \"Big Brother\". The estate of George Orwell and the television rightsholder to the novel Nineteen Eighty-Four considered the commercial to be a copyright infringement and sent a cease-and-desist letter to Apple and Chiat\\Day in April 1984.\nOriginally a subject of contention within Apple, it has subsequently been called a watershed event and a masterpiece in advertising. In 1995, The Clio Awards added it to its Hall of Fame, and Advertising Age placed it on the top of its list of 50 greatest commercials.","alt_names":["1984_(television_commercial)"],"name":"1984 (advertisement)","categories":["1983 works","1984 in American television","Advertisements","Advertising campaigns","All articles with dead external links","Apple Inc. advertising","Articles with dead external links from July 2011","English-language films","Films based on Nineteen Eighty-Four","Films directed by Ridley Scott","History of computing hardware","Super Bowl advertising","Television commercials","Use mdy dates from January 2014","Wikipedia articles needing page number citations from January 2014"],"tag_line":"\"1984\" is an American television commercial which introduced the Apple Macintosh personal computer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"lempel–ziv–welch","_score":0,"_source":{"description":"Lempel–Ziv–Welch (LZW) is a universal lossless data compression algorithm created by Abraham Lempel, Jacob Ziv, and Terry Welch. It was published by Welch in 1984 as an improved implementation of the LZ78 algorithm published by Lempel and Ziv in 1978. The algorithm is simple to implement, and has the potential for very high throughput in hardware implementations. It is the algorithm of the widely used Unix file compression utility compress, and is used in the GIF image format.","alt_names":["Lempel-Ziv-Welch"],"name":"Lempel–Ziv–Welch","categories":["All articles to be merged","Articles to be merged from December 2015","Articles with example pseudocode","Lossless compression algorithms","Wikipedia articles needing clarification from October 2012"],"tag_line":"Lempel–Ziv–Welch (LZW) is a universal lossless data compression algorithm created by Abraham Lempel, Jacob Ziv, and Terry Welch."}}
,{"_index":"throwtable","_type":"algorithm","_id":"common-operator-notation","_score":0,"_source":{"description":"In programming languages, scientific calculators and similar common operator notation or operator grammar is a way to define and analyse mathematical and other formal expressions. In this model a linear sequence of tokens are divided into two classes: operators and operands.\nOperands are objects upon which the operators operate. These include literal numbers and other constants as well as identifiers (names) which may represent anything from simple scalar variables to complex aggregated structures and objects, depending on the complexity and capability of the language at hand as well as usage context. One special type of operand is the parenthesis group. An expression enclosed in parentheses is typically recursively evaluated to be treated as a single operand on the next evaluation level.\nEach operator is given a position, precedence, and an associativity. The operator precedence is a number (from high to low or vice versa) that defines which operator that takes an operand surrounded by two operators of different precedence (or priority). Multiplication normally has higher precedence than addition, for example, so 3+4×5 = 3+(4×5) ≠ (3+4)×5.\nIn terms of operator position, an operator may be prefix, postfix, or infix. A prefix operator immediately precedes its operand, as in −x. A postfix operator immediately succeeds its operand, as in x! for instance. An infix operator is positioned in between a left and a right operand, as in x+y. Some languages, most notably the C-syntax family, stretches this conventional terminology and speaks also of ternary infix operators (a?b:c). Theoretically it would even be possible (but not necessarily practical) to define parenthesization as a unary bifix operation.\nOperator associativity, determines what happens when an operand is surrounded by operators of the same precedence, as in 1-2-3: An operator can be left-associative, right-associative, or non-associative. Left-associative operators are applied to operands in left-to-right order while right-associative operators are the other way round. The basic arithmetic operators are normally all left-associative, which means that 1-2-3 = (1-2)-3 ≠ 1-(2-3), for instance. This does not hold true for higher operators. For example, exponentiation is normally right-associative in mathematics, but is implemented as left-associative in some computer applications like Excel. In programming languages where assignment is implemented as an operator, that operator is often right-associative. If so, a statement like a := b := c would be equivalent to a := (b := c), which means that the value of c is copied to b which is then copied to a. An operator which is non-associative cannot compete for operands with operators of equal precedence. In Prolog for example, the infix operator :- is non-associative, so constructs such as a :- b :- c are syntax errors.\nUnary prefix operators such as − (negation) or sin (trigonometric function) are typically associative prefix operators. When more than one associative prefix or postfix operator of equal precedence precedes or succeeds an operand, the operators closest to the operand goes first. So −sin x = −(sin x), and sin -x = sin(-x).\nMathematically oriented languages (such as on scientific calculators) sometimes allow implicit multiplication with higher priority than prefix operators (such as sin), so that sin 2x+1 = (sin(2x))+1, for instance.\nHowever, prefix (and postfix) operators do not necessarily have higher precedence than all infix operators. Some (hypothetical) programming language may well have an operator called sin with a precedence lower than × but higher than + for instance. In such a language, sin 2·x+1 = sin(2·x)+1 would be true, instead of (sin 2)·x+1, as in mathematics.\nThe rules for expression evaluation are usually three-fold:\nTreat any sub-expression in parentheses as a single recursively-evaluated operand (there may be different kinds of parentheses though, with different semantics).\nBind operands to operators of higher precedence before those of lower precedence.\nFor equal precedence, bind operands to operators according to the associativity of the operators.\nSome more examples:\n1-2+3/4×5+6+7 = (((1-2)+((3/4)×5))+6)+7\n4 + -x + 3 = (4 + (-x)) + 3","alt_names":["Common_operator_notation"],"name":"Common operator notation","categories":["All articles lacking sources","All articles with unsourced statements","Articles lacking sources from August 2009","Articles with unsourced statements from November 2015","CS1 German-language sources (de)","Computer arithmetic","Operators (programming)","Wikipedia articles needing reorganization from July 2007"],"tag_line":"In programming languages, scientific calculators and similar common operator notation or operator grammar is a way to define and analyse mathematical and other formal expressions."}}
,{"_index":"throwtable","_type":"algorithm","_id":"collatz-conjecture","_score":0,"_source":{"description":"The Collatz conjecture is a conjecture in mathematics named after Lothar Collatz, who first proposed it in 1937. The conjecture is also known as the 3n + 1 conjecture, the Ulam conjecture (after Stanisław Ulam), Kakutani's problem (after Shizuo Kakutani), the Thwaites conjecture (after Sir Bryan Thwaites), Hasse's algorithm (after Helmut Hasse), or the Syracuse problem; the sequence of numbers involved is referred to as the hailstone sequence or hailstone numbers (because the values are usually subject to multiple descents and ascents like hailstones in a cloud), or as wondrous numbers.\nTake any natural number n. If n is even, divide it by 2 to get n / 2. If n is odd, multiply it by 3 and add 1 to obtain 3n + 1. Repeat the process (which has been called \"Half Or Triple Plus One\", or HOTPO) indefinitely. The conjecture is that no matter what number you start with, you will always eventually reach 1. The property has also been called oneness.\nPaul Erdős said about the Collatz conjecture: \"Mathematics may not be ready for such problems.\" He also offered $500 for its solution.","alt_names":["Collatz_conjecture"],"name":"Collatz conjecture","categories":["Arithmetic","Articles with French-language external links","Articles with inconsistent citation formats","CS1 Japanese-language sources (ja)","CS1 errors: invisible characters","Conjectures","Integer sequences","Number theory","Unsolved problems in mathematics"],"tag_line":"The Collatz conjecture is a conjecture in mathematics named after Lothar Collatz, who first proposed it in 1937."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ampersand","_score":0,"_source":{"description":"An ampersand is a logogram \"&\" representing the conjunction word \"and\", though to save confusion it is called a symbol. It originated as a ligature of the letters et, Latin for \"and\".\n\n","name":"Ampersand","categories":["Latin alphabet ligatures","Logic symbols","Pages containing links to subscription-only content","Typographical symbols","Use dmy dates from May 2012"],"tag_line":"An ampersand is a logogram \"&\" representing the conjunction word \"and\", though to save confusion it is called a symbol."}}
,{"_index":"throwtable","_type":"algorithm","_id":"points-of-the-compass","_score":0,"_source":{"description":"The points of the compass are points on a compass, specifically on the compass rose, marking divisions of the four cardinal directions: North, South, East, West. The number of points may be only the 4 cardinal points, or the 8 principal points adding the intercardinal (or ordinal) directions northeast (NE), southeast (SE), southwest (SW), and northwest (NW). In meteorological usage further intermediate points are added to give the sixteen points of a wind compass. Finally, at the most complete in European tradition, are found the full thirty-two points of the mariner's compass. In ancient China 24 points of the compass were used.\nIn the mariner's exercise of boxing the compass, all thirty-two points of the compass are named in clockwise order. The names of intermediate points are formed by the initials of the cardinal directions and their intermediate ordinal directions, and are very handy to refer to a heading (or course or azimuth) in a general or colloquial fashion, without having to resort to computing or recalling degrees. For most applications, the minor points have been superseded by degrees measured clockwise from North.\n^ David Boardman Graphicacy and Geography Teaching 1983 – Page 41 \"In particular they should learn that wind direction is always stated as the direction from which, and not to which, the wind is blowing. Once children have grasped these eight points they can learn the full sixteen points of the compass.\"\n^ Pamphlets on British shipping. 1785–1861 1785– Page 50 \"A deviation table having been formed by any of the processes now.so generally understood, either on the thirty-two points of the compass, the sixteen intermediate, or the eight principal points\"\n^ George Payn Quackenbos A Natural Philosophy: Embracing the Most Recent Discoveries 1860 \"Mentioning the mariner's compass: the points of the compass in their order is called boxing the compass. — The compass box is suspended within a larger box by means of two brass hoops, or gimbals as they are called, supported at opposite ...\"","alt_names":["Box_the_compass"],"name":"Points of the compass","categories":["All articles with unsourced statements","Articles with unsourced statements from July 2014","Navigational equipment","Orientation (geometry)","Units of angle"],"tag_line":"The points of the compass are points on a compass, specifically on the compass rose, marking divisions of the four cardinal directions: North, South, East, West."}}
,{"_index":"throwtable","_type":"algorithm","_id":"partial-application","_score":0,"_source":{"description":"Not to be confused with partial evaluation.\nIn computer science, partial application (or partial function application) refers to the process of fixing a number of arguments to a function, producing another function of smaller arity. Given a function , we might fix (or 'bind') the first argument, producing a function of type . Evaluation of this function might be represented as . Note that the result of partial function application in this case is a function that takes two arguments.","alt_names":["Partial_application"],"name":"Partial application","categories":["Functional programming","Implementation of functional programming languages"],"tag_line":"Not to be confused with partial evaluation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"conditional-(computer-programming)","_score":0,"_source":{"description":"In computer science, conditional statements, conditional expressions and conditional constructs are features of a programming language, which perform different computations or actions depending on whether a programmer-specified boolean condition evaluates to true or false. Apart from the case of branch predication, this is always achieved by selectively altering the control flow based on some condition.\nIn imperative programming languages, the term \"conditional statement\" is usually used, whereas in functional programming, the terms \"conditional expression\" or \"conditional construct\" are preferred, because these terms all have distinct meanings.\nAlthough dynamic dispatch is not usually classified as a conditional construct, it is another way to select between alternatives at runtime.","alt_names":["Conditional_(programming)"],"name":"Conditional (computer programming)","categories":["All articles with unsourced statements","Articles with example C code","Articles with example Haskell code","Articles with example Pascal code","Articles with example pseudocode","Articles with unsourced statements from November 2015","Conditional constructs","Pages with syntax highlighting errors"],"tag_line":"In computer science, conditional statements, conditional expressions and conditional constructs are features of a programming language, which perform different computations or actions depending on whether a programmer-specified boolean condition evaluates to true or false."}}
,{"_index":"throwtable","_type":"algorithm","_id":"eight-queens-puzzle","_score":0,"_source":{"description":"The eight queens puzzle is the problem of placing eight chess queens on an 8×8 chessboard so that no two queens threaten each other. Thus, a solution requires that no two queens share the same row, column, or diagonal. The eight queens puzzle is an example of the more general n-queens problem of placing n queens on an n×n chessboard, where solutions exist for all natural numbers n with the exception of n=2 and n=3.","alt_names":["Eight_queens_puzzle"],"name":"Eight queens puzzle","categories":["1848 in chess","Articles with example Pascal code","Chess problems","Enumerative combinatorics","Mathematical chess problems","Mathematical problems","Recreational mathematics","Use dmy dates from June 2011"],"tag_line":"The eight queens puzzle is the problem of placing eight chess queens on an 8×8 chessboard so that no two queens threaten each other."}}
,{"_index":"throwtable","_type":"algorithm","_id":"amicable-numbers","_score":0,"_source":{"description":"Amicable numbers are two different numbers so related that the sum of the proper divisors of each is equal to the other number. (A proper divisor of a number is a positive factor of that number other than the number itself. For example, the proper divisors of 6 are 1, 2, and 3.) A pair of amicable numbers constitutes an aliquot sequence of period 2. A related concept is that of a perfect number, which is a number that equals the sum of its own proper divisors, in other words a number which forms an aliquot sequence of period 1. Numbers that are members of an aliquot sequence with period greater than 2 are known as sociable numbers.\nFor example, the smallest pair of amicable numbers is (220, 284); for the proper divisors of 220 are 1, 2, 4, 5, 10, 11, 20, 22, 44, 55 and 110, of which the sum is 284; and the proper divisors of 284 are 1, 2, 4, 71 and 142, of which the sum is 220.\nThe first 20 amicable pairs are: (220, 284), (1184, 1210), (2620, 2924), (5020, 5564), (6232, 6368), (10744, 10856), (12285, 14595), (17296, 18416), (63020, 76084), (66928, 66992), (67095, 71145), (69615, 87633), (79750, 88730), (100485, 124155), (122265, 139815), (122368, 123152), (141664, 153176), (142310, 168730), ... (sequence A259180 in OEIS). (Also see  A002025 and  A002046)\n\n","alt_names":["Amicable_number"],"name":"Amicable numbers","categories":["All articles containing potentially dated statements","Articles containing potentially dated statements from December 2015","CS1 Italian-language sources (it)","Divisor function","Integer sequences","Number theory","Wikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with Wikisource reference","Wikipedia articles incorporating text from the 1911 Encyclopædia Britannica"],"tag_line":"Amicable numbers are two different numbers so related that the sum of the proper divisors of each is equal to the other number."}}
,{"_index":"throwtable","_type":"algorithm","_id":"modular-arithmetic","_score":0,"_source":{"description":"In mathematics, modular arithmetic is a system of arithmetic for integers, where numbers \"wrap around\" upon reaching a certain value—the modulus. The modern approach to modular arithmetic was developed by Carl Friedrich Gauss in his book Disquisitiones Arithmeticae, published in 1801.\nA familiar use of modular arithmetic is in the 12-hour clock, in which the day is divided into two 12-hour periods. If the time is 7:00 now, then 8 hours later it will be 3:00. Usual addition would suggest that the later time should be 7+8=15, but this is not the answer because clock time \"wraps around\" every 12 hours; in 12-hour time, there is no \"15 o'clock\". Likewise, if the clock starts at 12:00 (noon) and 21 hours elapse, then the time will be 9:00 the next day, rather than 33:00. Because the hour number starts over after it reaches 12, this is arithmetic modulo 12. According to the definition below, 12 is congruent not only to 12 itself, but also to 0, so the time called \"12:00\" could also be called \"0:00\", since 12 is congruent to 0 modulo 12.","alt_names":["Modular_arithmetic"],"name":"Modular arithmetic","categories":["All articles needing additional references","Articles needing additional references from August 2013","Finite rings","Group theory","Modular arithmetic"],"tag_line":"In mathematics, modular arithmetic is a system of arithmetic for integers, where numbers \"wrap around\" upon reaching a certain value—the modulus."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ada-(programming-language)","_score":0,"_source":{"description":"Ada is a structured, statically typed, imperative, wide-spectrum, and object-oriented high-level computer programming language, extended from Pascal and other languages. It has built-in language support for design-by-contract, extremely strong typing, explicit concurrency, offering tasks, synchronous message passing, protected objects, and non-determinism. Ada improves code safety and maintainability by using the compiler to find errors in favor of runtime errors. Ada is an international standard; the current version (known as Ada 2012) is defined by ISO/IEC 8652:2012.\nAda was originally designed by a team led by Jean Ichbiah of CII Honeywell Bull under contract to the United States Department of Defense (DoD) from 1977 to 1983 to supersede the hundreds of programming languages then used by the DoD. Ada was named after Ada Lovelace (1815–1852), who is credited as being the first computer programmer.","alt_names":["Ada_(programming_language)"],"name":"Ada (programming language)","categories":[".NET programming languages","Ada (programming language)","All articles with unsourced statements","Articles with DMOZ links","Articles with unsourced statements from June 2015","Avionics programming languages","Commons category with local link same as on Wikidata","High Integrity Programming Language","Multi-paradigm programming languages","Programming language standards","Programming languages created in the 1980s","Programming languages with an ISO standard","Statically typed programming languages","Systems programming languages","Wikipedia articles with GND identifiers"],"tag_line":"Ada is a structured, statically typed, imperative, wide-spectrum, and object-oriented high-level computer programming language, extended from Pascal and other languages."}}
,{"_index":"throwtable","_type":"algorithm","_id":"exponentiation","_score":0,"_source":{"description":"Exponentiation is a mathematical operation, written as bn, involving two numbers, the base b and the exponent n. When n is a positive integer, exponentiation corresponds to repeated multiplication of the base: that is, bn is the product of multiplying n bases:\n\nIn that case, bn is called the n-th power of b, or b raised to the power n.\nThe exponent is usually shown as a superscript to the right of the base. Some common exponents have their own names: the exponent 2 (or 2nd power) is called the square of b (b2) or b squared; the exponent 3 (or 3rd power) is called the cube of b (b3) or b cubed. The exponent −1 of b, or 1 / b, is called the reciprocal of b.\nWhen n is a negative integer and b is not zero, bn is naturally defined as 1/b−n, preserving the property bn × bm = bn + m.\nThe definition of exponentiation can be extended to allow any real or complex exponent. Exponentiation by integer exponents can also be defined for a wide variety of algebraic structures, including matrices.\nExponentiation is used extensively in many fields, including economics, biology, chemistry, physics, and computer science, with applications such as compound interest, population growth, chemical reaction kinetics, wave behavior, and public-key cryptography.","name":"Exponentiation","categories":["All articles with unsourced statements","Articles with unsourced statements from April 2014","Articles with unsourced statements from July 2010","Binary operations","Exponentials","Good articles"],"tag_line":"Exponentiation is a mathematical operation, written as bn, involving two numbers, the base b and the exponent n. When n is a positive integer, exponentiation corresponds to repeated multiplication of the base: that is, bn is the product of multiplying n bases:\n\nIn that case, bn is called the n-th power of b, or b raised to the power n.\nThe exponent is usually shown as a superscript to the right of the base."}}
,{"_index":"throwtable","_type":"algorithm","_id":"one-instruction-set-computer","_score":0,"_source":{"description":"A one instruction set computer (OISC), sometimes called an ultimate reduced instruction set computer (URISC), is an abstract machine that uses only one instruction – obviating the need for a machine language opcode. With a judicious choice for the single instruction and given infinite resources, an OISC is capable of being a universal computer in the same manner as traditional computers that have multiple instructions. OISCs have been recommended as aids in teaching computer architecture and have been used as computational models in structural computing research.","alt_names":["One_instruction_set_computer"],"name":"One instruction set computer","categories":["Esoteric programming languages","Models of computation","Pages containing cite templates with deprecated parameters"],"tag_line":"A one instruction set computer (OISC), sometimes called an ultimate reduced instruction set computer (URISC), is an abstract machine that uses only one instruction – obviating the need for a machine language opcode."}}
,{"_index":"throwtable","_type":"algorithm","_id":"cryptographic-hash-function","_score":0,"_source":{"description":"A cryptographic hash function is a hash function which is considered practically impossible to invert, that is, to recreate the input data from its hash value alone. These one-way hash functions have been called \"the workhorses of modern cryptography\". The input data is often called the message, and the hash value is often called the message digest or simply the digest.\nThe ideal cryptographic hash function has four main properties:\nit is easy to compute the hash value for any given message\nit is infeasible to generate a message from its hash\nit is infeasible to modify a message without changing the hash\nit is infeasible to find two different messages with the same hash.\nCryptographic hash functions have many information security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\n^ Schneier, Bruce. \"Cryptanalysis of MD5 and SHA: Time for a New Standard\". Computerworld. Retrieved 15 October 2014.","alt_names":["Cryptographic_hash_function"],"name":"Cryptographic hash function","categories":["Cryptographic hash functions","Cryptographic primitives","Cryptography","Hashing"],"tag_line":"A cryptographic hash function is a hash function which is considered practically impossible to invert, that is, to recreate the input data from its hash value alone."}}
,{"_index":"throwtable","_type":"algorithm","_id":"conversion-of-units-of-temperature","_score":0,"_source":{"description":"This is a compendium of temperature conversion formulas and comparisons among eight different temperature scales, several of which have long been obsolete.","alt_names":["Temperature_conversion"],"name":"Conversion of units of temperature","categories":["Conversion of units of measurement","Units of temperature"],"tag_line":"This is a compendium of temperature conversion formulas and comparisons among eight different temperature scales, several of which have long been obsolete."}}
,{"_index":"throwtable","_type":"algorithm","_id":"list-of-integrals-of-trigonometric-functions","_score":0,"_source":{"description":"The following is a list of integrals (antiderivative functions) of trigonometric functions. For antiderivatives involving both exponential and trigonometric functions, see List of integrals of exponential functions. For a complete list of antiderivative functions, see Lists of integrals. For the special antiderivatives involving trigonometric functions, see Trigonometric integral.\nGenerally, if the function  is any trigonometric function, and  is its derivative,\n\nIn all formulas the constant a is assumed to be nonzero, and C denotes the constant of integration.","alt_names":["List_of_integrals_of_trigonometric_functions"],"name":"List of integrals of trigonometric functions","categories":["Integrals","Mathematics-related lists","Trigonometry"],"tag_line":"The following is a list of integrals (antiderivative functions) of trigonometric functions."}}
,{"_index":"throwtable","_type":"algorithm","_id":"header-(computing)","_score":0,"_source":{"description":"In information technology, header refers to supplemental data placed at the beginning of a block of data being stored or transmitted. In data transmission, the data following the header are sometimes called the payload or body.\nIt is vital that header composition follow a clear and unambiguous specification or format, to allow for parsing.","alt_names":["Header_(computing)"],"name":"Header (computing)","categories":["Computer data","Data transmission"],"tag_line":"In information technology, header refers to supplemental data placed at the beginning of a block of data being stored or transmitted."}}
,{"_index":"throwtable","_type":"algorithm","_id":"palindrome","_score":0,"_source":{"description":"A palindrome is a word, phrase, number, or other sequence of characters which reads the same backward or forward. Allowances may be made for adjustments to capital letters, punctuation, and word dividers. Famous examples include \"A man, a plan, a canal, Panama!\", \"Amor, Roma\", \"race car\", \"stack cats\", \"step on no pets\", \"taco cat\", \"put it up\", \"Was it a car or a cat I saw?\" and \"No 'x' in Nixon\".\nComposing literature in palindromes is an example of constrained writing.\nThe word \"palindrome\" was coined by the English playwright Ben Jonson in the 17th century from the Greek roots palin (πάλιν; \"again\") and dromos (δρóμος; \"way, direction\").","alt_names":["semordnilap"],"name":"Palindrome","categories":["All articles with unsourced statements","Articles containing Ancient Greek-language text","Articles containing Latin-language text","Articles with unsourced statements from August 2011","Articles with unsourced statements from August 2015","Articles with unsourced statements from January 2009","Articles with unsourced statements from October 2011","Commons category with local link same as on Wikidata","Palindromes","Vague or ambiguous time from January 2011","Wikipedia articles with GND identifiers"],"tag_line":"A palindrome is a word, phrase, number, or other sequence of characters which reads the same backward or forward."}}
,{"_index":"throwtable","_type":"algorithm","_id":"brownian-tree","_score":0,"_source":{"description":"A Brownian tree, whose name is derived from Robert Brown via Brownian motion, is a form of computer art that was briefly popular in the 1990s, when home computers started to have sufficient power to simulate Brownian motion. Brownian trees are mathematical models of dendritic structures associated with the physical process known as diffusion-limited aggregation.\nA Brownian tree is built with these steps: first, a \"seed\" is placed somewhere on the screen. Then, a particle is placed in a random position of the screen, and moved randomly until it bumps against the seed. The particle is left there, and another particle is placed in a random position and moved until it bumps against the seed or any previous particle, and so on.\n\nThe resulting tree can have many different shapes, depending on principally three factors:\nthe seed position\nthe initial particle position (anywhere on the screen, from a circle surrounding the seed, from the top of the screen, etc.)\nthe moving algorithm (usually random, but for example a particle can be deleted if it goes too far from the seed, etc.)\nParticle color can change between iterations, giving interesting effects.\nAt the time of their popularity (helped by a Scientific American article in the Computer Recreations section, December 1988), a common computer took hours, and even days, to generate a small tree. Today's computers can generate trees with tens of thousands of particles in minutes or seconds.\nThese trees can also be grown easily in an electrodeposition cell, and are the direct result of diffusion-limited aggregation.","alt_names":["Brownian_tree"],"name":"Brownian tree","categories":["All articles lacking sources","Articles lacking sources from December 2009","Computer art","Stochastic processes"],"tag_line":"A Brownian tree, whose name is derived from Robert Brown via Brownian motion, is a form of computer art that was briefly popular in the 1990s, when home computers started to have sufficient power to simulate Brownian motion."}}
,{"_index":"throwtable","_type":"algorithm","_id":"least-common-multiple","_score":0,"_source":{"description":"In arithmetic and number theory, the least common multiple (also called the lowest common multiple or smallest common multiple) of two integers a and b, usually denoted by LCM(a, b), is the smallest positive integer that is divisible by both a and b. Since division of integers by zero is undefined, this definition has meaning only if a and b are both different from zero. However, some authors define lcm(a,0) as 0 for all a, which is the result of taking the lcm to be the least upper bound in the lattice of divisibility.\nThe LCM is familiar from grade-school arithmetic as the \"lowest common denominator\" (LCD) that must be determined before fractions can be added, subtracted or compared . The LCM of more than two integers is also well-defined: it is the smallest positive integer that is divisible by each of them.","alt_names":["Least_common_multiple"],"name":"Least common multiple","categories":["Arithmetic","Elementary arithmetic"],"tag_line":"In arithmetic and number theory, the least common multiple (also called the lowest common multiple or smallest common multiple) of two integers a and b, usually denoted by LCM(a, b), is the smallest positive integer that is divisible by both a and b."}}
,{"_index":"throwtable","_type":"algorithm","_id":"conjugate-transpose","_score":0,"_source":{"description":"In mathematics, the conjugate transpose or Hermitian transpose of an m-by-n matrix A with complex entries is the n-by-m matrix A* obtained from A by taking the transpose and then taking the complex conjugate of each entry (i.e., negating their imaginary parts but not their real parts). The conjugate transpose is formally defined by\n\nwhere the subscripts denote the i,j-th entry, for 1 ≤ i ≤ n and 1 ≤ j ≤ m, and the overbar denotes a scalar complex conjugate. (The complex conjugate of , where a and b are reals, is .)\nThis definition can also be written as\n\nwhere  denotes the transpose and  denotes the matrix with complex conjugated entries.\nOther names for the conjugate transpose of a matrix are Hermitian conjugate, bedaggered matrix, adjoint matrix or transjugate. The conjugate transpose of a matrix A can be denoted by any of these symbols:\n or , commonly used in linear algebra\n (sometimes pronounced as \"A dagger\"), universally used in quantum mechanics\n, although this symbol is more commonly used for the Moore–Penrose pseudoinverse\nIn some contexts,  denotes the matrix with complex conjugated entries, and the conjugate transpose is then denoted by  or .","alt_names":["conjugate_transpose"],"name":"Conjugate transpose","categories":["Linear algebra","Matrices"],"tag_line":"In mathematics, the conjugate transpose or Hermitian transpose of an m-by-n matrix A with complex entries is the n-by-m matrix A* obtained from A by taking the transpose and then taking the complex conjugate of each entry (i.e., negating their imaginary parts but not their real parts)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ackermann-function","_score":0,"_source":{"description":"In computability theory, the Ackermann function, named after Wilhelm Ackermann, is one of the simplest and earliest-discovered examples of a total computable function that is not primitive recursive. All primitive recursive functions are total and computable, but the Ackermann function illustrates that not all total computable functions are primitive recursive.\nAfter Ackermann's publication of his function (which had three nonnegative integer arguments), many authors modified it to suit various purposes, so that today \"the Ackermann function\" may refer to any of numerous variants of the original function. One common version, the two-argument Ackermann–Péter function, is defined as follows for nonnegative integers m and n:\n\nIts value grows rapidly, even for small inputs. For example A(4,2) is an integer of 19,729 decimal digits.","alt_names":["Ackermann_function"],"name":"Ackermann function","categories":["Arithmetic","Computability theory","Large integers","Special functions","Theory of computation"],"tag_line":"In computability theory, the Ackermann function, named after Wilhelm Ackermann, is one of the simplest and earliest-discovered examples of a total computable function that is not primitive recursive."}}
,{"_index":"throwtable","_type":"algorithm","_id":"caesar-cipher","_score":0,"_source":{"description":"In cryptography, a Caesar cipher, also known as Caesar's cipher, the shift cipher, Caesar's code or Caesar shift, is one of the simplest and most widely known encryption techniques. It is a type of substitution cipher in which each letter in the plaintext is replaced by a letter some fixed number of positions down the alphabet. For example, with a left shift of 3, D would be replaced by A, E would become B, and so on. The method is named after Julius Caesar, who used it in his private correspondence.\nThe encryption step performed by a Caesar cipher is often incorporated as part of more complex schemes, such as the Vigenère cipher, and still has modern application in the ROT13 system. As with all single-alphabet substitution ciphers, the Caesar cipher is easily broken and in modern practice offers essentially no communication security.","alt_names":["Caesar_cipher"],"name":"Caesar cipher","categories":["Articles with hAudio microformats","Classical ciphers","Featured articles","Group theory","Julius Caesar","Spoken articles"],"tag_line":"In cryptography, a Caesar cipher, also known as Caesar's cipher, the shift cipher, Caesar's code or Caesar shift, is one of the simplest and most widely known encryption techniques."}}
,{"_index":"throwtable","_type":"algorithm","_id":"look-and-say-sequence","_score":0,"_source":{"description":"In mathematics, the look-and-say sequence is the sequence of integers beginning as follows:\n1, 11, 21, 1211, 111221, 312211, 13112221, 1113213211, ... (sequence A005150 in OEIS).\nTo generate a member of the sequence from the previous member, read off the digits of the previous member, counting the number of digits in groups of the same digit. For example:\n1 is read off as \"one 1\" or 11.\n11 is read off as \"two 1s\" or 21.\n21 is read off as \"one 2, then one 1\" or 1211.\n1211 is read off as \"one 1, then one 2, then two 1s\" or 111221.\n111221 is read off as \"three 1s, then two 2s, then one 1\" or 312211.\nThe look-and-say sequence was introduced and analyzed by John Conway.\nThe idea of the look-and-say sequence is similar to that of run-length encoding.\nIf we start with any digit d from 0 to 9 then d will remain indefinitely as the last digit of the sequence. For d different from 1, the sequence starts as follows:\nd, 1d, 111d, 311d, 13211d, 111312211d, 31131122211d, …\nIlan Vardi has called this sequence, starting with d = 3, the Conway sequence (sequence A006715 in OEIS). (for d = 2, see  A006751)","alt_names":["Look_and_say_sequence"],"name":"Look-and-say sequence","categories":["Algebraic numbers","All articles needing additional references","Articles needing additional references from May 2012","Base-dependent integer sequences","CS1 errors: chapter ignored","Mathematical constants"],"tag_line":"In mathematics, the look-and-say sequence is the sequence of integers beginning as follows:\n1, 11, 21, 1211, 111221, 312211, 13112221, 1113213211, ... (sequence A005150 in OEIS)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"euler's-sum-of-powers-conjecture","_score":0,"_source":{"description":"Euler's conjecture is a disproved conjecture in mathematics related to Fermat's last theorem. It was proposed by Leonhard Euler in 1769. It states that for all integers n and k greater than 1, if the sum of n kth powers of non-zero integers is itself a kth power, then n is greater than or equal to k.\nIn symbols, the conjecture falsely states that if  where  and  are non-zero integers, then .\nThe conjecture represents an attempt to generalize Fermat's last theorem, which is the special case n = 2: if , then .\nAlthough the conjecture holds for the case k = 3 (which follows from Fermat's last theorem for the third powers), it was disproved for k = 4 and k = 5. It is unknown whether the conjecture fails or holds for any value k ≥ 6.","alt_names":["Euler's_sum_of_powers_conjecture"],"name":"Euler's sum of powers conjecture","categories":["All articles with dead external links","All articles with unsourced statements","Articles with dead external links from April 2015","Articles with unsourced statements from October 2014","Diophantine equations","Disproved conjectures"],"tag_line":"Euler's conjecture is a disproved conjecture in mathematics related to Fermat's last theorem."}}
,{"_index":"throwtable","_type":"algorithm","_id":"canny-edge-detector","_score":0,"_source":{"description":"The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. It was developed by John F. Canny in 1986. Canny also produced a computational theory of edge detection explaining why the technique works.","alt_names":["Canny_edge_detector"],"name":"Canny edge detector","categories":["Feature detection (computer vision)"],"tag_line":"The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images."}}
,{"_index":"throwtable","_type":"algorithm","_id":"list-comprehension","_score":0,"_source":{"description":"A list comprehension is a syntactic construct available in some programming languages for creating a list based on existing lists. It follows the form of the mathematical set-builder notation (set comprehension) as distinct from the use of map and filter functions.","alt_names":["List_comprehension"],"name":"List comprehension","categories":["All articles to be merged","Articles to be merged from November 2014","Articles with example Haskell code","Articles with example Python code","Articles with example Racket code","Articles with example code","Programming constructs"],"tag_line":"A list comprehension is a syntactic construct available in some programming languages for creating a list based on existing lists."}}
,{"_index":"throwtable","_type":"algorithm","_id":"haversine-formula","_score":0,"_source":{"description":"The haversine formula is an equation important in navigation, giving great-circle distances between two points on a sphere from their longitudes and latitudes. It is a special case of a more general formula in spherical trigonometry, the law of haversines, relating the sides and angles of spherical triangles. The first table of haversines in English was published by James Andrew in 1805.\nFlorian Cajori credits an earlier use by José de Mendoza y Ríos in 1801 The term haversine was coined in 1835 by James Inman.\nThese names follow from the fact that they are customarily written in terms of the haversine function, given by haversin(θ) = sin2(θ/2). The formulas could equally be written in terms of any multiple of the haversine, such as the older versine function (twice the haversine). Prior to the advent of computers, the elimination of division and multiplication by factors of two proved convenient enough that tables of haversine values and logarithms were included in 19th and early 20th century navigation and trigonometric texts. These days, the haversine form is also convenient in that it has no coefficient in front of the sin2 function.","alt_names":["Haversine_formula"],"name":"Haversine formula","categories":["CS1 Spanish-language sources (es)","Geolocation","Spherical trigonometry"],"tag_line":"The haversine formula is an equation important in navigation, giving great-circle distances between two points on a sphere from their longitudes and latitudes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"conway's-game-of-life","_score":0,"_source":{"description":"The Game of Life, also known simply as Life, is a cellular automaton devised by the British mathematician John Horton Conway in 1970.\nThe \"game\" is a zero-player game, meaning that its evolution is determined by its initial state, requiring no further input. One interacts with the Game of Life by creating an initial configuration and observing how it evolves or, for advanced players, by creating patterns with particular properties.","alt_names":["Conway's_Game_of_Life"],"name":"Conway's Game of Life","categories":["All articles with dead external links","All articles with unsourced statements","Articles with DMOZ links","Articles with dead external links from June 2015","Articles with unsourced statements from April 2015","Cellular automaton rules","Self-organization"],"tag_line":"The Game of Life, also known simply as Life, is a cellular automaton devised by the British mathematician John Horton Conway in 1970."}}
,{"_index":"throwtable","_type":"algorithm","_id":"hough-transform","_score":0,"_source":{"description":"The Hough transform is a feature extraction technique used in image analysis, computer vision, and digital image processing. The purpose of the technique is to find imperfect instances of objects within a certain class of shapes by a voting procedure. This voting procedure is carried out in a parameter space, from which object candidates are obtained as local maxima in a so-called accumulator space that is explicitly constructed by the algorithm for computing the Hough transform.\nThe classical Hough transform was concerned with the identification of lines in the image, but later the Hough transform has been extended to identifying positions of arbitrary shapes, most commonly circles or ellipses. The Hough transform as it is universally used today was invented by Richard Duda and Peter Hart in 1972, who called it a \"generalized Hough transform\" after the related 1962 patent of Paul Hough. The transform was popularized in the computer vision community by Dana H. Ballard through a 1981 journal article titled \"Generalizing the Hough transform to detect arbitrary shapes\".","alt_names":["Hough_transform"],"name":"Hough transform","categories":["Feature detection (computer vision)"],"tag_line":"The Hough transform is a feature extraction technique used in image analysis, computer vision, and digital image processing."}}
,{"_index":"throwtable","_type":"algorithm","_id":"named-parameter","_score":0,"_source":{"description":"In computer programming, named parameters, pass-by-name, or keyword arguments refer to a computer language's support for function calls that clearly state the name of each parameter within the function call itself.","alt_names":["Named_parameter"],"name":"Named parameter","categories":["All articles needing additional references","Articles needing additional references from August 2014","Articles with example code","Subroutines"],"tag_line":"In computer programming, named parameters, pass-by-name, or keyword arguments refer to a computer language's support for function calls that clearly state the name of each parameter within the function call itself."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sedol","_score":0,"_source":{"description":"SEDOL stands for Stock Exchange Daily Official List, a list of security identifiers used in the United Kingdom and Ireland for clearing purposes. The numbers are assigned by the London Stock Exchange, on request by the security issuer. SEDOLs serve as the National Securities Identifying Number for all securities issued in the United Kingdom and are therefore part of the security's ISIN as well. The SEDOL Masterfile (SMF) provides reference data on millions of global multi-asset securities each uniquely identified at the market level using a universal SEDOL code.","name":"SEDOL","categories":["All articles lacking in-text citations","All articles lacking sources","Articles lacking in-text citations from October 2013","Articles lacking sources from May 2013","Security identifier types"],"tag_line":"SEDOL stands for Stock Exchange Daily Official List, a list of security identifiers used in the United Kingdom and Ireland for clearing purposes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"ranking","_score":0,"_source":{"description":"A ranking is a relationship between a set of items such that, for any two items, the first is either 'ranked higher than', 'ranked lower than' or 'ranked equal to' the second. In mathematics, this is known as a weak order or total preorder of objects. It is not necessarily a total order of objects because two different objects can have the same ranking. The rankings themselves are totally ordered. For example, materials are totally preordered by hardness, while degrees of hardness are totally ordered.\nBy reducing detailed measures to a sequence of ordinal numbers, rankings make it possible to evaluate complex information according to certain criteria. Thus, for example, an Internet search engine may rank the pages it finds according to an estimation of their relevance, making it possible for the user quickly to select the pages they are likely to want to see.\nAnalysis of data obtained by ranking commonly requires non-parametric statistics.","name":"Ranking","categories":["All articles needing additional references","All articles needing cleanup","All articles that may contain original research","Articles needing additional references from November 2008","Articles needing additional references from September 2011","Articles needing cleanup from January 2010","Articles that may contain original research from July 2008","Cleanup tagged articles without a reason field from January 2010","International rankings","Nonparametric statistics","Wikipedia pages needing cleanup from January 2010"],"tag_line":"A ranking is a relationship between a set of items such that, for any two items, the first is either 'ranked higher than', 'ranked lower than' or 'ranked equal to' the second."}}
,{"_index":"throwtable","_type":"algorithm","_id":"qr-decomposition","_score":0,"_source":{"description":"In linear algebra, a QR decomposition (also called a QR factorization) of a matrix is a decomposition of a matrix A into a product A = QR of an orthogonal matrix Q and an upper triangular matrix R. QR decomposition is often used to solve the linear least squares problem, and is the basis for a particular eigenvalue algorithm, the QR algorithm.","alt_names":["QR_decomposition"],"name":"QR decomposition","categories":["All articles to be expanded","Articles to be expanded from December 2009","Articles to be expanded from November 2015","Matrix decompositions","Numerical linear algebra"],"tag_line":"In linear algebra, a QR decomposition (also called a QR factorization) of a matrix is a decomposition of a matrix A into a product A = QR of an orthogonal matrix Q and an upper triangular matrix R. QR decomposition is often used to solve the linear least squares problem, and is the basis for a particular eigenvalue algorithm, the QR algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"wireworld","_score":0,"_source":{"description":"Wireworld is a cellular automaton first proposed by Brian Silverman in 1987, as part of his program Phantom Fish Tank. It subsequently became more widely known as a result of an article in the \"Computer Recreations\" column of Scientific American. Wireworld is particularly suited to simulating electronic logic elements, or \"gates\", and, despite the simplicity of the rules, Wireworld is Turing-complete.\n\n","name":"Wireworld","categories":["Cellular automaton rules","Wikipedia external links cleanup from November 2012","Wikipedia spam cleanup from November 2012"],"tag_line":"Wireworld is a cellular automaton first proposed by Brian Silverman in 1987, as part of his program Phantom Fish Tank."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quaternion","_score":0,"_source":{"description":"In mathematics, the quaternions are a number system that extends the complex numbers. They were first described by Irish mathematician William Rowan Hamilton in 1843 and applied to mechanics in three-dimensional space. A feature of quaternions is that multiplication of two quaternions is noncommutative. Hamilton defined a quaternion as the quotient of two directed lines in a three-dimensional space or equivalently as the quotient of two vectors.\nQuaternions find uses in both theoretical and applied mathematics, in particular for calculations involving three-dimensional rotations such as in three-dimensional computer graphics, computer vision and crystallographic texture analysis. In practical applications, they can be used alongside other methods, such as Euler angles and rotation matrices, or as an alternative to them, depending on the application.\nIn modern mathematical language, quaternions form a four-dimensional associative normed division algebra over the real numbers, and therefore also a domain. In fact, the quaternions were the first noncommutative division algebra to be discovered. The algebra of quaternions is often denoted by H (for Hamilton), or in blackboard bold by  (Unicode U+210D, ℍ). It can also be given by the Clifford algebra classifications Cℓ0,2(R) ≅ Cℓ03,0(R). The algebra H holds a special place in analysis since, according to the Frobenius theorem, it is one of only two finite-dimensional division rings containing the real numbers as a proper subring, the other being the complex numbers. These rings are also Euclidean Hurwitz algebras, of which quaternions are the largest associative algebra.\nThe unit quaternions can therefore be thought of as a choice of a group structure on the 3-sphere S3 that gives the group Spin(3), which is isomorphic to SU(2) and also to the universal cover of SO(3).","name":"Quaternion","categories":["All articles needing additional references","All articles with dead external links","Articles needing additional references from October 2015","Articles with dead external links from October 2009","Quaternions"],"tag_line":"In mathematics, the quaternions are a number system that extends the complex numbers."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quine-(computing)","_score":0,"_source":{"description":"A quine is a non-empty computer program which takes no input and produces a copy of its own source code as its only output. The standard terms for these programs in the computability theory and computer science literature are \"self-replicating programs\", \"self-reproducing programs\", and \"self-copying programs\".\nA quine is a fixed point of an execution environment, when the execution environment is viewed as a function. Quines are possible in any Turing complete programming language, as a direct consequence of Kleene's recursion theorem. For amusement, programmers sometimes attempt to develop the shortest possible quine in any given programming language.\nThe name \"quine\" was coined by Douglas Hofstadter, in his popular science book Gödel, Escher, Bach: An Eternal Golden Braid, in the honor of philosopher Willard Van Orman Quine (1908–2000), who made an extensive study of indirect self-reference, and in particular for the following paradox-producing expression, known as Quine's paradox:\n\n\"Yields falsehood when preceded by its quotation\" yields falsehood when preceded by its quotation.\n\nIn some languages, particularly scripting languages, an empty source file is a fixed point of the language, being a valid program that produces no output. Such an empty program, submitted as \"the world's smallest self reproducing program\", once won the \"worst abuse of the rules\" prize in the International Obfuscated C Code Contest.","alt_names":["Quine_(computing)"],"name":"Quine (computing)","categories":["All articles lacking reliable references","Articles lacking reliable references from February 2014","Articles with example C code","Articles with specifically marked weasel-worded phrases from April 2013","CS1 Russian-language sources (ru)","Pages with syntax highlighting errors","Source code","Test items","Willard Van Orman Quine"],"tag_line":"A quine is a non-empty computer program which takes no input and produces a copy of its own source code as its only output."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sierpinski-carpet","_score":0,"_source":{"description":"The Sierpinski carpet is a plane fractal first described by Wacław Sierpiński in 1916. The carpet is one generalization of the Cantor set to two dimensions; another is the Cantor dust.\nThe technique of subdividing a shape into smaller copies of itself, removing one or more copies, and continuing recursively can be extended to other shapes. For instance, subdividing an equilateral triangle into four equilateral triangles, removing the middle triangle, and recursing leads to the Sierpinski triangle. In three dimensions, a similar construction based on cubes produces the Menger sponge.","alt_names":["Sierpinski_carpet"],"name":"Sierpinski carpet","categories":["CS1 French-language sources (fr)","Curves","Fractals","Science and technology in Poland","Topological spaces"],"tag_line":"The Sierpinski carpet is a plane fractal first described by Wacław Sierpiński in 1916."}}
,{"_index":"throwtable","_type":"algorithm","_id":"variable-length-quantity","_score":0,"_source":{"description":"A variable-length quantity (VLQ) is a universal code that uses an arbitrary number of binary octets (eight-bit bytes) to represent an arbitrarily large integer. It was defined for use in the standard MIDI file format to save additional space for a resource constrained system, and is also used in the later Extensible Music Format (XMF). A VLQ is essentially a base-128 representation of an unsigned integer with the addition of the eighth bit to mark continuation of bytes. See the example below.\nBase-128 is also used in ASN.1 BER encoding to encode tag numbers and Object Identifiers. It is also used in the WAP environment, where it is called variable length unsigned integer or uintvar. The DWARF debugging format defines a variant called LEB128 (or ULEB128 for unsigned numbers), where the least significant group of 7 bits are encoded in the first byte and the most significant bits are in the last byte (so effectively it is the little-endian analog of a variable-length quantity). Google's protocol buffers use a similar format to have compact representation of integer values, as does Oracle's Portable Object Format (POF) and the Microsoft .NET Framework's \"7-bit encoded int\" in the BinaryReader and BinaryWriter classes.\nIt's also used extensively in web browsers for source mapping - which contain a lot of integer line & column number mappings - to keep the size of the map to a minimum.","alt_names":["Variable-length_quantity"],"name":"Variable-length quantity","categories":["Data types","MIDI"],"tag_line":"A variable-length quantity (VLQ) is a universal code that uses an arbitrary number of binary octets (eight-bit bytes) to represent an arbitrarily large integer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"nth-root","_score":0,"_source":{"description":"In mathematics, the nth root of a number x, where n is a positive integer, is a number r which, when raised to the power n yields x\n\nwhere n is the degree of the root. A root of degree 2 is called a square root and a root of degree 3, a cube root. Roots of higher degree are referred by using ordinal numbers, as in fourth root, twentieth root, etc.\nFor example:\n2 is a square root of 4, since 22 = 4.\n−2 is also a square root of 4, since (−2)2 = 4.\nA real number or complex number has n roots of degree n. While the roots of 0 are not distinct (all equaling 0), the n nth roots of any other real or complex number are all distinct. If n is even and x is real and positive, one of its nth roots is positive, one is negative, and the rest are complex but not real; if n is even and x is real and negative, none of the nth roots is real. If n is odd and x is real, one nth root is real and has the same sign as x , while the other roots are not real. Finally, if x is not real, then none of its nth roots is real.\nRoots are usually written using the radical symbol or radix  or , with  or  denoting the square root,  denoting the cube root,  denoting the fourth root, and so on. In the expression , n is called the index,  is the radical sign or radix, and x is called the radicand. Since the radical symbol denotes a function, when a number is presented under the radical symbol it must return only one result, so a non-negative real root, called the principal nth root, is preferred rather than others; if the only real root is negative, as for the cube root of –8, again the real root is considered the principal root. An unresolved root, especially one using the radical symbol, is often referred to as a surd or a radical. Any expression containing a radical, whether it is a square root, a cube root, or a higher root, is called a radical expression, and if it contains no transcendental functions or transcendental numbers it is called an algebraic expression.\nIn calculus, roots are treated as special cases of exponentiation, where the exponent is a fraction:\n\nRoots are particularly important in the theory of infinite series; the root test determines the radius of convergence of a power series. Nth roots can also be defined for complex numbers, and the complex roots of 1 (the roots of unity) play an important role in higher mathematics. Galois theory can be used to determine which algebraic numbers can be expressed using roots, and to prove the Abel-Ruffini theorem, which states that a general polynomial equation of degree five or higher cannot be solved using roots alone; this result is also known as \"the insolubility of the quintic\".","alt_names":["Nth_root"],"name":"Nth root","categories":["Articles containing non-English-language text","CS1 Latin-language sources (la)","Elementary algebra"],"tag_line":"In mathematics, the nth root of a number x, where n is a positive integer, is a number r which, when raised to the power n yields x\n\nwhere n is the degree of the root."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fisher–yates-shuffle","_score":0,"_source":{"description":"The Fisher–Yates shuffle (named after Ronald Fisher and Frank Yates), also known as the Knuth shuffle (after Donald Knuth), is an algorithm for generating a random permutation of a finite set—in plain terms, for randomly shuffling the set. A variant of the Fisher–Yates shuffle, known as Sattolo's algorithm, may be used to generate random cyclic permutations of length n instead. The Fisher–Yates shuffle is unbiased, so that every permutation is equally likely. The modern version of the algorithm is also rather efficient, requiring only time proportional to the number of items being shuffled and no additional storage space.\nFisher–Yates shuffling is similar to randomly picking numbered tickets (combinatorics: distinguishable objects) out of a hat without replacement until there are none left.","alt_names":["Knuth_shuffle"],"name":"Fisher–Yates shuffle","categories":["CS1 errors: external links","Combinatorial algorithms","Monte Carlo methods","Permutations","Randomness"],"tag_line":"The Fisher–Yates shuffle (named after Ronald Fisher and Frank Yates), also known as the Knuth shuffle (after Donald Knuth), is an algorithm for generating a random permutation of a finite set—in plain terms, for randomly shuffling the set."}}
,{"_index":"throwtable","_type":"algorithm","_id":"higher-order-function","_score":0,"_source":{"description":"In mathematics and computer science, a higher-order function (also functional, functional form or functor; not to be confused with the functor concept in category theory) is a function that does at least one of the following:\ntakes one or more functions as arguments,\nreturns a function as its result.\nAll other functions are first-order functions. In mathematics higher-order functions are also known as operators or functionals. The differential operator in calculus is a common example, since it maps a function to its derivative, also a function.\nIn the untyped lambda calculus, all functions are higher-order; in a typed lambda calculus, from which most functional programming languages are derived, higher-order functions are values with types of the form .\n\n","alt_names":["Higher-order_function"],"name":"Higher-order function","categories":["All accuracy disputes","All articles lacking in-text citations","All articles with unsourced statements","Articles lacking in-text citations from September 2013","Articles with disputed statements from September 2015","Articles with example C code","Articles with example Erlang code","Articles with example Haskell code","Articles with example JavaScript code","Articles with example Pascal code","Articles with example Python code","Articles with example Scheme code","Articles with unsourced statements from November 2014","Functional programming","Higher-order functions","Lambda calculus","Subroutines"],"tag_line":"In mathematics and computer science, a higher-order function (also functional, functional form or functor; not to be confused with the functor concept in category theory) is a function that does at least one of the following:\ntakes one or more functions as arguments,\nreturns a function as its result."}}
,{"_index":"throwtable","_type":"algorithm","_id":"resampling-(statistics)","_score":0,"_source":{"description":"In statistics, resampling is any of a variety of methods for doing one of the following:\nEstimating the precision of sample statistics (medians, variances, percentiles) by using subsets of available data (jackknifing) or drawing randomly with replacement from a set of data points (bootstrapping)\nExchanging labels on data points when performing significance tests (permutation tests, also called exact tests, randomization tests, or re-randomization tests)\nValidating models by using random subsets (bootstrapping, cross validation)\nCommon resampling techniques include bootstrapping, jackknifing and permutation tests.","alt_names":["Permutation_test"],"name":"Resampling (statistics)","categories":["Articles needing more detailed references","Monte Carlo methods","Nonparametric statistics","Resampling (statistics)","Statistical inference"],"tag_line":"In statistics, resampling is any of a variety of methods for doing one of the following:\nEstimating the precision of sample statistics (medians, variances, percentiles) by using subsets of available data (jackknifing) or drawing randomly with replacement from a set of data points (bootstrapping)\nExchanging labels on data points when performing significance tests (permutation tests, also called exact tests, randomization tests, or re-randomization tests)\nValidating models by using random subsets (bootstrapping, cross validation)\nCommon resampling techniques include bootstrapping, jackknifing and permutation tests."}}
,{"_index":"throwtable","_type":"algorithm","_id":"catalan-number","_score":0,"_source":{"description":"In combinatorial mathematics, the Catalan numbers form a sequence of natural numbers that occur in various counting problems, often involving recursively-defined objects. They are named after the Belgian mathematician Eugène Charles Catalan (1814–1894).\nUsing zero-based numbering, the nth Catalan number is given directly in terms of binomial coefficients by\n\nThe first Catalan numbers for n = 0, 1, 2, 3, … are\n1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, 58786, 208012, 742900, 2674440, 9694845, 35357670, 129644790, 477638700, 1767263190, 6564120420, 24466267020, 91482563640, 343059613650, 1289904147324, 4861946401452, … (sequence A000108 in OEIS).","alt_names":["Catalan_number"],"name":"Catalan number","categories":["All articles with unsourced statements","Articles containing proofs","Articles with unsourced statements from June 2015","Enumerative combinatorics","Factorial and binomial topics","Integer sequences"],"tag_line":"In combinatorial mathematics, the Catalan numbers form a sequence of natural numbers that occur in various counting problems, often involving recursively-defined objects."}}
,{"_index":"throwtable","_type":"algorithm","_id":"numerical-integration","_score":0,"_source":{"description":"In numerical analysis, numerical integration constitutes a broad family of algorithms for calculating the numerical value of a definite integral, and by extension, the term is also sometimes used to describe the numerical solution of differential equations. This article focuses on calculation of definite integrals. The term numerical quadrature (often abbreviated to quadrature) is more or less a synonym for numerical integration, especially as applied to one-dimensional integrals. Some authors refer to numerical integration over more than one dimension as cubature; others take quadrature to include higher-dimensional integration.\nThe basic problem in numerical integration is to compute an approximate solution to a definite integral\n\nto a given degree of accuracy. If f(x) is a smooth function integrated over a small number of dimensions, and the domain of integration is bounded, there are many methods for approximating the integral to the desired precision.\n\n","alt_names":["Numerical_integration"],"name":"Numerical integration","categories":["Articles with example Python code","CS1 errors: external links","Numerical analysis","Numerical integration (quadrature)"],"tag_line":"In numerical analysis, numerical integration constitutes a broad family of algorithms for calculating the numerical value of a definite integral, and by extension, the term is also sometimes used to describe the numerical solution of differential equations."}}
,{"_index":"throwtable","_type":"algorithm","_id":"perfect-number","_score":0,"_source":{"description":"In number theory, a perfect number is a positive integer that is equal to the sum of its proper positive divisors, that is, the sum of its positive divisors excluding the number itself (also known as its aliquot sum). Equivalently, a perfect number is a number that is half the sum of all of its positive divisors (including itself) i.e. σ1(n) = 2n.\nThis definition is ancient, appearing as early as Euclid's Elements (VII.22) where it is called τέλειος ἀριθμός (perfect, ideal, or complete number). Euclid also proved a formation rule (IX.36) whereby  is an even perfect number whenever  is what is now called a Mersenne prime. Much later, Euler proved that all even perfect numbers are of this form. This is known as the Euclid–Euler theorem.\nIt is not known whether there are any odd perfect numbers, nor whether infinitely many perfect numbers exist.","alt_names":["Perfect_numbers"],"name":"Perfect number","categories":["All articles containing potentially dated statements","Articles containing potentially dated statements from December 2015","Articles with inconsistent citation formats","Divisor function","Integer sequences","Unsolved problems in mathematics"],"tag_line":"In number theory, a perfect number is a positive integer that is equal to the sum of its proper positive divisors, that is, the sum of its positive divisors excluding the number itself (also known as its aliquot sum)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"benford","_score":0,"_source":{"description":"Benford is a surname. Notable people with the surname include:\nFrank Benford, American scientist who gives his name to:\nBenford's law, which holds that in large sets of data, individual numbers are more likely to start with lower digits\n\nGregory Benford, American science fiction author and astrophysicist\nTommy Benford, American jazz drummer\nFictional characters:\nMark Benford, main character of the TV series FlashForward","name":"Benford","categories":["All set index articles","Surnames"],"tag_line":"Benford is a surname."}}
,{"_index":"throwtable","_type":"algorithm","_id":"1000","_score":0,"_source":{"description":"This article is about the single year 1000; see 1000s, 990s, 10th century, 11th century for events or processes with \"approximate date\" 1000.\n\nYear 1000 (M) was a leap year starting on Monday (link will display the full calendar) of the Julian calendar. It was also the last year of the 10th century as well as the last year of the 1st millennium of the Dionysian era ending on December 31st, but the first year of the 1000s decade.\nThe year falls well into the period of Old World history known as the Middle Ages; in Europe, it is sometimes and by convention considered the boundary date between the Early Middle Ages and the High Middle Ages. The Muslim world was in its \"Golden Age\". China was in its Song dynasty, Japan was in its \"classical\" Heian period. India was divided into a number of lesser empires, such as the Rashtrakuta Dynasty, Pala Empire (Kamboja Pala dynasty; Mahipala), Chola dynasty (Raja Raja Chola I), Yadava dynasty, etc. Sub-Saharan Africa was still in the prehistoric period, although Arab slave trade was beginning to be an important factor in the formation of the Sahelian kingdoms. The pre-Columbian New World was in a time of general transition in many regions. Wari and Tiwanaku cultures receded in power and influence while Chachapoya and Chimú cultures rose toward florescence in South America. In Mesoamerica, the Maya Terminal Classic period saw the decline of many grand polities of the Petén like Palenque and Tikal yet a renewed vigor and greater construction phases of sites in the Yucatán region like Chichén Itzá and Uxmal. Mitla, with Mixtec influence, became the more important site of the Zapotec, overshadowing the waning Monte Albán. Cholula flourished in central Mexico, as did Tula, the center of Toltec culture.\nWorld population is estimated to have been between c. 250 and 310 million.\n^ 310 million: United Nations Department of Economic and Social Affairs, Population Division. 254 million: Jean-Noël Biraben, 1980, \"An Essay Concerning Mankind's Evolution\", Population, Selected Papers, Vol. 4, pp. 1–13.","name":"1000","categories":["1000","All articles with unsourced statements","Articles with unsourced statements from December 2013"],"tag_line":"This article is about the single year 1000; see 1000s, 990s, 10th century, 11th century for events or processes with \"approximate date\" 1000."}}
,{"_index":"throwtable","_type":"algorithm","_id":"schwartzian-transform","_score":0,"_source":{"description":"The Schwartzian transform is a computer science programming idiom used to improve the efficiency of sorting a list of items. This idiom is appropriate for comparison-based sorting when the ordering is actually based on the ordering of a certain property (the key) of the elements, where computing that property is an intensive operation that should be performed a minimal number of times. The Schwartzian transform is notable in that it does not use named temporary arrays.\nThe idiom is named after Randal L. Schwartz, who first demonstrated it in Perl shortly after the release of Perl 5 in 1994. The term \"Schwartzian transform\" applied solely to Perl programming for a number of years, but it has later been adopted by some users of other languages, such as Python, to refer to similar idioms in those languages. However, the algorithm was already in use in other languages (under no specific name) before it was popularized among the Perl community in the form of that particular idiom by Schwartz. The term \"Schwartzian transform\" indicates a specific idiom, and not the algorithm in general.\nFor example, to sort the word list (\"aaaa\",\"a\",\"aa\") according to word length: first build the list ([\"aaaa\",4],[\"a\",1],[\"aa\",2]), then sort it according to the numeric values getting ([\"a\",1],[\"aa\",2],[\"aaaa\",4]), then strip off the numbers and you get (\"a\",\"aa\",\"aaaa\"). That was the algorithm in general, so it does not count as a transform. To make it a true Schwartzian transform, it would be done in Perl like this:\n\nThe Schwartzian transform is a version of a Lisp idiom known as decorate-sort-undecorate, which avoids recomputing the sort keys by temporarily associating them with the input items. This approach is similar to memoization, which avoids repeating the calculation of the key corresponding to a specific input value. By comparison, this idiom assures that each input item's key is calculated exactly once, which may still result in repeating some calculations if the input data contains duplicate items.","alt_names":["Decorate-sort-undecorate"],"name":"Schwartzian transform","categories":["All articles needing style editing","Articles with example Perl code","Articles with example Racket code","Perl","Programming idioms","Sorting algorithms","Wikipedia articles needing style editing from September 2014"],"tag_line":"The Schwartzian transform is a computer science programming idiom used to improve the efficiency of sorting a list of items."}}
,{"_index":"throwtable","_type":"algorithm","_id":"root-of-unity","_score":0,"_source":{"description":"In mathematics, a root of unity, occasionally called a de Moivre number, is any complex number that gives 1 when raised to some positive integer power n. Roots of unity are used in many branches of mathematics, and are especially important in number theory, the theory of group characters, and the discrete Fourier transform.\nIn field theory and ring theory the notion of root of unity also applies to any ring with a multiplicative identity element. Any algebraically closed field has exactly n nth roots of unity, if n is not divisible by the characteristic of the field.","alt_names":["Roots_of_unity"],"name":"Root of unity","categories":["1 (number)","Algebraic numbers","All articles needing additional references","All articles with unsourced statements","Articles needing additional references from April 2012","Articles with unsourced statements from May 2011","Complex numbers","Cyclotomic fields","Polynomials","Use dmy dates from September 2010"],"tag_line":"In mathematics, a root of unity, occasionally called a de Moivre number, is any complex number that gives 1 when raised to some positive integer power n. Roots of unity are used in many branches of mathematics, and are especially important in number theory, the theory of group characters, and the discrete Fourier transform."}}
,{"_index":"throwtable","_type":"algorithm","_id":"trabb-pardo–knuth-algorithm","_score":0,"_source":{"description":"The Trabb Pardo–Knuth algorithm is a program introduced by Donald Knuth and Luis Trabb Pardo to illustrate the evolution of computer programming languages.\nIn their 1977 work \"The Early Development of Programming Languages\", Trabb Pardo and Knuth introduced a trivial program that involved arrays, indexing, mathematical functions, subroutines, I/O, conditionals and iteration. They then wrote implementations of the algorithm in several early programming languages to show how such concepts were expressed.\nThe simpler Hello world program has been used for much the same purpose.\n\n","alt_names":["Trabb_Pardo-Knuth_algorithm"],"name":"Trabb Pardo–Knuth algorithm","categories":["All articles lacking in-text citations","All articles needing additional references","Articles lacking in-text citations from November 2011","Articles needing additional references from November 2011","Articles with example ALGOL 60 code","Donald Knuth","Programming language topics","Test items"],"tag_line":"The Trabb Pardo–Knuth algorithm is a program introduced by Donald Knuth and Luis Trabb Pardo to illustrate the evolution of computer programming languages."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shellsort","_score":0,"_source":{"description":"Shellsort, also known as Shell sort or Shell's method, is an in-place comparison sort. It can be seen as either a generalization of sorting by exchange (bubble sort) or sorting by insertion (insertion sort). The method starts by sorting pairs of elements far apart from each other, then progressively reducing the gap between elements to be compared. Starting with far apart elements can move some out-of-place elements into position faster than a simple nearest neighbor exchange. Donald Shell published the first version of this sort in 1959. The running time of Shellsort is heavily dependent on the gap sequence it uses. For many practical variants, determining their time complexity remains an open problem.","alt_names":["Shell_sort"],"name":"Shellsort","categories":["All articles with unsourced statements","Articles with unsourced statements from June 2015","Comparison sorts","Pages with duplicate reference names","Pages with reference errors","Sorting algorithms","Use dmy dates from September 2010"],"tag_line":"Shellsort, also known as Shell sort or Shell's method, is an in-place comparison sort."}}
,{"_index":"throwtable","_type":"algorithm","_id":"stem-and-leaf-display","_score":0,"_source":{"description":"A stem-and-leaf display is a complicated device for presenting quantitative data in a graphical format, similar to a histogram, to assist in visualizing the shape of a distribution. They evolved from Arthur Bowley's work in the early 1900s, and are useful tools in exploratory data analysis. Stemplots became more commonly used in the 1980s after the publication of John Tukey's book on exploratory data analysis in 1977. The popularity during those years is attributable to their use of monospaced (typewriter) typestyles that allowed computer technology of the time to easily produce the graphics. Modern computers' superior graphic capabilities have meant these techniques are less often used.\nA stem-and-leaf display is often called a stemplot, but the latter term often refers to another chart type. A simple stem plot may refer to plotting a matrix of y values onto a common x axis, and identifying the common x value with a vertical line, and the individual y values with symbols on the line.\nUnlike histograms, stem-and-leaf displays retain the original data to at least two significant digits, and put the data in order, thereby easing the move to order-based inference and non-parametric statistics.\nA basic stem-and-leaf display contains two columns separated by a vertical line. The left column contains the stems and the right column contains the leaves.","alt_names":["Stem-and-leaf_plot"],"name":"Stem-and-leaf display","categories":["Exploratory data analysis","Statistical charts and diagrams"],"tag_line":"A stem-and-leaf display is a complicated device for presenting quantitative data in a graphical format, similar to a histogram, to assist in visualizing the shape of a distribution."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quantum-algorithm","_score":0,"_source":{"description":"In quantum computing, a quantum algorithm is an algorithm which runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation. A classical (or non-quantum) algorithm is a finite sequence of instructions, or a step-by-step procedure for solving a problem, where each step or instruction can be performed on a classical computer. Similarly, a quantum algorithm is a step-by-step procedure, where each of the steps can be performed on a quantum computer. Although all classical algorithms can also be performed on a quantum computer, the term quantum algorithm is usually used for those algorithms which seem inherently quantum, or use some essential feature of quantum computation such as quantum superposition or quantum entanglement.\nAll problems which can be solved on a quantum computer can be solved on a classical computer. In particular, problems which are undecidable using classical computers remain undecidable using quantum computers. What makes quantum algorithms interesting is that they might be able to solve some problems faster than classical algorithms.\nThe most well known algorithms are Shor's algorithm for factoring, and Grover's algorithm for searching an unstructured database or an unordered list. Shor's algorithms runs exponentially faster than the best known classical algorithm for factoring, the general number field sieve. Grover's algorithm runs quadratically faster than the best possible classical algorithm for the same task.","name":"Quantum algorithm","categories":["All articles with unsourced statements","Articles with unsourced statements from December 2014","Quantum algorithms","Quantum computing","Quantum information science","Theoretical computer science","Use dmy dates from September 2011"],"tag_line":"In quantum computing, a quantum algorithm is an algorithm which runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"match-rating-approach","_score":0,"_source":{"description":"The match rating approach (MRA) is a phonetic algorithm developed by Western Airlines in 1977 for the indexation and comparison of homophonous names.\nThe algorithm itself has a simple set of encoding rules but a more lengthy set of comparison rules. The main mechanism being the similarity comparison which calculates the number of unmatched characters by comparing the strings from left to right and then from right to left and removing identical characters. This value is subtracted from 6 and then compared to a minimum threshold. The minimum threshold is defined by table A and is dependent upon the length of the strings.\nThe encoded name is known (perhaps incorrectly) as a personal numeric identifier (PNI). The PNI codex can never contain more than 6 alpha only characters.\nMatch rating approach performs well with names containing the letter \"y\" unlike the original flavour of the NYSIIS algorithm. For example, the surnames \"Smith\" and \"Smyth\" are successfully matched.\nMRA does not perform well with encoded names that differ in length by more than 2.","name":"Match rating approach","categories":["All Wikipedia articles needing context","All pages needing cleanup","Phonetic algorithms","Wikipedia articles needing context from October 2009","Wikipedia introduction cleanup from October 2009"],"tag_line":"The match rating approach (MRA) is a phonetic algorithm developed by Western Airlines in 1977 for the indexation and comparison of homophonous names."}}
,{"_index":"throwtable","_type":"algorithm","_id":"simon's-problem","_score":0,"_source":{"description":"In computational complexity theory and quantum computing, Simon's problem is a computational problem in the model of decision tree complexity or query complexity, conceived by Daniel Simon in 1994. Simon exhibited a quantum algorithm, usually called Simon's algorithm, that solves the problem exponentially faster than any (deterministic or probabilistic) classical algorithm.\nSimon's algorithm uses  queries to the black box, whereas the best classical probabilistic algorithm necessarily needs at least  queries. It is also known that Simon's algorithm is optimal in the sense that any quantum algorithm to solve this problem requires  queries. This problem yields an oracle separation between BPP and BQP, unlike the separation provided by the Deutsch-Jozsa algorithm, which separates P and EQP.\nAlthough the problem itself is of little practical value it is interesting because it provides an exponential speedup over any classical algorithm. Moreover, it was also the inspiration for Shor's algorithm. Both problems are special cases of the abelian hidden subgroup problem, which is now known to have efficient quantum algorithms.","name":"Simon's problem","categories":["All articles with unsourced statements","Articles with unsourced statements from May 2012","Quantum algorithms"],"tag_line":"In computational complexity theory and quantum computing, Simon's problem is a computational problem in the model of decision tree complexity or query complexity, conceived by Daniel Simon in 1994."}}
,{"_index":"throwtable","_type":"algorithm","_id":"quantum-fourier-transform","_score":0,"_source":{"description":"In quantum computing, the quantum Fourier transform is a linear transformation on quantum bits, and is the quantum analogue of the discrete Fourier transform. The quantum Fourier transform is a part of many quantum algorithms, notably Shor's algorithm for factoring and computing the discrete logarithm, the quantum phase estimation algorithm for estimating the eigenvalues of a unitary operator, and algorithms for the hidden subgroup problem.\nThe quantum Fourier transform can be performed efficiently on a quantum computer, with a particular decomposition into a product of simpler unitary matrices. Using a simple decomposition, the discrete Fourier transform on  amplitudes can be implemented as a quantum circuit consisting of only  Hadamard gates and controlled phase shift gates, where  is the number of qubits. This can be compared with the classical discrete Fourier transform, which takes  gates (where  is the number of bits), which is exponentially more than . However, the quantum Fourier transform acts on a quantum state, whereas the classical Fourier transform acts on a vector, so not every task that uses the classical Fourier transform can take advantage of this exponential speedup.\nThe best quantum Fourier transform algorithms known today require only  gates to achieve an efficient approximation.","name":"Quantum Fourier transform","categories":["Quantum algorithms","Transforms"],"tag_line":"In quantum computing, the quantum Fourier transform is a linear transformation on quantum bits, and is the quantum analogue of the discrete Fourier transform."}}
,{"_index":"throwtable","_type":"algorithm","_id":"featherstone's-algorithm","_score":0,"_source":{"description":"Featherstone's algorithm is a technique used for computing the effects of forces applied to a structure of joints and links (an \"open kinematic chain\") such as a skeleton used in ragdoll physics.\nThe Featherstone's algorithm uses a reduced coordinate representation. This is in contrast to the more popular Lagrange multiplier method, which uses maximal coordinates. Brian Mirtich's PhD Thesis has a very clear and detailed description of the algorithm. Baraff's paper \"Linear-time dynamics using Lagrange multipliers\" has a discussion and comparison of both algorithms.","name":"Featherstone's algorithm","categories":["Algorithms and data structures stubs","All stub articles","Computational physics","Computer physics engines","Computer science stubs","Mechanics"],"tag_line":"Featherstone's algorithm is a technique used for computing the effects of forces applied to a structure of joints and links (an \"open kinematic chain\") such as a skeleton used in ragdoll physics."}}
,{"_index":"throwtable","_type":"algorithm","_id":"modified-due-date-scheduling-heuristic","_score":0,"_source":{"description":"The modified due-date scheduling heuristic is used in scheduling tasks to resources (for example, to answer the question \"What order should we make sandwiches in, so as to please our customers?\").\nIt assumes that the objective of the scheduling process is to minimise the total amount of time spent on tasks after their due dates. This strategy is most relevant when completing all tasks carries a certainty that at least some of them will be completed late.\nThe modified due date forms the basis of an algorithm that attempts first to complete tasks early or on time, and second to complete tasks as soon as possible when the requested due date is unattainable: Given a list of tasks, with a range of due dates (dj), and a range of times it takes to complete the tasks (pj), then at any moment (t) you should do the task that has the smallest modified due date. The modified due date itself is the highest of either the due date, or the completion date if you started the task now:\nmddj = max( dj, t+pj ).\nThus, if the due date of the PB and J is 5 minutes from now, but its time to complete is 6 minutes, the modified due date of the PB and J is 6 minutes from now. Imagine a second customer is requesting that a Reuben be prepared in 7 minutes from now, but the Reuben takes 5 minutes to complete: in this case, the MDD of the Reuben is 7 minutes; the PB and J has a smaller MDD, and thus you should make the PB and J first, while the second customer waits patiently for the Reuben to be started. In this case, the PB and J is 1 minute late, and the Reuben is 4 minutes late. Both sandwiches are made late, but the total lateness is only 5 minutes. If you had made the Reuben first, the second customer may have gotten the sandwich early, but the first customer would have received their sandwich 6 minutes late.","name":"Modified due-date scheduling heuristic","categories":["All Wikipedia articles needing context","All articles lacking sources","All pages needing cleanup","All stub articles","Articles lacking sources from October 2007","Computer science stubs","Scheduling algorithms","Wikipedia articles needing context from October 2009","Wikipedia introduction cleanup from October 2009"],"tag_line":"The modified due-date scheduling heuristic is used in scheduling tasks to resources (for example, to answer the question \"What order should we make sandwiches in, so as to please our customers?\")."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sequence-step-algorithm","_score":0,"_source":{"description":"A sequence step algorithm (SQS-AL) is an algorithm implemented in a discrete event simulation system to maximize resource utilization. This is achieved by running through two main nested loops: A sequence step loop and a replication loop. For each sequence step, each replication loop is a simulation run that collects crew idle time for activities in that sequence step. The collected crew idle times are, then, used to determine resource arrival dates for user-specified confidence levels. The process of collecting the crew idle times and determining crew arrival times for activities on a considered sequence step is repeated from the first to the last sequence step.","name":"Sequence step algorithm","categories":["Algorithms and data structures stubs","All articles covered by WikiProject Wikify","All articles needing expert attention","All articles with too few wikilinks","All orphaned articles","All stub articles","Articles covered by WikiProject Wikify from March 2013","Articles needing expert attention from October 2009","Articles needing expert attention with no reason or talk parameter","Articles needing unspecified expert attention","Articles with too few wikilinks from March 2013","Computer science stubs","Network theory","Orphaned articles from February 2009","Project management","Scheduling algorithms"],"tag_line":"A sequence step algorithm (SQS-AL) is an algorithm implemented in a discrete event simulation system to maximize resource utilization."}}
,{"_index":"throwtable","_type":"algorithm","_id":"multilevel-queue","_score":0,"_source":{"description":"Multi-level queueing, used at least since the late 1950s/early 1960s, is a queue with a predefined number of levels. Unlike the multilevel feedback queue, items get assigned to a particular level at insert (using some predefined algorithm), and thus cannot be moved to another level. Items get removed from the queue by removing all items from a level, and then moving to the next. If an item is added to a level above, the \"fetching\" restarts from there. Each level of the queue is free to use its own scheduling, thus adding greater flexibility than merely having multiple levels in a queue.","name":"Multilevel queue","categories":["All articles needing additional references","All stub articles","Articles needing additional references from September 2014","Computer science stubs","Scheduling algorithms"],"tag_line":"Multi-level queueing, used at least since the late 1950s/early 1960s, is a queue with a predefined number of levels."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fino","_score":0,"_source":{"description":"In computer science, FINO (Sometimes seen as \"FISH\", for First In, Still Here) is a humorous scheduling algorithm. It is an acronym for \"First In Never Out\" as opposed to traditional \"first in first out\" (FIFO) and \"last in first out\" (LIFO) algorithms.\nFINO works by withholding all scheduled tasks permanently. No matter how many tasks are scheduled at any time, no task ever actually takes place. This makes FINO extremely simple to implement, but useless in practice. A stateful FINO queue can be used to implement a memory leak.\nA mention of FINO appears in the Signetics 25120 write-only memory joke datasheet.","name":"FINO","categories":["All stub articles","Computer humor","Computer science stubs","Scheduling algorithms"],"tag_line":"In computer science, FINO (Sometimes seen as \"FISH\", for First In, Still Here) is a humorous scheduling algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"coffman–graham-algorithm","_score":0,"_source":{"description":"In job shop scheduling and graph drawing, the Coffman–Graham algorithm is an algorithm, named after Edward G. Coffman, Jr. and Ronald Graham, for arranging the elements of a partially ordered set into a sequence of levels. The algorithm chooses an arrangement such that an element that comes after another in the order is assigned to a lower level, and such that each level has a number of elements that does not exceed a fixed width bound W. When W = 2, it uses the minimum possible number of distinct levels, and in general it uses at most 2 − 2/W times as many levels as necessary.","name":"Coffman–Graham algorithm","categories":["Graph drawing","Optimization algorithms and methods","Scheduling algorithms"],"tag_line":"In job shop scheduling and graph drawing, the Coffman–Graham algorithm is an algorithm, named after Edward G. Coffman, Jr. and Ronald Graham, for arranging the elements of a partially ordered set into a sequence of levels."}}
,{"_index":"throwtable","_type":"algorithm","_id":"least-slack-time-scheduling","_score":0,"_source":{"description":"Least slack time (LST) scheduling is a scheduling algorithm. It assigns priority based on the slack time of a process. Slack time is the amount of time left after a job if the job was started now. This algorithm is also known as least laxity first. Its most common use is in embedded systems, especially those with multiple processors. It imposes the simple constraint that each process on each available processor possesses the same run time, and that individual processes do not have an affinity to a certain processor. This is what lends it a suitability to embedded systems.","name":"Least slack time scheduling","categories":["All articles lacking sources","Articles lacking sources from December 2009","Scheduling algorithms"],"tag_line":"Least slack time (LST) scheduling is a scheduling algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"heterogeneous-earliest-finish-time","_score":0,"_source":{"description":"Heterogeneous Earliest Finish Time (or HEFT) is a heuristic to schedule a set of dependent tasks onto a network of heterogeneous workers taking communication time into account. For inputs HEFT takes a set of tasks, represented as a directed acyclic graph, a set of workers, the times to execute each task on each worker, and the times to communicate the results from each job to each of its children between each pair of workers. It descends from list scheduling algorithms.","name":"Heterogeneous Earliest Finish Time","categories":["Pages containing cite templates with deprecated parameters","Scheduling algorithms"],"tag_line":"Heterogeneous Earliest Finish Time (or HEFT) is a heuristic to schedule a set of dependent tasks onto a network of heterogeneous workers taking communication time into account."}}
,{"_index":"throwtable","_type":"algorithm","_id":"graphical-path-method","_score":0,"_source":{"description":"The Graphical Path Method (GPM) is a mathematically based algorithm used in project management for planning, scheduling and resource control. GPM represents logical relationships of dated objects – such as activities, milestones, and benchmarks – in a time-scaled network diagram.","name":"Graphical path method","categories":["Critical Path Scheduling","Project management","Scheduling algorithms"],"tag_line":"The Graphical Path Method (GPM) is a mathematically based algorithm used in project management for planning, scheduling and resource control."}}
,{"_index":"throwtable","_type":"algorithm","_id":"earliest-deadline-first-scheduling","_score":0,"_source":{"description":"Earliest deadline first (EDF) or least time to go is a dynamic scheduling algorithm used in real-time operating systems to place processes in a priority queue. Whenever a scheduling event occurs (task finishes, new task released, etc.) the queue will be searched for the process closest to its deadline. This process is the next to be scheduled for execution.\nEDF is an optimal scheduling algorithm on preemptive uniprocessors, in the following sense: if a collection of independent jobs, each characterized by an arrival time, an execution requirement and a deadline, can be scheduled (by any algorithm) in a way that ensures all the jobs complete by their deadline, the EDF will schedule this collection of jobs so they all complete by their deadline.\nWith scheduling periodic processes that have deadlines equal to their periods, EDF has a utilization bound of 100%. Thus, the schedulability test for EDF is:\n\nwhere the  are the worst-case computation-times of the  processes and the  are their respective inter-arrival periods (assumed to be equal to the relative deadlines).\nThat is, EDF can guarantee that all deadlines are met provided that the total CPU utilization is not more than 100%. Compared to fixed priority scheduling techniques like rate-monotonic scheduling, EDF can guarantee all the deadlines in the system at higher loading.\nHowever, when the system is overloaded, the set of processes that will miss deadlines is largely unpredictable (it will be a function of the exact deadlines and time at which the overload occurs.) This is a considerable disadvantage to a real time systems designer. The algorithm is also difficult to implement in hardware and there is a tricky issue of representing deadlines in different ranges (deadlines can't be more precise than the granularity of the clock used for the scheduling). If a modular arithmetic is used to calculate future deadlines relative to now, the field storing a future relative deadline must accommodate at least the value of the ((\"duration\" {of the longest expected time to completion} * 2) + \"now\"). Therefore EDF is not commonly found in industrial real-time computer systems.\nInstead, most real-time computer systems use fixed priority scheduling (usually rate-monotonic scheduling). With fixed priorities, it is easy to predict that overload conditions will cause the low-priority processes to miss deadlines, while the highest-priority process will still meet its deadline.\nThere is a significant body of research dealing with EDF scheduling in real-time computing; it is possible to calculate worst case response times of processes in EDF, to deal with other types of processes than periodic processes and to use servers to regulate overloads.","name":"Earliest deadline first scheduling","categories":["All Wikipedia articles needing clarification","Processor scheduling algorithms","Real-time computing","Wikipedia articles needing clarification from June 2013"],"tag_line":"Earliest deadline first (EDF) or least time to go is a dynamic scheduling algorithm used in real-time operating systems to place processes in a priority queue."}}
,{"_index":"throwtable","_type":"algorithm","_id":"anticipatory-scheduling","_score":0,"_source":{"description":"Anticipatory scheduling is an algorithm for scheduling hard disk input/output (I/O scheduling). It seeks to increase the efficiency of disk utilization by \"anticipating\" future synchronous read operations.\n\"Deceptive idleness\" is a situation where a process appears to be finished reading from the disk when it is actually processing data in preparation of the next read operation. This will cause a normal work-conserving I/O scheduler to switch to servicing I/O from an unrelated process. This situation is detrimental to the throughput of synchronous reads, as it degenerates into a seeking workload. Anticipatory scheduling overcomes deceptive idleness by pausing for a short time (a few milliseconds) after a read operation in anticipation of another close-by read requests.\nAnticipatory scheduling yields significant improvements in disk utilization for some workloads. In some situations the Apache web server may achieve up to 71% more throughput from using anticipatory scheduling.\nThe Linux anticipatory scheduler may reduce performance on disks using Tagged Command Queuing (TCQ), high performance disks, and hardware RAID arrays. An anticipatory scheduler (AS) was the default Linux kernel scheduler between 2.6.0 and 2.6.18, by which time it was replaced by the CFQ scheduler.\nAs of kernel version 2.6.33, the Anticipatory scheduler (AS) has been removed from the Linux kernel. The reason being that while useful, the scheduler's effects could be achieved through tuned use of other schedulers (mostly CFQ, which can also be configured to idle with the slice_idle tunable). Since the anticipatory scheduler added maintenance overhead while not improving the workload coverage of the Linux kernel, it was deemed redundant.","name":"Anticipatory scheduling","categories":["All stub articles","Disk scheduling algorithms","Linux kernel features","Linux stubs","Operating system kernels"],"tag_line":"Anticipatory scheduling is an algorithm for scheduling hard disk input/output (I/O scheduling)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"i/o-scheduling","_score":0,"_source":{"description":"Input/output (I/O) scheduling is the method that computer operating systems use to decide in which order the block I/O operations will be submitted to storage volumes. I/O scheduling is sometimes called disk scheduling.\nI/O scheduling usually has to work with hard disk drives that have long access times for requests placed far away from the current position of the disk head (this operation is called a seek). To minimize the effect this has on system performance, most I/O schedulers implement a variant of the elevator algorithm that reorders the incoming randomly ordered requests so the associated data would be accessed with minimal arm/head movement.\nI/O schedulers can have many purposes depending on the goals; common purposes include the following:\nTo minimize time wasted by hard disk seeks\nTo prioritize a certain processes' I/O requests\nTo give a share of the disk bandwidth to each running process\nTo guarantee that certain requests will be issued before a particular deadline\nCommon scheduling disciplines include the following:\nRandom scheduling (RSS)\nFirst In, First Out (FIFO), also known as First Come First Served (FCFS)\nLast In, First Out (LIFO)\nShortest seek first, also known as Shortest Seek / Service Time First (SSTF)\nElevator algorithm, also known as SCAN (including its variants, C-SCAN, LOOK, and C-LOOK)\nN-Step-SCAN SCAN of N records at a time\nFSCAN, N-Step-SCAN where N equals queue size at start of the SCAN cycle\nCompletely Fair Queuing (CFQ) on Linux\nAnticipatory scheduling\nNoop scheduler\nDeadline scheduler\nmClock scheduler","name":"I/O scheduling","categories":["All articles needing additional references","Articles needing additional references from February 2013","Disk scheduling algorithms"],"tag_line":"Input/output (I/O) scheduling is the method that computer operating systems use to decide in which order the block I/O operations will be submitted to storage volumes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dichotomic-search","_score":0,"_source":{"description":"In computer science, a dichotomic search is a search algorithm that operates by selecting between two distinct alternatives (dichotomies) at each step. It is a specific type of divide and conquer algorithm. A well-known example is binary search.\nAbstractly, a dichotomic search can be viewed as following edges of an implicit binary tree structure until it reaches a leaf (a goal or final state). This creates a theoretical tradeoff between the number of possible states and the running time: given k comparisons, the algorithm can only reach O(2k) possible states and/or possible goals.\nSome dichotomic searches only have results at the leaves of the tree, such as the Huffman tree used in Huffman compression, or the implicit classification tree used in Twenty Questions. Other dichotomic searches also have results in at least some internal nodes of the tree, such as a dichotomic search table for Morse code. There is thus some looseness in the definition. Though there may indeed be only two paths from any node, there are thus three possibilities at each step: choose one onwards path or the other, or stop at this node.\n\nDichotomic searches are often used in repair manuals, sometimes graphically illustrated with a flowchart similar to a fault tree.","name":"Dichotomic search","categories":["All articles lacking in-text citations","All stub articles","Articles lacking in-text citations from December 2014","Computer science stubs","Search algorithms"],"tag_line":"In computer science, a dichotomic search is a search algorithm that operates by selecting between two distinct alternatives (dichotomies) at each step."}}
,{"_index":"throwtable","_type":"algorithm","_id":"foreground-background","_score":0,"_source":{"description":"Foreground-background is a scheduling algorithm that is used to control execution of multiple processes on a single processor. It is based on two waiting lists, the first one is called foreground because this is the one in which all processes initially enter, and the second one is called background because all processes, after using all of their execution time in foreground, are moved to background.\nWhen a process becomes ready it begins its execution in foreground immediately, forcing the processor to give up execution of current process in the background and execute newly created process for a predefined period. This period is usually 2 or more quanta. If the process is not finished after its execution in the foreground it is moved to background waiting list where it will be executed only when the foreground list is empty. After being moved to background, process is then run longer than before, usually 4 quanta. The time of execution is increased because the process obviously needs more than 2 quanta to finish (this is the reason it was moved to background). This gives the process the opportunity to finish within this newly designated time. If the process does not finish after this, it is then preempted and moved to the end of the background list.\nThe advantage of the foreground-background algorithm is that it gives process the opportunity to execute immediately after its creation, but scheduling in the background list is pure round-robin scheduling.","name":"Foreground-background","categories":["All articles lacking sources","Articles lacking sources from February 2007","Processor scheduling algorithms"],"tag_line":"Foreground-background is a scheduling algorithm that is used to control execution of multiple processes on a single processor."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shortest-remaining-time","_score":0,"_source":{"description":"Shortest remaining time, also known as shortest remaining time first (SRTF), is a scheduling method that is a preemptive version of shortest job next scheduling. In this scheduling algorithm, the process with the smallest amount of time remaining until completion is selected to execute. Since the currently executing process is the one with the shortest amount of time remaining by definition, and since that time should only reduce as execution progresses, processes will always run until they complete or a new process is added that requires a smaller amount of time.\nShortest remaining time is advantageous because short processes are handled very quickly. The system also requires very little overhead since it only makes a decision when a process completes or a new process is added, and when a new process is added the algorithm only needs to compare the currently executing process with the new process, ignoring all other processes currently waiting to execute.\nLike shortest job first, it has the potential for process starvation; long processes may be held off indefinitely if short processes are continually added. This threat can be minimal when process times follow a heavy-tailed distribution.\nLike shortest job next scheduling, shortest remaining time scheduling is rarely used outside of specialized environments because it requires accurate estimations of the runtime of all processes that are waiting to execute.","name":"Shortest remaining time","categories":["Processor scheduling algorithms"],"tag_line":"Shortest remaining time, also known as shortest remaining time first (SRTF), is a scheduling method that is a preemptive version of shortest job next scheduling."}}
,{"_index":"throwtable","_type":"algorithm","_id":"n-step-scan","_score":0,"_source":{"description":"N-Step-SCAN (also referred to as N-Step LOOK) is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests. It segments the request queue into subqueues of length N. Breaking the queue into segments of N requests makes service guarantees possible. Subsequent requests entering the request queue won't get pushed into N sized subqueues which are already full by the elevator algorithm. As such, starvation is eliminated and guarantees of service within N requests is possible.\nAnother way to look at N-step SCAN is this: A buffer for N requests is kept. All the requests in this buffer are serviced in any particular sweep. All the incoming requests in this period are not added to this buffer but are kept up in a separate buffer. When these top N requests are serviced, the IO scheduler chooses the next N requests and this process continues. This allows for better throughput and avoids starvation.","name":"N-Step-SCAN","categories":["All articles needing additional references","Articles needing additional references from February 2008","Disk scheduling algorithms"],"tag_line":"N-Step-SCAN (also referred to as N-Step LOOK) is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests."}}
,{"_index":"throwtable","_type":"algorithm","_id":"proportional-share-scheduling","_score":0,"_source":{"description":"Proportional Share Scheduling is a type of scheduling which preallocates certain amount of CPU time to each of the processes. In a proportional share algorithm every job has a weight, and jobs receive a share of the available resources proportional to the weight of every job.","name":"Proportional share scheduling","categories":["All articles needing additional references","All stub articles","Articles needing additional references from December 2009","Computer science stubs","Processor scheduling algorithms"],"tag_line":"Proportional Share Scheduling is a type of scheduling which preallocates certain amount of CPU time to each of the processes."}}
,{"_index":"throwtable","_type":"algorithm","_id":"adaptive-projected-subgradient-method","_score":0,"_source":{"description":"The adaptive projected subgradient method (APSM) is an algorithm, the goal of which is to minimize iteratively a sequence of cost functions.\nThis algorithmic \"tool\" is general and has been used in several tasks, such as: online/adaptive parameter estimation, online classification,  adaptive distributed learning, just to name a few. The algorithm can be used in both linear and non-linear scenarios (using kernels).\n^ Yamada, I.; Ogura, N. (2003). \"Adaptive projected subgradient method and its applications to set theoretic adaptive filtering\". The Thirty-seventh Asilomar Conference on Signals, Systems & Computers, 2003. p. 600. doi:10.1109/ACSSC.2003.1291982. ISBN 0-7803-8104-1. \n^ Yamada, I.; Ogura, N. (2005). \"Adaptive Projected Subgradient Method for Asymptotic Minimization of Sequence of Nonnegative Convex Functions\". Numerical Functional Analysis and Optimization 25 (7–8): 593. doi:10.1081/NFA-200045806. \n^ Slavakis, Konstantinos; Yamada, Isao (5 August 2011). \"The adaptive projected subgradient method constrained by families of quasi-nonexpansive mappings and its application to online learning\". Ithaca, New York: Cornell University.\n^ Slavakis, Konstantinos, Sergios Theodoridis, and Isao Yamada. \"Online kernel-based classification using adaptive projection algorithms.\" Signal Processing, IEEE Transactions on 56.7 (2008): 2781-2796.\n^ OĞUZ, ÖZKAN. PERFORMANCE OF A NON-LINEAR ADAPTIVE BEAMFORMER ALGORITHM FOR SIGNAL-OF-INTEREST EXTRACTION. Diss. MIDDLE EAST TECHNICAL UNIVERSITY, 2015.\n^ Chouvardas, Symeon, Konstantinos Slavakis, and Sergios Theodoridis. \"Adaptive robust distributed learning in diffusion sensor networks.\" Signal Processing, IEEE Transactions on 59.10 (2011): 4692-4707.\n^ Slavakis, K.; Bouboulis, P.; Theodoridis, S. (2012-02-01). \"Adaptive Multiregression in Reproducing Kernel Hilbert Spaces: The Multiaccess MIMO Channel Case\". IEEE Transactions on Neural Networks and Learning Systems 23 (2): 260–276. doi:10.1109/TNNLS.2011.2178321. ISSN 2162-237X. \n^ Bouboulis, P.; Slavakis, K.; Theodoridis, S. (2012-03-01). \"Adaptive Learning in Complex Reproducing Kernel Hilbert Spaces Employing Wirtinger's Subgradients\". IEEE Transactions on Neural Networks and Learning Systems 23 (3): 425–438. doi:10.1109/TNNLS.2011.2179810. ISSN 2162-237X. \n^ Theodoridis, Sergios (2013). Academic Press Library in Signal Processing. CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces- Konstantinos Slavakis, Pantelis Bouboulis and Sergios Theodoridis: ACADEMIC PRESS. ISBN 978-0-12-397226-2. \n^ \"Source code for Kernel APSM\".","name":"Adaptive projected subgradient method","categories":["Algorithms and data structures stubs","All orphaned articles","All stub articles","Applied mathematics stubs","Computer science stubs","Estimation theory","Gradient methods","Machine learning","Orphaned articles from June 2013","Statistics stubs"],"tag_line":"The adaptive projected subgradient method (APSM) is an algorithm, the goal of which is to minimize iteratively a sequence of cost functions."}}
,{"_index":"throwtable","_type":"algorithm","_id":"incremental-heuristic-search","_score":0,"_source":{"description":"Incremental heuristic search algorithms combine both incremental and heuristic search to speed up searches of sequences of similar search problems, which is important in domains that are only incompletely known or change dynamically. Incremental search has been studied at least since the late 1960s. Incremental search algorithms reuse information from previous searches to speed up the current search and solve search problems potentially much faster than solving them repeatedly from scratch. Similarly, heuristic search has been studied at least since the late 1960s. Heuristic search algorithms, often based on A*, use heuristic knowledge in the form of approximations of the goal distances to focus the search and solve search problems potentially much faster than uninformed search algorithms. The resulting search problems, sometimes called dynamic path planning problems, are graph search problems where paths have to be found repeatedly because the topology of the graph, its edge costs, the start vertex or the goal vertices change over time.\nSo far, three main classes of incremental heuristic search algorithms have been developed:\nThe first class restarts A* at the point where its current search deviates from the previous one (example: Fringe Saving A*).\nThe second class updates the h-values from the previous search during the current search to make them more informed (example: Generalized Adaptive A*).\nThe third class updates the g-values from the previous search during the current search to correct them when necessary, which can be interpreted as transforming the A* search tree from the previous search into the A* search tree for the current search (examples: Lifelong Planning A*, D*, D* Lite).\nAll three classes of incremental heuristic search algorithms are different from other replanning algorithms, such as planning by analogy, in that their plan quality does not deteriorate with the number of replanning episodes.","name":"Incremental heuristic search","categories":["Artificial intelligence","Robot control","Search algorithms"],"tag_line":"Incremental heuristic search algorithms combine both incremental and heuristic search to speed up searches of sequences of similar search problems, which is important in domains that are only incompletely known or change dynamically."}}
,{"_index":"throwtable","_type":"algorithm","_id":"graphplan","_score":0,"_source":{"description":"Graphplan is an algorithm for automated planning developed by Avrim Blum and Merrick Furst in 1995. Graphplan takes as input a planning problem expressed in STRIPS and produces, if one is possible, a sequence of operations for reaching a goal state.\nThe name graphplan is due to the use of a novel planning graph, to reduce the amount of search needed to find the solution from straightforward exploration of the state space graph.\nIn the state space graph:\nthe nodes are possible states,\nand the edges indicate reachability through a certain action.\nOn the contrary, in Graphplan's planning graph:\nthe nodes are actions and atomic facts, arranged into alternate levels,\nand the edges are of two kinds:\nfrom an atomic fact to the actions for which it is a condition,\nfrom an action to the atomic facts it makes true or false.\n\nthe first level contains true atomic facts identifying the initial state.\nLists of incompatible facts that cannot be true at the same time and incompatible actions that cannot be executed together are also maintained.\nThe algorithm then iteratively extends the planning graph, proving that there are no solutions of length l-1 before looking for plans of length l by backward chaining: supposing the goals are true, Graphplan looks for the actions and previous states from which the goals can be reached, pruning as many of them as possible thanks to incompatibility information.\nA closely related approach to planning is the Planning as Satisfiability (Satplan). Both reduce the automated planning problem to search for plans of different fixed horizon lengths.","name":"Graphplan","categories":["Automated planning and scheduling","Search algorithms"],"tag_line":"Graphplan is an algorithm for automated planning developed by Avrim Blum and Merrick Furst in 1995."}}
,{"_index":"throwtable","_type":"algorithm","_id":"late-move-reductions","_score":0,"_source":{"description":"Late Move Reductions (LMR) is a non-game specific enhancement to the alpha-beta algorithm and its variants which attempts to examine a game search tree more efficiently. It uses the assumption that good game-specific move ordering causes a program to search the most likely moves early. If a cut-off is going to happen in a search, the first few moves are the ones most likely to cause them. In games like chess, most programs search winning captures and \"killers\" first. LMR will reduce the search depth for moves searched later at a given node. This allows the program to search deeper along the critical lines, and play better.\nMost chess programs will search the first several moves at a node to full depth. Often, they do not reduce moves considered to be very tactical, such as captures or promotions. If the score of the move at a reduced depth is smaller than the alpha, the move is assumed to be bad. However, if the score is larger than alpha, the reduced tells us nothing so we will have to do a full search (fail-low).\nThis search reduction can lead to a different search space than the pure alpha-beta method which can give different results. Care must be taken to select the reduction criteria or the search will miss some deep threats.","name":"Late Move Reductions","categories":["All articles lacking in-text citations","Articles lacking in-text citations from March 2014","Computer chess","Search algorithms"],"tag_line":"Late Move Reductions (LMR) is a non-game specific enhancement to the alpha-beta algorithm and its variants which attempts to examine a game search tree more efficiently."}}
,{"_index":"throwtable","_type":"algorithm","_id":"jump-search","_score":0,"_source":{"description":"In computer science, a jump search or block search refers to a search algorithm for ordered lists. It works by first checking all items Lkm, where  and m is the block size, until an item is found that is larger than the search key. To find the exact position of the search key in the list a linear search is performed on the sublist L[(k-1)m, km].\nThe optimal value of m is √n, where n is the length of the list L. Because both steps of the algorithm look at, at most, √n items the algorithm runs in O(√n) time. This is better than a linear search, but worse than a binary search. The advantage over the latter is that a jump search only needs to jump backwards once, while a binary can jump backwards up to log n times. This can be important if a jumping backwards takes significantly more time than jumping forward.\nThe algorithm can be modified by performing multiple levels of jump search on the sublists, before finally performing the linear search. For an k-level jump search the optimum block size ml for the lth level (counting from 1) is n(k-l)/k. The modified algorithm will perform k backward jumps and runs in O(kn1/(k+1)) time.","name":"Jump search","categories":["Search algorithms"],"tag_line":"In computer science, a jump search or block search refers to a search algorithm for ordered lists."}}
,{"_index":"throwtable","_type":"algorithm","_id":"linear-search","_score":0,"_source":{"description":"In computer science, linear search or sequential search is a method for finding a particular value in a list that checks each element in sequence until the desired element is found or the list is exhausted. The list need not be ordered.\nLinear search is the simplest search algorithm; it is a special case of brute-force search. Its worst case cost is proportional to the number of elements in the list. Its expected cost is also proportional to the number of elements if all elements are searched equally. If the list has more than a few elements and is searched often, then more complicated search methods such as binary search or hashing may be appropriate. Those methods have faster search times but require additional resources to attain that speed.","name":"Linear search","categories":["All articles needing additional references","Articles needing additional references from November 2010","Articles with example Java code","Articles with example pseudocode","Search algorithms"],"tag_line":"In computer science, linear search or sequential search is a method for finding a particular value in a list that checks each element in sequence until the desired element is found or the list is exhausted."}}
,{"_index":"throwtable","_type":"algorithm","_id":"baum–welch-algorithm","_score":0,"_source":{"description":"In electrical engineering, computer science, statistical computing and bioinformatics, the Baum–Welch algorithm is used to find the unknown parameters of a hidden Markov model (HMM). It makes use of the forward-backward algorithm and is named for Leonard E. Baum and Lloyd R. Welch.","name":"Baum–Welch algorithm","categories":["Bioinformatics algorithms","Markov models","Pages using citations with accessdate and no URL","Statistical algorithms"],"tag_line":"In electrical engineering, computer science, statistical computing and bioinformatics, the Baum–Welch algorithm is used to find the unknown parameters of a hidden Markov model (HMM)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"look-ahead-(backtracking)","_score":0,"_source":{"description":"In backtracking algorithms, look ahead is the generic term for a subprocedure that attempts to foresee the effects of choosing a branching variable to evaluate or one of its values. The two main aims of look-ahead are to choose a variable to evaluate next and the order of values to assign to it.","name":"Look-ahead (backtracking)","categories":["Constraint programming","Search algorithms"],"tag_line":"In backtracking algorithms, look ahead is the generic term for a subprocedure that attempts to foresee the effects of choosing a branching variable to evaluate or one of its values."}}
,{"_index":"throwtable","_type":"algorithm","_id":"glr-parser","_score":0,"_source":{"description":"A GLR parser (GLR standing for \"generalized LR\", where L stands for \"left-to-right\" and R stands for \"rightmost (derivation)\") is an extension of an LR parser algorithm to handle nondeterministic and ambiguous grammars. The theoretical foundation was provided in a 1974 paper by Bernard Lang (along with other general Context-Free parsers such as GLL). It describes a systematic way to produce such algorithms, and provides uniform results regarding correctness proofs, complexity with respect to grammar classes, and optimization techniques. The first actual implementation of GLR was described in a 1984 paper by Masaru Tomita, it has also been referred to as a \"parallel parser\". Tomita presented five stages in his original work, though in practice it is the second stage that is recognized as the GLR parser.\nThough the algorithm has evolved since its original forms, the principles have remained intact. As shown by an earlier publication, Lang was primarily interested in more easily used and more flexible parsers for extensible programming languages. Tomita's goal was to parse natural language text thoroughly and efficiently. Standard LR parsers cannot accommodate the nondeterministic and ambiguous nature of natural language, and the GLR algorithm can.","name":"GLR parser","categories":["All articles covered by WikiProject Wikify","All articles lacking in-text citations","All articles with unsourced statements","All pages needing cleanup","Articles covered by WikiProject Wikify from February 2015","Articles lacking in-text citations from May 2011","Articles with unsourced statements from May 2011","Parsing algorithms","Wikipedia introduction cleanup from February 2015"],"tag_line":"A GLR parser (GLR standing for \"generalized LR\", where L stands for \"left-to-right\" and R stands for \"rightmost (derivation)\") is an extension of an LR parser algorithm to handle nondeterministic and ambiguous grammars."}}
,{"_index":"throwtable","_type":"algorithm","_id":"simple-lr-parser","_score":0,"_source":{"description":"In computer science, a Simple LR or SLR parser is a type of LR parser with small parse tables and a relatively simple parser generator algorithm. As with other types of LR(1) parser, an SLR parser is quite efficient at finding the single correct bottom-up parse in a single left-to-right scan over the input stream, without guesswork or backtracking. The parser is mechanically generated from a formal grammar for the language.\nSLR and the more-general methods LALR parser and Canonical LR parser have identical methods and similar tables at parse time; they differ only in the mathematical grammar analysis algorithms used by the parser generator tool. SLR and LALR generators create tables of identical size and identical parser states. SLR generators accept fewer grammars than do LALR generators like yacc and Bison. Many computer languages don't readily fit the restrictions of SLR, as is. Bending the language's natural grammar into SLR grammar form requires more compromises and grammar hackery. So LALR generators have become much more widely used than SLR generators, despite being somewhat more complicated tools. SLR methods remain a useful learning step in college classes on compiler theory.\nSLR and LALR were both developed by Frank DeRemer as the first practical uses of Donald Knuth's LR parser theory. The tables created for real grammars by full LR methods were impractically large, larger than most computer memories of that decade, with 100 times or more parser states than the SLR and LALR methods.","name":"Simple LR parser","categories":["All articles lacking sources","All articles with unsourced statements","Articles lacking sources from December 2012","Articles with unsourced statements from January 2015","Articles with unsourced statements from June 2012","Parsing algorithms"],"tag_line":"In computer science, a Simple LR or SLR parser is a type of LR parser with small parse tables and a relatively simple parser generator algorithm."}}
,{"_index":"throwtable","_type":"algorithm","_id":"shunting-yard-algorithm","_score":0,"_source":{"description":"In computer science, the shunting-yard algorithm is a method for parsing mathematical expressions specified in infix notation. It can be used to produce output in Reverse Polish notation (RPN) or as an abstract syntax tree (AST). The algorithm was invented by Edsger Dijkstra and named the \"shunting yard\" algorithm because its operation resembles that of a railroad shunting yard. Dijkstra first described the Shunting Yard Algorithm in the Mathematisch Centrum report MR 34/61.\nLike the evaluation of RPN, the shunting yard algorithm is stack-based. Infix expressions are the form of mathematical notation most people are used to, for instance 3+4 or 3+4*(2−1). For the conversion there are two text variables (strings), the input and the output. There is also a stack that holds operators not yet added to the output queue. To convert, the program reads each symbol in order and does something based on that symbol.\nThe shunting-yard algorithm has been later generalized into operator-precedence parsing.","name":"Shunting-yard algorithm","categories":["All articles lacking in-text citations","Articles lacking in-text citations from August 2013","Dutch inventions","Parsing algorithms"],"tag_line":"In computer science, the shunting-yard algorithm is a method for parsing mathematical expressions specified in infix notation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"string-kernel","_score":0,"_source":{"description":"In machine learning and data mining, a string kernel is a kernel function that operates on strings, i.e. finite sequences of symbols that need not be of the same length. String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings a and b are, the higher the value of a string kernel K(a, b) will be.\nUsing string kernels with kernelized learning algorithms such as support vector machines allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued feature vectors. String kernels are used in domains where sequence data are to be clustered or classified, e.g. in text mining and gene analysis.","name":"String kernel","categories":["Algorithms on strings","Kernel methods for machine learning","Natural language processing","String similarity measures"],"tag_line":"In machine learning and data mining, a string kernel is a kernel function that operates on strings, i.e."}}
,{"_index":"throwtable","_type":"algorithm","_id":"fnn-algorithm","_score":0,"_source":{"description":"The false nearest neighbor algorithm is an algorithm for estimating the embedding dimension. The concept was proposed by Kennel et al. The main idea is to examine how the number of neighbors of a point along a signal trajectory change with increasing embedding dimension. In too low an embedding dimension, many of the neighbors will be false, but in an appropriate embedding dimension or higher, the neighbors are real. With increasing dimension, the false neighbors will no longer be neighbors. Therefore, by examining how the number of neighbors change as a function of dimension, an appropriate embedding can be determined.","name":"FNN algorithm","categories":["Algorithms and data structures stubs","All Wikipedia articles needing context","All pages needing cleanup","All stub articles","Computer science stubs","Dynamical systems","Nonlinear time series analysis","Statistical algorithms","Wikipedia articles needing context from October 2009","Wikipedia introduction cleanup from October 2009"],"tag_line":"The false nearest neighbor algorithm is an algorithm for estimating the embedding dimension."}}
,{"_index":"throwtable","_type":"algorithm","_id":"operator-precedence-parser","_score":0,"_source":{"description":"In computer science, an operator precedence parser is a bottom-up parser that interprets an operator-precedence grammar. For example, most calculators use operator precedence parsers to convert from the human-readable infix notation relying on order of operations to a format that is optimized for evaluation such as Reverse Polish notation (RPN).\nEdsger Dijkstra's shunting yard algorithm is commonly used to implement operator precedence parsers. Other algorithms include the precedence climbing method and the top down operator precedence method.","name":"Operator-precedence parser","categories":["All articles with unsourced statements","Articles with example C code","Articles with unsourced statements from November 2010","Parsing algorithms"],"tag_line":"In computer science, an operator precedence parser is a bottom-up parser that interprets an operator-precedence grammar."}}
,{"_index":"throwtable","_type":"algorithm","_id":"forest-fire-model","_score":0,"_source":{"description":"In applied mathematics, a forest-fire model is any of a number of dynamical systems displaying self-organized criticality. Note, however, that according to Pruessner et al. (2002, 2004) the forest-fire model does not behave critically on very large, i.e. physically relevant scales. Early versions go back to Henley (1989) and Drossel and Schwabl (1992). The model is defined as a cellular automaton on a grid with Ld cells. L is the sidelength of the grid and d is its dimension. A cell can be empty, occupied by a tree, or burning. The model of Drossel and Schwabl (1992) is defined by four rules which are executed simultaneously:\nA burning cell turns into an empty cell\nA tree will burn if at least one neighbor is burning\nA tree ignites with probability f even if no neighbor is burning\nAn empty space fills with a tree with probability p\nThe controlling parameter of the model is p/f which gives the average number of trees planted between two lightning strikes (see Schenk et al. (1996) and Grassberger (1993)). In order to exhibit a fractal frequency-size distribution of clusters a double separation of time scales is necessary\n\nwhere Tsmax is the burn time of the largest cluster. The scaling behavior is not simple, however ( Grassberger 1993,2002 and Pruessner et al. 2002,2004).\nA cluster is defined as a coherent set of cells, all of which have the same state. Cells are coherent if they can reach each other via nearest neighbor relations. In most cases, the von Neumann neighborhood (four adjacent cells) is considered.\nThe first condition  allows large structures to develop, while the second condition  keeps trees from popping up alongside a cluster while burning.\nIn landscape ecology, the forest fire model is used to illustrate the role of the fuel mosaic in the wildfire regime. The importance of the fuel mosaic on wildfire spread is debated. Parsimonious models such as the forest fire model can help to explore the role of the fuel mosaic and its limitations in explaining observed patterns.","alt_names":["Forest-fire_model"],"name":"Forest-fire model","categories":["Self-organization"],"tag_line":"In applied mathematics, a forest-fire model is any of a number of dynamical systems displaying self-organized criticality."}}
,{"_index":"throwtable","_type":"algorithm","_id":"serialization","_score":0,"_source":{"description":"In computer science, in the context of data storage, serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer, or transmitted across a network connection link) and reconstructed later in the same or another computer environment. When the resulting series of bits is reread according to the serialization format, it can be used to create a semantically identical clone of the original object. For many complex objects, such as those that make extensive use of references, this process is not straightforward. Serialization of object-oriented objects does not include any of their associated methods with which they were previously inextricably linked.\nThis process of serializing an object is also called marshalling an object. The opposite operation, extracting a data structure from a series of bytes, is deserialization (which is also called unmarshalling).","name":"Serialization","categories":["All articles needing style editing","All articles with links needing disambiguation","All articles with unsourced statements","Articles with example code","Articles with links needing disambiguation from December 2015","Articles with unsourced statements from March 2014","Data serialization formats","Data structures","Persistence","Wikipedia articles needing style editing from March 2012"],"tag_line":"In computer science, in the context of data storage, serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer, or transmitted across a network connection link) and reconstructed later in the same or another computer environment."}}
,{"_index":"throwtable","_type":"algorithm","_id":"mandelbrot-set","_score":0,"_source":{"description":"The Mandelbrot set is the set of complex numbers c for which the sequence (c, c² + c, (c²+c)² + c, ((c²+c)²+c)² + c, (((c²+c)²+c)²+c)² + c, …) does not approach infinity. The set is closely related to Julia sets (which include similarly complex shapes) and is named after the mathematician Benoit Mandelbrot, who studied and popularized it. Mandelbrot set images are made by sampling complex numbers and determining for each whether the result tends towards infinity when a particular mathematical operation is iterated on it. Treating the real and imaginary parts of each number as image coordinates, pixels are colored according to how rapidly the sequence diverges, if at all.\nMore precisely, the Mandelbrot set is the set of values of c in the complex plane for which the orbit of 0 under iteration of the complex quadratic polynomial\n\nremains bounded. That is, a complex number c is part of the Mandelbrot set if, when starting with z0 = 0 and applying the iteration repeatedly, the absolute value of zn remains bounded however large n gets. This can also be represented as\n\nFor example, letting c = 1 gives the sequence 0, 1, 2, 5, 26,…, which tends to infinity. As this sequence is unbounded, 1 is not an element of the Mandelbrot set. On the other hand, c = −1 gives the sequence 0, −1, 0, −1, 0,…, which is bounded, and so −1 belongs to the Mandelbrot set.\nImages of the Mandelbrot set display an elaborate boundary that reveals progressively ever-finer recursive detail at increasing magnifications. The \"style\" of this repeating detail depends on the region of the set being examined. The set's boundary also incorporates smaller versions of the main shape, so the fractal property of self-similarity applies to the entire set, and not just to its parts.\nThe Mandelbrot set has become popular outside mathematics both for its aesthetic appeal and as an example of a complex structure arising from the application of simple rules, and is one of the best-known examples of mathematical visualization.\n^ \"Mandelbrot Set Explorer: Mathematical Glossary\". Retrieved 2007-10-07. \n^ \"Escape Radius, Mu-Ency at MROB\". Retrieved 2015-10-21.","alt_names":["Mandelbrot_set"],"name":"Mandelbrot set","categories":["Articles containing video clips","Articles with DMOZ links","Articles with example pseudocode","Complex dynamics","Fractals","Wikipedia articles needing clarification from May 2010"],"tag_line":"The Mandelbrot set is the set of complex numbers c for which the sequence (c, c² + c, (c²+c)² + c, ((c²+c)²+c)² + c, (((c²+c)²+c)²+c)² + c, …) does not approach infinity."}}
,{"_index":"throwtable","_type":"algorithm","_id":"balanced-ternary","_score":0,"_source":{"description":"Balanced ternary is a non-standard positional numeral system (a balanced form), useful for comparison logic. While it is a ternary (base 3) number system, in the standard (unbalanced) ternary system, digits have values 0, 1 and 2. The digits in the balanced ternary system have values −1, 0, and 1.\nDifferent sources use different glyphs used to represent the three digits in balanced ternary. In this article, T (which resembles a ligature of the minus sign and 1) represents −1, while 0 and 1 represent themselves. Other conventions include using '−' and '+' to represent −1 and 1 respectively, or using Greek letter theta (Θ), which resembles a minus sign in a circle, to represent −1.\nIn Setun printings, −1 is represented as overturned 1: \"1\".","alt_names":["Balanced_ternary"],"name":"Balanced ternary","categories":["CS1 Russian-language sources (ru)","Computer arithmetic","Non-standard positional numeral systems","Ternary computers"],"tag_line":"Balanced ternary is a non-standard positional numeral system (a balanced form), useful for comparison logic."}}
,{"_index":"throwtable","_type":"algorithm","_id":"josephus-problem","_score":0,"_source":{"description":"In computer science and mathematics, the Josephus Problem (or Josephus permutation) is a theoretical problem related to a certain counting-out game.\nPeople are standing in a circle waiting to be executed. Counting begins at a specified point in the circle and proceeds around the circle in a specified direction. After a specified number of people are skipped, the next person is executed. The procedure is repeated with the remaining people, starting with the next person, going in the same direction and skipping the same number of people, until only one person remains, and is freed.\nThe problem — given the number of people, starting point, direction, and number to be skipped — is to choose the position in the initial circle to avoid execution.","alt_names":["Josephus_problem"],"name":"Josephus problem","categories":["Combinatorics","Computational problems","Mathematical problems","Pages using citations with accessdate and no URL","Permutations"],"tag_line":"In computer science and mathematics, the Josephus Problem (or Josephus permutation) is a theoretical problem related to a certain counting-out game."}}
,{"_index":"throwtable","_type":"algorithm","_id":"euler-method","_score":0,"_source":{"description":"In mathematics and computational science, the Euler method is a SN-order numerical procedure for solving ordinary differential equations (ODEs) with a given initial value. It is the most basic explicit method for numerical integration of ordinary differential equations and is the simplest Runge–Kutta method. The Euler method is named after Leonhard Euler, who treated it in his book Institutionum calculi integralis (published 1768–70).\nThe Euler method is a first-order method, which means that the local error (error per step) is proportional to the square of the step size, and the global error (error at a given time) is proportional to the step size. The Euler method often serves as the basis to construct more complex methods.","alt_names":["Euler_method"],"name":"Euler method","categories":["All articles needing expert attention","All articles that are too technical","Articles needing expert attention from May 2015","First order methods","Numerical differential equations","Runge–Kutta methods","Wikipedia articles that are too technical from May 2015"],"tag_line":"In mathematics and computational science, the Euler method is a SN-order numerical procedure for solving ordinary differential equations (ODEs) with a given initial value."}}
,{"_index":"throwtable","_type":"algorithm","_id":"kaprekar-number","_score":0,"_source":{"description":"In mathematics, a Kaprekar number for a given base is a non-negative integer, the representation of whose square in that base can be split into two parts that add up to the original number again. For instance, 45 is a Kaprekar number, because 45² = 2025 and 20+25 = 45. The Kaprekar numbers are named after D. R. Kaprekar.","alt_names":["Kaprekar_number"],"name":"Kaprekar number","categories":["Base-dependent integer sequences"],"tag_line":"In mathematics, a Kaprekar number for a given base is a non-negative integer, the representation of whose square in that base can be split into two parts that add up to the original number again."}}
,{"_index":"throwtable","_type":"algorithm","_id":"environment-variable","_score":0,"_source":{"description":"Environment variables are a set of dynamic named values that can affect the way running processes will behave on a computer.\nThey are part of the environment in which a process runs. For example, a running process can query the value of the TEMP environment variable to discover a suitable location to store temporary files, or the HOME or USERPROFILE variable to find the directory structure owned by the user running the process.\nThey were introduced in their modern form in 1979 with Version 7 Unix, so are included in all Unix operating system flavors and variants from that point onward including Linux and OS X. From PC DOS 2.0 in 1982, all succeeding Microsoft operating systems including Microsoft Windows, and OS/2 also have included them as a feature, although with somewhat different syntax, usage and standard variable names.","alt_names":["Environment_variable"],"name":"Environment variable","categories":["All articles to be expanded","All articles with empty sections","All articles with unsourced statements","Articles to be expanded from August 2014","Articles with empty sections from August 2014","Articles with unsourced statements from August 2014","CS1 German-language sources (de)","Environment variables","Operating system technology"],"tag_line":"Environment variables are a set of dynamic named values that can affect the way running processes will behave on a computer."}}
,{"_index":"throwtable","_type":"algorithm","_id":"parametric-polymorphism","_score":0,"_source":{"description":"In programming languages and type theory, parametric polymorphism is a way to make a language more expressive, while still maintaining full static type-safety. Using parametric polymorphism, a function or a data type can be written generically so that it can handle values identically without depending on their type. Such functions and data types are called generic functions and generic datatypes respectively and form the basis of generic programming.\nFor example, a function append that joins two lists can be constructed so that it does not care about the type of elements: it can append lists of integers, lists of real numbers, lists of strings, and so on. Let the type variable a denote the type of elements in the lists. Then append can be typed forall a. [a] × [a] -> [a], where [a] denotes the type of lists with elements of type a. We say that the type of append is parameterized by a for all values of a. (Note that since there is only one type variable, the function cannot be applied to just any pair of lists: the pair, as well as the result list, must consist of the same type of elements.) For each place where append is applied, a value is decided for a.\nFollowing Christopher Strachey, parametric polymorphism may be contrasted with ad hoc polymorphism, in which a single polymorphic function can have a number of distinct and potentially heterogeneous implementations depending on the type of argument(s) to which it is applied. Thus, ad hoc polymorphism can generally only support a limited number of such distinct types, since a separate implementation has to be provided for each type.\n^ Pierce 2002.\n^ Strachey 1967.","alt_names":["Parametric_Polymorphism"],"name":"Parametric polymorphism","categories":["All articles to be expanded","Articles to be expanded from November 2013","CS1 French-language sources (fr)","Generic programming","Pages with citations lacking titles","Polymorphism (computer science)","Type theory"],"tag_line":"In programming languages and type theory, parametric polymorphism is a way to make a language more expressive, while still maintaining full static type-safety."}}
,{"_index":"throwtable","_type":"algorithm","_id":"combination","_score":0,"_source":{"description":"In mathematics, a combination is a way of selecting items from a collection, such that (unlike permutations) the order of selection does not matter. In smaller cases it is possible to count the number of combinations. For example, given three fruits, say an apple, an orange and a pear, there are three combinations of two that can be drawn from this set: an apple and a pear; an apple and an orange; or a pear and an orange. More formally, a k-combination of a set S is a subset of k distinct elements of S. If the set has n elements, the number of k-combinations is equal to the binomial coefficient\n\nwhich can be written using factorials as  whenever , and which is zero when . The set of all k-combinations of a set S is sometimes denoted by .\nCombinations refer to the combination of n things taken k at a time without repetition. To refer to combinations in which repetition is allowed, the terms k-selection, k-multiset, or k-combination with repetition are often used. If, in the above example, it was possible to have two of any one kind of fruit there would be 3 more 2-selections: one with two apples, one with two oranges, and one with two pears.\nAlthough the set of three fruits was small enough to write a complete list of combinations, with large sets this becomes impractical. For example, a poker hand can be described as a 5-combination (k = 5) of cards from a 52 card deck (n = 52). The 5 cards of the hand are all distinct, and the order of cards in the hand does not matter. There are 2,598,960 such combinations, and the chance of drawing any one hand at random is 1 / 2,598,960.","name":"Combination","categories":["All articles with unsourced statements","Articles with unsourced statements from April 2012","CS1 Chinese-language sources (zh)","Combinatorics"],"tag_line":"In mathematics, a combination is a way of selecting items from a collection, such that (unlike permutations) the order of selection does not matter."}}
,{"_index":"throwtable","_type":"algorithm","_id":"linear-congruential-generator","_score":0,"_source":{"description":"A linear congruential generator (LCG) is an algorithm that yields a sequence of pseudo-randomized numbers calculated with a discontinuous piecewise linear equation. The method represents one of the oldest and best-known pseudorandom number generator algorithms. The theory behind them is relatively easy to understand, and they are easily implemented and fast, especially on computer hardware which can provide modulo arithmetic by storage-bit truncation.\nThe generator is defined by the recurrence relation:\n\nwhere  is the sequence of pseudorandom values, and\n – the \"modulus\"\n – the \"multiplier\"\n – the \"increment\"\n – the \"seed\" or \"start value\"\nare integer constants that specify the generator. If c = 0, the generator is often called a multiplicative congruential generator (MCG), or Lehmer RNG. If c ≠ 0, the method is called a mixed congruential generator.","alt_names":["linear_congruential_generator"],"name":"Linear congruential generator","categories":["Modular arithmetic","Pseudorandom number generators"],"tag_line":"A linear congruential generator (LCG) is an algorithm that yields a sequence of pseudo-randomized numbers calculated with a discontinuous piecewise linear equation."}}
,{"_index":"throwtable","_type":"algorithm","_id":"morse-code","_score":0,"_source":{"description":"Morse code is a method of transmitting text information as a series of on-off tones, lights, or clicks that can be directly understood by a skilled listener or observer without special equipment. The International Morse Code encodes the ISO basic Latin alphabet, some extra Latin letters, the Arabic numerals and a small set of punctuation and procedural signals (prosigns) as standardized sequences of short and long signals called \"dots\" and \"dashes\", or \"dits\" and \"dahs\", as in amateur radio practice. Because many non-English natural languages use more than the 26 Roman letters, extensions to the Morse alphabet exist for those languages.\nEach Morse code symbol represents either a text character (letter or numeral) or a prosign and is represented by a unique sequence of dots and dashes. The duration of a dash is three times the duration of a dot. Each dot or dash is followed by a short silence, equal to the dot duration. The letters of a word are separated by a space equal to three dots (one dash), and the words are separated by a space equal to seven dots. The dot duration is the basic unit of time measurement in code transmission. To increase the speed of the communication, the code was designed so that the length of each character in Morse varies approximately inversely to its frequency of occurrence in English. Thus the most common letter in English, the letter \"E\", has the shortest code, a single dot.\nMorse code is used by some amateur radio operators, although knowledge of and proficiency with it is no longer required for licensing in most countries. Pilots and air traffic controllers usually need only a cursory understanding. Aeronautical navigational aids, such as VORs and NDBs, constantly identify in Morse code. Compared to voice, Morse code is less sensitive to poor signal conditions, yet still comprehensible to humans without a decoding device. Morse is therefore a useful alternative to synthesized speech for sending automated data to skilled listeners on voice channels. Many amateur radio repeaters, for example, identify with Morse, even though they are used for voice communications.\n\nIn an emergency, Morse code can be sent by improvised methods that can be easily \"keyed\" on and off, making it one of the simplest and most versatile methods of telecommunication. The most common distress signal is SOS or three dots, three dashes and three dots, internationally recognized by treaty.\n\n","alt_names":["Morse_code"],"name":"Morse code","categories":["All articles with dead external links","All articles with unsourced statements","American inventions","Articles with DMOZ links","Articles with dead external links from June 2013","Articles with hAudio microformats","Articles with unsourced statements from July 2011","Articles with unsourced statements from October 2015","Latin-alphabet representations","Listen template using plain parameter","Morse code","Pages with content uneditable in VisualEditor","Wikipedia pages semi-protected against vandalism","Wikipedia protected pages without expiry"],"tag_line":"Morse code is a method of transmitting text information as a series of on-off tones, lights, or clicks that can be directly understood by a skilled listener or observer without special equipment."}}
,{"_index":"throwtable","_type":"algorithm","_id":"dragon-curve","_score":0,"_source":{"description":"A dragon curve is any member of a family of self-similar fractal curves, which can be approximated by recursive methods such as Lindenmayer systems.","alt_names":["dragon_curve"],"name":"Dragon curve","categories":["Fractal curves","Pages containing cite templates with deprecated parameters","Paper folding"],"tag_line":"A dragon curve is any member of a family of self-similar fractal curves, which can be approximated by recursive methods such as Lindenmayer systems."}}
,{"_index":"throwtable","_type":"algorithm","_id":"pernicious-number","_score":0,"_source":{"description":"In number theory, a pernicious number is a positive integer where the Hamming weight (or digit sum) of its binary representation is prime.","alt_names":["Pernicious_number"],"name":"Pernicious number","categories":["Number theory"],"tag_line":"In number theory, a pernicious number is a positive integer where the Hamming weight (or digit sum) of its binary representation is prime."}}
,{"_index":"throwtable","_type":"algorithm","_id":"currying","_score":0,"_source":{"description":"In mathematics and computer science, currying is the technique of translating the evaluation of a function that takes multiple arguments (or a tuple of arguments) into evaluating a sequence of functions, each with a single argument. It was introduced by Moses Schönfinkel and later developed by Haskell Curry.\nUncurrying is the dual transformation to currying, and can be seen as a form of defunctionalization. It takes a function f(x) that returns another function g(y) as a result, and yields a new function f′(x, y) that takes a number of additional parameters and applies them to the function returned by function f. The process can be iterated.\n\n","name":"Currying","categories":["Articles with example Java code","Articles with inconsistent citation formats","Functional programming","Higher-order functions","Lambda calculus"],"tag_line":"In mathematics and computer science, currying is the technique of translating the evaluation of a function that takes multiple arguments (or a tuple of arguments) into evaluating a sequence of functions, each with a single argument."}}
,{"_index":"throwtable","_type":"algorithm","_id":"machin-like-formula","_score":0,"_source":{"description":"In mathematics, Machin-like formulae are a popular technique for computing π to a large number of digits. They are generalizations of John Machin's formula from 1706:\n\nwhich he used to compute π to 100 decimal places.\nMachin-like formulas have the form:\n\nWhere  and  are positive integers such that ,  is a signed non-zero integer, and  is a positive integer.\nThese formulae are used in conjunction with the Taylor series expansion for arctangent:","alt_names":["Machin-like_formula"],"name":"Machin-like formula","categories":["All articles needing additional references","Articles needing additional references from October 2011","CS1 French-language sources (fr)","Pi algorithms"],"tag_line":"In mathematics, Machin-like formulae are a popular technique for computing π to a large number of digits."}}
,{"_index":"throwtable","_type":"algorithm","_id":"tic-tac-toe","_score":0,"_source":{"description":"Tic-tac-toe (also known as Noughts and crosses or Xs and Os) is a paper-and-pencil game for two players, X and O, who take turns marking the spaces in a 3×3 grid. The player who succeeds in placing three of their marks in a horizontal, vertical, or diagonal row wins the game.\nThe following example game is won by the first player, X:\n\nPlayers soon discover that best play from both parties leads to a draw. Hence, Tic-tac-toe is most often played by young children.\nBecause of the simplicity of Tic-tac-toe, it is often used as a pedagogical tool for teaching the concepts of good sportsmanship and the branch of artificial intelligence that deals with the searching of game trees. It is straightforward to write a computer program to play Tic-tac-toe perfectly, to enumerate the 765 essentially different positions (the state space complexity), or the 26,830 possible games up to rotations and reflections (the game tree complexity) on this space.\nThe game can be generalized to an m,n,k-game in which two players alternate placing stones of their own color on an m×n board, with the goal of getting k of their own color in a row. Tic-tac-toe is the (3,3,3)-game.","name":"Tic-tac-toe","categories":["Abstract strategy games","All articles with dead external links","Articles with dead external links from November 2015","Commons category with local link same as on Wikidata","Mathematical games","Paper-and-pencil games","Tic-tac-toe"],"tag_line":"Tic-tac-toe (also known as Noughts and crosses or Xs and Os) is a paper-and-pencil game for two players, X and O, who take turns marking the spaces in a 3×3 grid."}}
,{"_index":"throwtable","_type":"algorithm","_id":"system-time","_score":0,"_source":{"description":"In computer science and computer programming, system time represents a computer system's notion of the passing of time. In this sense, time also includes the passing of days on the calendar.\nSystem time is measured by a system clock, which is typically implemented as a simple count of the number of ticks that have transpired since some arbitrary starting date, called the epoch. For example, Unix and POSIX-compliant systems encode system time (\"Unix time\") as the number of seconds elapsed since the start of the Unix epoch at 1 January 1970 00:00:00 UT, with exceptions for leap seconds. Systems that implement the 32-bit and 64-bit versions of the Windows API, such as Windows 9x and Windows NT, provide the system time as both SYSTEMTIME, represented as a year/month/day/hour/minute/second/milliseconds value, and FILETIME, represented as a count of the number of 100-nanosecond ticks since 1 January 1601 00:00:00 UT as reckoned in the proleptic Gregorian calendar.\nSystem time can be converted into calendar time, which is a form more suitable for human comprehension. For example, the Unix system time 1000000000 seconds since the beginning of the epoch translates into the calendar time 9 September 2001 01:46:40 UT. Library subroutines that handle such conversions may also deal with adjustments for timezones, daylight saving time (DST), leap seconds, and the user's locale settings. Library routines are also generally provided that convert calendar times into system times.\n\n","alt_names":["System_time"],"name":"System time","categories":["Computer programming","Computer real-time clocks","Operating system technology"],"tag_line":"In computer science and computer programming, system time represents a computer system's notion of the passing of time."}}
,{"_index":"throwtable","_type":"algorithm","_id":"voronoi-diagram","_score":0,"_source":{"description":"In mathematics, a Voronoi diagram is a partitioning of a plane into regions based on distance to points in a specific subset of the plane. That set of points (called seeds, sites, or generators) is specified beforehand, and for each seed there is a corresponding region consisting of all points closer to that seed than to any other. These regions are called Voronoi cells. The Voronoi diagram of a set of points is dual to its Delaunay triangulation. Put simply, it's a diagram created by taking pairs of points that are close together and drawing a line that is equidistant between them and perpendicular to the line connecting them. That is, all points on the lines in the diagram are equidistant to the nearest two (or more) source points.\nIt is named after Georgy Voronoy, and is also called a Voronoi tessellation, a Voronoi decomposition, a Voronoi partition, or a Dirichlet tessellation (after Peter Gustav Lejeune Dirichlet). Voronoi diagrams have practical and theoretical applications to a large number of fields, mainly in science and technology but also including visual art.","alt_names":["Voronoi_diagram"],"name":"Voronoi diagram","categories":["Commons category with local link same as on Wikidata","Computational geometry","Diagrams","Discrete geometry","Russian inventions","Ukrainian inventions"],"tag_line":"In mathematics, a Voronoi diagram is a partitioning of a plane into regions based on distance to points in a specific subset of the plane."}}
,{"_index":"throwtable","_type":"algorithm","_id":"sierpinski-triangle","_score":0,"_source":{"description":"The Sierpinski triangle (also with the original orthography Sierpiński), also called the Sierpinski gasket or the Sierpinski Sieve, is a fractal and attractive fixed set with the overall shape of an equilateral triangle, subdivided recursively into smaller equilateral triangles. Originally constructed as a curve, this is one of the basic examples of self-similar sets, i.e., it is a mathematically generated pattern that can be reproducible at any magnification or reduction. It is named after the Polish mathematician Wacław Sierpiński but appeared as a decorative pattern many centuries prior to the work of Sierpiński.","alt_names":["Sierpinski_triangle"],"name":"Sierpinski triangle","categories":["CS1 maint: Explicit use of et al.","Cellular automaton patterns","Commons category with local link same as on Wikidata","Curves","Factorial and binomial topics","Fractals","Science and technology in Poland","Topological spaces","Triangles","Wikipedia articles with GND identifiers"],"tag_line":"The Sierpinski triangle (also with the original orthography Sierpiński), also called the Sierpinski gasket or the Sierpinski Sieve, is a fractal and attractive fixed set with the overall shape of an equilateral triangle, subdivided recursively into smaller equilateral triangles."}}
,{"_index":"throwtable","_type":"algorithm","_id":"roman-numerals","_score":0,"_source":{"description":"Roman numerals, the numeric system used in ancient Rome, employs combinations of letters from the Latin alphabet to signify values. The numbers 1 to 10 can be expressed in Roman numerals as follows:\nI, II, III, IV, V, VI, VII, VIII, IX, X.\nThe Roman numeral system is a cousin of Etruscan numerals. Use of Roman numerals continued after the decline of the Roman Empire. From the 14th century on, Roman numerals began to be replaced in most contexts by more convenient Hindu-Arabic numerals; however, this process was gradual, and the use of Roman numerals in some minor applications continues to this day.","alt_names":["Roman_numerals"],"name":"Roman numerals","categories":["Articles containing Dutch-language text","Articles containing French-language text","Articles containing Latin-language text","Articles containing Russian-language text","Articles containing Spanish-language text","CS1 French-language sources (fr)","Commons category with page title same as on Wikidata","Latin alphabet","Numeral systems","Numerals","Roman mathematics","Use mdy dates from May 2012"],"tag_line":"Roman numerals, the numeric system used in ancient Rome, employs combinations of letters from the Latin alphabet to signify values."}}
,{"_index":"throwtable","_type":"algorithm","_id":"stable-marriage-problem","_score":0,"_source":{"description":"In mathematics, economics, and computer science, the stable marriage problem (also stable matching problem or SMP) is the problem of finding a stable matching between two equally sized sets of elements given an ordering of preferences for each element. A matching is a mapping from the elements of one set to the elements of the other set. A matching is stable whenever it is not the case that both the following conditions hold.\n\nIn other words, a matching is stable when there does not exist any match (A, B) by which both A and B are individually better off than they would be with the element to which they are currently matched.\nThe stable marriage problem has been stated as follows:\nGiven n men and n women, where each person has ranked all members of the opposite sex in order of preference, marry the men and women together such that there are no two people of opposite sex who would both rather have each other than their current partners. When there are no such pairs of people, the set of marriages is deemed stable.\nNote that the requirement that the marriages be heterosexual distinguishes this problem from the stable roommates problem.","alt_names":["Stable_marriage_problem"],"name":"Stable marriage problem","categories":["Combinatorics","Cooperative games","Game theory","Matching","Mathematical problems"],"tag_line":"In mathematics, economics, and computer science, the stable marriage problem (also stable matching problem or SMP) is the problem of finding a stable matching between two equally sized sets of elements given an ordering of preferences for each element."}}
,{"_index":"throwtable","_type":"algorithm","_id":"universal-turing-machine","_score":0,"_source":{"description":"In computer science, a universal Turing machine (UTM) is a Turing machine that can simulate an arbitrary Turing machine on arbitrary input. The universal machine essentially achieves this by reading both the description of the machine to be simulated as well as the input thereof from its own tape. Alan Turing introduced this machine in 1936–1937. This model is considered by some (for example, Martin Davis (2000)) to be the origin of the stored program computer—used by John von Neumann (1946) for the \"Electronic Computing Instrument\" that now bears von Neumann's name: the von Neumann architecture. It is also known as universal computing machine, universal machine (UM), machine U, U.\nIn terms of computational complexity, a multi-tape universal Turing machine need only be slower by logarithmic factor compared to the machines it simulates.","alt_names":["Universal_Turing_machine"],"name":"Universal Turing machine","categories":["Pages using duplicate arguments in template calls","Turing machine","Use dmy dates from September 2010"],"tag_line":"In computer science, a universal Turing machine (UTM) is a Turing machine that can simulate an arbitrary Turing machine on arbitrary input."}}
,{"_index":"throwtable","_type":"algorithm","_id":"catamorphism","_score":0,"_source":{"description":"In category theory, the concept of catamorphism (from Greek: κατά = downwards or according to; μορφή = form or shape) denotes the unique homomorphism from an initial algebra into some other algebra.\nIn functional programming, catamorphisms provide generalizations of folds of lists to arbitrary algebraic data types, which can be described as initial algebras. The dual concept is that of anamorphism that generalize unfolds. See also Hylomorphism.","name":"Catamorphism","categories":["Category theory","Functional programming","Iteration in programming","Morphisms","Recursion schemes"],"tag_line":"In category theory, the concept of catamorphism (from Greek: κατά = downwards or according to; μορφή = form or shape) denotes the unique homomorphism from an initial algebra into some other algebra."}}
,{"_index":"throwtable","_type":"algorithm","_id":"longest-common-subsequence-problem","_score":0,"_source":{"description":"The longest common subsequence (LCS) problem is the problem of finding the longest subsequence common to all sequences in a set of sequences (often just two sequences). It differs from problems of finding common substrings: unlike substrings, subsequences are not required to occupy consecutive positions within the original sequences. The longest common subsequence problem is a classic computer science problem, the basis of data comparison programs such as the diff utility, and has applications in bioinformatics. It is also widely used by revision control systems such as Git for reconciling multiple changes made to a revision-controlled collection of files.","alt_names":["Longest_common_subsequence"],"name":"Longest common subsequence problem","categories":["All articles needing additional references","Articles needing additional references from March 2013","Combinatorics","Dynamic programming","NP-complete problems","Polynomial-time problems","Problems on strings"],"tag_line":"The longest common subsequence (LCS) problem is the problem of finding the longest subsequence common to all sequences in a set of sequences (often just two sequences)."}}
,{"_index":"throwtable","_type":"algorithm","_id":"abstract-syntax-notation-one","_score":0,"_source":{"description":"Abstract Syntax Notation One (ASN.1) is a standard and notation that describes rules and structures for representing, encoding, transmitting, and decoding data in telecommunications and computer networking. The formal rules enable representation of objects that are independent of machine-specific encoding techniques. Formal notation makes it possible to automate the task of validating whether a specific instance of data representation abides by the specifications. In other words, software tools can be used for the validation.\nASN.1 is a joint standard of the International Organization for Standardization (ISO), International Electrotechnical Commission (IEC), and International Telecommunication Union Telecommunication Standardization Sector ITU-T, originally defined in 1984 as part of CCITT X.409:1984. ASN.1 moved to its own standard, X.208, in 1988 due to wide applicability. The substantially revised 1995 version is covered by the X.680 series. The latest available version is dated 2008, and is backward compatible with the 1995 version.","alt_names":["Abstract_Syntax_Notation_One"],"name":"Abstract Syntax Notation One","categories":["All articles lacking in-text citations","All articles with unsourced statements","Articles containing explicitly cited English-language text","Articles lacking in-text citations from May 2009","Articles with unsourced statements from September 2011","Data modeling languages","Data serialization formats","ITU-T recommendations"],"tag_line":"Abstract Syntax Notation One (ASN.1) is a standard and notation that describes rules and structures for representing, encoding, transmitting, and decoding data in telecommunications and computer networking."}}
,{"_index":"throwtable","_type":"algorithm","_id":"interactive-programming","_score":0,"_source":{"description":"Interactive programming is the procedure of writing parts of a program while it is already active. This focuses on the program text as the main interface for a running process, rather than an interactive application, where the program is designed in development cycles and used thereafter (usually by a so-called \"user\", in distinction to the \"developer\"). Consequently, here, the activity of writing a program becomes part of the program itself.\nIt thus forms a specific instance of interactive computation as an extreme opposite to batch processing, where neither writing the program nor its use happens in an interactive way. The principle of rapid feedback in Extreme Programming is radicalized and becomes more explicit.\nSynonyms: Live coding, on-the-fly-programming, just in time programming, conversational programming","alt_names":["Interactive_programming"],"name":"Interactive programming","categories":["Programming paradigms"],"tag_line":"Interactive programming is the procedure of writing parts of a program while it is already active."}}
,{"_index":"throwtable","_type":"algorithm","_id":"day","_score":0,"_source":{"description":"A day is a unit of time. In common usage, it is an interval equal to 24 hours. It also can mean the consecutive period of time during which the Sun is above the horizon, also known as daytime. The period of time during which the Earth completes one rotation with respect to the Sun is called a solar day.\nSeveral definitions of this universal human concept are used according to context, need and convenience. In 1967, the second was redefined in terms of the wavelength of light, and was designated the SI base unit of time. The unit of measurement \"day\", redefined in 1967 as 86 400 SI seconds and symbolized d, is not an SI unit, but is accepted for use with SI. A civil day is usually also 86 400 seconds, plus or minus a possible leap second in Coordinated Universal Time (UTC), and, in some locations, occasionally plus or minus an hour when changing from or to daylight saving time. The word day may also refer to a day of the week or to a calendar date, as in answer to the question \"On which day?\" The life patterns of humans and many other species are related to Earth's solar day and the day-night cycle (see circadian rhythms).\nIn recent decades the average length of a solar day on Earth has been about 86 400.002 seconds (24.000 000 6 hours) and there are about 365.242 2 solar days in one mean tropical year. Because celestial orbits are not perfectly circular, and thus objects travel at different speeds at various positions in their orbit, a solar day is not the same length of time throughout the orbital year. A day, understood as the span of time it takes for the Earth to make one entire rotation with respect to the celestial background or a distant star (assumed to be fixed), is called a stellar day. This period of rotation is about 4 minutes less than 24 hours (23 hours 56 minutes and 4.1 seconds) and there are about 366.242 2 stellar days in one mean tropical year (one stellar day more than the number of solar days). Mainly due to tidal effects, the Earth's rotational period is not constant, resulting in further minor variations for both solar days and stellar \"days\". Other planets and moons also have stellar and solar days.","name":"Day","categories":["All articles with unsourced statements","Articles with unsourced statements from October 2015","Day","Orders of magnitude (time)","Wikipedia articles that may have off-topic sections","Wikipedia articles with GND identifiers","Wikipedia indefinitely move-protected pages"],"tag_line":"A day is a unit of time."}}
,{"_index":"throwtable","_type":"algorithm","_id":"playing-card","_score":0,"_source":{"description":"A playing card is a piece of specially prepared heavy paper, thin cardboard, plastic-coated paper, cotton-paper blend, or thin plastic, marked with distinguishing motifs and used as one of a set for playing card games. Playing cards are typically palm-sized for convenient handling.\nA complete set of cards is called a pack (UK English), deck (US English), or set (Universal), and the subset of cards held at one time by a player during a game is commonly called a hand. A pack of cards may be used for playing a variety of card games, with varying elements of skill and chance, some of which are played for money (e.g., poker and blackjack games at a casino). Playing cards are also used for illusions, cardistry, building card structures, cartomancy and memory sport.\nThe front (or \"face\") of each card carries markings that distinguish it from the other cards in the pack and determine its use under the rules of the game being played. The back of each card is identical for all cards in any particular pack, and usually of a single colour or formalized design. Usually every card will be smooth; however, some packs have braille to allow blind people to read the card number and suit. The backs of playing cards are sometimes used for advertising. For most games, the cards are assembled into a pack mechanically in an unvarying sequence, so their order must be randomized by shuffling when play begins.\nDedicated deck card games have sets that are used only for a specific game. The cards described in this article are used for many games and share a common origin stemming from the standards set in Mamluk Egypt. These sets divide their cards into four suits each consisting of three face cards and numbered or \"pip\" cards.\n\n","alt_names":["Playing-cards"],"name":"Playing card","categories":["All articles with unsourced statements","Articles containing Chinese-language text","Articles with unsourced statements from December 2011","Articles with unsourced statements from November 2012","CS1 Italian-language sources (it)","Chinese inventions","Game equipment","History of card decks","Paper products","Playing cards","Tarot","Wikipedia articles with GND identifiers"],"tag_line":"A playing card is a piece of specially prepared heavy paper, thin cardboard, plastic-coated paper, cotton-paper blend, or thin plastic, marked with distinguishing motifs and used as one of a set for playing card games."}}
,{"_index":"throwtable","_type":"algorithm","_id":"minesweeper-(video-game)","_score":0,"_source":{"description":"Minesweeper is a single-player puzzle video game. The objective of the game is to clear a rectangular board containing hidden \"mines\" without detonating any of them, with help from clues about the number of neighboring mines in each field. The game originates from the 1960s, and has been written for many computing platforms in use today. It has many variations and offshoots.","alt_names":["Minesweeper_(computer_game)"],"name":"Minesweeper (video game)","categories":["1989 video games","All articles with unsourced statements","Articles with DMOZ links","Articles with unsourced statements from December 2011","Articles with unsourced statements from March 2014","Linux games","Minesweeper (video game)","NP-complete problems","Puzzle video games","Use dmy dates from September 2010","Wikipedia indefinitely move-protected pages","Windows games"],"tag_line":"Minesweeper is a single-player puzzle video game."}}
,{"_index":"throwtable","_type":"algorithm","_id":".net-framework","_score":0,"_source":{"description":".NET Framework (pronounced dot net) is a software framework developed by Microsoft that runs primarily on Microsoft Windows. It includes a large class library known as Framework Class Library (FCL) and provides language interoperability (each language can use code written in other languages) across several programming languages. Programs written for .NET Framework execute in a software environment (as contrasted to hardware environment), known as Common Language Runtime (CLR), an application virtual machine that provides services such as security, memory management, and exception handling. FCL and CLR together constitute .NET Framework.\nFCL provides user interface, data access, database connectivity, cryptography, web application development, numeric algorithms, and network communications. Programmers produce software by combining their own source code with .NET Framework and other libraries. .NET Framework is intended to be used by most new applications created for the Windows platform. Microsoft also produces an integrated development environment largely for .NET software called Visual Studio.\n.NET Framework started out as a proprietary framework, although the company worked to standardize the software stack almost immediately, even before its first release. Despite the standardization efforts, developers—particularly those in the free and open-source software communities—expressed their uneasiness with the selected terms and the prospects of any free and open-source implementation, especially with regard to software patents. Since then, Microsoft has changed .NET development to more closely follow a contemporary model of a community-developed software project, including issuing an update to its patent that promises to address the concerns.\n.NET Framework family also includes two versions for mobile or embedded device use. A reduced version of the framework, .NET Compact Framework, is available on Windows CE platforms, including Windows Mobile devices such as smartphones. Additionally, .NET Micro Framework is targeted at severely resource-constrained devices.","alt_names":[".NET_Framework"],"name":".NET Framework","categories":[".NET Framework","2002 software","All articles with unsourced statements","Articles with unsourced statements from February 2015","Articles with unsourced statements from June 2013","Computing platforms","Cross-platform software","Microsoft application programming interfaces","Microsoft development tools","Use dmy dates from October 2014"],"tag_line":".NET Framework (pronounced dot net) is a software framework developed by Microsoft that runs primarily on Microsoft Windows."}}
,{"_index":"throwtable","_type":"algorithm","_id":"1896-summer-olympics","_score":0,"_source":{"description":"The 1896 Summer Olympics (Modern Greek: Θερινοί Ολυμπιακοί Αγώνες 1896, Therinoí Olympiakoí Agó̱nes 1896), officially known as the Games of the I Olympiad, was a multi-sport event held in Athens, Greece, from 6 to 15 April 1896. It was the first international Olympic Games held in the Modern era. Because Ancient Greece was the birthplace of the Olympic Games, Athens was considered to be an appropriate choice to stage the inaugural modern Games. It was unanimously chosen as the host city during a congress organised by Pierre de Coubertin, a French pedagogue and historian, in Paris, on 23 June 1894. The International Olympic Committee (IOC) was also instituted during this congress.\nDespite many obstacles and setbacks, the 1896 Olympics were regarded as a great success. The Games had the largest international participation of any sporting event to that date. The Panathinaiko Stadium, the only Olympic stadium used in the 1800s, overflowed with the largest crowd ever to watch a sporting event. The highlight for the Greeks was the marathon victory by their compatriot Spyridon Louis. The most successful competitor was German wrestler and gymnast Carl Schuhmann, who won four events.\nAfter the Games, Coubertin and the IOC were petitioned by several prominent figures, including Greece's King George and some of the American competitors in Athens, to hold all the following Games in Athens. However, the 1900 Summer Olympics were already planned for Paris and, except for the Intercalated Games of 1906, the Olympics did not return to Greece until the 2004 Summer Olympics, 108 years later.","alt_names":["1896_Summer_Olympics"],"name":"1896 Summer Olympics","categories":["1896 Summer Olympics","1896 in Greece","1896 in Greek sport","1896 in multi-sport events","19th century in Athens","Articles containing Greek-language text","CS1 Spanish-language sources (es)","Commons category with local link same as on Wikidata","EngvarB from April 2014","Featured articles","History of Greece (1863–1909)","International sports competitions hosted by Greece","Olympic Games in Greece","Sports events in Athens","Summer Olympic Games","Use dmy dates from February 2015","Wikipedia articles with GND identifiers","Wikipedia articles with LCCN identifiers","Wikipedia articles with VIAF identifiers","Wikipedia indefinitely move-protected pages"],"tag_line":"The 1896 Summer Olympics (Modern Greek: Θερινοί Ολυμπιακοί Αγώνες 1896, Therinoí Olympiakoí Agó̱nes 1896), officially known as the Games of the I Olympiad, was a multi-sport event held in Athens, Greece, from 6 to 15 April 1896."}}
,{"_index":"throwtable","_type":"algorithm","_id":"superman-red/superman-blue","_score":0,"_source":{"description":"Superman Red/Superman Blue refers to two different comic book storylines published by DC Comics featuring Superman.\n\n","alt_names":["Superman_Red/Superman_Blue"],"name":"Superman Red/Superman Blue","categories":["1963 in comics","1998 in comics","Comics caption less artist","Comics infobox image less alt text","Crossover comics","Single issue indices","Story arc pop","Superman storylines"],"tag_line":"Superman Red/Superman Blue refers to two different comic book storylines published by DC Comics featuring Superman.\n\n"}}
]

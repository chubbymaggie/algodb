[
{"_index":"throwtable","_type":"category","_id":"computer-algebra","_score":0,"_source":{"description":"In computational mathematics, computer algebra, also called symbolic computation or algebraic computation is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although, properly speaking, computer algebra should be a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have not any given value and are thus manipulated as symbols (therefore the name of symbolic computation).\nSoftware applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.\nAt the beginning of computer algebra, circa 1970, when the long-known algorithms were first put on computers, they turned out to be highly inefficient. Therefore, a large part of the work of the researchers in the field consisted in revisiting classical algebra in order to make it effective and to discover efficient algorithms to implement this effectiveness. A typical example of this kind of work is the computation of polynomial greatest common divisors, which is required to simplify fractions. Surprisingly, the classical Euclid's algorithm turned out to be inefficient for polynomials over infinite fields, and thus new algorithms needed to be developed. The same was also true for the classical algorithms from linear algebra.\nComputer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs. It is also used for complete scientific computations, when purely numerical methods fail, like in public key cryptography or for some non-linear problems.\n^ Kaltofen, Erich (1982), \"Factorization of polynomials\", in Buchberger, B.; Loos, R.; Collins, G., Computer Algebra, Springer Verlag, CiteSeerX: 10.1.1.39.7916","tag_line":"In computational mathematics, computer algebra, also called symbolic computation or algebraic computation is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects.","algorithms":["bareiss-algorithm","berlekamp–zassenhaus-algorithm","faugère's-f4-and-f5-algorithms","pollard's-kangaroo-algorithm"],"name":"Computer algebra","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"bioinformatics-algorithms","_score":0,"_source":{"description":"Algorithms used in bioinformatics.","tag_line":"Algorithms used in bioinformatics.","algorithms":["baum–welch-algorithm","blast","blast2go","flower-pollination-algorithm","hirschberg's-algorithm","island-algorithm","kabsch-algorithm","needleman–wunsch-algorithm","neighbor-joining","pairwise-algorithm","sequential-pattern-mining","smith–waterman-algorithm","spades-(software)","uclust","ukkonen's-algorithm","upgma","velvet-assembler"],"name":"Bioinformatics algorithms","children":["genetic-algorithms","sequence-alignment-algorithms","substring-indices"]}}
,{"_index":"throwtable","_type":"category","_id":"checksum-algorithms","_score":0,"_source":{"description":"This is a list of hash functions, including cyclic redundancy checks, checksum functions, and cryptographic hash functions.","tag_line":"This is a list of hash functions, including cyclic redundancy checks, checksum functions, and cryptographic hash functions.","algorithms":["checksum","adler-32","bsd-checksum","damm-algorithm","fletcher's-checksum","iso-7064","luhn-algorithm","luhn-mod-n-algorithm","md5","sha-1","sha-2","sysv-checksum","verhoeff-algorithm"],"name":"Checksum algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"graph-algorithms","_score":0,"_source":{"description":"The following is a list of algorithms along with one-line descriptions for each.\n\n","tag_line":"The following is a list of algorithms along with one-line descriptions for each.\n\n","algorithms":["knuth's-simpath-algorithm","a*-search-algorithm","algorithmic-version-for-szemerédi-regularity-partition","alpha–beta-pruning","b*","barabási–albert-model","belief-propagation","bellman–ford-algorithm","bidirectional-search","borůvka's-algorithm","bottleneck-traveling-salesman-problem","breadth-first-search","bron–kerbosch-algorithm","chaitin's-algorithm","christofides-algorithm","clique-percolation-method","color-coding","contraction-hierarchies","courcelle's-theorem","cuthill–mckee-algorithm","d*","depth-first-search","depth-limited-search","dijkstra's-algorithm","dijkstra–scholten-algorithm","dinic's-algorithm","disparity-filter-algorithm-of-weighted-network","edmonds'-algorithm","blossom-algorithm","edmonds–karp-algorithm","euler-tour-technique","fkt-algorithm","flooding-algorithm","floyd–warshall-algorithm","force-directed-graph-drawing","ford–fulkerson-algorithm","fringe-search","girvan–newman-algorithm","goal-node-(computer-science)","graph-kernel","havel–hakimi-algorithm","hierarchical-clustering-of-networks","hopcroft–karp-algorithm","iterative-deepening-a*","iterative-compression","johnson's-algorithm","journal-of-graph-algorithms-and-applications","jump-point-search","junction-tree-algorithm","k-shortest-path-routing","karger's-algorithm","kleitman–wang-algorithms","kosaraju's-algorithm","kruskal's-algorithm","lexicographic-breadth-first-search","misra-&-gries-edge-coloring-algorithm","nearest-neighbour-algorithm","network-simplex-algorithm","path-based-strong-component-algorithm","prim's-algorithm","proof-number-search","push–relabel-maximum-flow-algorithm","reverse-delete-algorithm","rocha–thatte-cycle-detection-algorithm","sethi–ullman-algorithm","shortest-path-faster-algorithm","sma*","spectral-layout","suurballe's-algorithm","tarjan's-off-line-lowest-common-ancestors-algorithm","tarjan's-strongly-connected-components-algorithm","topological-sorting","travelling-salesman-problem","tree-traversal","widest-path-problem","yen's-algorithm"],"name":"Graph algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"asymmetric-key-algorithms","_score":0,"_source":{"description":"Public-key cryptography refers to a set of cryptographic algorithms that are based on mathematical problems that currently admit no efficient solution -- particularly those inherent in certain integer factorization, discrete logarithm, and elliptic curve relationships. It is computationally easy for a user to generate a public and private key-pair and to use it for encryption and decryption. The strength lies in the \"impossibility\" (computational impracticality) for a properly generated private key to be determined from its corresponding public key. Thus the public key may be published without compromising security. Security depends only on keeping the private key private. Public key algorithms, unlike symmetric key algorithms, do not require a secure channel for the initial exchange of one (or more) secret keys between the parties.\nBecause of the computational complexity of asymmetric encryption, it is typically only used for short messages, typically the transfer of a symmetric encryption key. This symmetric key is then used to encrypt the rest of the potentially long & heavy conversation. The symmetric encryption/decryption is based on simpler algorithms and is much faster.\nMessage authentication involves hashing the message to produce a \"digest,\" and encrypting the digest with the private key to produce a digital signature. Thereafter anyone can verify this signature by (1) computing the hash of the message, (2) decrypting the signature with the signer's public key, and (3) comparing the computed digest with the decrypted digest. Equality between the digests confirms the message is unmodified since it was signed, and that the signer, and no one else, intentionally performed the signature operation — presuming the signer's private key has remained secret to the signer. The security of such procedure depends on a hash algorithm of such quality that it is computationally impossible to alter or find a substitute message that produces the same digest - but studies have shown that even with the MD5 and SHA-1 algorithms, producing an altered or substitute message is not impossible. The current hashing standard for encryption is SHA-2. The message itself can also be used in place of the digest.\nPublic-key algorithms are fundamental security ingredients in cryptosystems, applications and protocols. They underpin various Internet standards, such as Transport Layer Security (TLS), S/MIME, PGP, and GPG. Some public key algorithms provide key distribution and secrecy (e.g., Diffie–Hellman key exchange), some provide digital signatures (e.g., Digital Signature Algorithm), and some provide both (e.g., RSA).\nPublic-key cryptography finds application in, amongst others, the IT security discipline information security. Information security (IS) is concerned with all aspects of protecting electronic information assets against security threats. Public-key cryptography is used as a method of assuring the confidentiality, authenticity and non-repudiability of electronic communications and data storage.","tag_line":"Public-key cryptography refers to a set of cryptographic algorithms that are based on mathematical problems that currently admit no efficient solution -- particularly those inherent in certain integer factorization, discrete logarithm, and elliptic curve relationships.","algorithms":["coppersmith-method","discrete-logarithm-records","three-pass-protocol","three-pass-protocol","primality-test","schoof's-algorithm","schoof–elkies–atkin-algorithm","three-pass-protocol","three-pass-protocol","xtr"],"name":"Asymmetric-key algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"distributed-algorithms","_score":0,"_source":{"description":"A distributed algorithm is an algorithm designed to run on computer hardware constructed from interconnected processors. Distributed algorithms are used in many varied application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation.\nDistributed algorithms are a sub-type of parallel algorithm, typically executed concurrently, with separate parts of the algorithm being run simultaneously on independent processors, and having limited information about what the other parts of the algorithm are doing. One of the major challenges in developing and implementing distributed algorithms is successfully coordinating the behavior of the independent parts of the algorithm in the face of processor failures and unreliable communications links. The choice of an appropriate distributed algorithm to solve a given problem depends on both the characteristics of the problem, and characteristics of the system the algorithm will run on such as the type and probability of processor or link failures, the kind of inter-process communication that can be performed, and the level of timing synchronization between separate processes.","tag_line":"A distributed algorithm is an algorithm designed to run on computer hardware constructed from interconnected processors.","algorithms":["distributed-algorithm","berkeley-algorithm","bully-algorithm","cannon's-algorithm","chandra–toueg-consensus-algorithm","chang-and-roberts-algorithm","cristian's-algorithm","distributed-minimum-spanning-tree","edge-chasing","hs-algorithm","lamport-timestamps","local-algorithm","logical-clock","parallel-algorithm","parallel-tebd","paxos-(computer-science)","raft-(computer-science)","ricart–agrawala-algorithm","snapshot-algorithm","suzuki-kasami-algorithm","synchronizer-(algorithm)","vector-clock","verification-based-message-passing-algorithms-in-compressed-sensing"],"name":"Distributed algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"cryptographically-secure-pseudorandom-number-generators","_score":0,"_source":{"description":"ISAAC (indirection, shift, accumulate, add, and count) is a cryptographically secure pseudorandom number generator and a stream cipher designed by Robert J. Jenkins Jr. in 1996.\n^ Robert J. Jenkins Jr., ISAAC. Fast Software Encryption 1996, pp41–49.","tag_line":"ISAAC (indirection, shift, accumulate, add, and count) is a cryptographically secure pseudorandom number generator and a stream cipher designed by Robert J. Jenkins Jr. in 1996.","algorithms":["dual-ec-drbg"],"name":"Cryptographically secure pseudorandom number generators","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"integer-factorization-algorithms","_score":0,"_source":{"description":"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.\nWhen the numbers are very large, no efficient, non-quantum integer factorization algorithm is known; an effort by several researchers concluded in 2009, factoring a 232-digit number (RSA-768), utilizing hundreds of machines over a span of two years. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.\nNot all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.\nMany cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure.\n\n","tag_line":"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers.","algorithms":["integer-factorization","algebraic-group-factorisation-algorithm","congruence-of-squares","continued-fraction-factorization","dixon's-factorization-method","euler's-factorization-method","factor-base","general-number-field-sieve","lattice-sieving","lenstra-elliptic-curve-factorization","pollard's-p-−-1-algorithm","pollard's-rho-algorithm","quadratic-sieve","quantum-algorithm-for-linear-systems-of-equations","rational-sieve","rsa-factoring-challenge","rsa-numbers","shanks'-square-forms-factorization","shor's-algorithm","special-number-field-sieve","trial-division","williams'-p-+-1-algorithm"],"name":"Integer factorization algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"discrete-transforms","_score":0,"_source":{"description":"In signal processing, discrete transforms are mathematical transforms, often linear transforms, of signals between discrete domains, such as between discrete time and discrete frequency.\nMany common integral transforms used in signal processing have their discrete counterparts. For example, for the Fourier transform the counterpart is the discrete Fourier transform.\nIn addition to spectral analysis of signals, discrete transforms play important role in data compression, signal detection, digital filtering and correlation analysis.\nTransforms between a discrete domain and a continuous domain are not discrete transforms. For example, the discrete-time Fourier transform and the Z-transform, from discrete time to continuous frequency, and the Fourier series, from continuous time to discrete frequency, are outside the class of discrete transforms.\nClassical signal processing deals with one-dimensional discrete transforms. Other application areas, such as image processing, computer vision, high definition television, visual telephony, etc. make use of two-dimensional and in general, multidimensional discrete transforms.","tag_line":"In signal processing, discrete transforms are mathematical transforms, often linear transforms, of signals between discrete domains, such as between discrete time and discrete frequency.","algorithms":["cyclotomic-fast-fourier-transform","fast-fourier-transform"],"name":"Discrete transforms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"genetic-programming","_score":0,"_source":{"description":"In artificial intelligence, genetic programming (GP) is an evolutionary algorithm-based methodology inspired by biological evolution to find computer programs that perform a user-defined task. Essentially GP is a set of instructions and a fitness function to measure how well a computer has performed a task. It is a specialization of genetic algorithms (GA) where each individual is a computer program. It is a machine learning technique used to optimize a population of computer programs according to a fitness landscape determined by a program's ability to perform a given computational task.","tag_line":"In artificial intelligence, genetic programming (GP) is an evolutionary algorithm-based methodology inspired by biological evolution to find computer programs that perform a user-defined task.","algorithms":["genetic-programming","gene-expression-programming","santa-fe-trail-problem","schema-(genetic-algorithms)"],"name":"Genetic programming","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"geometric-algorithms","_score":0,"_source":{"description":"This category deals with algorithms in geometry. See also \"Computational geometry\".","tag_line":"This category deals with algorithms in geometry.","algorithms":["bounding-sphere","bowyer–watson-algorithm","bregman-divergence","centroidal-voronoi-tessellation","cgal","closest-pair-of-points-problem","cone-algorithm","criss-cross-algorithm","euclidean-shortest-path","geometric-design","geometric-modeling","gilbert–johnson–keerthi-distance-algorithm","jts-topology-suite","largest-empty-rectangle","line-segment-intersection","linear-programming","list-of-numerical-computational-geometry-topics","lloyd's-algorithm","midpoint-circle-algorithm","minkowski-portal-refinement","möller–trumbore-intersection-algorithm","nesting-algorithm","planar-straight-line-graph","proximity-problems","prune-and-search","ramer–douglas–peucker-algorithm","shoelace-formula","stencil-jumping","sweep-line-algorithm","symmetrization-methods","velocity-obstacle"],"name":"Geometric algorithms","children":["computer-graphics-algorithms","computer-aided-design","digital-geometry","mesh-generation","researchers-in-geometric-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"matrix-multiplication-algorithms","_score":0,"_source":{"description":"In computer science, Cannon's algorithm is a distributed algorithm for matrix multiplication for two-dimensional meshes first described in 1969 by Lynn Elliot Cannon.\nIt is especially suitable for computers laid out in an N × N mesh. While Cannon's algorithm works well in homogeneous 2D grids, extending it to heterogeneous 2D grids has been shown to be difficult.\nThe main advantage of the algorithm is that its storage requirements remain constant and are independent of the number of processors.\nThe Scalable Universal Matrix Multiplication Algorithm (SUMMA) is a more practical algorithm that requires less workspace and overcomes the need for a square 2D grid. It is used by the ScaLAPACK, PLAPACK, and Elemental libraries.","tag_line":"In computer science, Cannon's algorithm is a distributed algorithm for matrix multiplication for two-dimensional meshes first described in 1969 by Lynn Elliot Cannon.","algorithms":["matrix-multiplication-algorithm","cache-oblivious-matrix-multiplication","cannon's-algorithm","coppersmith–winograd-algorithm","freivalds'-algorithm","strassen-algorithm"],"name":"Matrix multiplication algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"numerical-analysis","_score":0,"_source":{"description":"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).\nOne of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection (YBC 7289), which gives a sexagesimal numerical approximation of , the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in astronomy, carpentry and construction.\nNumerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of , modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\nNumerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st century also the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.\nBefore the advent of modern computers numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.","tag_line":"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).","algorithms":["fee-method","interval-contractor","jenkins–traub-algorithm","level-set-method","minimax-approximation-algorithm","predictor–corrector-method"],"name":"Numerical analysis","children":["mesh-generation","mathematical-optimization","monte-carlo-methods","numerical-analysts","numerical-differential-equations","numerical-linear-algebra","numerical-software","polynomials","root-finding-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"pseudorandom-number-generators","_score":0,"_source":{"description":"A pseudorandom number generator (PRNG), also known as a deterministic random bit generator (DRBG), is an algorithm for generating a sequence of numbers whose properties approximate the properties of sequences of random numbers. The PRNG-generated sequence is not truly random, because it is completely determined by a relatively small set of initial values, called the PRNG's seed (which may include truly random values). Although sequences that are closer to truly random can be generated using hardware random number generators, pseudorandom number generators are important in practice for their speed in number generation and their reproducibility.\nPRNGs are central in applications such as simulations (e.g. for the Monte Carlo method), electronic games (e.g. for procedural generation), and cryptography. Cryptographic applications require the output not to be predictable from earlier outputs, and more elaborate algorithms, which do not inherit the linearity of simpler PRNGs, are needed.\nGood statistical properties are a central requirement for the output of a PRNG. In general, careful mathematical analysis is required to have any confidence that a PRNG generates numbers that are sufficiently close to random to suit the intended use. John von Neumann cautioned about the misinterpretation of a PRNG as a truly random generator, and joked that \"Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.\"","tag_line":"A pseudorandom number generator (PRNG), also known as a deterministic random bit generator (DRBG), is an algorithm for generating a sequence of numbers whose properties approximate the properties of sequences of random numbers.","algorithms":["dual-ec-drbg","feedback-with-carry-shift-registers","ziggurat-algorithm"],"name":"Pseudorandom number generators","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"phonetic-algorithms","_score":0,"_source":{"description":"Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English. The goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. Soundex is the most widely known of all phonetic algorithms (in part because it is a standard feature of popular database software such as DB2, PostgreSQL, MySQL, Ingres, MS SQL Server and Oracle) and is often used (incorrectly) as a synonym for \"phonetic algorithm\". Improvements to Soundex are the basis for many modern phonetic algorithms.","tag_line":"Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English.","algorithms":["phonetic-algorithm","caverphone","daitch–mokotoff-soundex","match-rating-approach","metaphone","new-york-state-identification-and-intelligence-system","soundex"],"name":"Phonetic algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"pseudo-polynomial-time-algorithms","_score":0,"_source":{"description":"In computational complexity theory, a numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input, but is exponential in the length of the input – the number of bits required to represent it.\nAn NP-complete problem with known pseudo-polynomial time algorithms is called weakly NP-complete. An NP-complete problem is called strongly NP-complete if it is proven that it cannot be solved by a pseudo-polynomial time algorithm unless P=NP. The strong/weak kinds of NP-hardness are defined analogously.\n\n","tag_line":"In computational complexity theory, a numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input, but is exponential in the length of the input – the number of bits required to represent it.","algorithms":["pseudo-polynomial-time"],"name":"Pseudo-polynomial time algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"fixed-points-(mathematics)","_score":0,"_source":{"description":"In mathematics, a fixed point (sometimes shortened to fixpoint, also known as an invariant point) of a function is an element of the function's domain that is mapped to itself by the function. That is to say, c is a fixed point of the function f(x) if and only if f(c) = c. This means f(f(...f(c)...)) = fn(c) = c, an important terminating consideration when recursively computing f. A set of fixed points is sometimes called a fixed set.\nFor example, if f is defined on the real numbers by\n\nthen 2 is a fixed point of f, because f(2) = 2.\nNot all functions have fixed points: for example, if f is a function defined on the real numbers as f(x) = x + 1, then it has no fixed points, since x is never equal to x + 1 for any real number. In graphical terms, a fixed point means the point (x, f(x)) is on the line y = x, or in other words the graph of f has a point in common with that line.\nPoints which come back to the same value after a finite number of iterations of the function are known as periodic points; a fixed point is a periodic point with period equal to one. In projective geometry, a fixed point of a projectivity has been called a double point.","tag_line":"In mathematics, a fixed point (sometimes shortened to fixpoint, also known as an invariant point) of a function is an element of the function's domain that is mapped to itself by the function.","algorithms":["cycle-detection"],"name":"Fixed points (mathematics)","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"recursion","_score":0,"_source":{"description":"Recursion is the process of repeating items in a self-similar way. For instance, when the surfaces of two mirrors are exactly parallel with each other, the nested images that occur are a form of infinite recursion. The term has a variety of meanings specific to a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, in which it refers to a method of defining functions in which the function being defined is applied within its own definition. Specifically, this defines an infinite number of instances (function values), using a finite expression that for some instances may refer to other instances, but in such a way that no loop or infinite chain of references can occur. The term is also used more generally to describe a process of repeating objects in a self-similar way.","tag_line":"Recursion is the process of repeating items in a self-similar way.","algorithms":["tree-traversal"],"name":"Recursion","children":["fixed-points-(mathematics)","recurrence-relations"]}}
,{"_index":"throwtable","_type":"category","_id":"recurrence-relations","_score":0,"_source":{"description":"In mathematics, a recurrence relation is an equation that recursively defines a sequence or multidimensional array of values, once one or more initial terms are given: each further term of the sequence or array is defined as a function of the preceding terms.\nThe term difference equation sometimes (and for the purposes of this article) refers to a specific type of recurrence relation. However, \"difference equation\" is frequently used to refer to any recurrence relation.","tag_line":"In mathematics, a recurrence relation is an equation that recursively defines a sequence or multidimensional array of values, once one or more initial terms are given: each further term of the sequence or array is defined as a function of the preceding terms.","algorithms":["master-theorem"],"name":"Recurrence relations","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"stochastic-algorithms","_score":0,"_source":{"algorithms":["estimation-of-distribution-algorithm","least-mean-squares-filter","randomized-algorithm","simultaneous-perturbation-stochastic-approximation","stochastic-computing","stochastic-diffusion-search","stochastic-universal-sampling"],"name":"Stochastic algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"internet-search-algorithms","_score":0,"_source":{"description":"Web indexing (or Internet indexing) refers to various methods for indexing the contents of a website or of the Internet as a whole. Individual websites or intranets may use a back-of-the-book index, while search engines usually use keywords and metadata to provide a more useful vocabulary for Internet or onsite searching. With the increase in the number of periodicals that have articles online, web indexing is also becoming important for periodical websites.\nBack-of-the-book-style web indexes may be called \"web site A-Z indexes\". The implication with \"A-Z\" is that there is an alphabetical browse view or interface. This interface differs from that of a browse through layers of hierarchical categories (also known as a taxonomy) which are not necessarily alphabetical, but are also found on some web sites. Although an A-Z index could be used to index multiple sites, rather than the multiple pages of a single site, this is unusual.\nMetadata web indexing involves assigning keywords or phrases to web pages or web sites within a metadata tag (or \"meta-tag\") field, so that the web page or web site can be retrieved with a search engine that is customized to search the keywords field. This may or may not involve using keywords restricted to a controlled vocabulary list. This method is commonly used by search engine indexing.","tag_line":"Web indexing (or Internet indexing) refers to various methods for indexing the contents of a website or of the Internet as a whole.","algorithms":["focused-crawler","pagerank"],"name":"Internet search algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"calendar-algorithms","_score":0,"_source":{"description":"Algorithms for calculation of certain calendar dates.","tag_line":"Algorithms for calculation of certain calendar dates.","algorithms":["astronomical-algorithm","calendrical-calculation","determination-of-the-day-of-the-week","doomsday-rule","top-nodes-algorithm","zeller's-congruence"],"name":"Calendar algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"computational-geometry","_score":0,"_source":{"description":"Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with history stretching back to antiquity. An ancient precursor is the Sanskrit treatise Shulba Sutras , or \"Rules of the Chord\", that is a book of algorithms written in 800 BCE. The book prescribes step-by-step procedures for constructing geometric objects like altars using a peg and chord.\nComputational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation.\nThe main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.\nOther important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), computer vision (3D reconstruction).\nThe main branches of computational geometry are:\nCombinatorial computational geometry, also called algorithmic geometry, which deals with geometric objects as discrete entities. A groundlaying book in the subject by Preparata and Shamos dates the first use of the term \"computational geometry\" in this sense by 1975.\nNumerical computational geometry, also called machine geometry, computer-aided geometric design (CAGD), or geometric modeling, which deals primarily with representing real-world objects in forms suitable for computer computations in CAD/CAM systems. This branch may be seen as a further development of descriptive geometry and is often considered a branch of computer graphics or CAD. The term \"computational geometry\" in this meaning has been in use since 1971.","tag_line":"Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry.","algorithms":["coreset","euclidean-shortest-path","polynomial-time-algorithm-for-approximating-the-volume-of-convex-bodies"],"name":"Computational geometry","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"combinatorial-optimization","_score":0,"_source":{"description":"In applied mathematics and theoretical computer science, combinatorial optimization is a topic that consists of finding an optimal object from a finite set of objects. In many such problems, exhaustive search is not feasible. It operates on the domain of those optimization problems, in which the set of feasible solutions is discrete or can be reduced to discrete, and in which the goal is to find the best solution. Some common problems involving combinatorial optimization are the traveling salesman problem (\"TSP\") and the minimum spanning tree problem (\"MST\").\nCombinatorial optimization is a subset of mathematical optimization that is related to operations research, algorithm theory, and computational complexity theory. It has important applications in several fields, including artificial intelligence, machine learning, mathematics, auction theory, and software engineering.\nSome research literature considers discrete optimization to consist of integer programming together with combinatorial optimization (which in turn is composed of optimization problems dealing with graph structures) although all of these topics have closely intertwined research literature. It often involves determining the way to efficiently allocate resources used to find solutions to mathematical problems.","tag_line":"In applied mathematics and theoretical computer science, combinatorial optimization is a topic that consists of finding an optimal object from a finite set of objects.","algorithms":["submodular-set-function","a*-search-algorithm","b*","bottleneck-traveling-salesman-problem","branch-and-bound","branch-and-cut","combinatorial-search","criss-cross-algorithm","dijkstra's-algorithm","harmony-search","job-shop-scheduling","kernighan–lin-algorithm","lin–kernighan-heuristic"],"name":"Combinatorial optimization","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"sorting-algorithms","_score":0,"_source":{"description":"A sorting algorithm is an algorithm that puts elements of a list in a certain order. The most-used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the use of other algorithms (such as search and merge algorithms) which require input data to be in sorted lists; it is also often useful for canonicalizing data and for producing human-readable output. More formally, the output must satisfy two conditions:\nThe output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);\nThe output is a permutation (reordering) of the input.\nFurther, the data is often taken to be in an array, which allows random access, rather than a list, which only allows sequential access, though often algorithms can be applied with suitable modification to either type of data.\nSince the dawn of computing, the sorting problem has attracted a great deal of research, perhaps due to the complexity of solving it efficiently despite its simple, familiar statement. For example, bubble sort was analyzed as early as 1956. A fundamental limit of comparison sorting algorithms is that they require linearithmic time – O(n log n) – in the worst case, though better performance is possible on real-world data (such as almost-sorted data), and algorithms not based on comparisons, such as counting sort, can have better performance. Although many consider sorting a solved problem – asymptotically optimal algorithms have been known since the mid-20th century – useful new algorithms are still being invented, with the now widely used Timsort dating to 2002, and the library sort being first published in 2006.\nSorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, time-space tradeoffs, and upper and lower bounds.","tag_line":"A sorting algorithm is an algorithm that puts elements of a list in a certain order.","algorithms":["sorting-algorithm","adaptive-heap-sort","adaptive-sort","bead-sort","binary-prioritization","bitonic-sorter","block-sort","bogosort","bubble-sort","bucket-sort","cache-oblivious-distribution-sort","cartesian-tree","cocktail-sort","comb-sort","comparison-sort","counting-sort","cubesort","cycle-sort","dutch-national-flag-problem","elevator-algorithm","external-sorting","flashsort","funnelsort","gnome-sort","heapsort","insertion-sort","integer-sorting","internal-sort","library-sort","median-cut","merge-algorithm","merge-sort","odd–even-sort","pancake-sorting","pigeonhole-sort","polyphase-merge-sort","proxmap-sort","qsort","quantum-sort","quicksort","radix-sort","samplesort","schwartzian-transform","selection-sort","smoothsort","sort-(c++)","sorting-network","spaghetti-sort","splaysort","spreadsort","stooge-sort","timsort","topological-sorting","tournament-sort","tree-sort","x-+-y-sorting"],"name":"Sorting algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"combinatorial-algorithms","_score":0,"_source":{"algorithms":["bees-algorithm","criss-cross-algorithm","cycle-detection","fisher–yates-shuffle","greedy-algorithm","heap's-algorithm","kernighan–lin-algorithm","lemke–howson-algorithm","lin–kernighan-heuristic","loopless-algorithm","robinson–schensted-correspondence","robinson–schensted–knuth-correspondence","smawk-algorithm","steinhaus–johnson–trotter-algorithm","tompkins–paige-algorithm"],"name":"Combinatorial algorithms","children":["combinatorial-optimization","computational-geometry","graph-algorithms","sorting-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"type-2-encryption-algorithms","_score":0,"_source":{"algorithms":["nsa-cryptography","skipjack-(cipher)"],"name":"Type 2 encryption algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"data-mining-algorithms","_score":0,"_source":{"algorithms":["alpha-algorithm","apriori-algorithm","fsa-red-algorithm","gsp-algorithm","teiresias-algorithm","winepi"],"name":"Data mining algorithms","children":["classification-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"image-processing","_score":0,"_source":{"description":"In imaging science, image processing is processing of images using mathematical operations by using any form of signal processing for which the input is an image, such as a photograph or video frame; the output of image processing may be either an image or a set of characteristics or parameters related to the image. Most image-processing techniques involve treating the image as a two-dimensional signal and applying standard signal-processing techniques to it.\nImage processing usually refers to digital image processing, but optical and analog image processing also are possible. This article is about general techniques that apply to all of them. The acquisition of images (producing the input image in the first place) is referred to as imaging.\nClosely related to image processing are computer graphics and computer vision. In computer graphics, images are manually made from physical models of objects, environments, and lighting, instead of being acquired (via imaging devices such as cameras) from natural scenes, as in most animated movies. Computer vision, on the other hand, is often considered high-level image processing out of which a machine/computer/software intends to decipher the physical contents of an image or a sequence of images (e.g., videos or 3D full-body magnetic resonance scans).\nIn modern sciences and technologies, images also gain much broader scopes due to the ever growing importance of scientific visualization (of often large-scale complex scientific/experimental data). Examples include microarray data in genetic research, or real-time multi-asset portfolio trading in finance.","tag_line":"In imaging science, image processing is processing of images using mathematical operations by using any form of signal processing for which the input is an image, such as a photograph or video frame; the output of image processing may be either an image or a set of characteristics or parameters related to the image.","algorithms":["false-radiosity","firefly-algorithm","floyd–steinberg-dithering","level-set-method","shepp–logan-phantom"],"name":"Image processing","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"hash-functions","_score":0,"_source":{"description":"A hash function is any function that can be used to map data of arbitrary size to data of fixed size. The values returned by a hash function are called hash values, hash codes, hash sums, or simply hashes. One use is a data structure called a hash table, widely used in computer software for rapid data lookup. Hash functions accelerate table or database lookup by detecting duplicated records in a large file. An example is finding similar stretches in DNA sequences. They are also useful in cryptography. A cryptographic hash function allows one to easily verify that some input data maps to a given hash value, but if the input data is unknown, it is deliberately difficult to reconstruct it (or equivalent alternatives) by knowing the stored hash value. This is used for assuring integrity of transmitted data, and is the building block for HMACs, which provide message authentication.\nHash functions are related to (and often confused with) checksums, check digits, fingerprints, randomization functions, error-correcting codes, and ciphers. Although these concepts overlap to some extent, each has its own uses and requirements and is designed and optimized differently. The Hash Keeper database maintained by the American National Drug Intelligence Center, for instance, is more aptly described as a catalogue of file fingerprints than of hash values.","tag_line":"A hash function is any function that can be used to map data of arbitrary size to data of fixed size.","algorithms":["k-independent-hashing"],"name":"Hash functions","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"heuristic-algorithms","_score":0,"_source":{"description":"In computer science, artificial intelligence, and mathematical optimization, a heuristic is a technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution. This is achieved by trading optimality, completeness, accuracy, or precision for speed. In a way, it can be considered a shortcut.","tag_line":"In computer science, artificial intelligence, and mathematical optimization, a heuristic is a technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution.","algorithms":["2-opt","3-opt","bat-algorithm","heuristiclab","kernighan–lin-algorithm","lin–kernighan-heuristic","luus–jaakola","monte-carlo-tree-search","nearest-neighbour-algorithm","simulated-annealing","social-cognitive-optimization"],"name":"Heuristic algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"automatic-memory-management","_score":0,"_source":{"description":"In computer science, garbage collection (GC) is a form of automatic memory management. The garbage collector, or just collector, attempts to reclaim garbage, or memory occupied by objects that are no longer in use by the program. Garbage collection was invented by John McCarthy around 1959 to abstract away manual memory management in Lisp.\nGarbage collection is often portrayed as the opposite of manual memory management, which requires the programmer to specify which objects to deallocate and return to the memory system. However, many systems use a combination of approaches, including other techniques such as stack allocation and region inference. Like other memory management techniques, garbage collection may take a significant proportion of total processing time in a program and, as a result, can have significant influence on performance.\nResources other than memory, such as network sockets, database handles, user interaction windows, and file and device descriptors, are not typically handled by garbage collection. Methods used to manage such resources, particularly destructors, may suffice to manage memory as well, leaving no need for GC. Some GC systems allow such other resources to be associated with a region of memory that, when collected, causes the other resource to be reclaimed; this is called finalization. Finalization may introduce complications limiting its usability, such as intolerable latency between disuse and reclaim of especially limited resources, or a lack of control over which thread performs the work of reclaiming.","tag_line":"In computer science, garbage collection (GC) is a form of automatic memory management.","algorithms":["mark-compact-algorithm"],"name":"Automatic memory management","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"memory-management-algorithms","_score":0,"_source":{"description":"Algorithms used for memory management.","tag_line":"Algorithms used for memory management.","algorithms":["adaptive-replacement-cache","buddy-memory-allocation","cache-algorithms","least-frequently-used","lirs-caching-algorithm","local-replacement-algorithm","mark-compact-algorithm","page-replacement-algorithm","pseudo-lru","slob"],"name":"Memory management algorithms","children":["automatic-memory-management"]}}
,{"_index":"throwtable","_type":"category","_id":"numerical-linear-algebra","_score":0,"_source":{"description":"Numerical linear algebra is the study of algorithms for performing linear algebra computations, most notably matrix operations, on computers. It is often a fundamental part of engineering and computational science problems, such as image and signal processing, telecommunication, computational finance, materials science simulations, structural biology, data mining, bioinformatics, fluid dynamics, and many other areas. Such software relies heavily on the development, analysis, and implementation of state-of-the-art algorithms for solving various numerical linear algebra problems, in large part because of the role of matrices in finite difference and finite element methods.\nCommon problems in numerical linear algebra include computing the following: LU decomposition, QR decomposition, singular value decomposition, eigenvalues.","tag_line":"Numerical linear algebra is the study of algorithms for performing linear algebra computations, most notably matrix operations, on computers.","algorithms":["bareiss-algorithm","block-lanczos-algorithm","coppersmith–winograd-algorithm","gaussian-elimination","gradient-method","lu-reduction","pivot-element","rrqr-factorization"],"name":"Numerical linear algebra","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"first-order-methods","_score":0,"_source":{"description":"In optimization, gradient method is an algorithm to solve problems of the form\n\nwith the search directions defined by the gradient of the function at the current point. Examples of gradient method are the gradient descent and the conjugate gradient.","tag_line":"In optimization, gradient method is an algorithm to solve problems of the form\n\nwith the search directions defined by the gradient of the function at the current point.","algorithms":["frank–wolfe-algorithm","gradient-descent","gradient-method"],"name":"First order methods","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"polynomials","_score":0,"_source":{"description":"In mathematics, a polynomial is an expression consisting of variables (or indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents. An example of a polynomial of a single indeterminate (or variable), x, is x2 − 4x + 7, which is a quadratic polynomial. An example in three variables is x3 + 2xyz2 + yz + 1.\nPolynomials appear in a wide variety of areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated problems in the sciences; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, central concepts in algebra and algebraic geometry.","tag_line":"In mathematics, a polynomial is an expression consisting of variables (or indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents.","algorithms":["polylogarithmic-function"],"name":"Polynomials","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"online-algorithms","_score":0,"_source":{"description":"In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i.e., in the order that the input is fed to the algorithm, without having the entire input available from the start.\nIn contrast, an offline algorithm is given the whole problem data from the beginning and is required to output an answer which solves the problem at hand.\nAs an example, consider the sorting algorithms selection sort and insertion sort: Selection sort repeatedly selects the minimum element from the unsorted remainder and places it at the front, which requires access to the entire input; it is thus an offline algorithm. On the other hand, insertion sort considers one input element per iteration and produces a partial solution without considering future elements. Thus insertion sort is an online algorithm.\nNote that insertion sort produces the optimum result, i.e., a correctly sorted list. For many problems, online algorithms cannot match the performance of offline algorithms. If the ratio between the performance of an online algorithm and an optimal offline algorithm is bounded, the online algorithm is called competitive.\nNot every online algorithm has an offline counterpart.","tag_line":"In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i.e., in the order that the input is fed to the algorithm, without having the entire input available from the start.","algorithms":["online-algorithm","adversary-model","competitive-analysis-(online-algorithm)","k-server-problem","least-frequently-used","lirs-caching-algorithm","list-update-problem","metrical-task-system","page-replacement-algorithm"],"name":"Online algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"signal-processing-stubs","_score":0,"_source":{"description":"A harmonic spectrum is a spectrum containing only frequency components whose frequencies are whole number multiples of the fundamental frequency; such frequencies are known as harmonics.\nIn other words, if  is the fundamental frequency, then a harmonic spectrum has the form\n\nA standard result of Fourier analysis is that a function has a harmonic spectrum if and only if it is periodic.","tag_line":"A harmonic spectrum is a spectrum containing only frequency components whose frequencies are whole number multiples of the fundamental frequency; such frequencies are known as harmonics.","algorithms":["fast-walsh–hadamard-transform"],"name":"Signal processing stubs","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"parsing-algorithms","_score":0,"_source":{"description":"Parsing or syntactic analysis is the process of analysing a string of symbols, either in natural language or in computer languages, conforming to the rules of a formal grammar. The term parsing comes from Latin pars (orationis), meaning part (of speech).\nThe term has slightly different meanings in different branches of linguistics and computer science. Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence, sometimes with the aid of devices such as sentence diagrams. It usually emphasizes the importance of grammatical divisions such as subject and predicate.\nWithin computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information.\nThe term is also used in psycholinguistics when describing language comprehension. In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) \"in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc.\" This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences.\nWithin computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters.","tag_line":"Parsing or syntactic analysis is the process of analysing a string of symbols, either in natural language or in computer languages, conforming to the rules of a formal grammar.","algorithms":["cyk-algorithm","earley-parser","glr-parser","inside–outside-algorithm","lalr-parser","lr-parser","operator-precedence-parser","parsing-expression-grammar","shunting-yard-algorithm","simple-lr-parser"],"name":"Parsing algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"algorithms-on-strings","_score":0,"_source":{"description":"In computer science, a substring index is a data structure which gives substring search in a text or text collection in sublinear time. If you have a document  of length , or a set of documents  of total length , you can locate all occurrences of a pattern  in  time. (See Big O notation.)\nThe phrase full-text index is also often used for an index of all substrings of a text. But is ambiguous, as it is also used for regular word indexes such as inverted files and document retrieval. See full text search.\nSubstring indexes include:\nSuffix tree\nSuffix array\nN-gram index, an inverted file for all N-grams of the text\nCompressed suffix array\nFM-index\nLZ-index","tag_line":"In computer science, a substring index is a data structure which gives substring search in a text or text collection in sublinear time.","algorithms":["boyer–moore-string-search-algorithm","hunt–mcilroy-algorithm","string-kernel","ukkonen's-algorithm","wagner–fischer-algorithm"],"name":"Algorithms on strings","children":["parsing-algorithms","string-similarity-measures","string-collation-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"genetic-algorithms","_score":0,"_source":{"description":"In the field of artificial intelligence, a genetic algorithm (GA) is a search heuristic that mimics the process of natural selection. This heuristic (also sometimes called a metaheuristic) is routinely used to generate useful solutions to optimization and search problems. Genetic algorithms belong to the larger class of evolutionary algorithms (EA), which generate solutions to optimization problems using techniques inspired by natural evolution, such as inheritance, mutation, selection, and crossover.","tag_line":"In the field of artificial intelligence, a genetic algorithm (GA) is a search heuristic that mimics the process of natural selection.","algorithms":["genetic-algorithm","chromosome-(genetic-algorithm)","clonal-selection-algorithm","crossover-(genetic-algorithm)","cultural-algorithm","defining-length","edge-recombination-operator","evolver-(software)","fitness-function","fitness-proportionate-selection","genetic-algorithm","gene-expression-programming","genetic-algorithm-scheduling","genetic-algorithms-in-economics","genetic-fuzzy-systems","genetic-memory-(computer-science)","genetic-operator","genetic-programming","holland's-schema-theorem","hyperneat","inheritance-(genetic-algorithm)","list-of-genetic-algorithm-applications","mutation-(genetic-algorithm)","neuroevolution-of-augmenting-topologies","parallel-metaheuristic","population-based-incremental-learning","premature-convergence","promoter-based-genetic-algorithm","quality-control-and-genetic-algorithms","reward-based-selection","santa-fe-trail-problem","schema-(genetic-algorithms)","search-based-software-engineering","selection-(genetic-algorithm)","speciation-(genetic-algorithm)","stochastic-universal-sampling","tournament-selection","truncation-selection"],"name":"Genetic algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"sequence-alignment-algorithms","_score":0,"_source":{"description":"In bioinformatics, a sequence alignment is a way of arranging the sequences of DNA, RNA, or protein to identify regions of similarity that may be a consequence of functional, structural, or evolutionary relationships between the sequences. Aligned sequences of nucleotide or amino acid residues are typically represented as rows within a matrix. Gaps are inserted between the residues so that identical or similar characters are aligned in successive columns. Sequence alignments are also used for non-biological sequences, such as those present in natural language or in financial data.","tag_line":"In bioinformatics, a sequence alignment is a way of arranging the sequences of DNA, RNA, or protein to identify regions of similarity that may be a consequence of functional, structural, or evolutionary relationships between the sequences.","algorithms":["hirschberg's-algorithm","needleman–wunsch-algorithm","smith–waterman-algorithm"],"name":"Sequence alignment algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"lossless-compression-algorithms","_score":0,"_source":{"description":"Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast, lossy compression permits reconstruction only of an approximation of the original data, though this usually improves compression rates (and therefore reduces file sizes).\nLossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by the LAME MP3 encoder and other lossy audio encoders).\nLossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data could be deleterious. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods. Lossless audio formats are most often used for archiving or production purposes, while smaller lossy audio files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary.","tag_line":"Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data.","algorithms":["lossless-compression","7z","adam7-algorithm","adaptive-coding","algorithm-bstw","brotli","burrows–wheeler-transform","byte-pair-encoding","bzip2","canonical-huffman-code","chain-code","context-tree-weighting","context-adaptive-binary-arithmetic-coding","deflate","dictionary-coder","dynamic-markov-compression","embedded-zerotrees-of-wavelet-transforms","felics","huffman-coding","huffyuv","incompressible-string","incremental-encoding","lempel–ziv–markov-chain-algorithm","lempel–ziv–oberhumer","lempel–ziv–stac","lempel–ziv–storer–szymanski","lempel–ziv–welch","liblzg","lossless-compression","lz4-(compression-algorithm)","lz77-and-lz78","lzjb","lzrw","lzwl","lzx-(algorithm)","microsoft-point-to-point-compression","move-to-front-transform","package-merge-algorithm","prediction-by-partial-matching","prefix-code","quad-(compressor)","sequitur-algorithm","ut-video-codec-suite","zopfli","zpaq"],"name":"Lossless compression algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"compression-algorithms","_score":0,"_source":{"description":"In digital signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by identifying unnecessary information and removing it. The process of reducing the size of a data file is referred to as data compression. In the context of data transmission, it is called source coding (encoding done at the source of the data before it is stored or transmitted) in opposition to channel coding.\nCompression is useful because it helps reduce resource usage, such as data storage space or transmission capacity. Because compressed data must be decompressed to use, this extra processing imposes computational or other costs through decompression; this situation is far from being a free lunch. Data compression is subject to a space–time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data.","tag_line":"In digital signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation.","algorithms":["geohash-36","power-quality-compression-algorithm"],"name":"Compression algorithms","children":["lossless-compression-algorithms","lossy-compression-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"broken-cryptography-algorithms","_score":0,"_source":{"description":"Category for cryptographic algorithms or primitives that have been broken.","tag_line":"Category for cryptographic algorithms or primitives that have been broken.","algorithms":["crypt-(c)","dual-ec-drbg","merkle–hellman-knapsack-cryptosystem","ms-chap","wired-equivalent-privacy"],"name":"Broken cryptography algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"type-1-encryption-algorithms","_score":0,"_source":{"algorithms":["nsa-product-types","baton","nsa-cryptography","saville"],"name":"Type 1 encryption algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"data-clustering-algorithms","_score":0,"_source":{"description":"This category contains algorithms used for cluster analysis.","tag_line":"This category contains algorithms used for cluster analysis.","algorithms":["affinity-propagation","basic-sequential-algorithmic-scheme","birch","canopy-clustering-algorithm","cluster-weighted-modeling","cobweb-(clustering)","constrained-clustering","cure-data-clustering-algorithm","data-stream-clustering","dbscan","expectation–maximization-algorithm","flame-clustering","fuzzy-clustering","information-bottleneck-method","k-q-flats","k-means-clustering","k-means++","k-medians-clustering","k-medoids","k-svd","linde–buzo–gray-algorithm","mean-shift","nearest-neighbor-chain-algorithm","neighbor-joining","optics-algorithm","subclu","upgma"],"name":"Data clustering algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"classification-algorithms","_score":0,"_source":{"description":"This category is about statistical classification algorithms. For more information, see Statistical classification.","tag_line":"This category is about statistical classification algorithms.","algorithms":["statistical-classification","adaboost","alopex","boosting-(machine-learning)","boruta-(algorithm)","brownboost","c4.5-algorithm","sukhotin's-algorithm","co-training","coboosting","compositional-pattern-producing-network","decision-boundary","generalization-error","group-method-of-data-handling","id3-algorithm","information-fuzzy-networks","k-nearest-neighbors-algorithm","kernel-method","large-margin-nearest-neighbor","learning-vector-quantization","logitboost","margin-classifier","margin-infused-relaxed-algorithm","multi-label-classification","multiclass-classification","multifactor-dimensionality-reduction","multispectral-pattern-recognition","nearest-centroid-classifier","perceptron","random-forest","random-subspace-method","relevance-vector-machine","rules-extraction-system-family","support-vector-machine","syntactic-pattern-recognition","types-of-artificial-neural-networks","winnow-(algorithm)"],"name":"Classification algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"join-algorithms","_score":0,"_source":{"description":"A SQL join clause combines records from two or more tables in a relational database. It creates a set that can be saved as a table or used as it is. A JOIN is a means for combining fields from two tables (or more) by using values common to each. ANSI-standard SQL specifies five types of JOIN: INNER, LEFT OUTER, RIGHT OUTER, FULL OUTER and CROSS. As a special case, a table (base table, view, or joined table) can JOIN to itself in a self-join.\nA programmer writes a JOIN statement to identify the records for joining. If the evaluated predicate is true, the combined record is then produced in the expected format, a record set or a temporary table.\n\n","tag_line":"A SQL join clause combines records from two or more tables in a relational database.","algorithms":["block-nested-loop","hash-join","nested-loop-join","sort-merge-join"],"name":"Join algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"digital-geometry","_score":0,"_source":{"description":"Digital geometry deals with discrete sets (usually discrete point sets) considered to be digitized models or images of objects of the 2D or 3D Euclidean space.\nSimply put, digitizing is replacing an object by a discrete set of its points. The images we see on the TV screen, the raster display of a computer, or in newspapers are in fact digital images.\nIts main application areas are computer graphics and image analysis.\nMain aspects of study are:\nConstructing digitized representations of objects, with the emphasis on precision and efficiency (either by means of synthesis, see, for example, Bresenham's line algorithm or digital disks, or by means of digitization and subsequent processing of digital images).\nStudy of properties of digital sets; see, for example, Pick's theorem, digital convexity, digital straightness, or digital planarity.\nTransforming digitized representations of objects, for example (A) into simplified shapes such as (i) skeletons, by repeated removal of simple points such that the digital topology of an image does not change, or (ii) medial axis, by calculating local maxima in a distance transform of the given digitized object representation, or (B) into modified shapes using mathematical morphology.\nReconstructing \"real\" objects or their properties (area, length, curvature, volume, surface area, and so forth) from digital images.\nStudy of digital curves, digital surfaces, and digital manifolds.\nDesigning tracking algorithms for digital objects.\nFunctions on digital space.\nDigital geometry heavily overlaps with discrete geometry and may be considered as a part thereof.","tag_line":"Digital geometry deals with discrete sets (usually discrete point sets) considered to be digitized models or images of objects of the 2D or 3D Euclidean space.","algorithms":["bresenham's-line-algorithm","digital-differential-analyzer-(graphics-algorithm)","midpoint-circle-algorithm"],"name":"Digital geometry","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"mesh-generation","_score":0,"_source":{"description":"Mesh generation is the practice of generating a polygonal or polyhedral mesh that approximates a geometric domain. The term \"grid generation\" is often used interchangeably. Typical uses are for rendering to a computer screen or for physical simulation such as finite element analysis or computational fluid dynamics. The input model form can vary greatly but common sources are CAD, NURBS, B-rep, STL (file format) or a point cloud. The field is highly interdisciplinary, with contributions found in mathematics, computer science, and engineering.\nThree-dimensional meshes created for finite element analysis need to consist of tetrahedra, pyramids, prisms or hexahedra. Those used for the finite volume method can consist of arbitrary polyhedra. Those used for finite difference methods usually need to consist of piecewise structured arrays of hexahedra known as multi-block structured meshes. A mesh is otherwise a discretization of a domain existing in one, two or three dimensions.","tag_line":"Mesh generation is the practice of generating a polygonal or polyhedral mesh that approximates a geometric domain.","algorithms":["marching-cubes"],"name":"Mesh generation","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"routing-algorithms","_score":0,"_source":{"description":"Routing is the process of selecting best paths in a network. In the past, the term routing also meant forwarding network traffic among networks. However, that latter function is better described as forwarding. Routing is performed for many kinds of networks, including the telephone network (circuit switching), electronic data networks (such as the Internet), and transportation networks. This article is concerned primarily with routing in electronic data networks using packet switching technology.\nIn packet switching networks, routing directs packet forwarding (the transit of logically addressed network packets from their source toward their ultimate destination) through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though they are not specialized hardware and may suffer from limited performance. The routing process usually directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Thus, constructing routing tables, which are held in the router's memory, is very important for efficient routing. Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.\nIn case of overlapping/equal routes, algorithms consider the following elements to decide which routes to install into the routing table (sorted by priority):\nPrefix-Length: where longer subnet masks are preferred (independent of whether it is within a routing protocol or over different routing protocol)\nMetric: where a lower metric/cost is preferred (only valid within one and the same routing protocol)\nAdministrative distance: where a route learned from a more reliable routing protocol is preferred (only valid between different routing protocols)\nRouting, in a more narrow sense of the term, is often contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices. In large networks, structured addressing (routing, in the narrow sense) outperforms unstructured addressing (bridging). Routing has become the dominant form of addressing on the Internet. Bridging is still widely used within localized environments.","tag_line":"Routing is the process of selecting best paths in a network.","algorithms":["a*-search-algorithm","b*","backpressure-routing","contraction-hierarchies","diffusing-update-algorithm","dijkstra's-algorithm","distance-vector-routing-protocol","edge-disjoint-shortest-pair-algorithm","expected-transmission-count","flooding-(computer-networking)","flooding-algorithm","floyd–warshall-algorithm","geographic-routing","iterative-deepening-a*","link-state-routing-protocol","luleå-algorithm","max-min-fairness","mentor-routing-algorithm","optimization-mechanism","sma*","suurballe's-algorithm","temporally-ordered-routing-algorithm","weighted-fair-queueing"],"name":"Routing algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"networking-algorithms","_score":0,"_source":{"description":"Algorithms for Computer System.","tag_line":"Algorithms for Computer System.","algorithms":["backpressure-routing","chung-kwei-(algorithm)","exponential-backoff","generic-cell-rate-algorithm","karn's-algorithm","luleå-algorithm","nagle's-algorithm"],"name":"Networking algorithms","children":["network-scheduling-algorithms","routing-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"mathematical-optimization","_score":0,"_source":{"description":"In mathematics, computer science and operations research, mathematical optimization (alternatively, optimization or mathematical programming) is the selection of a best element (with regard to some criteria) from some set of available alternatives.\nIn the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations comprises a large area of applied mathematics. More generally, optimization includes finding \"best available\" values of some objective function given a defined domain (or a set of constraints), including a variety of different types of objective functions and different types of domains.","tag_line":"In mathematics, computer science and operations research, mathematical optimization (alternatively, optimization or mathematical programming) is the selection of a best element (with regard to some criteria) from some set of available alternatives.","algorithms":["2-opt","3-opt","adaptive-dimensional-search","constructive-cooperative-coevolution","dynamic-programming","genetic-algorithm","genetic-programming","hardness-of-approximation","interval-contractor","job-shop-scheduling","klee–minty-cube","lemke's-algorithm","level-set-method","lloyd's-algorithm","meta-optimization","odds-algorithm","parallel-metaheuristic","pattern-search-(optimization)","shuffled-frog-leaping-algorithm","special-ordered-set","ternary-search"],"name":"Mathematical optimization","children":["genetic-programming"]}}
,{"_index":"throwtable","_type":"category","_id":"string-matching-algorithms","_score":0,"_source":{"description":"In computer science, string searching algorithms, sometimes called string matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.\nLet Σ be an alphabet (finite set). Formally, both the pattern and searched text are vectors of elements of Σ. The Σ may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (Σ = {0,1}) or DNA alphabet (Σ = {A,C,G,T}) in bioinformatics.\nIn practice, how the string is encoded can affect the feasible string search algorithms. In particular if a variable width encoding is in use then it is slow (time proportional to N) to find the Nth character. This will significantly slow down many of the more advanced search algorithms. A possible solution is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.","tag_line":"In computer science, string searching algorithms, sometimes called string matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.","algorithms":["string-searching-algorithm","aho–corasick-algorithm","apostolico–giancarlo-algorithm","bitap-algorithm","boyer–moore-string-search-algorithm","boyer–moore–horspool-algorithm","commentz-walter-algorithm","knuth–morris–pratt-algorithm","rabin–karp-algorithm","raita-algorithm","zhu–takaoka-string-matching-algorithm"],"name":"String matching algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"regular-expressions","_score":0,"_source":{"description":"In theoretical computer science and formal language theory, a regular expression (abbreviated regex or regexp and sometimes called a rational expression) is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching, i.e. \"find and replace\"-like operations. The concept arose in the 1950s, when the American mathematician Stephen Kleene formalized the description of a regular language, and came into common use with the Unix text processing utilities ed, an editor, and grep (global regular expression print), a filter.\nRegular expressions are so useful in computing that the various systems to specify regular expressions have evolved to provide both a basic and extended standard for the grammar and syntax; modern regular expressions heavily augment the standard. Regular expression processors are found in several search engines, search and replace dialogs of several word processors and text editors, and in the command lines of text processing utilities, such as sed and AWK.\nMany programming languages provide regular expression capabilities, some built-in, for example Perl, JavaScript, Ruby, AWK, and Tcl, and others via a standard library, for example .NET languages, Java, Python, POSIX C and C++ (since C++11). Most other languages offer regular expressions via a library.","tag_line":"In theoretical computer science and formal language theory, a regular expression (abbreviated regex or regexp and sometimes called a rational expression) is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching, i.e.","algorithms":["kleene's-algorithm","redos"],"name":"Regular expressions","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"computational-physics","_score":0,"_source":{"description":"Computational physics is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists. Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science.\nIt is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics, a third way that supplements theory and experiment.\n^ \n^","tag_line":"Computational physics is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists.","algorithms":["astronomical-algorithm","featherstone's-algorithm","multicanonical-ensemble","parallel-tebd","vegas-algorithm","wang-and-landau-algorithm"],"name":"Computational physics","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"processor-scheduling-algorithms","_score":0,"_source":{"algorithms":["deadline-monotonic-scheduling","earliest-deadline-first-scheduling","fair-share-scheduling","foreground-background","gang-scheduling","lottery-scheduling","multilevel-feedback-queue","processor-affinity","proportional-share-scheduling","rate-monotonic-scheduling","round-robin-scheduling","shortest-job-next","shortest-remaining-time","starvation-(computer-science)","yds-algorithm"],"name":"Processor scheduling algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"data-mining","_score":0,"_source":{"description":"Data mining (the analysis step of the \"knowledge discovery in databases\" process, or KDD), an interdisciplinary subfield of computer science is the computational process of discovering patterns in large data sets (\"big data\") involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems. The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use. Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.\nThe term is a misnomer, because the goal is the extraction of patterns and knowledge from large amount of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence, machine learning, and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics – or, when referring to actual methods, artificial intelligence and machine learning – are more appropriate.\nThe actual data mining task is the automatic or semi-automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.\nThe related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.","tag_line":"Data mining (the analysis step of the \"knowledge discovery in databases\" process, or KDD), an interdisciplinary subfield of computer science is the computational process of discovering patterns in large data sets (\"big data\") involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems.","algorithms":["local-outlier-factor","multifactor-dimensionality-reduction","multiple-kernel-learning","sequential-pattern-mining"],"name":"Data mining","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"signal-processing","_score":0,"_source":{"description":"Signal processing is an enabling technology that encompasses the fundamental theory, applications, algorithms, and implementations of processing or transferring information contained in many different physical, symbolic, or abstract formats broadly designated as signals. It uses mathematical, statistical, computational, heuristic, and linguistic representations, formalisms, and techniques for representation, modelling, analysis, synthesis, discovery, recovery, sensing, acquisition, extraction, learning, security, or forensics.","tag_line":"Signal processing is an enabling technology that encompasses the fundamental theory, applications, algorithms, and implementations of processing or transferring information contained in many different physical, symbolic, or abstract formats broadly designated as signals.","algorithms":["fast-folding-algorithm"],"name":"Signal processing","children":["multidimensional-signal-processing","estimation-theory","radar-signal-processing","signal-processing-stubs"]}}
,{"_index":"throwtable","_type":"category","_id":"statistical-algorithms","_score":0,"_source":{"algorithms":["algorithms-for-calculating-variance","baum–welch-algorithm","buzen's-algorithm","canopy-clustering-algorithm","elston–stewart-algorithm","expectation–maximization-algorithm","fnn-algorithm","gauss–newton-algorithm","iterated-filtering","iterative-proportional-fitting","k-means-clustering","k-means++","k-medians-clustering","k-medoids","lander–green-algorithm","levenberg–marquardt-algorithm","metropolis–hastings-algorithm","multicanonical-ensemble","nested-sampling-algorithm","odds-algorithm","ransac","vegas-algorithm","wang-and-landau-algorithm","yamartino-method","ziggurat-algorithm"],"name":"Statistical algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"string-similarity-measures","_score":0,"_source":{"description":"In mathematics and computer science, a string metric (also known as a string similarity metric or string distance function) is a metric that measures distance (\"inverse similarity\") between two text strings for approximate string matching or comparison and in fuzzy string searching. A necessary requirement for a string metric (e.g. in contrast to string matching) is fulfillment of the triangle inequality. For example the strings \"Sam\" and \"Samuel\" can be considered to be close. A string metric provides a number indicating an algorithm-specific indication of distance.\nThe most widely known string metric is a rudimentary one called the Levenshtein Distance (also known as Edit Distance). It operates between two input strings, returning a number equivalent to the number of substitutions and deletions needed in order to transform one input string into another. Simplistic string metrics such as Levenshtein distance have expanded to include phonetic, token, grammatical and character-based methods of statistical comparisons.\nString metrics are used heavily in information integration and are currently used in areas including fraud detection, fingerprint analysis, plagiarism detection, ontology merging, DNA analysis, RNA analysis, image analysis, evidence-based machine learning, database data deduplication, data mining, Web interfaces, e.g. Ajax-style suggestions as you type, data integration, and semantic knowledge integration.","tag_line":"In mathematics and computer science, a string metric (also known as a string similarity metric or string distance function) is a metric that measures distance (\"inverse similarity\") between two text strings for approximate string matching or comparison and in fuzzy string searching.","algorithms":["string-kernel","wagner–fischer-algorithm"],"name":"String similarity measures","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"approximation-algorithms","_score":0,"_source":{"description":"In computer science and operations research, approximation algorithms are algorithms used to find approximate solutions to optimization problems. Approximation algorithms are often associated with NP-hard problems; since it is unlikely that there can ever be efficient polynomial-time exact algorithms solving NP-hard problems, one settles for polynomial-time sub-optimal solutions. Unlike heuristics, which usually only find reasonably good solutions reasonably fast, one wants provable solution quality and provable run-time bounds. Ideally, the approximation is optimal up to a small constant factor (for instance within 5% of the optimal solution). Approximation algorithms are increasingly being used for problems where exact polynomial-time algorithms are known but are too expensive due to the input size. A typical example for an approximation algorithm is the one for vertex cover in graphs: find an uncovered edge and add both endpoints to the vertex cover, until none remain. It is clear that the resulting cover is at most twice as large as the optimal one. This is a constant factor approximation algorithm with a factor of 2.\nNP-hard problems vary greatly in their approximability; some, such as the bin packing problem, can be approximated within any factor greater than 1 (such a family of approximation algorithms is often called a polynomial time approximation scheme or PTAS). Others are impossible to approximate within any constant, or even polynomial factor unless P = NP, such as the maximum clique problem.\nNP-hard problems can often be expressed as integer programs (IP) and solved exactly in exponential time. Many approximation algorithms emerge from the linear programming relaxation of the integer program.\nNot all approximation algorithms are suitable for all practical applications. They often use IP/LP/Semidefinite solvers, complex data structures or sophisticated algorithmic techniques which lead to difficult implementation problems. Also, some approximation algorithms have impractical running times even though they are polynomial time, for example O(n2156) . Yet the study of even very expensive algorithms is not a completely theoretical pursuit as they can yield valuable insights. A classic example is the initial PTAS for Euclidean TSP due to Sanjeev Arora which had prohibitive running time, yet within a year, Arora refined the ideas into a linear time algorithm. Such algorithms are also worthwhile in some applications where the running times and cost can be justified e.g. computational biology, financial engineering, transportation planning, and inventory management. In such scenarios, they must compete with the corresponding direct IP formulations.\nAnother limitation of the approach is that it applies only to optimization problems and not to \"pure\" decision problems like satisfiability, although it is often possible to conceive optimization versions of such problems, such as the maximum satisfiability problem (Max SAT).\nInapproximability has been a fruitful area of research in computational complexity theory since the 1990 result of Feige, Goldwasser, Lovász, Safra and Szegedy on the inapproximability of Independent Set. After Arora et al. proved the PCP theorem a year later, it has now been shown that Johnson's 1974 approximation algorithms for Max SAT, Set Cover, Independent Set and Coloring all achieve the optimal approximation ratio, assuming P != NP.","tag_line":"In computer science and operations research, approximation algorithms are algorithms used to find approximate solutions to optimization problems.","algorithms":["approximation-algorithm","submodular-set-function","alpha-max-plus-beta-min-algorithm","approximation-preserving-reduction","apx","bidimensionality","christofides-algorithm","domination-analysis","gap-reduction","hardness-of-approximation","k-approximation-of-k-hitting-set","karloff–zwick-algorithm","l-reduction","method-of-conditional-probabilities","nearest-neighbour-algorithm","polynomial-time-algorithm-for-approximating-the-volume-of-convex-bodies","polynomial-time-approximation-scheme","property-testing","unique-games-conjecture"],"name":"Approximation algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"algorithm-description-languages","_score":0,"_source":{"description":"Pseudocode is an informal high-level description of the operating principle of a computer program or other algorithm.\nIt uses the structural conventions of a programming language, but is intended for human reading rather than machine reading. Pseudocode typically omits details that are essential for machine understanding of the algorithm, such as variable declarations, system-specific code and some subroutines. The programming language is augmented with natural language description details, where convenient, or with compact mathematical notation. The purpose of using pseudocode is that it is easier for people to understand than conventional programming language code, and that it is an efficient and environment-independent description of the key principles of an algorithm. It is commonly used in textbooks and scientific publications that are documenting various algorithms, and also in planning of computer program development, for sketching out the structure of the program before the actual coding takes place.\nNo standard for pseudocode syntax exists, as a program in pseudocode is not an executable program. Pseudocode resembles, but should not be confused with skeleton programs which can be compiled without errors. Flowcharts, drakon-charts and Unified Modeling Language (UML) charts can be thought of as a graphical alternative to pseudocode, but are more spacious on paper.","tag_line":"Pseudocode is an informal high-level description of the operating principle of a computer program or other algorithm.","algorithms":["flowchart","pidgin-code","pluscal","pseudocode"],"name":"Algorithm description languages","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"substring-indices","_score":0,"_source":{"description":"In computer science, a substring index is a data structure which gives substring search in a text or text collection in sublinear time. If you have a document  of length , or a set of documents  of total length , you can locate all occurrences of a pattern  in  time. (See Big O notation.)\nThe phrase full-text index is also often used for an index of all substrings of a text. But is ambiguous, as it is also used for regular word indexes such as inverted files and document retrieval. See full text search.\nSubstring indexes include:\nSuffix tree\nSuffix array\nN-gram index, an inverted file for all N-grams of the text\nCompressed suffix array\nFM-index\nLZ-index","tag_line":"In computer science, a substring index is a data structure which gives substring search in a text or text collection in sublinear time.","algorithms":["ukkonen's-algorithm"],"name":"Substring indices","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"lossy-compression-algorithms","_score":0,"_source":{"description":"In information technology, lossy compression is the class of data encoding methods that uses inexact approximations (or partial data discarding) to represent the content. These techniques are used to reduce data size for storage, handling, and transmitting content. Different versions of the photo of the cat at the right show how higher degrees of approximation create coarser images as more details are removed. This is opposed to lossless data compression which does not degrade the image. The amount of data reduction possible using lossy compression is often much higher than through lossless techniques.\nWell-designed lossy compression technology often reduces file sizes significantly before degradation is noticed by the end-user. Even when noticeable by the user, further data reduction may be desirable (e.g., for real-time communication, to reduce transmission times, or to reduce storage needs).\nLossy compression is most commonly used to compress multimedia data (audio, video, and images), especially in applications such as streaming media and internet telephony. By contrast, lossless compression is typically required for text and data files, such as bank records and text articles. In many cases it is advantageous to make a master lossless file which is to be used to produce new compressed files; for example, a multi-megabyte file can be used at full size to produce a full-page advertisement in a glossy magazine, and a 10 kilobyte lossy copy can be made for a small image on a web page.","tag_line":"In information technology, lossy compression is the class of data encoding methods that uses inexact approximations (or partial data discarding) to represent the content.","algorithms":["3dc","adaptive-scalable-texture-compression","apple-video","block-truncation-coding","bloom-filter","color-cell-compression","felics","fractal-compression","gwic","microsoft-video-1","opus-(audio-format)","quicktime-graphics","s2tc","s3-texture-compression","vector-quantization","vocoder","wavelet-scalar-quantization"],"name":"Lossy compression algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"computer-arithmetic-algorithms","_score":0,"_source":{"description":"This category contains articles pertaining to algorithms that are used in arbitrary precision arithmetic. This includes algorithms for multiplication and division, as well as algorithms for the efficient evaluation of mathematical constants and special functions to high precision.\nSee also Category:Number theoretic algorithms for arbitrary-precision integer and cryptography algorithms.","tag_line":"This category contains articles pertaining to algorithms that are used in arbitrary precision arithmetic.","algorithms":["addition-chain-exponentiation","agm-method","binary-splitting","computational-complexity-of-mathematical-operations","division-algorithm","double-dabble","exponentiation-by-squaring","fee-method","fürer's-algorithm","karatsuba-algorithm","methods-of-computing-square-roots","mpir-(mathematics-software)","multiplication-algorithm","quote-notation","schönhage–strassen-algorithm","shifting-nth-root-algorithm","spigot-algorithm","toom–cook-multiplication"],"name":"Computer arithmetic algorithms","children":["pi-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"concurrent-algorithms","_score":0,"_source":{"description":"In computer science, a concurrent algorithm is one that can be executed concurrently. Most standard computer algorithms are sequential algorithms, and assume that the algorithm is run from start to finish without any other processes executing. These often do not behave correctly when run concurrently, as demonstrated at right, and are often nondeterministic, as the actual sequence of computations is determined by the external scheduler. Concurrency often adds significant complexity to an algorithm, requiring concurrency control such as mutual exclusion to avoid problems such as race conditions.\nMany parallel algorithms are run concurrently, particularly distributed algorithms, though these are distinct concepts in general.","tag_line":"In computer science, a concurrent algorithm is one that can be executed concurrently.","algorithms":["concurrent-algorithm","ostrich-algorithm","parallel-algorithm","prefix-sum"],"name":"Concurrent algorithms","children":["concurrency-control-algorithms","distributed-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"primality-tests","_score":0,"_source":{"description":"A primality test is an algorithm for determining whether an input number is prime. Amongst other fields of mathematics, it is used for cryptography. Unlike integer factorization, primality tests do not generally give prime factors, only stating whether the input number is prime or not. Factorization is thought to be a computationally difficult problem, whereas primality testing is comparatively easy (its running time is polynomial in the size of the input). Some primality tests prove that a number is prime, while others like Miller–Rabin prove that a number is composite. Therefore, the latter might be called compositeness tests instead of primality tests.","tag_line":"A primality test is an algorithm for determining whether an input number is prime.","algorithms":["primality-test","adleman–pomerance–rumely-primality-test","sieve-of-eratosthenes"],"name":"Primality tests","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"cryptanalytic-algorithms","_score":0,"_source":{"algorithms":["berlekamp–massey-algorithm","reeds–sloane-algorithm"],"name":"Cryptanalytic algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"digital-signal-processing","_score":0,"_source":{"description":"Digital signal processing (DSP) is the numerical manipulation of signals, usually with the intention to measure, filter, produce or compress continuous analog signals. It is characterized by the use of digital signals to represent these signals as discrete time, discrete frequency, or other discrete domain signals in the form of a sequence of numbers or symbols to permit the digital processing of these signals.\nTheoretical analyses and derivations are typically performed on discrete-time signal models, created by the abstract process of sampling. Numerical methods require a digital signal, such as those produced by an analog-to-digital converter (ADC). The processed result might be a frequency spectrum or a set of statistics. But often it is another digital signal that is converted back to analog form by a digital-to-analog converter (DAC). Even if that whole sequence is more complex than analog processing and has a discrete value range, the application of computational power to signal processing allows for many advantages over analog processing in many applications, such as error detection and correction in transmission as well as data compression.\nDigital signal processing and analog signal processing are subfields of signal processing. DSP applications include audio and speech signal processing, sonar and radar signal processing, sensor array processing, spectral estimation, statistical signal processing, digital image processing, signal processing for communications, control of systems, biomedical signal processing, seismic data processing, among others. DSP algorithms have long been run on standard computers, as well as on specialized processors called digital signal processors, and on purpose-built hardware such as application-specific integrated circuit (ASICs). Currently, there are additional technologies used for digital signal processing including more powerful general purpose microprocessors, field-programmable gate arrays (FPGAs), digital signal controllers (mostly for industrial applications such as motor control), and stream processors, among others.\nDigital signal processing can involve linear or nonlinear operations. Nonlinear signal processing is closely related to nonlinear system identification and can be implemented in the time, frequency, and spatio-temporal domains.","tag_line":"Digital signal processing (DSP) is the numerical manipulation of signals, usually with the intention to measure, filter, produce or compress continuous analog signals.","algorithms":["fast-fourier-transform","fast-walsh–hadamard-transform","generalized-distributive-law","goertzel-algorithm","least-mean-squares-filter","ramer–douglas–peucker-algorithm","verification-based-message-passing-algorithms-in-compressed-sensing"],"name":"Digital signal processing","children":["discrete-transforms","fft-algorithms","image-processing","multidimensional-signal-processing"]}}
,{"_index":"throwtable","_type":"category","_id":"database-algorithms","_score":0,"_source":{"description":"Algorithms used for implementation of database management systems.","tag_line":"Algorithms used for implementation of database management systems.","algorithms":["algorithms-for-recovery-and-isolation-exploiting-semantics","chase-(algorithm)","write-ahead-logging"],"name":"Database algorithms","children":["join-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"evolutionary-algorithms","_score":0,"_source":{"description":"In artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators. Artificial evolution (AE) describes a process involving individual evolutionary algorithms; EAs are individual components that participate in an AE.\nEvolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape; this generality is shown by successes in fields as diverse as engineering, art, biology, economics, marketing, genetics, operations research, robotics, social sciences, physics, politics and chemistry.\nTechniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. The computer simulations Tierra and Avida attempt to model macroevolutionary dynamics.\nIn most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.\nA possible limitation  of many evolutionary algorithms is their lack of a clear genotype-phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism. Such indirect (aka generative or developmental) encodings also enable evolution to exploit the regularity in the environment. Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype-phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes.","tag_line":"In artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm.","algorithms":["evolutionary-algorithm","artificial-bee-colony-algorithm","artificial-immune-system","bat-algorithm","biogeography-based-optimization","cellular-evolutionary-algorithm","cma-es","computer-automated-design","constructive-cooperative-coevolution","cuckoo-search","cultural-algorithm","darwintunes","eagle-strategy","evolutionary-algorithm-for-landmark-detection","evolutionary-music","evolutionary-programming","evolved-antenna","firefly-algorithm","gaussian-adaptation","gene-expression-programming","genetic-programming","genetic-representation","harmony-search","hyperneat","learning-classifier-system","melomics","memetic-algorithm","meta-optimization","natural-evolution-strategy","neuroevolution","neuroevolution-of-augmenting-topologies","particle-swarm-optimization","promoter-based-genetic-algorithm","reward-based-selection","speciation-(genetic-algorithm)"],"name":"Evolutionary algorithms","children":["genetic-programming"]}}
,{"_index":"throwtable","_type":"category","_id":"computer-graphics-algorithms","_score":0,"_source":{"description":"Algorithms used in Computer graphics. See also Category:Computer graphics data structures.","tag_line":"Algorithms used in Computer graphics.","algorithms":["beier–neely-morphing-algorithm","bresenham's-line-algorithm","diamond-square-algorithm","digital-differential-analyzer-(graphics-algorithm)","even–odd-rule","false-radiosity","flood-fill","floyd–steinberg-dithering","geomipmapping","level-set-method","line-drawing-algorithm","marching-cubes","marching-squares","marching-tetrahedra","mclone","newell's-algorithm","painter's-algorithm","progressive-refinement","ramer–douglas–peucker-algorithm","ray-casting","recursive-xy-cut","roam","scanline-rendering","sgi-algorithm","simulated-fluorescence-process-algorithm","warnock-algorithm","xiaolin-wu's-line-algorithm"],"name":"Computer graphics algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"researchers-in-geometric-algorithms","_score":0,"_source":{"algorithms":["pankaj-k.-agarwal","lars-arge","david-avis","jon-bentley","jit-bose","timothy-m.-chan","bernard-chazelle","matthew-t.-dickerson","györgy-elekes","david-eppstein","leonidas-j.-guibas","john-hershberger","david-g.-kirkpatrick","jiří-matoušek-(mathematician)","nimrod-megiddo","joseph-o'rourke-(professor)","mark-overmars","john-reif","pierre-rosenstiehl","jörg-rüdiger-sack","michael-segal","raimund-seidel","michael-ian-shamos","subhash-suri","roberto-tamassia","shang-hua-teng","godfried-toussaint","frances-yao"],"name":"Researchers in geometric algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"machine-learning-algorithms","_score":0,"_source":{"description":"","tag_line":"","algorithms":["almeida–pineda-recurrent-backpropagation","bootstrap-aggregating","cn2-algorithm","constructing-skill-trees","diffusion-map","dynamic-time-warping","expectation–maximization-algorithm","fastica","forward–backward-algorithm","generec","genetic-algorithm-for-rule-set-production","hexq","k-nearest-neighbors-algorithm","kernel-methods-for-vector-output","leabra","linde–buzo–gray-algorithm","local-outlier-factor","logitboost","loss-functions-for-classification","manifold-alignment","minimum-redundancy-feature-selection","multiple-kernel-learning","non-negative-matrix-factorization","online-machine-learning","prefrontal-cortex-basal-ganglia-working-memory","pvlv","quadratic-unconstrained-binary-optimization","query-level-feature","quickprop","randomized-weighted-majority-algorithm","reinforcement-learning","rprop","state-action-reward-state-action","t-distributed-stochastic-neighbor-embedding","temporal-difference-learning","wake-sleep-algorithm","weighted-majority-algorithm"],"name":"Machine learning algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"iterative-methods","_score":0,"_source":{"description":"In computational mathematics, an iterative method is a mathematical procedure that generates a sequence of improving approximate solutions for a class of problems. A specific implementation of an iterative method, including the termination criteria, is an algorithm of the iterative method. An iterative method is called convergent if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic-based iterative methods are also common.\nIn the problems of finding the root of an equation (or a solution of a system of equations), an iterative method uses an initial guess to generate successive approximations to a solution. In contrast, direct methods attempt to solve the problem by a finite sequence of operations. In the absence of rounding errors, direct methods would deliver an exact solution (like solving a linear system of equations  by Gaussian elimination). Iterative methods are often the only choice for nonlinear equations. However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.","tag_line":"In computational mathematics, an iterative method is a mathematical procedure that generates a sequence of improving approximate solutions for a class of problems.","algorithms":["frank–wolfe-algorithm"],"name":"Iterative methods","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"computational-number-theory","_score":0,"_source":{"description":"In mathematics and computer science, computational number theory, also known as algorithmic number theory, is the study of algorithms for performing number theoretic computations.\n\n","tag_line":"In mathematics and computer science, computational number theory, also known as algorithmic number theory, is the study of algorithms for performing number theoretic computations.\n\n","algorithms":["odlyzko–schönhage-algorithm"],"name":"Computational number theory","children":["number-theoretic-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"monte-carlo-methods","_score":0,"_source":{"description":"Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other mathematical methods. Monte Carlo methods are mainly used in three distinct problem classes: optimization, numerical integration, and generating draws from a probability distribution.\nIn physics-related problems, Monte Carlo methods are quite useful for simulating systems with many coupled degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean-Vlasov processes, kinetic models of gases). Other examples include modeling phenomena with significant uncertainty in inputs such as the calculation of risk in business and, in math, evaluation of multidimensional definite integrals with complicated boundary conditions. In application to space and oil exploration problems, Monte Carlo–based predictions of failure, cost overruns and schedule overruns are routinely better than human intuition or alternative \"soft\" methods.\nIn principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (a.k.a. the sample mean) of independent samples of the variable. When the probability distribution of the variable is too complex, mathematicians often use a Markov Chain Monte Carlo (MCMC) sampler. The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution. By the ergodic theorem, the stationary probability distribution is approximated by the empirical measures of the random states of the MCMC sampler.\nIn other important problems we are interested in generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation. These flows of probability distributions can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depends on the distributions of the current random states (see McKean-Vlasov processes, nonlinear filtering equation). In other instances we are given a flow of probability distributions with an increasing level of sampling complexity (path spaces models with an increasing time horizon, Boltzmann-Gibbs measures associated with decreasing temperature parameters, and many others). These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain. A natural way to simulate these sophisticated nonlinear Markov processes is to sample a large number of copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled empirical measures. In contrast with traditional Monte Carlo and Markov chain Monte Carlo methodologies these mean field particle techniques rely on sequential interacting samples. The terminology mean field reflects the fact that each of the samples (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes) interacts with the empirical measures of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes.","tag_line":"Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results.","algorithms":["fisher–yates-shuffle","iterated-filtering","metropolis-light-transport","metropolis–hastings-algorithm","monte-carlo-tree-search","multicanonical-ensemble","simulated-annealing","vegas-algorithm"],"name":"Monte Carlo methods","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"numerical-analysts","_score":0,"_source":{"description":"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).\nOne of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection (YBC 7289), which gives a sexagesimal numerical approximation of , the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in astronomy, carpentry and construction.\nNumerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of , modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\nNumerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st century also the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.\nBefore the advent of modern computers numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.","tag_line":"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).","algorithms":["nimrod-megiddo"],"name":"Numerical analysts","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"numerical-software","_score":0,"_source":{"description":"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).\nOne of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection (YBC 7289), which gives a sexagesimal numerical approximation of , the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in astronomy, carpentry and construction.\nNumerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of , modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\nNumerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st century also the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.\nBefore the advent of modern computers numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.","tag_line":"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).","algorithms":["mpir-(mathematics-software)"],"name":"Numerical software","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"numerical-differential-equations","_score":0,"_source":{"description":"In computational fluid dynamics, the MacCormack method is a widely used discretization scheme for the numerical solution of hyperbolic partial differential equations. This second-order finite difference method was introduced by Robert W. MacCormack in 1969. The MacCormack method is elegant and easy to understand and program.\n\n","tag_line":"In computational fluid dynamics, the MacCormack method is a widely used discretization scheme for the numerical solution of hyperbolic partial differential equations.","algorithms":["pantelides-algorithm"],"name":"Numerical differential equations","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"root-finding-algorithms","_score":0,"_source":{"description":"A root-finding algorithm is a numerical method, or algorithm, for finding a value x such that f(x) = 0, for a given function f. Such an x is called a root of the function f.\nThis article is concerned with finding scalar, real or complex roots, approximated as floating point numbers. Finding integer roots or exact algebraic roots are separate problems, whose algorithms have little in common with those discussed here. (See: Diophantine equation for integer roots)\nFinding a root of f(x) − g(x) = 0 is the same as solving the equation f(x) = g(x). Here, x is called the unknown in the equation. Conversely, any equation can take the canonical form f(x) = 0, so equation solving is the same thing as computing (or finding) a root of a function.\nNumerical root-finding methods use iteration, producing a sequence of numbers that hopefully converge towards a limit, which is a root. The first values of this series are initial guesses. Many methods computes subsequent values by evaluating an auxiliary function on the preceding values. The limit is thus a fixed point of the auxiliary function, which is chosen for having the roots of the original equation as fixed points.\nThe behaviour of root-finding algorithms is studied in numerical analysis. Algorithms perform best when they take advantage of known characteristics of the given function. Thus an algorithm to find isolated real roots of a low-degree polynomial in one variable may bear little resemblance to an algorithm for complex roots of a \"black-box\" function which is not even known to be differentiable. Questions include ability to separate close roots, robustness against failures of continuity and differentiability, reliability despite inevitable numerical errors, and rate of convergence.","tag_line":"A root-finding algorithm is a numerical method, or algorithm, for finding a value x such that f(x) = 0, for a given function f. Such an x is called a root of the function f.\nThis article is concerned with finding scalar, real or complex roots, approximated as floating point numbers.","algorithms":["root-finding-algorithm","aberth-method","alpha-max-plus-beta-min-algorithm","bairstow's-method","brent's-method","durand–kerner-method","fast-inverse-square-root","graeffe's-method","halley's-method","householder's-method","inverse-quadratic-interpolation","jenkins–traub-algorithm","laguerre's-method","lehmer–schur-algorithm","methods-of-computing-square-roots","muller's-method","newton's-method","nth-root-algorithm","ridders'-method","secant-method","shifting-nth-root-algorithm","sidi's-generalized-secant-method","solving-quadratic-equations-with-continued-fractions","splitting-circle-method"],"name":"Root-finding algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"probabilistic-data-structures","_score":0,"_source":{"description":"In mathematics and computer science, a random tree is a tree or arborescence that is formed by a stochastic process. Types of random trees include:\nUniform spanning tree, a spanning tree of a given graph in which each different tree is equally likely to be selected\nRandom minimal spanning tree, spanning trees of a graph formed by choosing random edge weights and using the minimum spanning tree for those weights\nRandom binary tree, binary trees with a given number of nodes, formed by inserting the nodes in a random order or by selecting all possible trees uniformly at random\nRandom recursive tree, increasingly labelled trees, which can be generated using a simple stochastic growth rule.\nTreap or randomized binary search tree, a data structure that uses random choices to simulate a random binary tree for non-random update sequences\nRapidly exploring random tree, a fractal space-filling pattern used as a data structure for searching high-dimensional spaces\nBrownian tree, a fractal tree structure created by diffusion-limited aggregation processes\nRandom forest, a machine-learning classifier based on choosing random subsets of variables for each tree and using the most frequent tree output as the overall classification\nBranching process, a model of a population in which each individual has a random number of children","tag_line":"In mathematics and computer science, a random tree is a tree or arborescence that is formed by a stochastic process.","algorithms":["bloom-filter","rapidly-exploring-random-tree"],"name":"Probabilistic data structures","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"scheduling-algorithms","_score":0,"_source":{"description":"In computing, scheduling is the method by which work specified by some means is assigned to resources that complete the work. The work may be virtual computation elements such as threads, processes or data flows, which are in turn scheduled onto hardware resources such as processors, network links or expansion cards.\nA scheduler is what carries out the scheduling activity. Schedulers are often implemented so they keep all compute resources busy (as in load balancing), allow multiple users to share system resources effectively, or to achieve a target quality of service. Scheduling is fundamental to computation itself, and an intrinsic part of the execution model of a computer system; the concept of scheduling makes it possible to have computer multitasking with a single central processing unit (CPU).\nA scheduler may aim at one of many goals, for example, maximizing throughput (the total amount of work completed per time unit), minimizing response time (time from work becoming enabled until the first point it begins execution on resources), or minimizing latency (the time between work becoming enabled and its subsequent completion), maximizing fairness (equal CPU time to each process, or more generally appropriate times according to the priority and workload of each process). In practice, these goals often conflict (e.g. throughput versus latency), thus a scheduler will implement a suitable compromise. Preference is given to any one of the concerns mentioned above, depending upon the user's needs and objectives.\nIn real-time environments, such as embedded systems for automatic control in industry (for example robotics), the scheduler also must ensure that processes can meet deadlines; this is crucial for keeping the system stable. Scheduled tasks can also be distributed to remote devices across a network and managed through an administrative back end.","tag_line":"In computing, scheduling is the method by which work specified by some means is assigned to resources that complete the work.","algorithms":["atropos-scheduler","coffman–graham-algorithm","critical-path-method","dynamic-priority-scheduling","exponential-backoff","fifo-(computing-and-electronics)","fino","generalized-processor-sharing","graphical-path-method","heterogeneous-earliest-finish-time","interval-scheduling","least-slack-time-scheduling","list-scheduling","modified-due-date-scheduling-heuristic","multilevel-queue","starvation-(computer-science)","sequence-step-algorithm","top-nodes-algorithm"],"name":"Scheduling algorithms","children":["disk-scheduling-algorithms","processor-scheduling-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"randomized-algorithms","_score":0,"_source":{"description":"A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the \"average case\" over all possible choices of random bits. Formally, the algorithm's performance will be a random variable determined by the random bits; thus either the running time, or the output (or both) are random variables.\nOne has to distinguish between algorithms that use the random input to reduce the expected running time or memory usage, but always terminate with a correct result (Las Vegas algorithms) in a bounded amount of time, and probabilistic algorithms, which, depending on the random input, have a chance of producing an incorrect result (Monte Carlo algorithms) or fail to produce a result either by signalling a failure or failing to terminate.\nIn the second case, random performance and random output, the term \"algorithm\" for a procedure is somewhat questionable. In the case of random output, it is no longer formally effective. However, in some cases, probabilistic algorithms are the only practical means of solving a problem.\nIn common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior.","tag_line":"A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic.","algorithms":["randomized-algorithm","approximate-counting-algorithm","atlantic-city-algorithm","entropy-compression","expected-linear-time-mst-algorithm","freivalds'-algorithm","las-vegas-algorithm","list-update-problem","monte-carlo-algorithm","principle-of-deferred-decision","property-testing"],"name":"Randomized algorithms","children":["probabilistic-data-structures","stochastic-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"radar-signal-processing","_score":0,"_source":{"description":"Digital signal processing (DSP) is the numerical manipulation of signals, usually with the intention to measure, filter, produce or compress continuous analog signals. It is characterized by the use of digital signals to represent these signals as discrete time, discrete frequency, or other discrete domain signals in the form of a sequence of numbers or symbols to permit the digital processing of these signals.\nTheoretical analyses and derivations are typically performed on discrete-time signal models, created by the abstract process of sampling. Numerical methods require a digital signal, such as those produced by an analog-to-digital converter (ADC). The processed result might be a frequency spectrum or a set of statistics. But often it is another digital signal that is converted back to analog form by a digital-to-analog converter (DAC). Even if that whole sequence is more complex than analog processing and has a discrete value range, the application of computational power to signal processing allows for many advantages over analog processing in many applications, such as error detection and correction in transmission as well as data compression.\nDigital signal processing and analog signal processing are subfields of signal processing. DSP applications include audio and speech signal processing, sonar and radar signal processing, sensor array processing, spectral estimation, statistical signal processing, digital image processing, signal processing for communications, control of systems, biomedical signal processing, seismic data processing, among others. DSP algorithms have long been run on standard computers, as well as on specialized processors called digital signal processors, and on purpose-built hardware such as application-specific integrated circuit (ASICs). Currently, there are additional technologies used for digital signal processing including more powerful general purpose microprocessors, field-programmable gate arrays (FPGAs), digital signal controllers (mostly for industrial applications such as motor control), and stream processors, among others.\nDigital signal processing can involve linear or nonlinear operations. Nonlinear signal processing is closely related to nonlinear system identification and can be implemented in the time, frequency, and spatio-temporal domains.","tag_line":"Digital signal processing (DSP) is the numerical manipulation of signals, usually with the intention to measure, filter, produce or compress continuous analog signals.","algorithms":[],"name":"Radar signal processing","children":["digital-signal-processing"]}}
,{"_index":"throwtable","_type":"category","_id":"search-algorithms","_score":0,"_source":{"description":"In computer science, a search algorithm is an algorithm for finding an item with specified properties among a collection of items which are coded into a computer program, that look for clues to return what is wanted. The items may be stored individually as records in a database; or may be elements of a search space defined by a mathematical formula or procedure, such as the roots of an equation with integer variables; or a combination of the two, such as the Hamiltonian circuits of a graph.","tag_line":"In computer science, a search algorithm is an algorithm for finding an item with specified properties among a collection of items which are coded into a computer program, that look for clues to return what is wanted.","algorithms":["combinatorial-search","search-algorithm","a*-search-algorithm","all-nearest-smaller-values","alpha–beta-pruning","amplitude-amplification","any-angle-path-planning","b*","backjumping","backtracking","beam-search","beam-stack-search","best-bin-first","best-first-search","bidirectional-search","binary-search-algorithm","breadth-first-search","brute-force-search","d*","dancing-links","depth-first-search","depth-limited-search","dichotomic-search","difference-map-algorithm","dijkstra's-algorithm","exponential-search","fibonacci-search-technique","finger-search-tree","genetic-algorithm","god's-algorithm","graphplan","grover's-algorithm","hill-climbing","hopscotch-hashing","iterative-deepening-a*","incremental-heuristic-search","interpolation-search","jump-point-search","jump-search","k-independent-hashing","k-nearest-neighbors-algorithm","knuth's-algorithm-x","late-move-reductions","lexicographic-breadth-first-search","linear-hashing","linear-search","look-ahead-(backtracking)","mamf","mobilegeddon","mtd-f","null-move-heuristic","parallel-metaheuristic","proof-of-o(log*n)-time-complexity-of-union–find","proof-number-search","rainbow-table","rapidly-exploring-dense-trees","rapidly-exploring-random-tree","search-game","search-based-software-engineering","shuffled-frog-leaping-algorithm","sma*","sss*","stack-search","ternary-search","uniform-binary-search","universal-hashing","uuhash","variation-(game-tree)"],"name":"Search algorithms","children":["hashing","internet-search-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"estimation-theory","_score":0,"_source":{"description":"Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured/empirical data that has a random component. The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data. An estimator attempts to approximate the unknown parameters using the measurements.\nFor example, it is desired to estimate the proportion of a population of voters who will vote for a particular candidate. That proportion is the parameter sought; the estimate is based on a small random sample of voters.\nOr, for example, in radar the goal is to estimate the range of objects (airplanes, boats, etc.) by analyzing the two-way transit timing of received echoes of transmitted pulses. Since the reflected pulses are unavoidably embedded in electrical noise, their measured values are randomly distributed, so that the transit time must be estimated.\nIn estimation theory, two approaches are generally considered. \nThe probabilistic approach (described in this article) assumes that the measured data is random with probability distribution dependent on the parameters of interest\nThe set-membership approach assumes that the measured data vector belongs to a set which depends on the parameter vector.\nFor example, in electrical communication theory, the measurements which contain information regarding the parameters of interest are often associated with a noisy signal. Without randomness, or noise, the problem would be deterministic and estimation would not be needed.","tag_line":"Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured/empirical data that has a random component.","algorithms":["adaptive-projected-subgradient-method","expectation–maximization-algorithm"],"name":"Estimation theory","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"computational-statistics","_score":0,"_source":{"description":"Computational statistics, or statistical computing, is the interface between statistics and computer science. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education.\nThe terms 'computational statistics' and 'statistical computing' are often used interchangeably, although Carlo Lauro (a former president of the International Association for Statistical Computing) proposed making a distinction, defining 'statistical computing' as \"the application of computer science to statistics\", and 'computational statistics' as \"aiming at the design of algorithm for implementing statistical methods on computers, including the ones unthinkable before the computer age (e.g. bootstrap, simulation), as well as to cope with analytically intractable problems\" [sic].\nThe term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models.","tag_line":"Computational statistics, or statistical computing, is the interface between statistics and computer science.","algorithms":["bootstrap-aggregating","bootstrapping-populations","fastica","group-method-of-data-handling","twisting-properties","types-of-artificial-neural-networks"],"name":"Computational statistics","children":["data-mining"]}}
,{"_index":"throwtable","_type":"category","_id":"algorithms","_score":0,"_source":{"description":"In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ AL-gə-ri-dhəm) is a self-contained step-by-step set of operations to be performed. Algorithms exist that perform calculation, data processing, and automated reasoning.\nAn algorithm is an effective method that can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\nThe concept of algorithm has existed for centuries, however a partial formalization of what would become the modern algorithm began with attempts to solve the Entscheidungsproblem (the \"decision problem\") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define \"effective calculability\" or \"effective method\"; those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's \"Formulation 1\" of 1936, and Alan Turing's Turing machines of 1936–7 and 1939. Giving a formal definition of algorithms, corresponding to the intuitive notion, remains a challenging problem.","tag_line":"In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ AL-gə-ri-dhəm) is a self-contained step-by-step set of operations to be performed.","algorithms":["algorithm","list-of-algorithm-general-topics","list-of-algorithms","adaptive-algorithm","adaptive-dimensional-search","the-algorithm-auction","algorithm-characterizations","algorithm-design","algorithm-engineering","algorithmic-logic","algorithmics","the-art-of-computer-programming","avt-statistical-filtering-algorithm","boyer–moore-majority-vote-algorithm","british-museum-algorithm","cascade-learning-based-on-adaboost","chandy-misra-haas-algorithm:resource-model","chinese-whispers-(clustering-method)","collaborative-diffusion","dakota","devex-algorithm","divide-and-conquer-algorithms","domain-reduction-algorithm","driver-scheduling-problem","edgerank","flajolet–martin-algorithm","generalized-distributive-law","grey-wolf-optimizer","hakmem","hcs-clustering-algorithm","holographic-algorithm","hybrid-algorithm","hyphenation-algorithm","in-place-algorithm","jump-and-walk-algorithm","kinodynamic-planning","kisao","kleene's-algorithm","label-propagation-algorithm","lancichinetti-fortunato-radicchi-benchmark","lossy-count-algorithm","manhattan-address-algorithm","maze-generation-algorithm","maze-solving-algorithm","medical-algorithm","metis","one-pass-algorithm","out-of-core-algorithm","ping-pong-scheme","pointer-jumping","predictor–corrector-method","randomization-function","randomized-rounding","rendezvous-hashing","reservoir-sampling","rna22","run-time-algorithm-specialisation","sardinas–patterson-algorithm","sequential-algorithm","sieve-of-eratosthenes","simulation-algorithms-for-atomic-devs","simulation-algorithms-for-coupled-devs","streaming-algorithm","super-recursive-algorithm","timeline-of-algorithms","tomasulo-algorithm","hindley–milner-type-system","xor-swap-algorithm","xulvi-brunet---sokolov-algorithm","zassenhaus-algorithm"],"name":"Algorithms","children":["numerical-analysis","online-algorithms","pattern-matching","computational-physics","pseudo-polynomial-time-algorithms","pseudorandom-number-generators","quantum-algorithms","randomized-algorithms","recursion","scheduling-algorithms","search-algorithms","signal-processing","statistical-algorithms","computational-statistics","algorithms-on-strings","unicode-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"string-collation-algorithms","_score":0,"_source":{"algorithms":["iso-14651","unicode-collation-algorithm"],"name":"String collation algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"pi-algorithms","_score":0,"_source":{"description":"Approximations for the mathematical constant pi (π) in the history of mathematics reached an accuracy within 0.04% of the true value before the beginning of the Common Era (Archimedes). In Chinese mathematics, this was improved to approximations correct to what corresponds to about seven decimal digits by the 5th century.\nFurther progress was made only from the 15th century (Jamshīd al-Kāshī), and early modern mathematicians reached an accuracy of 35 digits by the 18th century (Ludolph van Ceulen), and 126 digits by the 19th century (Jurij Vega), surpassing the accuracy required for any conceivable application outside of pure mathematics.\nThe record of manual approximation of π is held by William Shanks, who calculated 527 digits correctly in the years preceding 1873. Since the mid 20th century, approximation of π has been the task of electronic digital computers; the current record (as of May 2015) is at 13.3 trillion digits, calculated in October 2014.","tag_line":"Approximations for the mathematical constant pi (π) in the history of mathematics reached an accuracy within 0.04% of the true value before the beginning of the Common Era (Archimedes).","algorithms":["bailey–borwein–plouffe-formula","borwein's-algorithm","chudnovsky-algorithm","fee-method","gauss–legendre-algorithm","liu-hui's-π-algorithm"],"name":"Pi algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"concurrency-control-algorithms","_score":0,"_source":{"algorithms":["banker's-algorithm","dekker's-algorithm","lamport's-bakery-algorithm","lamport's-distributed-mutual-exclusion-algorithm","maekawa's-algorithm","non-blocking-algorithm","peterson's-algorithm","raymond's-algorithm","spinlock","szymański's-algorithm","ticket-lock","timestamp-based-concurrency-control"],"name":"Concurrency control algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"cryptographic-hash-functions","_score":0,"_source":{"description":"A cryptographic hash function is a hash function which is considered practically impossible to invert, that is, to recreate the input data from its hash value alone. These one-way hash functions have been called \"the workhorses of modern cryptography\". The input data is often called the message, and the hash value is often called the message digest or simply the digest.\nThe ideal cryptographic hash function has four main properties:\nit is easy to compute the hash value for any given message\nit is infeasible to generate a message from its hash\nit is infeasible to modify a message without changing the hash\nit is infeasible to find two different messages with the same hash.\nCryptographic hash functions have many information security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.","tag_line":"A cryptographic hash function is a hash function which is considered practically impossible to invert, that is, to recreate the input data from its hash value alone.","algorithms":["crypt-(c)","md5","rainbow-table","sha-1","sha-2","universal-hashing"],"name":"Cryptographic hash functions","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"information-theoretically-secure-algorithms","_score":0,"_source":{"description":"A cryptosystem is information-theoretically secure if its security derives purely from information theory. That is, it cannot be broken even when the adversary has unlimited computing power. The adversary simply does not have enough information to break the encryption, so these cryptosystems are considered cryptanalytically unbreakable.\nAn encryption protocol that has information-theoretic security does not depend for its effectiveness on unproven assumptions about computational hardness, and such an algorithm is not vulnerable to future developments in computer power such as quantum computing. An example of an information-theoretically secure cryptosystem is the one-time pad. The concept of information-theoretically secure communication was introduced in 1949 by American mathematician Claude Shannon, the inventor of information theory, who used it to prove that the one-time pad system was secure. Information-theoretically secure cryptosystems have been used for the most sensitive governmental communications, such as diplomatic cables and high-level military communications, because of the great efforts enemy governments expend toward breaking them.\nAn interesting special case is perfect security: an encryption algorithm is perfectly secure if a ciphertext produced using it provides no information about the plaintext without knowledge of the key. If E is a perfectly secure encryption function, for any fixed message m there must exist for each ciphertext c at least one key k such that . It has been proved that any cipher with the perfect secrecy property must use keys with effectively the same requirements as one-time pad keys.\nIt is common for a cryptosystem to leak some information but nevertheless maintain its security properties even against an adversary that has unlimited computational resources. Such a cryptosystem would have information theoretic but not perfect security. The exact definition of security would depend on the cryptosystem in question.\nThere are a variety of cryptographic tasks for which information-theoretic security is a meaningful and useful requirement. A few of these are:\nSecret sharing schemes such as Shamir's are information-theoretically secure (and also perfectly secure) in that less than the requisite number of shares of the secret provide no information about the secret.\nMore generally, secure multiparty computation protocols often, but not always, have information-theoretic security.\nPrivate information retrieval with multiple databases can be achieved with information-theoretic privacy for the user's query.\nReductions between cryptographic primitives or tasks can often be achieved information-theoretically. Such reductions are important from a theoretical perspective, because they establish that primitive  can be realized if primitive  can be realized.\nSymmetric encryption can be constructed under an information-theoretic notion of security called entropic security, which assumes that the adversary knows almost nothing about the message being sent. The goal here is to hide all functions of the plaintext rather than all information about it.\nQuantum cryptography is largely part of information-theoretic cryptography.\nConventional secrecy entails encrypting messages. Beyond this, some scenarios require covert communication, a stronger type of secrecy which also hides the fact that communication is happening at all.","tag_line":"A cryptosystem is information-theoretically secure if its security derives purely from information theory.","algorithms":["information-theoretic-security","shamir's-secret-sharing"],"name":"Information-theoretically secure algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"type-3-encryption-algorithms","_score":0,"_source":{"description":"The vast majority of the National Security Agency's work on encryption is classified, but from time to time NSA participates in standards processes or otherwise publishes information about its cryptographic algorithms. The NSA has categorized encryption items into four product types, and algorithms into two suites. The following is a brief and incomplete summary of public knowledge about NSA algorithms and protocols.","tag_line":"The vast majority of the National Security Agency's work on encryption is classified, but from time to time NSA participates in standards processes or otherwise publishes information about its cryptographic algorithms.","algorithms":["nsa-cryptography"],"name":"Type 3 encryption algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"cryptographic-algorithms","_score":0,"_source":{"description":"In cryptography, encryption is the process of encoding messages or information in such a way that only authorized parties can read it. Encryption does not of itself prevent interception, but denies the message content to the interceptor. In an encryption scheme, the intended communication information or message, referred to as plaintext, is encrypted using an encryption algorithm, generating ciphertext that can only be read if decrypted. For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is in principle possible to decrypt the message without possessing the key, but, for a well-designed encryption scheme, large computational resources and skill are required. An authorized recipient can easily decrypt the message with the key provided by the originator to recipients, but not to unauthorized interceptors.","tag_line":"In cryptography, encryption is the process of encoding messages or information in such a way that only authorized parties can read it.","algorithms":["bach's-algorithm","barrett-reduction","block-cipher-mode-of-operation","cdmf","common-scrambling-algorithm","crypto++","cycles-per-byte","feedback-with-carry-shift-registers","generating-primes","hmac-based-one-time-password-algorithm","industrial-grade-prime","key-schedule","key-wrap","kochanski-multiplication","master-password","modular-exponentiation","montgomery-modular-multiplication","mosquito","nsa-product-types","pr-cpa-advantage","randomness-extractor","rc-algorithm","ring-learning-with-errors-key-exchange","s-box","scrypt","securelog","substitution-permutation-network","supersingular-isogeny-key-exchange","symmetric-key-algorithm","time-based-one-time-password-algorithm"],"name":"Cryptographic algorithms","children":["asymmetric-key-algorithms","broken-cryptography-algorithms","cryptanalytic-algorithms","cryptographic-hash-functions","cryptographically-secure-pseudorandom-number-generators","information-theoretically-secure-algorithms","integer-factorization-algorithms","primality-tests","type-1-encryption-algorithms","type-2-encryption-algorithms","type-3-encryption-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"multidimensional-signal-processing","_score":0,"_source":{"description":"In signal processing, multidimensional signal processing covers all signal processing done using multidimensional signals and systems. While multidimensional signal processing is a subset of signal processing, it is unique in the sense that it deals specifically with data that can only be adequately detailed using more than one dimension. In m-D digital signal processing, useful data is sampled in more than one dimension. Examples of this are image processing and multi-sensor radar detection. Both of these examples use multiple sensors to sample signals and form images based on the manipulation of these multiple signals. Processing in multi-dimension (m-D) requires more complex algorithms, compared to the 1-D case, to handle calculations such as the Fast Fourier Transform due to more degrees of freedom. In some cases, m-D signals and systems can be simplified into single dimension signal processing methods, if the considered systems are separable.\nTypically, multidimensional signal processing is directly associated with digital signal processing because its complexity warrants the use of computer modelling and computation. A multidimensional signal is similar to a single dimensional signal as far as manipulations that can be performed, such as sampling, Fourier analysis, and filtering. The actual computations of these manipulations grow with the number of dimensions.","tag_line":"In signal processing, multidimensional signal processing covers all signal processing done using multidimensional signals and systems.","algorithms":[],"name":"Multidimensional signal processing","children":["image-processing"]}}
,{"_index":"throwtable","_type":"category","_id":"error-detection-and-correction","_score":0,"_source":{"description":"In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. Many communication channels are subject to channel noise, and thus errors may be introduced during transmission from the source to a receiver. Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases.","tag_line":"In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels.","algorithms":["berlekamp–massey-algorithm","forward–backward-algorithm","iso-7064","k-independent-hashing","luhn-algorithm","verhoeff-algorithm"],"name":"Error detection and correction","children":["hash-functions"]}}
,{"_index":"throwtable","_type":"category","_id":"fft-algorithms","_score":0,"_source":{"description":"This category is for fast Fourier transform (FFT) algorithms, i.e. algorithms to compute the discrete Fourier transform (DFT) in O(N log N) time (or better, for approximate algorithms), where  is the number of discrete points.","tag_line":"This category is for fast Fourier transform (FFT) algorithms, i.e.","algorithms":["bruun's-fft-algorithm","butterfly-diagram","cooley–tukey-fft-algorithm","cyclotomic-fast-fourier-transform","fast-fourier-transform","fftw","goertzel-algorithm","prime-factor-fft-algorithm","rader's-fft-algorithm","split-radix-fft-algorithm","twiddle-factor"],"name":"FFT algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"external-memory-algorithms","_score":0,"_source":{"description":"Computer data storage, often called storage or memory, is a technology consisting of computer components and recording media used to retain digital data. It is a core function and fundamental component of computers.\nThe central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but larger and cheaper options farther away. Often the fast volatile technologies (which lose data when powered off) are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\"; however, \"memory\" is sometimes also used when referring to persistent storage.\nIn the Von Neumann architecture, the CPU consists of two main parts: control unit and arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.","tag_line":"Computer data storage, often called storage or memory, is a technology consisting of computer components and recording media used to retain digital data.","algorithms":["cache-oblivious-distribution-sort","cache-oblivious-matrix-multiplication","external-sorting","funnelsort"],"name":"External memory algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"computer-aided-design","_score":0,"_source":{"description":"Computer-aided design (CAD) is the use of computer systems to aid in the creation, modification, analysis, or optimization of a design. CAD software is used to increase the productivity of the designer, improve the quality of design, improve communications through documentation, and to create a database for manufacturing. CAD output is often in the form of electronic files for print, machining, or other manufacturing operations.\nComputer-aided design is used in many fields. Its use in designing electronic systems is known as electronic design automation, or EDA. In mechanical design it is known as mechanical design automation (MDA) or computer-aided design (CAD), which includes the process of creating a technical drawing with the use of computer software.\nCAD software for mechanical design uses either vector-based graphics to depict the objects of traditional drafting, or may also produce raster graphics showing the overall appearance of designed objects. However, it involves more than just shapes. As in the manual drafting of technical and engineering drawings, the output of CAD must convey information, such as materials, processes, dimensions, and tolerances, according to application-specific conventions.\nCAD may be used to design curves and figures in two-dimensional (2D) space; or curves, surfaces, and solids in three-dimensional (3D) space.\nCAD is an important industrial art extensively used in many applications, including automotive, shipbuilding, and aerospace industries, industrial and architectural design, prosthetics, and many more. CAD is also widely used to produce computer animation for special effects in movies, advertising and technical manuals, often called DCC digital content creation. The modern ubiquity and power of computers means that even perfume bottles and shampoo dispensers are designed using techniques unheard of by engineers of the 1960s. Because of its enormous economic importance, CAD has been a major driving force for research in computational geometry, computer graphics (both hardware and software), and discrete differential geometry.\nThe design of geometric models for object shapes, in particular, is occasionally called computer-aided geometric design (CAGD).","tag_line":"Computer-aided design (CAD) is the use of computer systems to aid in the creation, modification, analysis, or optimization of a design.","algorithms":["computer-automated-design","geometric-design","geometric-modeling"],"name":"Computer-aided design","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"network-scheduling-algorithms","_score":0,"_source":{"description":"On a node in packet switching communication network, a network scheduler, also called packet scheduler, is an arbiter program that manages the sequence of network packets in the transmit and receive queues of the network interface controller, which is a circular data buffer. There are several network schedulers available for the different operating system kernels, that implement many of the existing network scheduling algorithms.\nThe network scheduler logic decides, in a way similar to statistical multiplexers, which network packet to forward next from the buffer. The buffer works as a queuing system, storing the network packets temporarily until they are transmitted. The buffer space may be divided into different queues, with each of them holding the packets of one flow according to configured packet classification rules; for example, packets can be divided into flows by their source and destination IP addresses. Network scheduling algorithms and their associated settings determine how the network scheduler manages the buffer.\nAlso, network schedulers are enabling accomplishment of the active queue management and traffic shaping.","tag_line":"On a node in packet switching communication network, a network scheduler, also called packet scheduler, is an arbiter program that manages the sequence of network packets in the transmit and receive queues of the network interface controller, which is a circular data buffer.","algorithms":["active-queue-management","class-based-queueing","deficit-round-robin","delay-gradient-congestion-control","fair-queuing","generic-cell-rate-algorithm","hierarchical-fair-service-curve","interleaved-polling-with-adaptive-cycle-time","leaky-bucket","max-min-fairness","network-scheduler","proportionally-fair","random-early-detection","round-robin-scheduling","token-bucket","weighted-fair-queueing"],"name":"Network scheduling algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"number-theoretic-algorithms","_score":0,"_source":{"description":"This category deals with algorithms in number theory, especially primality testing and similar.","tag_line":"This category deals with algorithms in number theory, especially primality testing and similar.","algorithms":["ancient-egyptian-multiplication","baby-step-giant-step","binary-gcd-algorithm","chakravala-method","cipolla's-algorithm","computational-complexity-of-mathematical-operations","cornacchia's-algorithm","euclidean-algorithm","extended-euclidean-algorithm","generating-primes","integer-relation-algorithm","lehmer's-gcd-algorithm","modular-exponentiation","pocklington's-algorithm","pohlig–hellman-algorithm","pollard's-kangaroo-algorithm","pollard's-rho-algorithm-for-logarithms","tonelli–shanks-algorithm"],"name":"Number theoretic algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"pattern-matching","_score":0,"_source":{"description":"In computer science, pattern matching is the act of checking a given sequence of tokens for the presence of the constituents of some pattern. In contrast to pattern recognition, the match usually has to be exact. The patterns generally have the form of either sequences or tree structures. Uses of pattern matching include outputting the locations (if any) of a pattern within a token sequence, to output some component of the matched pattern, and to substitute the matching pattern with some other token sequence (i.e., search and replace).\nSequence patterns (e.g., a text string) are often described using regular expressions and matched using techniques such as backtracking.\nTree patterns are used in some programming languages as a general tool to process data based on its structure, e.g., Haskell, ML, Scala and the symbolic mathematics language Mathematica have special syntax for expressing tree patterns and a language construct for conditional execution and value retrieval based on it. For simplicity and efficiency reasons, these tree patterns lack some features that are available in regular expressions.\nOften it is possible to give alternative patterns that are tried one by one, which yields a powerful conditional programming construct. Pattern matching sometimes include support for guards.\nTerm rewriting and graph rewriting languages rely on pattern matching for the fundamental way a program evaluates into a result.","tag_line":"In computer science, pattern matching is the act of checking a given sequence of tokens for the presence of the constituents of some pattern.","algorithms":["backtracking","redos","reteoo","rna22","teiresias-algorithm"],"name":"Pattern matching","children":["phonetic-algorithms","regular-expressions","string-matching-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"quantum-algorithms","_score":0,"_source":{"description":"In quantum computing, a quantum algorithm is an algorithm which runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation. A classical (or non-quantum) algorithm is a finite sequence of instructions, or a step-by-step procedure for solving a problem, where each step or instruction can be performed on a classical computer. Similarly, a quantum algorithm is a step-by-step procedure, where each of the steps can be performed on a quantum computer. Although all classical algorithms can also be performed on a quantum computer, the term quantum algorithm is usually used for those algorithms which seem inherently quantum, or use some essential feature of quantum computation such as quantum superposition or quantum entanglement.\nAll problems which can be solved on a quantum computer can be solved on a classical computer. In particular, problems which are undecidable using classical computers remain undecidable using quantum computers. What makes quantum algorithms interesting is that they might be able to solve some problems faster than classical algorithms.\nThe most well known algorithms are Shor's algorithm for factoring, and Grover's algorithm for searching an unstructured database or an unordered list. Shor's algorithms runs exponentially faster than the best known classical algorithm for factoring, the general number field sieve. Grover's algorithm runs quadratically faster than the best possible classical algorithm for the same task.","tag_line":"In quantum computing, a quantum algorithm is an algorithm which runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation.","algorithms":["quantum-algorithm","amplitude-amplification","bht-algorithm","deutsch–jozsa-algorithm","grover's-algorithm","hidden-subgroup-problem","quantum-algorithm-for-linear-systems-of-equations","quantum-fourier-transform","quantum-phase-estimation-algorithm","shor's-algorithm","simon's-problem"],"name":"Quantum algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"disk-scheduling-algorithms","_score":0,"_source":{"description":"These are algorithms designed for disk scheduling tasks.","tag_line":"These are algorithms designed for disk scheduling tasks.","algorithms":["anticipatory-scheduling","elevator-algorithm","fscan","i/o-scheduling","look-algorithm","n-step-scan","shortest-seek-first"],"name":"Disk scheduling algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"hashing","_score":0,"_source":{"description":"In computing, a hash table (hash map) is a data structure used to implement an associative array, a structure that can map keys to values. A hash table uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.\nIdeally, the hash function will assign each key to a unique bucket, but it is possible that two keys will generate an identical hash causing both keys to point to the same bucket. Instead, most hash table designs assume that hash collisions—different keys that are assigned by the hash function to the same bucket—will occur and must be accommodated in some way.\nIn a well-dimensioned hash table, the average cost (number of instructions) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at (amortized) constant average cost per operation.\nIn many situations, hash tables turn out to be more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets.","tag_line":"In computing, a hash table (hash map) is a data structure used to implement an associative array, a structure that can map keys to values.","algorithms":["bloom-filter","hash-join","hopscotch-hashing","linear-hashing","perceptual-hashing","rabin–karp-algorithm","rendezvous-hashing","universal-hashing"],"name":"Hashing","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"unicode-algorithms","_score":0,"_source":{"description":"This category lists articles on algorithms developed by the Unicode Consortium for handling of characters and text.","tag_line":"This category lists articles on algorithms developed by the Unicode Consortium for handling of characters and text.","algorithms":["unicode-collation-algorithm","iso-14651"],"name":"Unicode algorithms","children":[]}}
]

[
{"_index":"throwtable","_type":"category","_id":"checksum-algorithms","_score":0,"_source":{"description":"This is a list of hash functions, including cyclic redundancy checks, checksum functions, and cryptographic hash functions.\n\n","tag_line":"This is a list of hash functions, including cyclic redundancy checks, checksum functions, and cryptographic hash functions.\n\n","algorithms":["checksum","adler-32","bsd-checksum","damm-algorithm","fletcher's-checksum","iso-7064","luhn-algorithm","luhn-mod-n-algorithm","md5","sha-1","sha-2","sysv-checksum","verhoeff-algorithm"],"name":"Checksum algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"distributed-algorithms","_score":0,"_source":{"description":"A distributed algorithm is an algorithm designed to run on computer hardware constructed from interconnected processors. Distributed algorithms are used in many varied application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation.\nDistributed algorithms are a sub-type of parallel algorithm, typically executed concurrently, with separate parts of the algorithm being run simultaneously on independent processors, and having limited information about what the other parts of the algorithm are doing. One of the major challenges in developing and implementing distributed algorithms is successfully coordinating the behavior of the independent parts of the algorithm in the face of processor failures and unreliable communications links. The choice of an appropriate distributed algorithm to solve a given problem depends on both the characteristics of the problem, and characteristics of the system the algorithm will run on such as the type and probability of processor or link failures, the kind of inter-process communication that can be performed, and the level of timing synchronization between separate processes.","tag_line":"A distributed algorithm is an algorithm designed to run on computer hardware constructed from interconnected processors.","algorithms":["distributed-algorithm","berkeley-algorithm","bully-algorithm","cannon's-algorithm","chandra–toueg-consensus-algorithm","chang-and-roberts-algorithm","cristian's-algorithm","distributed-minimum-spanning-tree","edge-chasing","hs-algorithm","lamport-timestamps","local-algorithm","logical-clock",["parallel-algorithm",null],"parallel-tebd","paxos-(computer-science)","raft-(computer-science)","ricart–agrawala-algorithm",["samplesort",null],"snapshot-algorithm","suzuki-kasami-algorithm","synchronizer-(algorithm)","vector-clock","verification-based-message-passing-algorithms-in-compressed-sensing"],"name":"Distributed algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"integer-factorization-algorithms","_score":0,"_source":{"description":"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.\nWhen the numbers are very large, no efficient, non-quantum integer factorization algorithm is known; an effort by several researchers concluded in 2009, factoring a 232-digit number (RSA-768), utilizing hundreds of machines took two years and the researchers estimated that a 1024-bit RSA modulus would take about a thousand times as long. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.\nNot all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.\nMany cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure.","tag_line":"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers.","algorithms":["integer-factorization","algebraic-group-factorisation-algorithm","congruence-of-squares","continued-fraction-factorization","dixon's-factorization-method","euler's-factorization-method","factor-base","general-number-field-sieve","lattice-sieving","lenstra-elliptic-curve-factorization","pollard's-p-−-1-algorithm","pollard's-rho-algorithm","quadratic-sieve","quantum-algorithm-for-linear-systems-of-equations","rational-sieve","rsa-factoring-challenge","rsa-numbers","shanks'-square-forms-factorization","shor's-algorithm","special-number-field-sieve","trial-division","williams'-p-+-1-algorithm"],"name":"Integer factorization algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"pseudorandom-number-generators","_score":0,"_source":{"description":"A pseudorandom number generator (PRNG), also known as a deterministic random bit generator (DRBG), is an algorithm for generating a sequence of numbers whose properties approximate the properties of sequences of random numbers. The PRNG-generated sequence is not truly random, because it is completely determined by a relatively small set of initial values, called the PRNG's seed (which may include truly random values). Although sequences that are closer to truly random can be generated using hardware random number generators, pseudorandom number generators are important in practice for their speed in number generation and their reproducibility.\nPRNGs are central in applications such as simulations (e.g. for the Monte Carlo method), electronic games (e.g. for procedural generation), and cryptography. Cryptographic applications require the output not to be predictable from earlier outputs, and more elaborate algorithms, which do not inherit the linearity of simpler PRNGs, are needed.\nGood statistical properties are a central requirement for the output of a PRNG. In general, careful mathematical analysis is required to have any confidence that a PRNG generates numbers that are sufficiently close to random to suit the intended use. John von Neumann cautioned about the misinterpretation of a PRNG as a truly random generator, and joked that \"Anyone who considers arithmetical methods of producing random digits is, of course, in a state of sin.\"","tag_line":"A pseudorandom number generator (PRNG), also known as a deterministic random bit generator (DRBG), is an algorithm for generating a sequence of numbers whose properties approximate the properties of sequences of random numbers.","algorithms":[["dual-ec-drbg",null],["feedback-with-carry-shift-registers",null],"ziggurat-algorithm"],"name":"Pseudorandom number generators","children":[null]}}
,{"_index":"throwtable","_type":"category","_id":"recurrence-relations","_score":0,"_source":{"description":"In mathematics, a recurrence relation is an equation that recursively defines a sequence or multidimensional array of values, once one or more initial terms are given: each further term of the sequence or array is defined as a function of the preceding terms.\nThe term difference equation sometimes (and for the purposes of this article) refers to a specific type of recurrence relation. However, \"difference equation\" is frequently used to refer to any recurrence relation.\n\n","tag_line":"In mathematics, a recurrence relation is an equation that recursively defines a sequence or multidimensional array of values, once one or more initial terms are given: each further term of the sequence or array is defined as a function of the preceding terms.","algorithms":["master-theorem"],"name":"Recurrence relations","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"matrix-multiplication-algorithms","_score":0,"_source":{"description":"In computer science, Cannon's algorithm is a distributed algorithm for matrix multiplication for two-dimensional meshes first described in 1969 by Lynn Elliot Cannon.\nIt is especially suitable for computers laid out in an N × N mesh. While Cannon's algorithm works well in homogeneous 2D grids, extending it to heterogeneous 2D grids has been shown to be difficult.\nThe main advantage of the algorithm is that its storage requirements remain constant and are independent of the number of processors.\nThe Scalable Universal Matrix Multiplication Algorithm (SUMMA) is a more practical algorithm that requires less workspace and overcomes the need for a square 2D grid. It is used by the ScaLAPACK, PLAPACK, and Elemental libraries.","tag_line":"In computer science, Cannon's algorithm is a distributed algorithm for matrix multiplication for two-dimensional meshes first described in 1969 by Lynn Elliot Cannon.","algorithms":["matrix-multiplication-algorithm",["cache-oblivious-matrix-multiplication",null],["cannon's-algorithm",null],"coppersmith–winograd-algorithm","freivalds'-algorithm","strassen-algorithm"],"name":"Matrix multiplication algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"stochastic-algorithms","_score":0,"_source":{"algorithms":["estimation-of-distribution-algorithm",["least-mean-squares-filter",null],["randomized-algorithm",null],["simultaneous-perturbation-stochastic-approximation",null],"stochastic-computing","stochastic-diffusion-search",["stochastic-universal-sampling",null]],"name":"Stochastic algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"genetic-programming","_score":0,"_source":{"description":"In artificial intelligence, genetic programming (GP) is an evolutionary algorithm-based methodology inspired by biological evolution to find computer programs that perform a user-defined task. Essentially GP is a set of instructions and a fitness function to measure how well a computer has performed a task. It is a specialization of genetic algorithms (GA) where each individual is a computer program. It is a machine learning technique used to optimize a population of computer programs according to a fitness landscape determined by a program's ability to perform a given computational task.","tag_line":"In artificial intelligence, genetic programming (GP) is an evolutionary algorithm-based methodology inspired by biological evolution to find computer programs that perform a user-defined task.","algorithms":[["genetic-programming",null],["gene-expression-programming","gene-expression-programming"],["santa-fe-trail-problem",null],["schema-(genetic-algorithms)",null]],"name":"Genetic programming","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"fixed-points-(mathematics)","_score":0,"_source":{"description":"In mathematics, a fixed point (sometimes shortened to fixpoint, also known as an invariant point) of a function is an element of the function's domain that is mapped to itself by the function. That is to say, c is a fixed point of the function f(x) if and only if f(c) = c. This means f(f(...f(c)...)) = fn(c) = c, an important terminating consideration when recursively computing f. A set of fixed points is sometimes called a fixed set.\nFor example, if f is defined on the real numbers by\n\nthen 2 is a fixed point of f, because f(2) = 2.\nNot all functions have fixed points: for example, if f is a function defined on the real numbers as f(x) = x + 1, then it has no fixed points, since x is never equal to x + 1 for any real number. In graphical terms, a fixed point means the point (x, f(x)) is on the line y = x, or in other words the graph of f has a point in common with that line.\nPoints which come back to the same value after a finite number of iterations of the function are known as periodic points; a fixed point is a periodic point with period equal to one. In projective geometry, a fixed point of a projectivity has been called a double point.\nIn Galois theory, the set of the fixed points of a set of field automorphisms is a field called the fixed field of the set of automorphisms.\n\n","tag_line":"In mathematics, a fixed point (sometimes shortened to fixpoint, also known as an invariant point) of a function is an element of the function's domain that is mapped to itself by the function.","algorithms":[["cycle-detection",null]],"name":"Fixed points (mathematics)","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"recursion","_score":0,"_source":{"description":"Recursion is the process of repeating items in a self-similar way. For instance, when the surfaces of two mirrors are exactly parallel with each other, the nested images that occur are a form of infinite recursion. The term has a variety of meanings specific to a variety of disciplines ranging from linguistics to logic. The most common application of recursion is in mathematics and computer science, in which it refers to a method of defining functions in which the function being defined is applied within its own definition. Specifically, this defines an infinite number of instances (function values), using a finite expression that for some instances may refer to other instances, but in such a way that no loop or infinite chain of references can occur. The term is also used more generally to describe a process of repeating objects in a self-similar way.","tag_line":"Recursion is the process of repeating items in a self-similar way.","algorithms":[["tree-traversal",null]],"name":"Recursion","children":["fixed-points-(mathematics)","recurrence-relations"]}}
,{"_index":"throwtable","_type":"category","_id":"geometric-algorithms","_score":0,"_source":{"description":"This category deals with algorithms in geometry. See also \"Computational geometry\".","tag_line":"This category deals with algorithms in geometry.","algorithms":["bounding-sphere","bowyer–watson-algorithm","bregman-divergence","centroidal-voronoi-tessellation","cgal","closest-pair-of-points-problem","cone-algorithm",["criss-cross-algorithm",null],["euclidean-shortest-path",null],"geometric-design","geometric-modeling","gilbert–johnson–keerthi-distance-algorithm","jts-topology-suite","largest-empty-rectangle","line-segment-intersection","linear-programming","list-of-numerical-computational-geometry-topics","lloyd's-algorithm","midpoint-circle-algorithm","minkowski-portal-refinement","möller–trumbore-intersection-algorithm","nesting-algorithm","planar-straight-line-graph","proximity-problems","prune-and-search",["ramer–douglas–peucker-algorithm",null],"shoelace-formula","stencil-jumping","sweep-line-algorithm","symmetrization-methods","velocity-obstacle","visibility-polygon"],"name":"Geometric algorithms","children":[null,"computer-graphics-algorithms","computer-aided-design","digital-geometry","graph-drawing","mesh-generation","researchers-in-geometric-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"computer-algebra","_score":0,"_source":{"description":"In computational mathematics, computer algebra, also called symbolic computation or algebraic computation is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although, properly speaking, computer algebra should be a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have not any given value and are thus manipulated as symbols (therefore the name of symbolic computation).\nSoftware applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.\nAt the beginning of computer algebra, circa 1970, when the long-known algorithms were first put on computers, they turned out to be highly inefficient. Therefore, a large part of the work of the researchers in the field consisted in revisiting classical algebra in order to make it effective and to discover efficient algorithms to implement this effectiveness. A typical example of this kind of work is the computation of polynomial greatest common divisors, which is required to simplify fractions. Surprisingly, the classical Euclid's algorithm turned out to be inefficient for polynomials over infinite fields, and thus new algorithms needed to be developed. The same was also true for the classical algorithms from linear algebra.\nComputer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs. It is also used for complete scientific computations, when purely numerical methods fail, like in public key cryptography or for some non-linear problems.","tag_line":"In computational mathematics, computer algebra, also called symbolic computation or algebraic computation is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects.","algorithms":["bareiss-algorithm","berlekamp–zassenhaus-algorithm","faugère's-f4-and-f5-algorithms","pollard's-kangaroo-algorithm"],"name":"Computer algebra","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"numerical-analysis","_score":0,"_source":{"description":"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).\nOne of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection (YBC 7289), which gives a sexagesimal numerical approximation of , the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in astronomy, carpentry and construction.\nNumerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of , modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\nNumerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st century also the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.\nBefore the advent of modern computers numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.","tag_line":"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).","algorithms":[["fee-method",null],"interval-contractor","jenkins–traub-algorithm",["level-set-method",null],"minimax-approximation-algorithm",["predictor–corrector-method",null]],"name":"Numerical analysis","children":[null,null,"first-order-methods","iterative-methods","mathematical-optimization",null,"monte-carlo-methods","numerical-analysts","numerical-differential-equations","numerical-linear-algebra","numerical-software","polynomials","root-finding-algorithms",null]}}
,{"_index":"throwtable","_type":"category","_id":"bioinformatics-algorithms","_score":0,"_source":{"description":"Algorithms used in bioinformatics.","tag_line":"Algorithms used in bioinformatics.","algorithms":["baum–welch-algorithm","blast","blast2go","flower-pollination-algorithm","hirschberg's-algorithm","island-algorithm","kabsch-algorithm","needleman–wunsch-algorithm","neighbor-joining","pairwise-algorithm","sequential-pattern-mining","smith–waterman-algorithm","spades-(software)","uclust","ukkonen's-algorithm","upgma","velvet-assembler"],"name":"Bioinformatics algorithms","children":["genetic-algorithms","sequence-alignment-algorithms","substring-indices"]}}
,{"_index":"throwtable","_type":"category","_id":"graph-algorithms","_score":0,"_source":{"description":"The following is a list of algorithms along with one-line descriptions for each.","tag_line":"The following is a list of algorithms along with one-line descriptions for each.","algorithms":["knuth's-simpath-algorithm",["a*-search-algorithm",null],"algorithmic-version-for-szemerédi-regularity-partition","alpha–beta-pruning",["b*",null],"barabási–albert-model","belief-propagation","bellman–ford-algorithm","bidirectional-search","borůvka's-algorithm",["bottleneck-traveling-salesman-problem",null],"breadth-first-search","bron–kerbosch-algorithm","chaitin's-algorithm",["christofides-algorithm",null],"clique-percolation-method","color-coding","contraction-hierarchies","courcelle's-theorem","cuthill–mckee-algorithm","d*","depth-first-search","depth-limited-search",["dijkstra's-algorithm",null],"dijkstra–scholten-algorithm","dinic's-algorithm","disparity-filter-algorithm-of-weighted-network","edmonds'-algorithm","blossom-algorithm","edmonds–karp-algorithm","euler-tour-technique","fkt-algorithm","flooding-algorithm","floyd–warshall-algorithm","force-directed-graph-drawing","ford–fulkerson-algorithm","fringe-search","girvan–newman-algorithm","goal-node-(computer-science)","graph-kernel","havel–hakimi-algorithm","hierarchical-clustering-of-networks","hopcroft–karp-algorithm","iterative-deepening-a*","iterative-compression","johnson's-algorithm","journal-of-graph-algorithms-and-applications","jump-point-search","junction-tree-algorithm","k-shortest-path-routing","karger's-algorithm","kleitman–wang-algorithms","kosaraju's-algorithm","kruskal's-algorithm","lexicographic-breadth-first-search","misra-&-gries-edge-coloring-algorithm",["nearest-neighbour-algorithm",null],"network-simplex-algorithm","path-based-strong-component-algorithm","prim's-algorithm","proof-number-search","push–relabel-maximum-flow-algorithm","reverse-delete-algorithm","rocha–thatte-cycle-detection-algorithm","sethi–ullman-algorithm","shortest-path-faster-algorithm","sma*","spectral-layout","stoer-wagner-algorithm","suurballe's-algorithm","tarjan's-off-line-lowest-common-ancestors-algorithm","tarjan's-strongly-connected-components-algorithm","topological-sorting","travelling-salesman-problem","tree-traversal","widest-path-problem","yen's-algorithm"],"name":"Graph algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"internet-search-algorithms","_score":0,"_source":{"description":"Distributed web crawling is a distributed computing technique whereby Internet search engines employ many computers to index the Internet via web crawling. Such systems may allow for users to voluntarily offer their own computing and bandwidth resources towards crawling web pages. By spreading the load of these tasks across many computers, costs that would otherwise be spent on maintaining large computing clusters are avoided.","tag_line":"Distributed web crawling is a distributed computing technique whereby Internet search engines employ many computers to index the Internet via web crawling.","algorithms":["focused-crawler","pagerank"],"name":"Internet search algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"stochastic-optimization","_score":0,"_source":{"description":"Stochastic optimization (SO) methods are optimization methods that generate and use random variables. For stochastic problems, the random variables appear in the formulation of the optimization problem itself, which involve random objective functions or random constraints, for example. Stochastic optimization methods also include methods with random iterates. Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization. Stochastic optimization methods generalize deterministic methods for deterministic problems.","tag_line":"Stochastic optimization (SO) methods are optimization methods that generate and use random variables.","algorithms":[["biogeography-based-optimization",null],["cma-es",null],["natural-evolution-strategy",null]],"name":"Stochastic optimization","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"asymmetric-key-algorithms","_score":0,"_source":{"description":"Public-key cryptography refers to a set of cryptographic algorithms that are based on mathematical problems that currently admit no efficient solution -- particularly those inherent in certain integer factorization, discrete logarithm, and elliptic curve relationships. It is computationally easy for a user to generate a public and private key-pair and to use it for encryption and decryption. The strength lies in the \"impossibility\" (computational impracticality) for a properly generated private key to be determined from its corresponding public key. Thus the public key may be published without compromising security. Security depends only on keeping the private key private. Public key algorithms, unlike symmetric key algorithms, do not require a secure channel for the initial exchange of one (or more) secret keys between the parties.\nBecause of the computational complexity of asymmetric encryption, it is typically only used for short messages, typically the transfer of a symmetric encryption key. This symmetric key is then used to encrypt the rest of the potentially long & heavy conversation. The symmetric encryption/decryption is based on simpler algorithms and is much faster.\nMessage authentication involves hashing the message to produce a \"digest,\" and encrypting the digest with the private key to produce a digital signature. Thereafter anyone can verify this signature by (1) computing the hash of the message, (2) decrypting the signature with the signer's public key, and (3) comparing the computed digest with the decrypted digest. Equality between the digests confirms the message is unmodified since it was signed, and that the signer, and no one else, intentionally performed the signature operation — presuming the signer's private key has remained secret. The security of such procedure depends on a hash algorithm of such quality that it is computationally impossible to alter or find a substitute message that produces the same digest - but studies have shown that even with the MD5 and SHA-1 algorithms, producing an altered or substitute message is not impossible. The current hashing standard for encryption is SHA-2. The message itself can also be used in place of the digest.\nPublic-key algorithms are fundamental security ingredients in cryptosystems, applications and protocols. They underpin various Internet standards, such as Transport Layer Security (TLS), S/MIME, PGP, and GPG. Some public key algorithms provide key distribution and secrecy (e.g., Diffie–Hellman key exchange), some provide digital signatures (e.g., Digital Signature Algorithm), and some provide both (e.g., RSA).\nPublic-key cryptography finds application in, amongst others, the IT security discipline information security. Information security (IS) is concerned with all aspects of protecting electronic information assets against security threats. Public-key cryptography is used as a method of assuring the confidentiality, authenticity and non-repudiability of electronic communications and data storage.","tag_line":"Public-key cryptography refers to a set of cryptographic algorithms that are based on mathematical problems that currently admit no efficient solution -- particularly those inherent in certain integer factorization, discrete logarithm, and elliptic curve relationships.","algorithms":["coppersmith-method","discrete-logarithm-records","three-pass-protocol","three-pass-protocol","primality-test","schoof's-algorithm","schoof–elkies–atkin-algorithm","three-pass-protocol","three-pass-protocol","xtr"],"name":"Asymmetric-key algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"markov-chain-monte-carlo","_score":0,"_source":{"description":"In statistics, Markov chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from a probability distribution based on constructing a Markov chain that has the desired distribution as its equilibrium distribution. The state of the chain after a number of steps is then used as a sample of the desired distribution. The quality of the sample improves as a function of the number of steps.\n\nRandom walk Monte Carlo methods make up a large subclass of MCMC methods.","tag_line":"In statistics, Markov chain Monte Carlo (MCMC) methods are a class of algorithms for sampling from a probability distribution based on constructing a Markov chain that has the desired distribution as its equilibrium distribution.","algorithms":[["metropolis–hastings-algorithm",null],["wang-and-landau-algorithm",null]],"name":"Markov chain Monte Carlo","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"phonetic-algorithms","_score":0,"_source":{"description":"A phonetic algorithm is an algorithm for indexing of words by their pronunciation. Most phonetic algorithms were developed for use with the English language; consequently, applying the rules to words in other languages might not give a meaningful result.\nThey are necessarily complex algorithms with many rules and exceptions, because English spelling and pronunciation is complicated by historical changes in pronunciation and words borrowed from many languages.\nAmong the best-known phonetic algorithms are:\nSoundex, which was developed to encode surnames for use in censuses. Soundex codes are four-character strings composed of a single letter followed by three numbers.\nDaitch–Mokotoff Soundex, which is a refinement of Soundex designed to better match surnames of Slavic and Germanic origin. Daitch–Mokotoff Soundex codes are strings composed of six numeric digits.\nKölner Phonetik: This is similar to Soundex, but more suitable for German words.\nMetaphone, Double Metaphone, and Metaphone 3 which are suitable for use with most English words, not just names. Metaphone algorithms are the basis for many popular spell checkers.\nNew York State Identification and Intelligence System (NYSIIS), which maps similar phonemes to the same letter. The result is a string that can be pronounced by the reader without decoding.\nMatch Rating Approach developed by Western Airlines in 1977 - this algorithm has an encoding and range comparison technique.\nCaverphone, created to assist in data matching between late 19th century and early 20th century electoral rolls, optimized for accents present in parts of New Zealand.","tag_line":"A phonetic algorithm is an algorithm for indexing of words by their pronunciation.","algorithms":["phonetic-algorithm","caverphone","daitch–mokotoff-soundex","match-rating-approach","metaphone","new-york-state-identification-and-intelligence-system","soundex"],"name":"Phonetic algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"pseudo-polynomial-time-algorithms","_score":0,"_source":{"description":"In computational complexity theory, a numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input, but is exponential in the length of the input – the number of bits required to represent it.\nAn NP-complete problem with known pseudo-polynomial time algorithms is called weakly NP-complete. An NP-complete problem is called strongly NP-complete if it is proven that it cannot be solved by a pseudo-polynomial time algorithm unless P=NP. The strong/weak kinds of NP-hardness are defined analogously.","tag_line":"In computational complexity theory, a numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input, but is exponential in the length of the input – the number of bits required to represent it.","algorithms":["pseudo-polynomial-time"],"name":"Pseudo-polynomial time algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"discrete-transforms","_score":0,"_source":{"description":"In signal processing, discrete transforms are mathematical transforms, often linear transforms, of signals between discrete domains, such as between discrete time and discrete frequency.\nMany common integral transforms used in signal processing have their discrete counterparts. For example, for the Fourier transform the counterpart is the discrete Fourier transform.\nIn addition to spectral analysis of signals, discrete transforms play important role in data compression, signal detection, digital filtering and correlation analysis.\nTransforms between a discrete domain and a continuous domain are not discrete transforms. For example, the discrete-time Fourier transform and the Z-transform, from discrete time to continuous frequency, and the Fourier series, from continuous time to discrete frequency, are outside the class of discrete transforms.\nClassical signal processing deals with one-dimensional discrete transforms. Other application areas, such as image processing, computer vision, high definition television, visual telephony, etc. make use of two-dimensional and in general, multidimensional discrete transforms.\n\n","tag_line":"In signal processing, discrete transforms are mathematical transforms, often linear transforms, of signals between discrete domains, such as between discrete time and discrete frequency.","algorithms":["cyclotomic-fast-fourier-transform",["fast-fourier-transform",null]],"name":"Discrete transforms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"cryptographically-secure-pseudorandom-number-generators","_score":0,"_source":{"description":"ISAAC (indirection, shift, accumulate, add, and count) is a cryptographically secure pseudorandom number generator and a stream cipher designed by Robert J. Jenkins Jr. in 1996.","tag_line":"ISAAC (indirection, shift, accumulate, add, and count) is a cryptographically secure pseudorandom number generator and a stream cipher designed by Robert J. Jenkins Jr. in 1996.","algorithms":[["dual-ec-drbg",null]],"name":"Cryptographically secure pseudorandom number generators","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"calendar-algorithms","_score":0,"_source":{"description":"Algorithms for calculation of certain calendar dates.","tag_line":"Algorithms for calculation of certain calendar dates.","algorithms":["astronomical-algorithm","calendrical-calculation","determination-of-the-day-of-the-week","doomsday-rule","top-nodes-algorithm","zeller's-congruence"],"name":"Calendar algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"exchange-algorithms","_score":0,"_source":{"description":"A greedy algorithm is an algorithm that follows the problem solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. In many problems, a greedy strategy does not in general produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a global optimal solution in a reasonable time.\nFor example, a greedy strategy for the traveling salesman problem (which is of a high computational complexity) is the following heuristic: \"At each stage visit an unvisited city nearest to the current city\". This heuristic need not find a best solution, but terminates in a reasonable number of steps; finding an optimal solution typically requires unreasonably many steps. In mathematical optimization, greedy algorithms solve combinatorial problems having the properties of matroids.","tag_line":"A greedy algorithm is an algorithm that follows the problem solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum.","algorithms":[["bareiss-algorithm",null],"bland's-rule",["criss-cross-algorithm",null],"gaussian-elimination","pivot-element","simplex-algorithm"],"name":"Exchange algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"sorting-algorithms","_score":0,"_source":{"description":"A sorting algorithm is an algorithm that puts elements of a list in a certain order. The most-used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the use of other algorithms (such as search and merge algorithms) which require input data to be in sorted lists; it is also often useful for canonicalizing data and for producing human-readable output. More formally, the output must satisfy two conditions:\nThe output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);\nThe output is a permutation (reordering) of the input.\nFurther, the data is often taken to be in an array, which allows random access, rather than a list, which only allows sequential access, though often algorithms can be applied with suitable modification to either type of data.\nSince the dawn of computing, the sorting problem has attracted a great deal of research, perhaps due to the complexity of solving it efficiently despite its simple, familiar statement. For example, bubble sort was analyzed as early as 1956. A fundamental limit of comparison sorting algorithms is that they require linearithmic time – O(n log n) – in the worst case, though better performance is possible on real-world data (such as almost-sorted data), and algorithms not based on comparisons, such as counting sort, can have better performance. Although many consider sorting a solved problem – asymptotically optimal algorithms have been known since the mid-20th century – useful new algorithms are still being invented, with the now widely used Timsort dating to 2002, and the library sort being first published in 2006.\nSorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, time-space tradeoffs, and upper and lower bounds.\n\n","tag_line":"A sorting algorithm is an algorithm that puts elements of a list in a certain order.","algorithms":["sorting-algorithm","adaptive-heap-sort","adaptive-sort","bead-sort","binary-prioritization","bitonic-sorter","block-sort","bogosort","bubble-sort","bucket-sort","cache-oblivious-distribution-sort","cartesian-tree","cocktail-sort","comb-sort","comparison-sort","counting-sort","cubesort","cycle-sort","dutch-national-flag-problem","elevator-algorithm","external-sorting","flashsort","funnelsort","gnome-sort","heapsort","insertion-sort","integer-sorting","internal-sort","library-sort","median-cut","merge-algorithm","merge-sort","odd–even-sort","pancake-sorting","pigeonhole-sort","polyphase-merge-sort","proxmap-sort","qsort","quantum-sort","quicksort","radix-sort","samplesort","schwartzian-transform","selection-sort","smoothsort","sort-(c++)","sorting-network","spaghetti-sort","splaysort","spreadsort","stooge-sort","timsort",["topological-sorting",null],"tournament-sort","tree-sort","x-+-y-sorting"],"name":"Sorting algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"combinatorial-algorithms","_score":0,"_source":{"algorithms":["bees-algorithm","criss-cross-algorithm","cycle-detection","fisher–yates-shuffle","greedy-algorithm","heap's-algorithm","kernighan–lin-algorithm","lemke–howson-algorithm","lin–kernighan-heuristic","loopless-algorithm","robinson–schensted-correspondence","robinson–schensted–knuth-correspondence","smawk-algorithm","steinhaus–johnson–trotter-algorithm","tompkins–paige-algorithm"],"name":"Combinatorial algorithms","children":["combinatorial-optimization","computational-geometry","exchange-algorithms","graph-algorithms","sorting-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"combinatorial-optimization","_score":0,"_source":{"description":"In applied mathematics and theoretical computer science, combinatorial optimization is a topic that consists of finding an optimal object from a finite set of objects. In many such problems, exhaustive search is not feasible. It operates on the domain of those optimization problems, in which the set of feasible solutions is discrete or can be reduced to discrete, and in which the goal is to find the best solution. Some common problems involving combinatorial optimization are the traveling salesman problem (\"TSP\") and the minimum spanning tree problem (\"MST\").\nCombinatorial optimization is a subset of mathematical optimization that is related to operations research, algorithm theory, and computational complexity theory. It has important applications in several fields, including artificial intelligence, machine learning, mathematics, auction theory, and software engineering.\nSome research literature considers discrete optimization to consist of integer programming together with combinatorial optimization (which in turn is composed of optimization problems dealing with graph structures) although all of these topics have closely intertwined research literature. It often involves determining the way to efficiently allocate resources used to find solutions to mathematical problems.","tag_line":"In applied mathematics and theoretical computer science, combinatorial optimization is a topic that consists of finding an optimal object from a finite set of objects.","algorithms":[["submodular-set-function",null],"a*-search-algorithm","b*","bottleneck-traveling-salesman-problem","branch-and-bound","branch-and-cut","combinatorial-search",["criss-cross-algorithm",null],"dijkstra's-algorithm","harmony-search","job-shop-scheduling",["kernighan–lin-algorithm",null],["lin–kernighan-heuristic",null]],"name":"Combinatorial optimization","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"image-processing","_score":0,"_source":{"description":"In imaging science, image processing is processing of images using mathematical operations by using any form of signal processing for which the input is an image, such as a photograph or video frame; the output of image processing may be either an image or a set of characteristics or parameters related to the image. Most image-processing techniques involve treating the image as a two-dimensional signal and applying standard signal-processing techniques to it.\nImage processing usually refers to digital image processing, but optical and analog image processing also are possible. This article is about general techniques that apply to all of them. The acquisition of images (producing the input image in the first place) is referred to as imaging.\nClosely related to image processing are computer graphics and computer vision. In computer graphics, images are manually made from physical models of objects, environments, and lighting, instead of being acquired (via imaging devices such as cameras) from natural scenes, as in most animated movies. Computer vision, on the other hand, is often considered high-level image processing out of which a machine/computer/software intends to decipher the physical contents of an image or a sequence of images (e.g., videos or 3D full-body magnetic resonance scans).\nIn modern sciences and technologies, images also gain much broader scopes due to the ever growing importance of scientific visualization (of often large-scale complex scientific/experimental data). Examples include microarray data in genetic research, or real-time multi-asset portfolio trading in finance.","tag_line":"In imaging science, image processing is processing of images using mathematical operations by using any form of signal processing for which the input is an image, such as a photograph or video frame; the output of image processing may be either an image or a set of characteristics or parameters related to the image.","algorithms":["false-radiosity","firefly-algorithm","floyd–steinberg-dithering","level-set-method","shepp–logan-phantom"],"name":"Image processing","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"numerical-linear-algebra","_score":0,"_source":{"description":"Numerical linear algebra is the study of algorithms for performing linear algebra computations, most notably matrix operations, on computers. It is often a fundamental part of engineering and computational science problems, such as image and signal processing, telecommunication, computational finance, materials science simulations, structural biology, data mining, bioinformatics, fluid dynamics, and many other areas. Such software relies heavily on the development, analysis, and implementation of state-of-the-art algorithms for solving various numerical linear algebra problems, in large part because of the role of matrices in finite difference and finite element methods.\nCommon problems in numerical linear algebra include computing the following: LU decomposition, QR decomposition, singular value decomposition, eigenvalues.","tag_line":"Numerical linear algebra is the study of algorithms for performing linear algebra computations, most notably matrix operations, on computers.","algorithms":[["bareiss-algorithm",null],"block-lanczos-algorithm",["coppersmith–winograd-algorithm",null],["gaussian-elimination",null],["gradient-method",null],"lu-reduction",["pivot-element",null],"rrqr-factorization"],"name":"Numerical linear algebra","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"automatic-memory-management","_score":0,"_source":{"description":"In computer science, garbage collection (GC) is a form of automatic memory management. The garbage collector, or just collector, attempts to reclaim garbage, or memory occupied by objects that are no longer in use by the program. Garbage collection was invented by John McCarthy around 1959 to abstract away manual memory management in Lisp.\nGarbage collection is often portrayed as the opposite of manual memory management, which requires the programmer to specify which objects to deallocate and return to the memory system. However, many systems use a combination of approaches, including other techniques such as stack allocation and region inference. Like other memory management techniques, garbage collection may take a significant proportion of total processing time in a program and, as a result, can have significant influence on performance.\nResources other than memory, such as network sockets, database handles, user interaction windows, and file and device descriptors, are not typically handled by garbage collection. Methods used to manage such resources, particularly destructors, may suffice to manage memory as well, leaving no need for GC. Some GC systems allow such other resources to be associated with a region of memory that, when collected, causes the other resource to be reclaimed; this is called finalization. Finalization may introduce complications limiting its usability, such as intolerable latency between disuse and reclaim of especially limited resources, or a lack of control over which thread performs the work of reclaiming.","tag_line":"In computer science, garbage collection (GC) is a form of automatic memory management.","algorithms":[["mark-compact-algorithm",null]],"name":"Automatic memory management","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"parsing-algorithms","_score":0,"_source":{"description":"Parsing or syntactic analysis is the process of analysing a string of symbols, either in natural language or in computer languages, conforming to the rules of a formal grammar. The term parsing comes from Latin pars (orationis), meaning part (of speech).\nThe term has slightly different meanings in different branches of linguistics and computer science. Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence, sometimes with the aid of devices such as sentence diagrams. It usually emphasizes the importance of grammatical divisions such as subject and predicate.\nWithin computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information.\nThe term is also used in psycholinguistics when describing language comprehension. In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) \"in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc.\" This term is especially common when discussing what linguistic cues help speakers to interpret garden-path sentences.\nWithin computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters.\n\n","tag_line":"Parsing or syntactic analysis is the process of analysing a string of symbols, either in natural language or in computer languages, conforming to the rules of a formal grammar.","algorithms":["cyk-algorithm",["earley-parser",null],"glr-parser","inside–outside-algorithm","lalr-parser","lr-parser","operator-precedence-parser","parsing-expression-grammar","shunting-yard-algorithm","simple-lr-parser"],"name":"Parsing algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"comparison-sorts","_score":0,"_source":{"description":"Cascade merge sort is similar to the polyphase merge sort but uses a simpler distribution. The merge is slower than a polyphase merge when there are fewer than six files, but faster when there are more than six.\n^ Bradley 1982, pp. 189–190","tag_line":"Cascade merge sort is similar to the polyphase merge sort but uses a simpler distribution.","algorithms":[["adaptive-heap-sort",null],["block-sort",null],["bogosort",null],["bubble-sort",null],["cache-oblivious-distribution-sort",null],["cocktail-sort",null],["comb-sort",null],["cubesort",null],["cycle-sort",null],["funnelsort",null],["gnome-sort",null],["heapsort",null],["insertion-sort",null],["library-sort",null],["merge-sort",null],"multi-key-quicksort",["odd–even-sort",null],["polyphase-merge-sort",null],["quicksort",null],["selection-sort",null],["smoothsort",null],["stooge-sort",null],["timsort",null]],"name":"Comparison sorts","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"heuristic-algorithms","_score":0,"_source":{"description":"In computer science, artificial intelligence, and mathematical optimization, a heuristic is a technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution. This is achieved by trading optimality, completeness, accuracy, or precision for speed. In a way, it can be considered a shortcut.","tag_line":"In computer science, artificial intelligence, and mathematical optimization, a heuristic is a technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution.","algorithms":[["2-opt",null],["3-opt",null],["bat-algorithm",null],"heuristiclab",["kernighan–lin-algorithm",null],["lin–kernighan-heuristic",null],"luus–jaakola","monte-carlo-tree-search",["nearest-neighbour-algorithm",null],"simulated-annealing","social-cognitive-optimization"],"name":"Heuristic algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"hash-functions","_score":0,"_source":{"description":"A hash function is any function that can be used to map data of arbitrary size to data of fixed size. The values returned by a hash function are called hash values, hash codes, hash sums, or simply hashes. One use is a data structure called a hash table, widely used in computer software for rapid data lookup. Hash functions accelerate table or database lookup by detecting duplicated records in a large file. An example is finding similar stretches in DNA sequences. They are also useful in cryptography. A cryptographic hash function allows one to easily verify that some input data maps to a given hash value, but if the input data is unknown, it is deliberately difficult to reconstruct it (or equivalent alternatives) by knowing the stored hash value. This is used for assuring integrity of transmitted data, and is the building block for HMACs, which provide message authentication.\nHash functions are related to (and often confused with) checksums, check digits, fingerprints, randomization functions, error-correcting codes, and ciphers. Although these concepts overlap to some extent, each has its own uses and requirements and is designed and optimized differently. The Hash Keeper database maintained by the American National Drug Intelligence Center, for instance, is more aptly described as a catalogue of file fingerprints than of hash values.","tag_line":"A hash function is any function that can be used to map data of arbitrary size to data of fixed size.","algorithms":[["k-independent-hashing",null]],"name":"Hash functions","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"polynomials","_score":0,"_source":{"description":"In mathematics, a polynomial is an expression consisting of variables (or indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents. An example of a polynomial of a single indeterminate (or variable), x, is x2 − 4x + 7, which is a quadratic polynomial. An example in three variables is x3 + 2xyz2 + yz + 1.\nPolynomials appear in a wide variety of areas of mathematics and science. For example, they are used to form polynomial equations, which encode a wide range of problems, from elementary word problems to complicated problems in the sciences; they are used to define polynomial functions, which appear in settings ranging from basic chemistry and physics to economics and social science; they are used in calculus and numerical analysis to approximate other functions. In advanced mathematics, polynomials are used to construct polynomial rings and algebraic varieties, central concepts in algebra and algebraic geometry.","tag_line":"In mathematics, a polynomial is an expression consisting of variables (or indeterminates) and coefficients, that involves only the operations of addition, subtraction, multiplication, and non-negative integer exponents.","algorithms":["polylogarithmic-function"],"name":"Polynomials","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"computational-geometry","_score":0,"_source":{"description":"Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with history stretching back to antiquity. An ancient precursor is the Sanskrit treatise Shulba Sutras , or \"Rules of the Chord\", that is a book of algorithms written in 800 BCE. The book prescribes step-by-step procedures for constructing geometric objects like altars using a peg and chord.\nComputational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation.\nThe main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.\nOther important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), computer vision (3D reconstruction).\nThe main branches of computational geometry are:\nCombinatorial computational geometry, also called algorithmic geometry, which deals with geometric objects as discrete entities. A groundlaying book in the subject by Preparata and Shamos dates the first use of the term \"computational geometry\" in this sense by 1975.\nNumerical computational geometry, also called machine geometry, computer-aided geometric design (CAGD), or geometric modeling, which deals primarily with representing real-world objects in forms suitable for computer computations in CAD/CAM systems. This branch may be seen as a further development of descriptive geometry and is often considered a branch of computer graphics or CAD. The term \"computational geometry\" in this meaning has been in use since 1971.","tag_line":"Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry.","algorithms":["coreset","euclidean-shortest-path",["polynomial-time-algorithm-for-approximating-the-volume-of-convex-bodies",null]],"name":"Computational geometry","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"graph-drawing","_score":0,"_source":{"description":"Graph drawing is an area of mathematics and computer science combining methods from geometric graph theory and information visualization to derive two-dimensional depictions of graphs arising from applications such as social network analysis, cartography, linguistics, and bioinformatics.\nA drawing of a graph or network diagram is a pictorial representation of the vertices and edges of a graph. This drawing should not be confused with the graph itself: very different layouts can correspond to the same graph. In the abstract, all that matters is which pairs of vertices are connected by edges. In the concrete, however, the arrangement of these vertices and edges within a drawing affects its understandability, usability, fabrication cost, and aesthetics. The problem gets worse, if the graph changes over time by adding and deleting edges (dynamic graph drawing) and the goal is to preserve the user's mental map.","tag_line":"Graph drawing is an area of mathematics and computer science combining methods from geometric graph theory and information visualization to derive two-dimensional depictions of graphs arising from applications such as social network analysis, cartography, linguistics, and bioinformatics.","algorithms":["coffman–graham-algorithm",["force-directed-graph-drawing",null],["journal-of-graph-algorithms-and-applications",null],["spectral-layout",null]],"name":"Graph drawing","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"online-algorithms","_score":0,"_source":{"description":"In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i.e., in the order that the input is fed to the algorithm, without having the entire input available from the start.\nIn contrast, an offline algorithm is given the whole problem data from the beginning and is required to output an answer which solves the problem at hand.\nAs an example, consider the sorting algorithms selection sort and insertion sort: Selection sort repeatedly selects the minimum element from the unsorted remainder and places it at the front, which requires access to the entire input; it is thus an offline algorithm. On the other hand, insertion sort considers one input element per iteration and produces a partial solution without considering future elements. Thus insertion sort is an online algorithm.\nNote that insertion sort produces the optimum result, i.e., a correctly sorted list. For many problems, online algorithms cannot match the performance of offline algorithms. If the ratio between the performance of an online algorithm and an optimal offline algorithm is bounded, the online algorithm is called competitive.\nNot every online algorithm has an offline counterpart.","tag_line":"In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i.e., in the order that the input is fed to the algorithm, without having the entire input available from the start.","algorithms":["online-algorithm","adversary-model","competitive-analysis-(online-algorithm)","k-server-problem",["least-frequently-used",null],["lirs-caching-algorithm",null],"list-update-problem","metrical-task-system",["page-replacement-algorithm",null]],"name":"Online algorithms","children":["online-sorts"]}}
,{"_index":"throwtable","_type":"category","_id":"travelling-salesman-problem","_score":0,"_source":{"description":"The travelling salesman problem (TSP) asks the following question: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city? It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science.\n\nTSP is a special case of the travelling purchaser problem and the Vehicle routing problem.\nIn the theory of computational complexity, the decision version of the TSP (where, given a length L, the task is to decide whether the graph has any tour shorter than L) belongs to the class of NP-complete problems. Thus, it is possible that the worst-case running time for any algorithm for the TSP increases superpolynomially (perhaps, specifically, exponentially) with the number of cities.\nThe problem was first formulated in 1930 and is one of the most intensively studied problems in optimization. It is used as a benchmark for many optimization methods. Even though the problem is computationally difficult, a large number of heuristics and exact methods are known, so that some instances with tens of thousands of cities can be solved completely and even problems with millions of cities can be approximated within a small fraction of 1%.\nThe TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing. In these applications, the concept city represents, for example, customers, soldering points, or DNA fragments, and the concept distance represents travelling times or cost, or a similarity measure between DNA fragments. The TSP also appears in astronomy, as astronomers observing many sources will want to minimise the time spent slewing the telescope between the sources. In many applications, additional constraints such as limited resources or time windows may be imposed.","tag_line":"The travelling salesman problem (TSP) asks the following question: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city?","algorithms":[["travelling-salesman-problem",null],"2-opt","3-opt",["christofides-algorithm",null],["lin–kernighan-heuristic",null],["nearest-neighbour-algorithm",null]],"name":"Travelling salesman problem","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"signal-processing-stubs","_score":0,"_source":{"description":"Signal processing is an enabling technology that encompasses the fundamental theory, applications, algorithms, and implementations of processing or transferring information contained in many different physical, symbolic, or abstract formats broadly designated as signals. It uses mathematical, statistical, computational, heuristic, and linguistic representations, formalisms, and techniques for representation, modelling, analysis, synthesis, discovery, recovery, sensing, acquisition, extraction, learning, security, or forensics.","tag_line":"Signal processing is an enabling technology that encompasses the fundamental theory, applications, algorithms, and implementations of processing or transferring information contained in many different physical, symbolic, or abstract formats broadly designated as signals.","algorithms":[["fast-walsh–hadamard-transform",null]],"name":"Signal processing stubs","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"data-mining-algorithms","_score":0,"_source":{"algorithms":["alpha-algorithm","apriori-algorithm","fsa-red-algorithm","gsp-algorithm","teiresias-algorithm","winepi"],"name":"Data mining algorithms","children":["classification-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"first-order-methods","_score":0,"_source":{"description":"In mathematics, a linear approximation is an approximation of a general function using a linear function (more precisely, an affine function). They are widely used in the method of finite differences to produce first order methods for solving or approximating solutions to equations.","tag_line":"In mathematics, a linear approximation is an approximation of a general function using a linear function (more precisely, an affine function).","algorithms":["frank–wolfe-algorithm","gradient-descent","gradient-method"],"name":"First order methods","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"type-2-encryption-algorithms","_score":0,"_source":{"algorithms":[["nsa-cryptography",null],"skipjack-(cipher)"],"name":"Type 2 encryption algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"algorithms-on-strings","_score":0,"_source":{"description":"Pertains to algorithms that operate on string datatypes.","tag_line":"Pertains to algorithms that operate on string datatypes.","algorithms":[["boyer–moore-string-search-algorithm",null],["hunt–mcilroy-algorithm",null],"string-kernel",["ukkonen's-algorithm",null],"wagner–fischer-algorithm"],"name":"Algorithms on strings","children":["parsing-algorithms",null,null,"string-similarity-measures",null,"string-collation-algorithms",null,null]}}
,{"_index":"throwtable","_type":"category","_id":"memory-management-algorithms","_score":0,"_source":{"description":"Algorithms used for memory management.","tag_line":"Algorithms used for memory management.","algorithms":["adaptive-replacement-cache","buddy-memory-allocation","cache-algorithms","least-frequently-used","lirs-caching-algorithm","local-replacement-algorithm","mark-compact-algorithm","page-replacement-algorithm","pseudo-lru","slob"],"name":"Memory management algorithms","children":["automatic-memory-management"]}}
,{"_index":"throwtable","_type":"category","_id":"data-clustering-algorithms","_score":0,"_source":{"description":"This category contains algorithms used for cluster analysis.","tag_line":"This category contains algorithms used for cluster analysis.","algorithms":["affinity-propagation","basic-sequential-algorithmic-scheme","birch","canopy-clustering-algorithm","cluster-weighted-modeling","cobweb-(clustering)","constrained-clustering","cure-data-clustering-algorithm","data-stream-clustering","dbscan","expectation–maximization-algorithm","flame-clustering","fuzzy-clustering","information-bottleneck-method","k-q-flats","k-means-clustering","k-means++","k-medians-clustering","k-medoids","k-svd","linde–buzo–gray-algorithm","mean-shift","nearest-neighbor-chain-algorithm",["neighbor-joining",null],"optics-algorithm","user:pakoch/sandbox","subclu",["upgma",null]],"name":"Data clustering algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"statistical-algorithms","_score":0,"_source":{"algorithms":["algorithms-for-calculating-variance",["baum–welch-algorithm",null],"buzen's-algorithm",["canopy-clustering-algorithm",null],"elston–stewart-algorithm",["expectation–maximization-algorithm",null],"fnn-algorithm",["gauss–newton-algorithm",null],["iterated-filtering",null],"iterative-proportional-fitting",["k-means-clustering",null],["k-means++",null],["k-medians-clustering",null],["k-medoids",null],"lander–green-algorithm",["levenberg–marquardt-algorithm",null],["metropolis–hastings-algorithm",null],["multicanonical-ensemble",null],"nested-sampling-algorithm",["odds-algorithm",null],"ransac",["vegas-algorithm",null],["wang-and-landau-algorithm",null],"yamartino-method",["ziggurat-algorithm",null]],"name":"Statistical algorithms","children":[null]}}
,{"_index":"throwtable","_type":"category","_id":"mathematical-optimization","_score":0,"_source":{"description":"In mathematics, computer science and operations research, mathematical optimization (alternatively, optimization or mathematical programming) is the selection of a best element (with regard to some criteria) from some set of available alternatives.\nIn the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations comprises a large area of applied mathematics. More generally, optimization includes finding \"best available\" values of some objective function given a defined domain (or a set of constraints), including a variety of different types of objective functions and different types of domains.","tag_line":"In mathematics, computer science and operations research, mathematical optimization (alternatively, optimization or mathematical programming) is the selection of a best element (with regard to some criteria) from some set of available alternatives.","algorithms":[["2-opt",null],["3-opt",null],["adaptive-dimensional-search",null],["constructive-cooperative-coevolution",null],"dynamic-programming",["genetic-algorithm",null],["genetic-programming","genetic-programming"],["hardness-of-approximation",null],["interval-contractor",null],["job-shop-scheduling",null],"klee–minty-cube","lemke's-algorithm",["level-set-method",null],["lloyd's-algorithm",null],["meta-optimization",null],"odds-algorithm",["parallel-metaheuristic",null],"pattern-search-(optimization)","shuffled-frog-leaping-algorithm","special-ordered-set","successive-linear-programming","ternary-search"],"name":"Mathematical optimization","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"lossless-compression-algorithms","_score":0,"_source":{"description":"Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast, lossy compression permits reconstruction only of an approximation of the original data, though this usually improves compression rates (and therefore reduces file sizes).\nLossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by the LAME MP3 encoder and other lossy audio encoders).\nLossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data could be deleterious. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods. Lossless audio formats are most often used for archiving or production purposes, while smaller lossy audio files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary.","tag_line":"Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data.","algorithms":["lossless-compression","7z","adam7-algorithm","adaptive-coding","algorithm-bstw","brotli","burrows–wheeler-transform","byte-pair-encoding","bzip2","canonical-huffman-code","chain-code","context-tree-weighting","context-adaptive-binary-arithmetic-coding","deflate","dictionary-coder","dynamic-markov-compression","embedded-zerotrees-of-wavelet-transforms","felics","huffman-coding","huffyuv","incompressible-string","incremental-encoding","lempel–ziv–markov-chain-algorithm","lempel–ziv–oberhumer","lempel–ziv–stac","lempel–ziv–storer–szymanski","lempel–ziv–welch","liblzg","lossless-compression","lz4-(compression-algorithm)","lz77-and-lz78","lzjb","lzrw","lzwl","lzx-(algorithm)","microsoft-point-to-point-compression","move-to-front-transform","package-merge-algorithm","prediction-by-partial-matching","prefix-code","quad-(compressor)","sequitur-algorithm","ut-video-codec-suite","zopfli","zpaq"],"name":"Lossless compression algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"mesh-generation","_score":0,"_source":{"description":"Mesh generation is the practice of generating a polygonal or polyhedral mesh that approximates a geometric domain. The term \"grid generation\" is often used interchangeably. Typical uses are for rendering to a computer screen or for physical simulation such as finite element analysis or computational fluid dynamics. The input model form can vary greatly but common sources are CAD, NURBS, B-rep, STL (file format) or a point cloud. The field is highly interdisciplinary, with contributions found in mathematics, computer science, and engineering.\nThree-dimensional meshes created for finite element analysis need to consist of tetrahedra, pyramids, prisms or hexahedra. Those used for the finite volume method can consist of arbitrary polyhedra. Those used for finite difference methods usually need to consist of piecewise structured arrays of hexahedra known as multi-block structured meshes. A mesh is otherwise a discretization of a domain existing in one, two or three dimensions.","tag_line":"Mesh generation is the practice of generating a polygonal or polyhedral mesh that approximates a geometric domain.","algorithms":[["marching-cubes",null]],"name":"Mesh generation","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"computational-physics","_score":0,"_source":{"description":"Computational physics is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists. Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science.\nIt is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics, a third way that supplements theory and experiment.","tag_line":"Computational physics is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists.","algorithms":[["astronomical-algorithm",null],"featherstone's-algorithm",["multicanonical-ensemble",null],["parallel-tebd",null],["vegas-algorithm",null],"wang-and-landau-algorithm"],"name":"Computational physics","children":["computational-fluid-dynamics",null]}}
,{"_index":"throwtable","_type":"category","_id":"network-flow","_score":0,"_source":{"description":"In graph theory, a flow network (also known as a transportation network) is a directed graph where each edge has a capacity and each edge receives a flow. The amount of flow on an edge cannot exceed the capacity of the edge. Often in operations research, a directed graph is called a network. The vertices are called nodes and the edges are called arcs. A flow must satisfy the restriction that the amount of flow into a node equals the amount of flow out of it, unless it is a source, which has only outgoing flow, or sink, which has only incoming flow. A network can be used to model traffic in a road system, circulation with demands, fluids in pipes, currents in an electrical circuit, or anything similar in which something travels through a network of nodes.","tag_line":"In graph theory, a flow network (also known as a transportation network) is a directed graph where each edge has a capacity and each edge receives a flow.","algorithms":[["dinic's-algorithm",null],["edmonds–karp-algorithm",null],["ford–fulkerson-algorithm",null],["iterative-compression",null],["network-simplex-algorithm",null],"out-of-kilter-algorithm",["push–relabel-maximum-flow-algorithm",null]],"name":"Network flow","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"compression-algorithms","_score":0,"_source":{"description":"In digital signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation. Compression can be either lossy or lossless. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression reduces bits by identifying unnecessary information and removing it. The process of reducing the size of a data file is referred to as data compression. In the context of data transmission, it is called source coding (encoding done at the source of the data before it is stored or transmitted) in opposition to channel coding.\nCompression is useful because it helps reduce resource usage, such as data storage space or transmission capacity. Because compressed data must be decompressed to use, this extra processing imposes computational or other costs through decompression; this situation is far from being a free lunch. Data compression is subject to a space–time complexity trade-off. For instance, a compression scheme for video may require expensive hardware for the video to be decompressed fast enough to be viewed as it is being decompressed, and the option to decompress the video in full before watching it may be inconvenient or require additional storage. The design of data compression schemes involves trade-offs among various factors, including the degree of compression, the amount of distortion introduced (when using lossy data compression), and the computational resources required to compress and decompress the data.\n\n","tag_line":"In digital signal processing, data compression, source coding, or bit-rate reduction involves encoding information using fewer bits than the original representation.","algorithms":["geohash-36","power-quality-compression-algorithm","reduced-offset-lempel-ziv"],"name":"Compression algorithms","children":["lossless-compression-algorithms","lossy-compression-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"processor-scheduling-algorithms","_score":0,"_source":{"algorithms":["deadline-monotonic-scheduling","earliest-deadline-first-scheduling","fair-share-scheduling","foreground-background","gang-scheduling","lottery-scheduling","multilevel-feedback-queue","processor-affinity","proportional-share-scheduling","rate-monotonic-scheduling",["round-robin-scheduling",null],"shortest-job-next","shortest-remaining-time","starvation-(computer-science)","yds-algorithm"],"name":"Processor scheduling algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"broken-cryptography-algorithms","_score":0,"_source":{"description":"Category for cryptographic algorithms or primitives that have been broken.","tag_line":"Category for cryptographic algorithms or primitives that have been broken.","algorithms":["crypt-(c)","dual-ec-drbg","ms-chap","wired-equivalent-privacy"],"name":"Broken cryptography algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"signal-processing","_score":0,"_source":{"description":"Signal processing is an enabling technology that encompasses the fundamental theory, applications, algorithms, and implementations of processing or transferring information contained in many different physical, symbolic, or abstract formats broadly designated as signals. It uses mathematical, statistical, computational, heuristic, and linguistic representations, formalisms, and techniques for representation, modelling, analysis, synthesis, discovery, recovery, sensing, acquisition, extraction, learning, security, or forensics.","tag_line":"Signal processing is an enabling technology that encompasses the fundamental theory, applications, algorithms, and implementations of processing or transferring information contained in many different physical, symbolic, or abstract formats broadly designated as signals.","algorithms":["fast-folding-algorithm"],"name":"Signal processing","children":[null,"estimation-theory",null,null,"signal-processing-stubs"]}}
,{"_index":"throwtable","_type":"category","_id":"routing-algorithms","_score":0,"_source":{"description":"Routing is the process of selecting best paths in a network. In the past, the term routing also meant forwarding network traffic among networks. However, that latter function is better described as forwarding. Routing is performed for many kinds of networks, including the telephone network (circuit switching), electronic data networks (such as the Internet), and transportation networks. This article is concerned primarily with routing in electronic data networks using packet switching technology.\nIn packet switching networks, routing directs packet forwarding (the transit of logically addressed network packets from their source toward their ultimate destination) through intermediate nodes. Intermediate nodes are typically network hardware devices such as routers, bridges, gateways, firewalls, or switches. General-purpose computers can also forward packets and perform routing, though they are not specialized hardware and may suffer from limited performance. The routing process usually directs forwarding on the basis of routing tables, which maintain a record of the routes to various network destinations. Thus, constructing routing tables, which are held in the router's memory, is very important for efficient routing. Most routing algorithms use only one network path at a time. Multipath routing techniques enable the use of multiple alternative paths.\nIn case of overlapping/equal routes, algorithms consider the following elements to decide which routes to install into the routing table (sorted by priority):\nPrefix-Length: where longer subnet masks are preferred (independent of whether it is within a routing protocol or over different routing protocol)\nMetric: where a lower metric/cost is preferred (only valid within one and the same routing protocol)\nAdministrative distance: where a route learned from a more reliable routing protocol is preferred (only valid between different routing protocols)\nRouting, in a more narrow sense of the term, is often contrasted with bridging in its assumption that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices. In large networks, structured addressing (routing, in the narrow sense) outperforms unstructured addressing (bridging). Routing has become the dominant form of addressing on the Internet. Bridging is still widely used within localized environments.","tag_line":"Routing is the process of selecting best paths in a network.","algorithms":[["a*-search-algorithm",null],["b*",null],["backpressure-routing",null],["contraction-hierarchies",null],"diffusing-update-algorithm",["dijkstra's-algorithm",null],"distance-vector-routing-protocol","edge-disjoint-shortest-pair-algorithm","expected-transmission-count","flooding-(computer-networking)",["flooding-algorithm",null],["floyd–warshall-algorithm",null],"geographic-routing",["iterative-deepening-a*",null],"link-state-routing-protocol",["luleå-algorithm",null],["max-min-fairness",null],"mentor-routing-algorithm","optimization-mechanism","pathfinding",["sma*",null],["suurballe's-algorithm",null],"temporally-ordered-routing-algorithm"],"name":"Routing algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"join-algorithms","_score":0,"_source":{"description":"A SQL join clause combines records from two or more tables in a relational database. It creates a set that can be saved as a table or used as it is. A JOIN is a means for combining fields from two tables (or more) by using values common to each. ANSI-standard SQL specifies five types of JOIN: INNER, LEFT OUTER, RIGHT OUTER, FULL OUTER and CROSS. As a special case, a table (base table, view, or joined table) can JOIN to itself in a self-join.\nA programmer writes a JOIN statement to identify the records for joining. If the evaluated predicate is true, the combined record is then produced in the expected format, a record set or a temporary table.","tag_line":"A SQL join clause combines records from two or more tables in a relational database.","algorithms":["block-nested-loop","hash-join","nested-loop-join","sort-merge-join"],"name":"Join algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"genetic-algorithms","_score":0,"_source":{"description":"In the field of artificial intelligence, a genetic algorithm (GA) is a search heuristic that mimics the process of natural selection. This heuristic (also sometimes called a metaheuristic) is routinely used to generate useful solutions to optimization and search problems. Genetic algorithms belong to the larger class of evolutionary algorithms (EA), which generate solutions to optimization problems using techniques inspired by natural evolution, such as inheritance, mutation, selection, and crossover.","tag_line":"In the field of artificial intelligence, a genetic algorithm (GA) is a search heuristic that mimics the process of natural selection.","algorithms":["genetic-algorithm","chromosome-(genetic-algorithm)","clonal-selection-algorithm","crossover-(genetic-algorithm)","cultural-algorithm","defining-length","edge-recombination-operator","evolver-(software)","fitness-function","fitness-proportionate-selection","genetic-algorithm","gene-expression-programming","genetic-algorithm-scheduling","genetic-algorithms-in-economics","genetic-fuzzy-systems","genetic-memory-(computer-science)","genetic-operator","genetic-programming","holland's-schema-theorem","hyperneat","inheritance-(genetic-algorithm)","list-of-genetic-algorithm-applications","mutation-(genetic-algorithm)","neuroevolution-of-augmenting-topologies","parallel-metaheuristic","population-based-incremental-learning","premature-convergence","promoter-based-genetic-algorithm","quality-control-and-genetic-algorithms","reward-based-selection","santa-fe-trail-problem","schema-(genetic-algorithms)","search-based-software-engineering","selection-(genetic-algorithm)","speciation-(genetic-algorithm)","stochastic-universal-sampling","tournament-selection","truncation-selection"],"name":"Genetic algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"string-matching-algorithms","_score":0,"_source":{"description":"In computer science, string searching algorithms, sometimes called string matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.\nLet Σ be an alphabet (finite set). Formally, both the pattern and searched text are vectors of elements of Σ. The Σ may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (Σ = {0,1}) or DNA alphabet (Σ = {A,C,G,T}) in bioinformatics.\nIn practice, how the string is encoded can affect the feasible string search algorithms. In particular if a variable width encoding is in use then it is slow (time proportional to N) to find the Nth character. This will significantly slow down many of the more advanced search algorithms. A possible solution is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.","tag_line":"In computer science, string searching algorithms, sometimes called string matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.","algorithms":["string-searching-algorithm","aho–corasick-algorithm","apostolico–giancarlo-algorithm","bitap-algorithm","boyer–moore-string-search-algorithm","boyer–moore–horspool-algorithm","commentz-walter-algorithm","knuth–morris–pratt-algorithm","rabin–karp-algorithm","raita-algorithm","zhu–takaoka-string-matching-algorithm"],"name":"String matching algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"string-similarity-measures","_score":0,"_source":{"description":"In mathematics and computer science, a string metric (also known as a string similarity metric or string distance function) is a metric that measures distance (\"inverse similarity\") between two text strings for approximate string matching or comparison and in fuzzy string searching. A necessary requirement for a string metric (e.g. in contrast to string matching) is fulfillment of the triangle inequality. For example the strings \"Sam\" and \"Samuel\" can be considered to be close. A string metric provides a number indicating an algorithm-specific indication of distance.\nThe most widely known string metric is a rudimentary one called the Levenshtein Distance (also known as Edit Distance). It operates between two input strings, returning a number equivalent to the number of substitutions and deletions needed in order to transform one input string into another. Simplistic string metrics such as Levenshtein distance have expanded to include phonetic, token, grammatical and character-based methods of statistical comparisons.\nString metrics are used heavily in information integration and are currently used in areas including fraud detection, fingerprint analysis, plagiarism detection, ontology merging, DNA analysis, RNA analysis, image analysis, evidence-based machine learning, database data deduplication, data mining, Web interfaces, e.g. Ajax-style suggestions as you type, data integration, and semantic knowledge integration.","tag_line":"In mathematics and computer science, a string metric (also known as a string similarity metric or string distance function) is a metric that measures distance (\"inverse similarity\") between two text strings for approximate string matching or comparison and in fuzzy string searching.","algorithms":[["string-kernel",null],["wagner–fischer-algorithm",null]],"name":"String similarity measures","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"sequence-alignment-algorithms","_score":0,"_source":{"description":"In bioinformatics, a sequence alignment is a way of arranging the sequences of DNA, RNA, or protein to identify regions of similarity that may be a consequence of functional, structural, or evolutionary relationships between the sequences. Aligned sequences of nucleotide or amino acid residues are typically represented as rows within a matrix. Gaps are inserted between the residues so that identical or similar characters are aligned in successive columns. Sequence alignments are also used for non-biological sequences, such as calculating the edit distance cost between strings in a natural language or in financial data.","tag_line":"In bioinformatics, a sequence alignment is a way of arranging the sequences of DNA, RNA, or protein to identify regions of similarity that may be a consequence of functional, structural, or evolutionary relationships between the sequences.","algorithms":[["hirschberg's-algorithm",null],["needleman–wunsch-algorithm",null],["smith–waterman-algorithm",null]],"name":"Sequence alignment algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"digital-geometry","_score":0,"_source":{"description":"Digital geometry deals with discrete sets (usually discrete point sets) considered to be digitized models or images of objects of the 2D or 3D Euclidean space.\nSimply put, digitizing is replacing an object by a discrete set of its points. The images we see on the TV screen, the raster display of a computer, or in newspapers are in fact digital images.\nIts main application areas are computer graphics and image analysis.\nMain aspects of study are:\nConstructing digitized representations of objects, with the emphasis on precision and efficiency (either by means of synthesis, see, for example, Bresenham's line algorithm or digital disks, or by means of digitization and subsequent processing of digital images).\nStudy of properties of digital sets; see, for example, Pick's theorem, digital convexity, digital straightness, or digital planarity.\nTransforming digitized representations of objects, for example (A) into simplified shapes such as (i) skeletons, by repeated removal of simple points such that the digital topology of an image does not change, or (ii) medial axis, by calculating local maxima in a distance transform of the given digitized object representation, or (B) into modified shapes using mathematical morphology.\nReconstructing \"real\" objects or their properties (area, length, curvature, volume, surface area, and so forth) from digital images.\nStudy of digital curves, digital surfaces, and digital manifolds.\nDesigning tracking algorithms for digital objects.\nFunctions on digital space.\nDigital geometry heavily overlaps with discrete geometry and may be considered as a part thereof.","tag_line":"Digital geometry deals with discrete sets (usually discrete point sets) considered to be digitized models or images of objects of the 2D or 3D Euclidean space.","algorithms":[["bresenham's-line-algorithm",null],["digital-differential-analyzer-(graphics-algorithm)",null],["midpoint-circle-algorithm",null]],"name":"Digital geometry","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"computational-fluid-dynamics","_score":0,"_source":{"description":"Computational fluid dynamics, usually abbreviated as CFD, is a branch of fluid mechanics that uses numerical analysis and algorithms to solve and analyze problems that involve fluid flows. Computers are used to perform the calculations required to simulate the interaction of liquids and gases with surfaces defined by boundary conditions. With high-speed supercomputers, better solutions can be achieved. Ongoing research yields software that improves the accuracy and speed of complex simulation scenarios such as transonic or turbulent flows. Initial experimental validation of such software is performed using a wind tunnel with the final validation coming in full-scale testing, e.g. flight tests.","tag_line":"Computational fluid dynamics, usually abbreviated as CFD, is a branch of fluid mechanics that uses numerical analysis and algorithms to solve and analyze problems that involve fluid flows.","algorithms":[["level-set-method",null]],"name":"Computational fluid dynamics","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"networking-algorithms","_score":0,"_source":{"description":"Algorithms for Computer System.","tag_line":"Algorithms for Computer System.","algorithms":["backpressure-routing","chung-kwei-(algorithm)","exponential-backoff","generic-cell-rate-algorithm","karn's-algorithm","luleå-algorithm","nagle's-algorithm"],"name":"Networking algorithms","children":["network-scheduling-algorithms","routing-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"type-1-encryption-algorithms","_score":0,"_source":{"algorithms":[["nsa-product-types",null],"baton","nsa-cryptography","saville"],"name":"Type 1 encryption algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"data-mining","_score":0,"_source":{"description":"Data mining is an interdisciplinary subfield of computer science. It is the computational process of discovering patterns in large data sets (\"big data\") involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems. The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use. Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.\nThe term is a misnomer, because the goal is the extraction of patterns and knowledge from large amount of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence, machine learning, and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics – or, when referring to actual methods, artificial intelligence and machine learning – are more appropriate.\nThe actual data mining task is the automatic or semi-automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.\nThe related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.","tag_line":"Data mining is an interdisciplinary subfield of computer science.","algorithms":[["local-outlier-factor",null],["multifactor-dimensionality-reduction",null],["multiple-kernel-learning",null],["sequential-pattern-mining",null]],"name":"Data mining","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"regular-expressions","_score":0,"_source":{"description":"In theoretical computer science and formal language theory, a regular expression (sometimes called a rational expression) is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching, i.e. \"find and replace\"-like operations. The concept arose in the 1950s, when the American mathematician Stephen Kleene formalized the description of a regular language, and came into common use with the Unix text processing utilities ed, an editor, and grep, a filter.\nIn modern usage, \"regular expressions\" are often distinguished from the derived, but fundamentally distinct concepts of regex or regexp, which no longer describe a regular language. See below for details.\nRegexps are so useful in computing that the various systems to specify regexps have evolved to provide both a basic and extended standard for the grammar and syntax; modern regexps heavily augment the standard. Regexp processors are found in several search engines, search and replace dialogs of several word processors and text editors, and in the command lines of text processing utilities, such as sed and AWK.\nMany programming languages provide regexp capabilities, some built-in, for example Perl, JavaScript, Ruby, AWK, and Tcl, and others via a standard library, for example .NET languages, Java, Python, POSIX C and C++ (since C++11). Most other languages offer regexps via a library.\n\n","tag_line":"In theoretical computer science and formal language theory, a regular expression (sometimes called a rational expression) is a sequence of characters that define a search pattern, mainly for use in pattern matching with strings, or string matching, i.e.","algorithms":[["kleene's-algorithm",null],["redos",null]],"name":"Regular expressions","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"classification-algorithms","_score":0,"_source":{"description":"This category is about statistical classification algorithms. For more information, see Statistical classification.","tag_line":"This category is about statistical classification algorithms.","algorithms":["statistical-classification","adaboost","alopex","boosting-(machine-learning)","boruta-(algorithm)","brownboost","c4.5-algorithm","sukhotin's-algorithm","co-training","coboosting","compositional-pattern-producing-network","decision-boundary","generalization-error","group-method-of-data-handling","id3-algorithm","information-fuzzy-networks","k-nearest-neighbors-algorithm","kernel-method","large-margin-nearest-neighbor","learning-vector-quantization","logitboost","margin-classifier","margin-infused-relaxed-algorithm","multi-label-classification","multiclass-classification","multifactor-dimensionality-reduction","multispectral-pattern-recognition","nearest-centroid-classifier","perceptron","random-forest","random-subspace-method","relevance-vector-machine","rules-extraction-system-family","support-vector-machine","syntactic-pattern-recognition","types-of-artificial-neural-networks","winnow-(algorithm)"],"name":"Classification algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"non-uniform-random-numbers","_score":0,"_source":{"description":"Pseudo-random number sampling or non-uniform pseudo-random variate generation is the numerical practice of generating pseudo-random numbers that are distributed according to a given probability distribution.\nMethods of sampling a non-uniform distribution are typically based on the availability of a pseudo-random number generator producing numbers X that are uniformly distributed. Computational algorithms are then used to manipulate a single random variate, X, or often several such variates, into a new random variate Y such that these values have the required distribution.\nHistorically, basic methods of pseudo-random number sampling were developed for Monte-Carlo simulations in the Manhattan project; they were first published by John von Neumann in the early 1950s.\n\n","tag_line":"Pseudo-random number sampling or non-uniform pseudo-random variate generation is the numerical practice of generating pseudo-random numbers that are distributed according to a given probability distribution.","algorithms":[["ziggurat-algorithm",null]],"name":"Non-uniform random numbers","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"concurrent-algorithms","_score":0,"_source":{"description":"In computer science, a concurrent algorithm is one that can be executed concurrently. Most standard computer algorithms are sequential algorithms, and assume that the algorithm is run from start to finish without any other processes executing. These often do not behave correctly when run concurrently, as demonstrated at right, and are often nondeterministic, as the actual sequence of computations is determined by the external scheduler. Concurrency often adds significant complexity to an algorithm, requiring concurrency control such as mutual exclusion to avoid problems such as race conditions.\nMany parallel algorithms are run concurrently, particularly distributed algorithms, though these are distinct concepts in general.","tag_line":"In computer science, a concurrent algorithm is one that can be executed concurrently.","algorithms":["concurrent-algorithm","ostrich-algorithm","parallel-algorithm","prefix-sum"],"name":"Concurrent algorithms","children":["concurrency-control-algorithms","distributed-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"computer-graphics-algorithms","_score":0,"_source":{"description":"Algorithms used in Computer graphics. See also Category:Computer graphics data structures.","tag_line":"Algorithms used in Computer graphics.","algorithms":["beier–neely-morphing-algorithm","bresenham's-line-algorithm","diamond-square-algorithm","digital-differential-analyzer-(graphics-algorithm)","even–odd-rule",["false-radiosity",null],"flood-fill",["floyd–steinberg-dithering",null],"geomipmapping",["level-set-method",null],"line-drawing-algorithm","marching-cubes","marching-squares","marching-tetrahedra","mclone","newell's-algorithm","painter's-algorithm","progressive-refinement",["ramer–douglas–peucker-algorithm",null],"ray-casting","recursive-xy-cut","roam","scanline-rendering","sgi-algorithm","simulated-fluorescence-process-algorithm","warnock-algorithm","xiaolin-wu's-line-algorithm"],"name":"Computer graphics algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"gene-expression-programming","_score":0,"_source":{"description":"In computer programming, gene expression programming (GEP) is an evolutionary algorithm that creates computer programs or models. These computer programs are complex tree structures that learn and adapt by changing their sizes, shapes, and composition, much like a living organism. And like living organisms, the computer programs of GEP are also encoded in simple linear chromosomes of fixed length. Thus, GEP is a genotype-phenotype system, benefiting from a simple genome to keep and transmit the genetic information and a complex phenotype to explore the environment and adapt to it.","tag_line":"In computer programming, gene expression programming (GEP) is an evolutionary algorithm that creates computer programs or models.","algorithms":[["gene-expression-programming",null]],"name":"Gene expression programming","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"numerical-analysts","_score":0,"_source":{"description":"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).\nOne of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection (YBC 7289), which gives a sexagesimal numerical approximation of , the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in astronomy, carpentry and construction.\nNumerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of , modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\nNumerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st century also the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.\nBefore the advent of modern computers numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.","tag_line":"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).","algorithms":[["nimrod-megiddo",null]],"name":"Numerical analysts","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"wavelets","_score":0,"_source":{"description":"A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases, and then decreases back to zero. It can typically be visualized as a \"brief oscillation\" like one might see recorded by a seismograph or heart monitor. Generally, wavelets are purposefully crafted to have specific properties that make them useful for signal processing. Wavelets can be combined, using a \"reverse, shift, multiply and integrate\" technique called convolution, with portions of a known signal to extract information from the unknown signal.\n\nFor example, a wavelet could be created to have a frequency of Middle C and a short duration of roughly a 32nd note. If this wavelet was to be convolved with a signal created from the recording of a song, then the resulting signal would be useful for determining when the Middle C note was being played in the song. Mathematically, the wavelet will correlate with the signal if the unknown signal contains information of similar frequency. This concept of correlation is at the core of many practical applications of wavelet theory.\nAs a mathematical tool, wavelets can be used to extract information from many different kinds of data, including – but certainly not limited to – audio signals and images. Sets of wavelets are generally needed to analyze data fully. A set of \"complementary\" wavelets will decompose data without gaps or overlap so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet based compression/decompression algorithms where it is desirable to recover the original information with minimal loss.\nIn formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square integrable functions.","tag_line":"A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases, and then decreases back to zero.","algorithms":[["embedded-zerotrees-of-wavelet-transforms",null]],"name":"Wavelets","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"monte-carlo-methods","_score":0,"_source":{"description":"Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other mathematical methods. Monte Carlo methods are mainly used in three distinct problem classes: optimization, numerical integration, and generating draws from a probability distribution.\nIn physics-related problems, Monte Carlo methods are quite useful for simulating systems with many coupled degrees of freedom, such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean-Vlasov processes, kinetic models of gases). Other examples include modeling phenomena with significant uncertainty in inputs such as the calculation of risk in business and, in math, evaluation of multidimensional definite integrals with complicated boundary conditions. In application to space and oil exploration problems, Monte Carlo–based predictions of failure, cost overruns and schedule overruns are routinely better than human intuition or alternative \"soft\" methods.\nIn principle, Monte Carlo methods can be used to solve any problem having a probabilistic interpretation. By the law of large numbers, integrals described by the expected value of some random variable can be approximated by taking the empirical mean (a.k.a. the sample mean) of independent samples of the variable. When the probability distribution of the variable is too complex, mathematicians often use a Markov Chain Monte Carlo (MCMC) sampler. The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution. By the ergodic theorem, the stationary probability distribution is approximated by the empirical measures of the random states of the MCMC sampler.\nIn other important problems we are interested in generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation. These flows of probability distributions can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depends on the distributions of the current random states (see McKean-Vlasov processes, nonlinear filtering equation). In other instances we are given a flow of probability distributions with an increasing level of sampling complexity (path spaces models with an increasing time horizon, Boltzmann-Gibbs measures associated with decreasing temperature parameters, and many others). These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain. A natural way to simulate these sophisticated nonlinear Markov processes is to sample a large number of copies of the process, replacing in the evolution equation the unknown distributions of the random states by the sampled empirical measures. In contrast with traditional Monte Carlo and Markov chain Monte Carlo methodologies these mean field particle techniques rely on sequential interacting samples. The terminology mean field reflects the fact that each of the samples (a.k.a. particles, individuals, walkers, agents, creatures, or phenotypes) interacts with the empirical measures of the process. When the size of the system tends to infinity, these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain, so that the statistical interaction between particles vanishes.\n\n","tag_line":"Monte Carlo methods (or Monte Carlo experiments) are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results.","algorithms":[["fisher–yates-shuffle",null],"iterated-filtering","metropolis-light-transport","metropolis–hastings-algorithm",["monte-carlo-tree-search",null],"multicanonical-ensemble",["simulated-annealing",null],"vegas-algorithm"],"name":"Monte Carlo methods","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"numerical-software","_score":0,"_source":{"description":"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).\nOne of the earliest mathematical writings is a Babylonian tablet from the Yale Babylonian Collection (YBC 7289), which gives a sexagesimal numerical approximation of , the length of the diagonal in a unit square. Being able to compute the sides of a triangle (and hence, being able to compute square roots) is extremely important, for instance, in astronomy, carpentry and construction.\nNumerical analysis continues this long tradition of practical mathematical calculations. Much like the Babylonian approximation of , modern numerical analysis does not seek exact answers, because exact answers are often impossible to obtain in practice. Instead, much of numerical analysis is concerned with obtaining approximate solutions while maintaining reasonable bounds on errors.\nNumerical analysis naturally finds applications in all fields of engineering and the physical sciences, but in the 21st century also the life sciences and even the arts have adopted elements of scientific computations. Ordinary differential equations appear in celestial mechanics (planets, stars and galaxies); numerical linear algebra is important for data analysis; stochastic differential equations and Markov chains are essential in simulating living cells for medicine and biology.\nBefore the advent of modern computers numerical methods often depended on hand interpolation in large printed tables. Since the mid 20th century, computers calculate the required functions instead. These same interpolation formulas nevertheless continue to be used as part of the software algorithms for solving differential equations.","tag_line":"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to general symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics).","algorithms":[["mpir-(mathematics-software)",null]],"name":"Numerical software","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"computational-number-theory","_score":0,"_source":{"description":"In mathematics and computer science, computational number theory, also known as algorithmic number theory, is the study of algorithms for performing number theoretic computations.\n\n","tag_line":"In mathematics and computer science, computational number theory, also known as algorithmic number theory, is the study of algorithms for performing number theoretic computations.\n\n","algorithms":["odlyzko–schönhage-algorithm"],"name":"Computational number theory","children":[null,"number-theoretic-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"root-finding-algorithms","_score":0,"_source":{"description":"A root-finding algorithm is a numerical method, or algorithm, for finding a value x such that f(x) = 0, for a given function f. Such an x is called a root of the function f.\nThis article is concerned with finding scalar, real or complex roots, approximated as floating point numbers. Finding integer roots or exact algebraic roots are separate problems, whose algorithms have little in common with those discussed here. (See: Diophantine equation for integer roots)\nFinding a root of f(x) − g(x) = 0 is the same as solving the equation f(x) = g(x). Here, x is called the unknown in the equation. Conversely, any equation can take the canonical form f(x) = 0, so equation solving is the same thing as computing (or finding) a root of a function.\nNumerical root-finding methods use iteration, producing a sequence of numbers that hopefully converge towards a limit, which is a root. The first values of this series are initial guesses. Many methods computes subsequent values by evaluating an auxiliary function on the preceding values. The limit is thus a fixed point of the auxiliary function, which is chosen for having the roots of the original equation as fixed points.\nThe behaviour of root-finding algorithms is studied in numerical analysis. Algorithms perform best when they take advantage of known characteristics of the given function. Thus an algorithm to find isolated real roots of a low-degree polynomial in one variable may bear little resemblance to an algorithm for complex roots of a \"black-box\" function which is not even known to be differentiable. Questions include ability to separate close roots, robustness against failures of continuity and differentiability, reliability despite inevitable numerical errors, and rate of convergence.","tag_line":"A root-finding algorithm is a numerical method, or algorithm, for finding a value x such that f(x) = 0, for a given function f. Such an x is called a root of the function f.\nThis article is concerned with finding scalar, real or complex roots, approximated as floating point numbers.","algorithms":["root-finding-algorithm","aberth-method",["alpha-max-plus-beta-min-algorithm",null],"bairstow's-method","brent's-method","durand–kerner-method","fast-inverse-square-root","graeffe's-method","halley's-method","householder's-method","inverse-quadratic-interpolation",["jenkins–traub-algorithm",null],"laguerre's-method","lehmer–schur-algorithm",["methods-of-computing-square-roots",null],"muller's-method","newton's-method","nth-root-algorithm","ridders'-method","secant-method",["shifting-nth-root-algorithm",null],"sidi's-generalized-secant-method","solving-quadratic-equations-with-continued-fractions","splitting-circle-method"],"name":"Root-finding algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"iterative-methods","_score":0,"_source":{"description":"In computational mathematics, an iterative method is a mathematical procedure that generates a sequence of improving approximate solutions for a class of problems. A specific implementation of an iterative method, including the termination criteria, is an algorithm of the iterative method. An iterative method is called convergent if the corresponding sequence converges for given initial approximations. A mathematically rigorous convergence analysis of an iterative method is usually performed; however, heuristic-based iterative methods are also common. In the problems of finding the root of an equation (or a solution of a system of equations), an iterative method uses an initial guess to generate successive approximations to a solution.\nIn contrast, direct methods attempt to solve the problem by a finite sequence of operations. In the absence of rounding errors, direct methods would deliver an exact solution (like solving a linear system of equations  by Gaussian elimination). Iterative methods are often the only choice for nonlinear equations. However, iterative methods are often useful even for linear problems involving a large number of variables (sometimes of the order of millions), where direct methods would be prohibitively expensive (and in some cases impossible) even with the best available computing power.\n\n","tag_line":"In computational mathematics, an iterative method is a mathematical procedure that generates a sequence of improving approximate solutions for a class of problems.","algorithms":[["frank–wolfe-algorithm",null]],"name":"Iterative methods","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"optimization-algorithms-and-methods","_score":0,"_source":{"description":"In numerical analysis, an iterative method is called locally convergent if the successive approximations produced by the method are guaranteed to converge to a solution when the initial approximation is already close enough to the solution. Iterative methods for nonlinear equations and their systems, such as Newton's method are usually only locally convergent.\nAn iterative method that converges for an arbitrary initial approximation is called globally convergent. Iterative methods for systems of linear equations are usually globally convergence.","tag_line":"In numerical analysis, an iterative method is called locally convergent if the successive approximations produced by the method are guaranteed to converge to a solution when the initial approximation is already close enough to the solution.","algorithms":["adaptive-coordinate-descent",["adaptive-dimensional-search",null],["alpha–beta-pruning",null],["artificial-bee-colony-algorithm",null],"auction-algorithm","augmented-lagrangian-method",["bees-algorithm",null],"benson's-algorithm","berndt–hall–hall–hausman-algorithm","big-m-method","bin-packing-problem",["bland's-rule",null],"bobyqa",["branch-and-bound",null],["branch-and-cut",null],"broyden–fletcher–goldfarb–shanno-algorithm",["cma-es",null],"cobyla",["coffman–graham-algorithm",null],"column-generation",["constructive-cooperative-coevolution",null],["criss-cross-algorithm",null],["cuckoo-search",null],["divide-and-conquer-algorithms",null],"dykstra's-projection-algorithm",["dynamic-programming",null],["eagle-strategy",null],["evolutionary-algorithm",null],["evolutionary-programming",null],["expectation–maximization-algorithm",null],"fernandez’s-method",["firefly-algorithm",null],"fourier–motzkin-elimination",["frank–wolfe-algorithm",null],"gauss–newton-algorithm","generalized-iterative-scaling",["genetic-algorithm",null],["genetic-algorithms-in-economics",null],"glowworm-swarm-optimization","golden-section-search",["gradient-descent",null],["gradient-method",null],"great-deluge-algorithm",["greedy-algorithm",null],"guided-local-search",["harmony-search",null],"hill-climbing","imperialist-competitive-algorithm","intelligent-water-drops-algorithm","interior-point-method",["interval-contractor",null],["job-shop-scheduling",null],"karmarkar's-algorithm","killer-heuristic",["lemke's-algorithm",null],"levenberg–marquardt-algorithm","limited-memory-bfgs","lincoa","local-search-(optimization)",["luus–jaakola",null],"maximum-subarray-problem","mcs-algorithm","mehrotra-predictor–corrector-method",["meta-optimization",null],"mm-algorithm",["natural-evolution-strategy",null],"negamax","nelder–mead-method",["network-simplex-algorithm",null],["newton's-method",null],"newuoa","nonlinear-conjugate-gradient-method",["parallel-metaheuristic",null],["particle-swarm-optimization",null],["pattern-search-(optimization)",null],"penalty-method","powell's-method","pseven","reactive-search-optimization",["search-based-software-engineering",null],"sequential-minimal-optimization",["shuffled-frog-leaping-algorithm",null],["simplex-algorithm",null],["simulated-annealing",null],"simultaneous-perturbation-stochastic-approximation",["social-cognitive-optimization",null],["special-ordered-set",null],"swarm-intelligence","tabu-search","tolmin-(optimization-software)","tree-rearrangement","truncated-newton-method","uobyqa","very-large-scale-neighborhood-search"],"name":"Optimization algorithms and methods","children":[null,"dynamic-programming",null,"gradient-methods","stochastic-optimization"]}}
,{"_index":"throwtable","_type":"category","_id":"randomized-algorithms","_score":0,"_source":{"description":"A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the \"average case\" over all possible choices of random bits. Formally, the algorithm's performance will be a random variable determined by the random bits; thus either the running time, or the output (or both) are random variables.\nOne has to distinguish between algorithms that use the random input to reduce the expected running time or memory usage, but always terminate with a correct result (Las Vegas algorithms) in a bounded amount of time, and probabilistic algorithms, which, depending on the random input, have a chance of producing an incorrect result (Monte Carlo algorithms) or fail to produce a result either by signalling a failure or failing to terminate.\nIn the second case, random performance and random output, the term \"algorithm\" for a procedure is somewhat questionable. In the case of random output, it is no longer formally effective. However, in some cases, probabilistic algorithms are the only practical means of solving a problem.\nIn common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior.","tag_line":"A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic.","algorithms":["randomized-algorithm","approximate-counting-algorithm","atlantic-city-algorithm","entropy-compression","expected-linear-time-mst-algorithm",["freivalds'-algorithm",null],"las-vegas-algorithm",["list-update-problem",null],"monte-carlo-algorithm","principle-of-deferred-decision",["property-testing",null]],"name":"Randomized algorithms","children":[null,"probabilistic-data-structures","stochastic-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"gradient-methods","_score":0,"_source":{"description":"Proximal gradient methods are a generalized form of projection used to solve non-differentiable convex optimization problems. Many interesting problems can be formulated as convex optimization problems of form\n\nwhere  are convex functions defined from  where some of the functions are non-differentiable, this rules out our conventional smooth optimization techniques like Steepest descent method, conjugate gradient method etc. There is a specific class of algorithms which can solve above optimization problem. These methods proceed by splitting, in that the functions  are used individually so as to yield an easily implementable algorithm. They are called proximal because each non smooth function among  is involved via its proximity operator. Iterative Shrinkage thresholding algorithm, projected Landweber, projected gradient, alternating projections, alternating-direction method of multipliers, alternating split Bregman are special instances of proximal algorithms. Details of proximal methods are discussed in Combettes and Pesquet. For the theory of proximal gradient methods from the perspective of and with applications to statistical learning theory, see proximal gradient methods for learning.","tag_line":"Proximal gradient methods are a generalized form of projection used to solve non-differentiable convex optimization problems.","algorithms":[["gradient-method",null],"adaptive-projected-subgradient-method",["frank–wolfe-algorithm",null],["gradient-descent",null],["nonlinear-conjugate-gradient-method",null]],"name":"Gradient methods","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"probabilistic-data-structures","_score":0,"_source":{"description":"In mathematics and computer science, a random tree is a tree or arborescence that is formed by a stochastic process. Types of random trees include:\nUniform spanning tree, a spanning tree of a given graph in which each different tree is equally likely to be selected\nRandom minimal spanning tree, spanning trees of a graph formed by choosing random edge weights and using the minimum spanning tree for those weights\nRandom binary tree, binary trees with a given number of nodes, formed by inserting the nodes in a random order or by selecting all possible trees uniformly at random\nRandom recursive tree, increasingly labelled trees, which can be generated using a simple stochastic growth rule.\nTreap or randomized binary search tree, a data structure that uses random choices to simulate a random binary tree for non-random update sequences\nRapidly exploring random tree, a fractal space-filling pattern used as a data structure for searching high-dimensional spaces\nBrownian tree, a fractal tree structure created by diffusion-limited aggregation processes\nRandom forest, a machine-learning classifier based on choosing random subsets of variables for each tree and using the most frequent tree output as the overall classification\nBranching process, a model of a population in which each individual has a random number of children","tag_line":"In mathematics and computer science, a random tree is a tree or arborescence that is formed by a stochastic process.","algorithms":[["bloom-filter",null],"rapidly-exploring-random-tree"],"name":"Probabilistic data structures","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"scheduling-algorithms","_score":0,"_source":{"description":"In computing, scheduling is the method by which work specified by some means is assigned to resources that complete the work. The work may be virtual computation elements such as threads, processes or data flows, which are in turn scheduled onto hardware resources such as processors, network links or expansion cards.\nA scheduler is what carries out the scheduling activity. Schedulers are often implemented so they keep all compute resources busy (as in load balancing), allow multiple users to share system resources effectively, or to achieve a target quality of service. Scheduling is fundamental to computation itself, and an intrinsic part of the execution model of a computer system; the concept of scheduling makes it possible to have computer multitasking with a single central processing unit (CPU).\nA scheduler may aim at one of many goals, for example, maximizing throughput (the total amount of work completed per time unit), minimizing response time (time from work becoming enabled until the first point it begins execution on resources), or minimizing latency (the time between work becoming enabled and its subsequent completion), maximizing fairness (equal CPU time to each process, or more generally appropriate times according to the priority and workload of each process). In practice, these goals often conflict (e.g. throughput versus latency), thus a scheduler will implement a suitable compromise. Preference is given to any one of the concerns mentioned above, depending upon the user's needs and objectives.\nIn real-time environments, such as embedded systems for automatic control in industry (for example robotics), the scheduler also must ensure that processes can meet deadlines; this is crucial for keeping the system stable. Scheduled tasks can also be distributed to remote devices across a network and managed through an administrative back end.","tag_line":"In computing, scheduling is the method by which work specified by some means is assigned to resources that complete the work.","algorithms":["atropos-scheduler",["coffman–graham-algorithm",null],"critical-path-method","dynamic-priority-scheduling",["exponential-backoff",null],"fifo-(computing-and-electronics)","fino","generalized-processor-sharing","graphical-path-method","heterogeneous-earliest-finish-time","interval-scheduling","least-slack-time-scheduling","list-scheduling","modified-due-date-scheduling-heuristic","multilevel-queue","starvation-(computer-science)","sequence-step-algorithm",["top-nodes-algorithm",null]],"name":"Scheduling algorithms","children":["disk-scheduling-algorithms",null,"processor-scheduling-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"online-sorts","_score":0,"_source":{"description":"Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm. It is based on the idea that the permutation to be sorted can be factored into cycles, which can individually be rotated to give a sorted result.\nUnlike nearly every other sort, items are never written elsewhere in the array simply to push them out of the way of the action. Each value is either written zero times, if it's already in its correct position, or written one time to its correct position. This matches the minimal number of overwrites required for a completed in-place sort.\nMinimizing the number of writes is useful when making writes to some huge data set is very expensive, such as with EEPROMs like Flash memory where each write reduces the lifespan of the memory.","tag_line":"Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm.","algorithms":[["cubesort",null],["cycle-sort",null],["insertion-sort",null],["library-sort",null],["polyphase-merge-sort",null]],"name":"Online sorts","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"numerical-differential-equations","_score":0,"_source":{"description":"In computational fluid dynamics, the MacCormack method is a widely used discretization scheme for the numerical solution of hyperbolic partial differential equations. This second-order finite difference method was introduced by Robert W. MacCormack in 1969. The MacCormack method is elegant and easy to understand and program.\n\n","tag_line":"In computational fluid dynamics, the MacCormack method is a widely used discretization scheme for the numerical solution of hyperbolic partial differential equations.","algorithms":["pantelides-algorithm"],"name":"Numerical differential equations","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"computer-arithmetic-algorithms","_score":0,"_source":{"description":"This category contains articles pertaining to algorithms that are used in arbitrary precision arithmetic. This includes algorithms for multiplication and division, as well as algorithms for the efficient evaluation of mathematical constants and special functions to high precision.\nSee also Category:Number theoretic algorithms for arbitrary-precision integer and cryptography algorithms.","tag_line":"This category contains articles pertaining to algorithms that are used in arbitrary precision arithmetic.","algorithms":["addition-chain-exponentiation","agm-method","binary-splitting","computational-complexity-of-mathematical-operations","division-algorithm","double-dabble","exponentiation-by-squaring","fee-method","fürer's-algorithm","karatsuba-algorithm","methods-of-computing-square-roots","mpir-(mathematics-software)","multiplication-algorithm","quote-notation","schönhage–strassen-algorithm","shifting-nth-root-algorithm","spigot-algorithm","toom–cook-multiplication"],"name":"Computer arithmetic algorithms","children":["pi-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"algorithm-description-languages","_score":0,"_source":{"description":"Pseudo code is an informal high-level description of the operating principle of a computer program or other algorithm.\nIt uses the structural conventions of a programming language, but is intended for human reading rather than machine reading. Pseudocode typically omits details that are essential for machine understanding of the algorithm, such as variable declarations, system-specific code and some subroutines. The programming language is augmented with natural language description details, where convenient, or with compact mathematical notation. The purpose of using pseudocode is that it is easier for people to understand than conventional programming language code, and that it is an efficient and environment-independent description of the key principles of an algorithm. It is commonly used in textbooks and scientific publications that are documenting various algorithms, and also in planning of computer program development, for sketching out the structure of the program before the actual coding takes place.\nNo standard for pseudocode syntax exists, as a program in pseudocode is not an executable program. Pseudocode resembles, but should not be confused with skeleton programs which can be compiled without errors. Flowcharts, drakon-charts and Unified Modeling Language (UML) charts can be thought of as a graphical alternative to pseudocode, but are more spacious on paper.\n\n","tag_line":"Pseudo code is an informal high-level description of the operating principle of a computer program or other algorithm.","algorithms":["flowchart","pidgin-code","pluscal","pseudocode"],"name":"Algorithm description languages","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"digital-signal-processing","_score":0,"_source":{"description":"Digital signal processing (DSP) is the numerical manipulation of signals, usually with the intention to measure, filter, produce or compress continuous analog signals. It is characterized by the use of digital signals to represent these signals as discrete time, discrete frequency, or other discrete domain signals in the form of a sequence of numbers or symbols to permit the digital processing of these signals.\nTheoretical analyses and derivations are typically performed on discrete-time signal models, created by the abstract process of sampling. Numerical methods require a digital signal, such as those produced by an analog-to-digital converter (ADC). The processed result might be a frequency spectrum or a set of statistics. But often it is another digital signal that is converted back to analog form by a digital-to-analog converter (DAC). Even if that whole sequence is more complex than analog processing and has a discrete value range, the application of computational power to signal processing allows for many advantages over analog processing in many applications, such as error detection and correction in transmission as well as data compression.\nDigital signal processing and analog signal processing are subfields of signal processing. DSP applications include audio and speech signal processing, sonar and radar signal processing, sensor array processing, spectral estimation, statistical signal processing, digital image processing, signal processing for communications, control of systems, biomedical signal processing, seismic data processing, among others. DSP algorithms have long been run on standard computers, as well as on specialized processors called digital signal processors, and on purpose-built hardware such as application-specific integrated circuit (ASICs). Currently, there are additional technologies used for digital signal processing including more powerful general purpose microprocessors, field-programmable gate arrays (FPGAs), digital signal controllers (mostly for industrial applications such as motor control), and stream processors, among others.\nDigital signal processing can involve linear or nonlinear operations. Nonlinear signal processing is closely related to nonlinear system identification and can be implemented in the time, frequency, and spatio-temporal domains.","tag_line":"Digital signal processing (DSP) is the numerical manipulation of signals, usually with the intention to measure, filter, produce or compress continuous analog signals.","algorithms":["fast-fourier-transform","fast-walsh–hadamard-transform",["generalized-distributive-law",null],"goertzel-algorithm","least-mean-squares-filter","ramer–douglas–peucker-algorithm",["verification-based-message-passing-algorithms-in-compressed-sensing",null]],"name":"Digital signal processing","children":["discrete-transforms","fft-algorithms","image-processing","wavelets"]}}
,{"_index":"throwtable","_type":"category","_id":"substring-indices","_score":0,"_source":{"description":"In computer science, a substring index is a data structure which gives substring search in a text or text collection in sublinear time. If you have a document  of length , or a set of documents  of total length , you can locate all occurrences of a pattern  in  time. (See Big O notation.)\nThe phrase full-text index is also often used for an index of all substrings of a text. But is ambiguous, as it is also used for regular word indexes such as inverted files and document retrieval. See full text search.\nSubstring indexes include:\nSuffix tree\nSuffix array\nN-gram index, an inverted file for all N-grams of the text\nCompressed suffix array\nFM-index\nLZ-index","tag_line":"In computer science, a substring index is a data structure which gives substring search in a text or text collection in sublinear time.","algorithms":[["ukkonen's-algorithm",null]],"name":"Substring indices","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"database-algorithms","_score":0,"_source":{"description":"Algorithms used for implementation of database management systems.","tag_line":"Algorithms used for implementation of database management systems.","algorithms":["algorithms-for-recovery-and-isolation-exploiting-semantics","chase-(algorithm)","write-ahead-logging"],"name":"Database algorithms","children":["join-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"lossy-compression-algorithms","_score":0,"_source":{"description":"In information technology, lossy compression is the class of data encoding methods that uses inexact approximations (or partial data discarding) to represent the content. These techniques are used to reduce data size for storage, handling, and transmitting content. Different versions of the photo of the cat at the right show how higher degrees of approximation create coarser images as more details are removed. This is opposed to lossless data compression which does not degrade the image. The amount of data reduction possible using lossy compression is often much higher than through lossless techniques.\nWell-designed lossy compression technology often reduces file sizes significantly before degradation is noticed by the end-user. Even when noticeable by the user, further data reduction may be desirable (e.g., for real-time communication, to reduce transmission times, or to reduce storage needs).\nLossy compression is most commonly used to compress multimedia data (audio, video, and images), especially in applications such as streaming media and internet telephony. By contrast, lossless compression is typically required for text and data files, such as bank records and text articles. In many cases it is advantageous to make a master lossless file which is to be used to produce new compressed files; for example, a multi-megabyte file can be used at full size to produce a full-page advertisement in a glossy magazine, and a 10 kilobyte lossy copy can be made for a small image on a web page.","tag_line":"In information technology, lossy compression is the class of data encoding methods that uses inexact approximations (or partial data discarding) to represent the content.","algorithms":["3dc","adaptive-scalable-texture-compression","apple-video","block-truncation-coding","bloom-filter","color-cell-compression",["felics",null],"fractal-compression","gwic","microsoft-video-1","opus-(audio-format)","quicktime-graphics","s2tc","s3-texture-compression","vector-quantization","vocoder","wavelet-scalar-quantization"],"name":"Lossy compression algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"primality-tests","_score":0,"_source":{"description":"A primality test is an algorithm for determining whether an input number is prime. Amongst other fields of mathematics, it is used for cryptography. Unlike integer factorization, primality tests do not generally give prime factors, only stating whether the input number is prime or not. Factorization is thought to be a computationally difficult problem, whereas primality testing is comparatively easy (its running time is polynomial in the size of the input). Some primality tests prove that a number is prime, while others like Miller–Rabin prove that a number is composite. Therefore, the latter might be called compositeness tests instead of primality tests.\n\n","tag_line":"A primality test is an algorithm for determining whether an input number is prime.","algorithms":[["primality-test",null],"adleman–pomerance–rumely-primality-test",["sieve-of-eratosthenes",null]],"name":"Primality tests","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"machine-learning-algorithms","_score":0,"_source":{"description":"","tag_line":"","algorithms":["almeida–pineda-recurrent-backpropagation","bootstrap-aggregating","cn2-algorithm","constructing-skill-trees","diffusion-map","dynamic-time-warping",["expectation–maximization-algorithm",null],"fastica",["forward–backward-algorithm",null],"generec","genetic-algorithm-for-rule-set-production","hexq",["k-nearest-neighbors-algorithm",null],"kernel-methods-for-vector-output","leabra",["linde–buzo–gray-algorithm",null],"local-outlier-factor",["logitboost",null],"loss-functions-for-classification","manifold-alignment","minimum-redundancy-feature-selection","multiple-kernel-learning","non-negative-matrix-factorization","online-machine-learning","prefrontal-cortex-basal-ganglia-working-memory","pvlv","quadratic-unconstrained-binary-optimization","user:quantares/sandbox","query-level-feature","quickprop","randomized-weighted-majority-algorithm","reinforcement-learning","rprop","state-action-reward-state-action","t-distributed-stochastic-neighbor-embedding","temporal-difference-learning","wake-sleep-algorithm","weighted-majority-algorithm"],"name":"Machine learning algorithms","children":[null]}}
,{"_index":"throwtable","_type":"category","_id":"evolutionary-algorithms","_score":0,"_source":{"description":"In artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators. Artificial evolution (AE) describes a process involving individual evolutionary algorithms; EAs are individual components that participate in an AE.\nEvolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape; this generality is shown by successes in fields as diverse as engineering, art, biology, economics, marketing, genetics, operations research, robotics, social sciences, physics, politics and chemistry.\nTechniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. The computer simulations Tierra and Avida attempt to model macroevolutionary dynamics.\nIn most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.\nA possible limitation  of many evolutionary algorithms is their lack of a clear genotype-phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism. Such indirect (aka generative or developmental) encodings also enable evolution to exploit the regularity in the environment. Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype-phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes.","tag_line":"In artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm.","algorithms":["evolutionary-algorithm","artificial-bee-colony-algorithm","artificial-immune-system","bat-algorithm","biogeography-based-optimization","cellular-evolutionary-algorithm","cma-es","computer-automated-design","constructive-cooperative-coevolution","cuckoo-search",["cultural-algorithm",null],"darwintunes","eagle-strategy","evolutionary-algorithm-for-landmark-detection","evolutionary-music","evolutionary-programming","evolved-antenna",["firefly-algorithm",null],"gaussian-adaptation",["gene-expression-programming",null],["genetic-programming",null],"genetic-representation",["harmony-search",null],["hyperneat",null],"learning-classifier-system","melomics","memetic-algorithm","meta-optimization","natural-evolution-strategy","neuroevolution",["neuroevolution-of-augmenting-topologies",null],"particle-swarm-optimization",["promoter-based-genetic-algorithm",null],["reward-based-selection",null],["speciation-(genetic-algorithm)",null]],"name":"Evolutionary algorithms","children":["gene-expression-programming",null,"genetic-programming"]}}
,{"_index":"throwtable","_type":"category","_id":"string-collation-algorithms","_score":0,"_source":{"algorithms":["iso-14651","unicode-collation-algorithm"],"name":"String collation algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"stable-sorts","_score":0,"_source":{"description":"Cascade merge sort is similar to the polyphase merge sort but uses a simpler distribution. The merge is slower than a polyphase merge when there are fewer than six files, but faster when there are more than six.\n^ Bradley 1982, pp. 189–190","tag_line":"Cascade merge sort is similar to the polyphase merge sort but uses a simpler distribution.","algorithms":[["block-sort",null],["bubble-sort",null],["bucket-sort",null],["cocktail-sort",null],["counting-sort",null],["cubesort",null],["gnome-sort",null],["insertion-sort",null],["library-sort",null],["merge-sort",null],["odd–even-sort",null],["pigeonhole-sort",null],["proxmap-sort",null],["radix-sort",null],["timsort",null]],"name":"Stable sorts","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"computational-statistics","_score":0,"_source":{"description":"Computational statistics, or statistical computing, is the interface between statistics and computer science. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education.\nThe terms 'computational statistics' and 'statistical computing' are often used interchangeably, although Carlo Lauro (a former president of the International Association for Statistical Computing) proposed making a distinction, defining 'statistical computing' as \"the application of computer science to statistics\", and 'computational statistics' as \"aiming at the design of algorithm for implementing statistical methods on computers, including the ones unthinkable before the computer age (e.g. bootstrap, simulation), as well as to cope with analytically intractable problems\" [sic].\nThe term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models.","tag_line":"Computational statistics, or statistical computing, is the interface between statistics and computer science.","algorithms":[["bootstrap-aggregating",null],"bootstrapping-populations",["fastica",null],["group-method-of-data-handling",null],"twisting-properties",["types-of-artificial-neural-networks",null]],"name":"Computational statistics","children":[null,"data-mining","markov-chain-monte-carlo",null,"non-uniform-random-numbers","variance-reduction"]}}
,{"_index":"throwtable","_type":"category","_id":"estimation-theory","_score":0,"_source":{"description":"Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured/empirical data that has a random component. The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data. An estimator attempts to approximate the unknown parameters using the measurements.\nFor example, it is desired to estimate the proportion of a population of voters who will vote for a particular candidate. That proportion is the parameter sought; the estimate is based on a small random sample of voters.\nOr, for example, in radar the goal is to estimate the range of objects (airplanes, boats, etc.) by analyzing the two-way transit timing of received echoes of transmitted pulses. Since the reflected pulses are unavoidably embedded in electrical noise, their measured values are randomly distributed, so that the transit time must be estimated.\nIn estimation theory, two approaches are generally considered. \nThe probabilistic approach (described in this article) assumes that the measured data is random with probability distribution dependent on the parameters of interest\nThe set-membership approach assumes that the measured data vector belongs to a set which depends on the parameter vector.\nFor example, in electrical communication theory, the measurements which contain information regarding the parameters of interest are often associated with a noisy signal. Without randomness, or noise, the problem would be deterministic and estimation would not be needed.","tag_line":"Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured/empirical data that has a random component.","algorithms":[["adaptive-projected-subgradient-method",null],["expectation–maximization-algorithm",null]],"name":"Estimation theory","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"algorithms","_score":0,"_source":{"description":"In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ AL-gə-ri-dhəm) is a self-contained step-by-step set of operations to be performed. Algorithms exist that perform calculation, data processing, and automated reasoning.\nAn algorithm is an effective method that can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\nThe concept of algorithm has existed for centuries, however a partial formalization of what would become the modern algorithm began with attempts to solve the Entscheidungsproblem (the \"decision problem\") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define \"effective calculability\" or \"effective method\"; those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's \"Formulation 1\" of 1936, and Alan Turing's Turing machines of 1936–7 and 1939. Giving a formal definition of algorithms, corresponding to the intuitive notion, remains a challenging problem.\n\n","tag_line":"In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ AL-gə-ri-dhəm) is a self-contained step-by-step set of operations to be performed.","algorithms":["algorithm","list-of-algorithm-general-topics","list-of-algorithms","adaptive-algorithm","adaptive-dimensional-search","the-algorithm-auction","algorithm-characterizations","algorithm-design","algorithm-engineering","algorithmic-logic","algorithmics","the-art-of-computer-programming","avt-statistical-filtering-algorithm","boyer–moore-majority-vote-algorithm","british-museum-algorithm","cascade-learning-based-on-adaboost","chandy-misra-haas-algorithm:resource-model","chinese-whispers-(clustering-method)","collaborative-diffusion","dakota","divide-and-conquer-algorithms","devex-algorithm","divide-and-conquer-algorithms","domain-reduction-algorithm","driver-scheduling-problem","edgerank","flajolet–martin-algorithm","generalized-distributive-law","grey-wolf-optimizer","hakmem","hcs-clustering-algorithm","holographic-algorithm","hybrid-algorithm","hyphenation-algorithm","in-place-algorithm","jump-and-walk-algorithm","kinodynamic-planning","kisao","kleene's-algorithm","label-propagation-algorithm","lancichinetti-fortunato-radicchi-benchmark","lossy-count-algorithm","manhattan-address-algorithm","maze-generation-algorithm","maze-solving-algorithm","medical-algorithm","metis","one-pass-algorithm","out-of-core-algorithm","ping-pong-scheme","pointer-jumping","predictor–corrector-method","randomization-function","randomized-rounding","rendezvous-hashing","reservoir-sampling","rna22","run-time-algorithm-specialisation","sardinas–patterson-algorithm","sequential-algorithm","sequential-algorithm","sieve-of-eratosthenes","simulation-algorithms-for-atomic-devs","simulation-algorithms-for-coupled-devs","streaming-algorithm","super-recursive-algorithm","timeline-of-algorithms","tomasulo-algorithm","hindley–milner-type-system","xor-swap-algorithm","xulvi-brunet---sokolov-algorithm","zassenhaus-algorithm"],"name":"Algorithms","children":["computer-algebra","algorithm-description-languages","approximation-algorithms","bioinformatics-algorithms","calendar-algorithms","checksum-algorithms","combinatorial-algorithms","compression-algorithms","computer-arithmetic-algorithms","concurrent-algorithms","cryptographic-algorithms","data-clustering-algorithms","data-mining-algorithms","database-algorithms","digital-signal-processing",null,"error-detection-and-correction","evolutionary-algorithms","external-memory-algorithms","geometric-algorithms",null,"heuristic-algorithms","machine-learning-algorithms","matrix-multiplication-algorithms","memory-management-algorithms","networking-algorithms","computational-number-theory","numerical-analysis","online-algorithms","optimization-algorithms-and-methods","pattern-matching","computational-physics","pseudo-polynomial-time-algorithms","pseudorandom-number-generators","quantum-algorithms","randomized-algorithms","recursion",null,null,"scheduling-algorithms","search-algorithms","selection-algorithms","signal-processing",null,"statistical-algorithms","computational-statistics",null,"algorithms-on-strings","unicode-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"search-algorithms","_score":0,"_source":{"description":"In computer science, a search algorithm is an algorithm for finding an item with specified properties among a collection of items which are coded into a computer program, that look for clues to return what is wanted. The items may be stored individually as records in a database; or may be elements of a search space defined by a mathematical formula or procedure, such as the roots of an equation with integer variables; or a combination of the two, such as the Hamiltonian circuits of a graph.","tag_line":"In computer science, a search algorithm is an algorithm for finding an item with specified properties among a collection of items which are coded into a computer program, that look for clues to return what is wanted.","algorithms":[["combinatorial-search",null],"search-algorithm",["a*-search-algorithm",null],"all-nearest-smaller-values",["alpha–beta-pruning",null],["amplitude-amplification",null],"any-angle-path-planning",["b*",null],"backjumping",["backtracking",null],"beam-search","beam-stack-search","best-bin-first","best-first-search",["bidirectional-search",null],"binary-search-algorithm",["breadth-first-search",null],"brute-force-search",["d*",null],"dancing-links",["depth-first-search",null],["depth-limited-search",null],"dichotomic-search","difference-map-algorithm",["dijkstra's-algorithm",null],"exponential-search","fibonacci-search-technique","finger-search-tree",["genetic-algorithm",null],"god's-algorithm","graphplan",["grover's-algorithm",null],["hill-climbing",null],"hopscotch-hashing",["iterative-deepening-a*",null],"incremental-heuristic-search","interpolation-search",["jump-point-search",null],"jump-search",["k-independent-hashing",null],["k-nearest-neighbors-algorithm",null],"knuth's-algorithm-x","late-move-reductions",["lexicographic-breadth-first-search",null],"linear-hashing","linear-search","look-ahead-(backtracking)","mamf","mobilegeddon","mtd-f","null-move-heuristic",["parallel-metaheuristic",null],"proof-of-o(log*n)-time-complexity-of-union–find",["proof-number-search",null],["rainbow-table",null],"rapidly-exploring-dense-trees",["rapidly-exploring-random-tree",null],"search-game",["search-based-software-engineering",null],["shuffled-frog-leaping-algorithm",null],["sma*",null],"sss*","stack-search",["ternary-search",null],"uniform-binary-search",["universal-hashing",null],"uuhash","variation-(game-tree)"],"name":"Search algorithms","children":[null,"hashing","internet-search-algorithms",null]}}
,{"_index":"throwtable","_type":"category","_id":"approximation-algorithms","_score":0,"_source":{"description":"In computer science and operations research, approximation algorithms are algorithms used to find approximate solutions to optimization problems. Approximation algorithms are often associated with NP-hard problems; since it is unlikely that there can ever be efficient polynomial-time exact algorithms solving NP-hard problems, one settles for polynomial-time sub-optimal solutions. Unlike heuristics, which usually only find reasonably good solutions reasonably fast, one wants provable solution quality and provable run-time bounds. Ideally, the approximation is optimal up to a small constant factor (for instance within 5% of the optimal solution). Approximation algorithms are increasingly being used for problems where exact polynomial-time algorithms are known but are too expensive due to the input size. A typical example for an approximation algorithm is the one for vertex cover in graphs: find an uncovered edge and add both endpoints to the vertex cover, until none remain. It is clear that the resulting cover is at most twice as large as the optimal one. This is a constant factor approximation algorithm with a factor of 2.\nNP-hard problems vary greatly in their approximability; some, such as the bin packing problem, can be approximated within any factor greater than 1 (such a family of approximation algorithms is often called a polynomial time approximation scheme or PTAS). Others are impossible to approximate within any constant, or even polynomial factor unless P = NP, such as the maximum clique problem.\nNP-hard problems can often be expressed as integer programs (IP) and solved exactly in exponential time. Many approximation algorithms emerge from the linear programming relaxation of the integer program.\nNot all approximation algorithms are suitable for all practical applications. They often use IP/LP/Semidefinite solvers, complex data structures or sophisticated algorithmic techniques which lead to difficult implementation problems. Also, some approximation algorithms have impractical running times even though they are polynomial time, for example O(n2156) . Yet the study of even very expensive algorithms is not a completely theoretical pursuit as they can yield valuable insights. A classic example is the initial PTAS for Euclidean TSP due to Sanjeev Arora which had prohibitive running time, yet within a year, Arora refined the ideas into a linear time algorithm. Such algorithms are also worthwhile in some applications where the running times and cost can be justified e.g. computational biology, financial engineering, transportation planning, and inventory management. In such scenarios, they must compete with the corresponding direct IP formulations.\nAnother limitation of the approach is that it applies only to optimization problems and not to \"pure\" decision problems like satisfiability, although it is often possible to conceive optimization versions of such problems, such as the maximum satisfiability problem (Max SAT).\nInapproximability has been a fruitful area of research in computational complexity theory since the 1990 result of Feige, Goldwasser, Lovász, Safra and Szegedy on the inapproximability of Independent Set. After Arora et al. proved the PCP theorem a year later, it has now been shown that Johnson's 1974 approximation algorithms for Max SAT, Set Cover, Independent Set and Coloring all achieve the optimal approximation ratio, assuming P != NP.","tag_line":"In computer science and operations research, approximation algorithms are algorithms used to find approximate solutions to optimization problems.","algorithms":["approximation-algorithm","submodular-set-function","alpha-max-plus-beta-min-algorithm","approximation-preserving-reduction","apx","bidimensionality","christofides-algorithm","domination-analysis","gap-reduction","hardness-of-approximation","k-approximation-of-k-hitting-set","karloff–zwick-algorithm","l-reduction","method-of-conditional-probabilities","nearest-neighbour-algorithm","polynomial-time-algorithm-for-approximating-the-volume-of-convex-bodies","polynomial-time-approximation-scheme","property-testing","token-reconfiguration","unique-games-conjecture"],"name":"Approximation algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"cryptanalytic-algorithms","_score":0,"_source":{"algorithms":["berlekamp–massey-algorithm","reeds–sloane-algorithm"],"name":"Cryptanalytic algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"researchers-in-geometric-algorithms","_score":0,"_source":{"algorithms":["pankaj-k.-agarwal","lars-arge","david-avis","jon-bentley","jit-bose","timothy-m.-chan","bernard-chazelle","matthew-t.-dickerson","györgy-elekes","david-eppstein","leonidas-j.-guibas","john-hershberger","david-g.-kirkpatrick","jiří-matoušek-(mathematician)","nimrod-megiddo","joseph-o'rourke-(professor)","franco-p.-preparata","john-reif","pierre-rosenstiehl","jörg-rüdiger-sack","michael-segal","raimund-seidel","michael-ian-shamos","subhash-suri","roberto-tamassia","shang-hua-teng","godfried-toussaint","frances-yao"],"name":"Researchers in geometric algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"network-scheduling-algorithms","_score":0,"_source":{"description":"On a node in packet switching communication network, a network scheduler, also called packet scheduler, is an arbiter program that manages the sequence of network packets in the transmit and receive queues of the network interface controller, which is a circular data buffer. There are several network schedulers available for the different operating system kernels, that implement many of the existing network scheduling algorithms.\nThe network scheduler logic decides, in a way similar to statistical multiplexers, which network packet to forward next from the buffer. The buffer works as a queuing system, storing the network packets temporarily until they are transmitted. The buffer space may be divided into different queues, with each of them holding the packets of one flow according to configured packet classification rules; for example, packets can be divided into flows by their source and destination IP addresses. Network scheduling algorithms and their associated settings determine how the network scheduler manages the buffer.\nAlso, network schedulers are enabling accomplishment of the active queue management and traffic shaping.","tag_line":"On a node in packet switching communication network, a network scheduler, also called packet scheduler, is an arbiter program that manages the sequence of network packets in the transmit and receive queues of the network interface controller, which is a circular data buffer.","algorithms":["active-queue-management","class-based-queueing","deficit-round-robin","delay-gradient-congestion-control","fair-queuing",["generic-cell-rate-algorithm",null],"hierarchical-fair-service-curve","interleaved-polling-with-adaptive-cycle-time","leaky-bucket","max-min-fairness","network-scheduler","proportionally-fair","random-early-detection","round-robin-scheduling","token-bucket","weighted-fair-queueing"],"name":"Network scheduling algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"type-3-encryption-algorithms","_score":0,"_source":{"description":"The vast majority of the National Security Agency's work on encryption is classified, but from time to time NSA participates in standards processes or otherwise publishes information about its cryptographic algorithms. The NSA has categorized encryption items into four product types, and algorithms into two suites. The following is a brief and incomplete summary of public knowledge about NSA algorithms and protocols.","tag_line":"The vast majority of the National Security Agency's work on encryption is classified, but from time to time NSA participates in standards processes or otherwise publishes information about its cryptographic algorithms.","algorithms":[["nsa-cryptography",null]],"name":"Type 3 encryption algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"cryptographic-algorithms","_score":0,"_source":{"description":"In cryptography, encryption is the process of encoding messages or information in such a way that only authorized parties can read it. Encryption does not of itself prevent interception, but denies the message content to the interceptor. In an encryption scheme, the intended communication information or message, referred to as plaintext, is encrypted using an encryption algorithm, generating ciphertext that can only be read if decrypted. For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is in principle possible to decrypt the message without possessing the key, but, for a well-designed encryption scheme, large computational resources and skill are required. An authorized recipient can easily decrypt the message with the key provided by the originator to recipients, but not to unauthorized interceptors.\n\n","tag_line":"In cryptography, encryption is the process of encoding messages or information in such a way that only authorized parties can read it.","algorithms":["bach's-algorithm","barrett-reduction","block-cipher-mode-of-operation","cdmf","common-scrambling-algorithm","crypto++","cycles-per-byte","feedback-with-carry-shift-registers","generating-primes","hmac-based-one-time-password-algorithm","industrial-grade-prime","key-schedule","key-wrap","kochanski-multiplication","master-password","modular-exponentiation","montgomery-modular-multiplication","mosquito","nsa-product-types","pr-cpa-advantage","randomness-extractor","rc-algorithm","ring-learning-with-errors-key-exchange","s-box","scrypt","securelog","substitution-permutation-network","supersingular-isogeny-key-exchange","symmetric-key-algorithm","time-based-one-time-password-algorithm"],"name":"Cryptographic algorithms","children":["asymmetric-key-algorithms","broken-cryptography-algorithms","cryptanalytic-algorithms","cryptographic-hash-functions","cryptographically-secure-pseudorandom-number-generators","information-theoretically-secure-algorithms","integer-factorization-algorithms","primality-tests","type-1-encryption-algorithms","type-2-encryption-algorithms","type-3-encryption-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"computer-aided-design","_score":0,"_source":{"description":"Computer-aided design (CAD) is the use of computer systems to aid in the creation, modification, analysis, or optimization of a design. CAD software is used to increase the productivity of the designer, improve the quality of design, improve communications through documentation, and to create a database for manufacturing. CAD output is often in the form of electronic files for print, machining, or other manufacturing operations.\nComputer-aided design is used in many fields. Its use in designing electronic systems is known as electronic design automation, or EDA. In mechanical design it is known as mechanical design automation (MDA) or computer-aided design (CAD), which includes the process of creating a technical drawing with the use of computer software.\nCAD software for mechanical design uses either vector-based graphics to depict the objects of traditional drafting, or may also produce raster graphics showing the overall appearance of designed objects. However, it involves more than just shapes. As in the manual drafting of technical and engineering drawings, the output of CAD must convey information, such as materials, processes, dimensions, and tolerances, according to application-specific conventions.\nCAD may be used to design curves and figures in two-dimensional (2D) space; or curves, surfaces, and solids in three-dimensional (3D) space.\nCAD is an important industrial art extensively used in many applications, including automotive, shipbuilding, and aerospace industries, industrial and architectural design, prosthetics, and many more. CAD is also widely used to produce computer animation for special effects in movies, advertising and technical manuals, often called DCC digital content creation. The modern ubiquity and power of computers means that even perfume bottles and shampoo dispensers are designed using techniques unheard of by engineers of the 1960s. Because of its enormous economic importance, CAD has been a major driving force for research in computational geometry, computer graphics (both hardware and software), and discrete differential geometry.\nThe design of geometric models for object shapes, in particular, is occasionally called computer-aided geometric design (CAGD).","tag_line":"Computer-aided design (CAD) is the use of computer systems to aid in the creation, modification, analysis, or optimization of a design.","algorithms":[["computer-automated-design",null],["geometric-design",null],["geometric-modeling",null]],"name":"Computer-aided design","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"cryptographic-hash-functions","_score":0,"_source":{"description":"A cryptographic hash function is a hash function which is considered practically impossible to invert, that is, to recreate the input data from its hash value alone. These one-way hash functions have been called \"the workhorses of modern cryptography\". The input data is often called the message, and the hash value is often called the message digest or simply the digest.\nThe ideal cryptographic hash function has four main properties:\nit is easy to compute the hash value for any given message\nit is infeasible to generate a message from its hash\nit is infeasible to modify a message without changing the hash\nit is infeasible to find two different messages with the same hash.\nCryptographic hash functions have many information security applications, notably in digital signatures, message authentication codes (MACs), and other forms of authentication. They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. Indeed, in information security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, even though all these terms stand for more general functions with rather different properties and purposes.\n^ Schneier, Bruce. \"Cryptanalysis of MD5 and SHA: Time for a New Standard\". Computerworld. Retrieved 15 October 2014.","tag_line":"A cryptographic hash function is a hash function which is considered practically impossible to invert, that is, to recreate the input data from its hash value alone.","algorithms":[["crypt-(c)",null],["md5",null],"rainbow-table",["sha-1",null],["sha-2",null],"universal-hashing"],"name":"Cryptographic hash functions","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"hashing","_score":0,"_source":{"description":"In computing, a hash table (hash map) is a data structure used to implement an associative array, a structure that can map keys to values. A hash table uses a hash function to compute an index into an array of buckets or slots, from which the desired value can be found.\nIdeally, the hash function will assign each key to a unique bucket, but it is possible that two keys will generate an identical hash causing both keys to point to the same bucket. Instead, most hash table designs assume that hash collisions—different keys that are assigned by the hash function to the same bucket—will occur and must be accommodated in some way.\nIn a well-dimensioned hash table, the average cost (number of instructions) for each lookup is independent of the number of elements stored in the table. Many hash table designs also allow arbitrary insertions and deletions of key-value pairs, at (amortized) constant average cost per operation.\nIn many situations, hash tables turn out to be more efficient than search trees or any other table lookup structure. For this reason, they are widely used in many kinds of computer software, particularly for associative arrays, database indexing, caches, and sets.\n\n","tag_line":"In computing, a hash table (hash map) is a data structure used to implement an associative array, a structure that can map keys to values.","algorithms":[["bloom-filter",null],["hash-join",null],["hopscotch-hashing",null],["linear-hashing",null],"perceptual-hashing",["rabin–karp-algorithm",null],["rendezvous-hashing",null],["universal-hashing",null]],"name":"Hashing","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"agreement-algorithms","_score":0,"_source":{"algorithms":["intersection-algorithm","marzullo's-algorithm"],"name":"Agreement algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"pattern-matching","_score":0,"_source":{"description":"In computer science, pattern matching is the act of checking a given sequence of tokens for the presence of the constituents of some pattern. In contrast to pattern recognition, the match usually has to be exact. The patterns generally have the form of either sequences or tree structures. Uses of pattern matching include outputting the locations (if any) of a pattern within a token sequence, to output some component of the matched pattern, and to substitute the matching pattern with some other token sequence (i.e., search and replace).\nSequence patterns (e.g., a text string) are often described using regular expressions and matched using techniques such as backtracking.\nTree patterns are used in some programming languages as a general tool to process data based on its structure, e.g., Haskell, ML, Scala and the symbolic mathematics language Mathematica have special syntax for expressing tree patterns and a language construct for conditional execution and value retrieval based on it. For simplicity and efficiency reasons, these tree patterns lack some features that are available in regular expressions.\nOften it is possible to give alternative patterns that are tried one by one, which yields a powerful conditional programming construct. Pattern matching sometimes include support for guards.\nTerm rewriting and graph rewriting languages rely on pattern matching for the fundamental way a program evaluates into a result.","tag_line":"In computer science, pattern matching is the act of checking a given sequence of tokens for the presence of the constituents of some pattern.","algorithms":["backtracking","redos","reteoo",["rna22",null],["teiresias-algorithm",null]],"name":"Pattern matching","children":["phonetic-algorithms","regular-expressions","string-matching-algorithms"]}}
,{"_index":"throwtable","_type":"category","_id":"pi-algorithms","_score":0,"_source":{"description":"Approximations for the mathematical constant pi (π) in the history of mathematics reached an accuracy within 0.04% of the true value before the beginning of the Common Era (Archimedes). In Chinese mathematics, this was improved to approximations correct to what corresponds to about seven decimal digits by the 5th century.\nFurther progress was made only from the 15th century (Jamshīd al-Kāshī), and early modern mathematicians reached an accuracy of 35 digits by the 18th century (Ludolph van Ceulen), and 126 digits by the 19th century (Jurij Vega), surpassing the accuracy required for any conceivable application outside of pure mathematics.\nThe record of manual approximation of π is held by William Shanks, who calculated 527 digits correctly in the years preceding 1873. Since the mid 20th century, approximation of π has been the task of electronic digital computers; the current record (as of May 2015) is at 13.3 trillion digits, calculated in October 2014.\n\n","tag_line":"Approximations for the mathematical constant pi (π) in the history of mathematics reached an accuracy within 0.04% of the true value before the beginning of the Common Era (Archimedes).","algorithms":["bailey–borwein–plouffe-formula","borwein's-algorithm","chudnovsky-algorithm",["fee-method",null],"gauss–legendre-algorithm","liu-hui's-π-algorithm"],"name":"Pi algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"string-sorting-algorithms","_score":0,"_source":{"description":"Burstsort and its variants are cache-efficient algorithms for sorting strings and are faster than radix sort for large data sets of common strings, first published in 2003.\nBurstsort algorithms use a trie to store prefixes of strings, with growable arrays of pointers as end nodes containing sorted, unique, suffixes (referred to as buckets). Some variants copy the string tails into the buckets. As the buckets grow beyond a predetermined threshold, the buckets are \"burst\", giving the sort its name. A more recent variant uses a bucket index with smaller sub-buckets to reduce memory usage. Most implementations delegate to multikey quicksort, an extension of three-way radix quicksort, to sort the contents of the buckets. By dividing the input into buckets with common prefixes, the sorting can be done in a cache-efficient manner.\nBurstsort was introduced as a sort that is similar to MSD Radix Sort, but is faster due to being aware of caching and related radixes being stored closer to each other due to specifics of trie structure. It exploits specifics of strings that are usually encountered in real world. And even though asymptotically it is the same as radix sort, with time complexity of O(wn) (w - word length and n - number of strings to be sorted), but due to more optimal memory distribution it tends to be twice as fast on big data sets of strings.","tag_line":"Burstsort and its variants are cache-efficient algorithms for sorting strings and are faster than radix sort for large data sets of common strings, first published in 2003.","algorithms":["american-flag-sort","burstsort",["multi-key-quicksort",null],["radix-sort",null]],"name":"String sorting algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"quantum-algorithms","_score":0,"_source":{"description":"In quantum computing, a quantum algorithm is an algorithm which runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation. A classical (or non-quantum) algorithm is a finite sequence of instructions, or a step-by-step procedure for solving a problem, where each step or instruction can be performed on a classical computer. Similarly, a quantum algorithm is a step-by-step procedure, where each of the steps can be performed on a quantum computer. Although all classical algorithms can also be performed on a quantum computer, the term quantum algorithm is usually used for those algorithms which seem inherently quantum, or use some essential feature of quantum computation such as quantum superposition or quantum entanglement.\nProblems which are undecidable using classical computers remain undecidable using quantum computers. What makes quantum algorithms interesting is that they might be able to solve some problems faster than classical algorithms.\nThe most well known algorithms are Shor's algorithm for factoring, and Grover's algorithm for searching an unstructured database or an unordered list. Shor's algorithms runs exponentially faster than the best known classical algorithm for factoring, the general number field sieve. Grover's algorithm runs quadratically faster than the best possible classical algorithm for the same task.\n\n","tag_line":"In quantum computing, a quantum algorithm is an algorithm which runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation.","algorithms":["quantum-algorithm","amplitude-amplification","bht-algorithm","deutsch–jozsa-algorithm","grover's-algorithm","hidden-subgroup-problem",["quantum-algorithm-for-linear-systems-of-equations",null],"quantum-fourier-transform","quantum-phase-estimation-algorithm",["shor's-algorithm",null],"simon's-problem"],"name":"Quantum algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"termination-algorithms","_score":0,"_source":{"description":"In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ AL-gə-ri-dhəm) is a self-contained step-by-step set of operations to be performed. Algorithms exist that perform calculation, data processing, and automated reasoning.\nAn algorithm is an effective method that can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\nThe concept of algorithm has existed for centuries, however a partial formalization of what would become the modern algorithm began with attempts to solve the Entscheidungsproblem (the \"decision problem\") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define \"effective calculability\" or \"effective method\"; those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's \"Formulation 1\" of 1936, and Alan Turing's Turing machines of 1936–7 and 1939. Giving a formal definition of algorithms, corresponding to the intuitive notion, remains a challenging problem.\n\n","tag_line":"In mathematics and computer science, an algorithm (/ˈælɡərɪðəm/ AL-gə-ri-dhəm) is a self-contained step-by-step set of operations to be performed.","algorithms":[["dijkstra–scholten-algorithm",null],"huang's-algorithm"],"name":"Termination algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"error-detection-and-correction","_score":0,"_source":{"description":"In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. Many communication channels are subject to channel noise, and thus errors may be introduced during transmission from the source to a receiver. Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases.","tag_line":"In information theory and coding theory with applications in computer science and telecommunication, error detection and correction or error control are techniques that enable reliable delivery of digital data over unreliable communication channels.","algorithms":["bcjr-algorithm",["berlekamp–massey-algorithm",null],"forward–backward-algorithm",["iso-7064",null],"k-independent-hashing",["luhn-algorithm",null],["verhoeff-algorithm",null]],"name":"Error detection and correction","children":[null,"hash-functions"]}}
,{"_index":"throwtable","_type":"category","_id":"information-theoretically-secure-algorithms","_score":0,"_source":{"description":"A cryptosystem is information-theoretically secure if its security derives purely from information theory. That is, it cannot be broken even when the adversary has unlimited computing power. The adversary simply does not have enough information to break the encryption, so these cryptosystems are considered cryptanalytically unbreakable.\nAn encryption protocol that has information-theoretic security does not depend for its effectiveness on unproven assumptions about computational hardness, and such an algorithm is not vulnerable to future developments in computer power such as quantum computing. An example of an information-theoretically secure cryptosystem is the one-time pad. The concept of information-theoretically secure communication was introduced in 1949 by American mathematician Claude Shannon, the inventor of information theory, who used it to prove that the one-time pad system was secure. Information-theoretically secure cryptosystems have been used for the most sensitive governmental communications, such as diplomatic cables and high-level military communications, because of the great efforts enemy governments expend toward breaking them.\nAn interesting special case is perfect security: an encryption algorithm is perfectly secure if a ciphertext produced using it provides no information about the plaintext without knowledge of the key. If E is a perfectly secure encryption function, for any fixed message m there must exist for each ciphertext c at least one key k such that . It has been proved that any cipher with the perfect secrecy property must use keys with effectively the same requirements as one-time pad keys.\nIt is common for a cryptosystem to leak some information but nevertheless maintain its security properties even against an adversary that has unlimited computational resources. Such a cryptosystem would have information theoretic but not perfect security. The exact definition of security would depend on the cryptosystem in question.\nThere are a variety of cryptographic tasks for which information-theoretic security is a meaningful and useful requirement. A few of these are:\nSecret sharing schemes such as Shamir's are information-theoretically secure (and also perfectly secure) in that less than the requisite number of shares of the secret provide no information about the secret.\nMore generally, secure multiparty computation protocols often, but not always, have information-theoretic security.\nPrivate information retrieval with multiple databases can be achieved with information-theoretic privacy for the user's query.\nReductions between cryptographic primitives or tasks can often be achieved information-theoretically. Such reductions are important from a theoretical perspective, because they establish that primitive  can be realized if primitive  can be realized.\nSymmetric encryption can be constructed under an information-theoretic notion of security called entropic security, which assumes that the adversary knows almost nothing about the message being sent. The goal here is to hide all functions of the plaintext rather than all information about it.\nQuantum cryptography is largely part of information-theoretic cryptography.\nConventional secrecy entails encrypting messages. Beyond this, some scenarios require covert communication, a stronger type of secrecy which also hides the fact that communication is happening at all. \n^ a b Shannon, Claude E. (October 1949). \"Communication Theory of Secrecy Systems\" (PDF). Bell System Technical Journal (USA: AT&T Corporation) 28 (4): 656–715. doi:10.1002/j.1538-7305.1949.tb00928.x. Retrieved 2011-12-21. \n^ Soltani, R.; Bash, B.; Goeckel, D.; Guha, S.; Towsley, D. (Sep 2014). \"Covert single-hop communication in a wireless network with distributed artificial noise generation\". Communication, Control, and Computing (Allerton), 2014 52nd Annual Allerton Conference on: 1078–1085.","tag_line":"A cryptosystem is information-theoretically secure if its security derives purely from information theory.","algorithms":["information-theoretic-security","shamir's-secret-sharing"],"name":"Information-theoretically secure algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"selection-algorithms","_score":0,"_source":{"description":"In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. This includes the cases of finding the minimum, maximum, and median elements. There are O(n) (worst-case linear time) selection algorithms, and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. Selection is a subproblem of more complex problems like the nearest neighbor and shortest path problems. Many selection algorithms are derived by generalizing a sorting algorithm, and conversely some sorting algorithms can be derived as repeated application of selection.\nThe simplest case of a selection algorithm is finding the minimum (or maximum) element by iterating through the list, keeping track of the running minimum – the minimum so far – (or maximum) and can be seen as related to the selection sort. Conversely, the hardest case of a selection algorithm is finding the median, and this necessarily takes n/2 storage. In fact, a specialized median-selection algorithm can be used to build a general selection algorithm, as in median of medians. The best-known selection algorithm is quickselect, which is related to quicksort; like quicksort, it has (asymptotically) optimal average performance, but poor worst-case performance, though it can be modified to give optimal worst-case performance as well.","tag_line":"In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic.","algorithms":["selection-algorithm","median-of-medians","floyd–rivest-algorithm","introselect","median-of-medians","quickselect"],"name":"Selection algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"fft-algorithms","_score":0,"_source":{"description":"This category is for fast Fourier transform (FFT) algorithms, i.e. algorithms to compute the discrete Fourier transform (DFT) in O(N log N) time (or better, for approximate algorithms), where  is the number of discrete points.","tag_line":"This category is for fast Fourier transform (FFT) algorithms, i.e.","algorithms":["bruun's-fft-algorithm","butterfly-diagram","cooley–tukey-fft-algorithm",["cyclotomic-fast-fourier-transform",null],["fast-fourier-transform",null],"fftw",["goertzel-algorithm",null],"prime-factor-fft-algorithm","rader's-fft-algorithm","split-radix-fft-algorithm","twiddle-factor"],"name":"FFT algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"dynamic-programming","_score":0,"_source":{"description":"In mathematics, management science, economics, computer science, and bioinformatics, dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions - ideally, using a memory-based data structure. The next time the same subproblem occurs, instead of recomputing its solution, one simply looks up the previously computed solution, thereby saving computation time at the expense of a (hopefully) modest expenditure in storage space. (Each of the subproblem solutions is indexed in some way, typically based on the values of its input parameters, so as to facilitate its lookup.) The act of storing solutions to subproblems is called \"memoization\".\nDynamic programming algorithms are used for optimization (for example, finding the shortest path between two points, or the fastest way to multiply many matrices). A dynamic programming algorithm will examine the previously solved subproblems and will combine their solutions to give the best solution for the given problem. The alternatives are many, such as using a greedy algorithm, which picks the locally optimal choice at each branch in the road. The locally optimal choice may be a poor choice for the overall solution. While a greedy algorithm does not guarantee an optimal solution, it is often faster to calculate. Fortunately, some greedy algorithms (such as minimum spanning trees) are proven to lead to the optimal solution.\nFor example, let's say that you have to get from point A to point B as fast as possible, in a given city, during rush hour. A dynamic programming algorithm will look at finding the shortest paths to points close to A, and use those solutions to eventually find the shortest path to B. On the other hand, a greedy algorithm will start you driving immediately and will pick the road that looks the fastest at every intersection. As you can imagine, this strategy might not lead to the fastest arrival time, since you might take some \"easy\" streets and then find yourself hopelessly stuck in a traffic jam.\nSometimes, applying memoization to a naive basic recursive solution already results in a dynamic programming solution with asymptotically optimal time complexity; however, the optimal solution to some problems requires more sophisticated dynamic programming algorithms. Some of these may be recursive as well but parametrized differently from the naive solution. Others can be more complicated and cannot be implemented as a recursive function with memoization. Examples of these are the two solutions to the Egg Dropping puzzle below.","tag_line":"In mathematics, management science, economics, computer science, and bioinformatics, dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions - ideally, using a memory-based data structure.","algorithms":[["dynamic-programming",null],["bellman–ford-algorithm",null],["dynamic-time-warping",null],"earley-parser",["floyd–warshall-algorithm",null],["forward–backward-algorithm",null],["hirschberg's-algorithm",null],"hunt–mcilroy-algorithm",["maximum-subarray-problem",null],["needleman–wunsch-algorithm",null],["smith–waterman-algorithm",null]],"name":"Dynamic programming","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"number-theoretic-algorithms","_score":0,"_source":{"description":"This category deals with algorithms in number theory, especially primality testing and similar.","tag_line":"This category deals with algorithms in number theory, especially primality testing and similar.","algorithms":["ancient-egyptian-multiplication","baby-step-giant-step","binary-gcd-algorithm","chakravala-method","cipolla's-algorithm",["computational-complexity-of-mathematical-operations",null],"cornacchia's-algorithm","euclidean-algorithm","extended-euclidean-algorithm",["generating-primes",null],"integer-relation-algorithm","lehmer's-gcd-algorithm",["modular-exponentiation",null],"pocklington's-algorithm","pohlig–hellman-algorithm",["pollard's-kangaroo-algorithm",null],"pollard's-rho-algorithm-for-logarithms","tonelli–shanks-algorithm"],"name":"Number theoretic algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"concurrency-control-algorithms","_score":0,"_source":{"algorithms":["banker's-algorithm","dekker's-algorithm","lamport's-bakery-algorithm","lamport's-distributed-mutual-exclusion-algorithm","maekawa's-algorithm","non-blocking-algorithm","peterson's-algorithm","raymond's-algorithm","spinlock","szymański's-algorithm","ticket-lock","timestamp-based-concurrency-control"],"name":"Concurrency control algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"external-memory-algorithms","_score":0,"_source":{"description":"Computer data storage, often called storage or memory, is a technology consisting of computer components and recording media used to retain digital data. It is a core function and fundamental component of computers.\nThe central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but larger and cheaper options farther away. Often the fast volatile technologies (which lose data when powered off) are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\"; however, \"memory\" is sometimes also used when referring to persistent storage.\nIn the Von Neumann architecture, the CPU consists of two main parts: control unit and arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.","tag_line":"Computer data storage, often called storage or memory, is a technology consisting of computer components and recording media used to retain digital data.","algorithms":[["cache-oblivious-distribution-sort",null],"cache-oblivious-matrix-multiplication",["external-sorting",null],["funnelsort",null]],"name":"External memory algorithms","children":[null]}}
,{"_index":"throwtable","_type":"category","_id":"disk-scheduling-algorithms","_score":0,"_source":{"description":"Shortest seek first (or shortest seek time first) is a secondary storage scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\n\n","tag_line":"Shortest seek first (or shortest seek time first) is a secondary storage scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\n\n","algorithms":["anticipatory-scheduling",["elevator-algorithm",null],"fscan","i/o-scheduling","look-algorithm","n-step-scan","shortest-seek-first"],"name":"Disk scheduling algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"unicode-algorithms","_score":0,"_source":{"description":"This category lists articles on algorithms developed by the Unicode Consortium for handling of characters and text.","tag_line":"This category lists articles on algorithms developed by the Unicode Consortium for handling of characters and text.","algorithms":[["unicode-collation-algorithm",null],["iso-14651",null]],"name":"Unicode algorithms","children":[]}}
,{"_index":"throwtable","_type":"category","_id":"variance-reduction","_score":0,"_source":{"description":"In mathematics, more specifically in the theory of Monte Carlo methods, variance reduction is a procedure used to increase the precision of the estimates that can be obtained for a given number of iterations. Every output random variable from the simulation is associated with a variance which limits the precision of the simulation results. In order to make a simulation statistically efficient, i.e., to obtain a greater precision and smaller confidence intervals for the output random variable of interest, variance reduction techniques can be used. The main ones are: Common random numbers, antithetic variates, control variates, importance sampling and stratified sampling. Under these headings are a variety of specialized techniques; for example, particle transport simulations make extensive use of \"weight windows\" and \"splitting/Russian roulette\" techniques, which are a form of importance sampling.","tag_line":"In mathematics, more specifically in the theory of Monte Carlo methods, variance reduction is a procedure used to increase the precision of the estimates that can be obtained for a given number of iterations.","algorithms":[["vegas-algorithm",null]],"name":"Variance reduction","children":[]}}
]

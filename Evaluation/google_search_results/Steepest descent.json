{
    "kind": "customsearch#search", 
    "url": {
        "type": "application/json", 
        "template": "https://www.googleapis.com/customsearch/v1?q={searchTerms}&num={count?}&start={startIndex?}&lr={language?}&safe={safe?}&cx={cx?}&cref={cref?}&sort={sort?}&filter={filter?}&gl={gl?}&cr={cr?}&googlehost={googleHost?}&c2coff={disableCnTwTranslation?}&hq={hq?}&hl={hl?}&siteSearch={siteSearch?}&siteSearchFilter={siteSearchFilter?}&exactTerms={exactTerms?}&excludeTerms={excludeTerms?}&linkSite={linkSite?}&orTerms={orTerms?}&relatedSite={relatedSite?}&dateRestrict={dateRestrict?}&lowRange={lowRange?}&highRange={highRange?}&searchType={searchType}&fileType={fileType?}&rights={rights?}&imgSize={imgSize?}&imgType={imgType?}&imgColorType={imgColorType?}&imgDominantColor={imgDominantColor?}&alt=json"
    }, 
    "items": [
        {
            "kind": "customsearch#result", 
            "title": "Gradient descent - Wikipedia, the free encyclopedia", 
            "displayLink": "en.wikipedia.org", 
            "htmlTitle": "<b>Gradient descent</b> - Wikipedia, the free encyclopedia", 
            "formattedUrl": "https://en.wikipedia.org/wiki/Gradient_descent", 
            "htmlFormattedUrl": "https://en.wikipedia.org/wiki/<b>Gradient</b>_<b>descent</b>", 
            "pagemap": {
                "cse_thumbnail": [
                    {
                        "width": "217", 
                        "src": "https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcREUf4MJd1p9FeAowsEcQbFHt3PbNM6CJunFoLLXTEonr8jhjQY1xF-4_M", 
                        "height": "232"
                    }
                ], 
                "cse_image": [
                    {
                        "src": "https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/350px-Gradient_descent.svg.png"
                    }
                ]
            }, 
            "snippet": "Gradient descent is a first-order optimization algorithm. To find a local minimum \nof a function using gradient descent, one takes steps proportional to the negative\n\u00a0...", 
            "htmlSnippet": "<b>Gradient descent</b> is a first-order optimization algorithm. To find a local minimum <br>\nof a function using <b>gradient descent</b>, one takes steps proportional to the negative<br>\n&nbsp;...", 
            "link": "https://en.wikipedia.org/wiki/Gradient_descent", 
            "cacheId": "H6_GiOPyBRoJ"
        }, 
        {
            "kind": "customsearch#result", 
            "title": "Method of steepest descent - Wikipedia, the free encyclopedia", 
            "displayLink": "en.wikipedia.org", 
            "htmlTitle": "Method of <b>steepest descent</b> - Wikipedia, the free encyclopedia", 
            "formattedUrl": "https://en.wikipedia.org/wiki/Method_of_steepest_descent", 
            "htmlFormattedUrl": "https://en.wikipedia.org/wiki/Method_of_<b>steepest</b>_<b>descent</b>", 
            "snippet": "In mathematics, the method of steepest descent or stationary phase method or \nsaddle-point method is an extension of Laplace's method for approximating an\u00a0...", 
            "htmlSnippet": "In mathematics, the method of <b>steepest descent</b> or stationary phase method or <br>\nsaddle-point method is an extension of Laplace&#39;s method for approximating an&nbsp;...", 
            "link": "https://en.wikipedia.org/wiki/Method_of_steepest_descent", 
            "cacheId": "u1ypRyv4UUsJ"
        }, 
        {
            "kind": "customsearch#result", 
            "title": "Stochastic gradient descent - Wikipedia, the free encyclopedia", 
            "displayLink": "en.wikipedia.org", 
            "htmlTitle": "Stochastic <b>gradient descent</b> - Wikipedia, the free encyclopedia", 
            "formattedUrl": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", 
            "htmlFormattedUrl": "https://en.wikipedia.org/wiki/Stochastic_<b>gradient</b>_<b>descent</b>", 
            "snippet": "Stochastic gradient descent is a gradient descent optimization method for \nminimizing an objective function that is written as a sum of differentiable functions\n.", 
            "htmlSnippet": "Stochastic <b>gradient descent</b> is a <b>gradient descent</b> optimization method for <br>\nminimizing an objective function that is written as a sum of differentiable functions<br>\n.", 
            "link": "https://en.wikipedia.org/wiki/Stochastic_gradient_descent", 
            "cacheId": "sgqbOX6ysbQJ"
        }, 
        {
            "kind": "customsearch#result", 
            "title": "Conjugate gradient method - Wikipedia, the free encyclopedia", 
            "displayLink": "en.wikipedia.org", 
            "htmlTitle": "Conjugate <b>gradient</b> method - Wikipedia, the free encyclopedia", 
            "formattedUrl": "https://en.wikipedia.org/wiki/Conjugate_gradient_method", 
            "htmlFormattedUrl": "https://en.wikipedia.org/wiki/Conjugate_<b>gradient</b>_method", 
            "pagemap": {
                "cse_thumbnail": [
                    {
                        "width": "176", 
                        "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSdWEbprvFAg0IOuJuzAEBZ_mVnPI1oV6632m_T8GU_ydloPNyPUZrhPn4", 
                        "height": "261"
                    }
                ], 
                "cse_image": [
                    {
                        "src": "https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Conjugate_gradient_illustration.svg/220px-Conjugate_gradient_illustration.svg.png"
                    }
                ]
            }, 
            "snippet": "[edit]. In both the original and the preconditioned conjugate\u00a0...", 
            "htmlSnippet": "[edit]. In both the original and the preconditioned conjugate&nbsp;...", 
            "link": "https://en.wikipedia.org/wiki/Conjugate_gradient_method", 
            "cacheId": "kiX8RvFON9YJ"
        }, 
        {
            "kind": "customsearch#result", 
            "title": "Nonlinear conjugate gradient method - Wikipedia, the free ...", 
            "displayLink": "en.wikipedia.org", 
            "htmlTitle": "Nonlinear conjugate <b>gradient</b> method - Wikipedia, the free <b>...</b>", 
            "formattedUrl": "https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method", 
            "htmlFormattedUrl": "https://en.wikipedia.org/wiki/Nonlinear_conjugate_<b>gradient</b>_method", 
            "snippet": "variables to minimize, its gradient \\nabla_x f indicates the direction of maximum \nincrease. One simply starts in the opposite (steepest descent) direction:.", 
            "htmlSnippet": "variables to minimize, its gradient \\nabla_x f indicates the direction of maximum <br>\nincrease. One simply starts in the opposite (<b>steepest descent</b>) direction:.", 
            "link": "https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method", 
            "cacheId": "zPaBjHOxaEAJ"
        }, 
        {
            "kind": "customsearch#result", 
            "title": "Coordinate descent - Wikipedia, the free encyclopedia", 
            "displayLink": "en.wikipedia.org", 
            "htmlTitle": "Coordinate <b>descent</b> - Wikipedia, the free encyclopedia", 
            "formattedUrl": "https://en.wikipedia.org/wiki/Coordinate_descent", 
            "htmlFormattedUrl": "https://en.wikipedia.org/wiki/Coordinate_<b>descent</b>", 
            "pagemap": {
                "cse_thumbnail": [
                    {
                        "width": "259", 
                        "src": "https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQLRF7DD2XNgA4sSgt-AxVQBT7isvl3J0qc9LT2zhvXri6EcKjPSaw3oUM", 
                        "height": "194"
                    }
                ], 
                "cse_image": [
                    {
                        "src": "https://upload.wikimedia.org/wikipedia/commons/5/50/Coordinate_descent.jpg"
                    }
                ]
            }, 
            "snippet": "It can be shown that this sequence has similar convergence properties as \nsteepest descent. No improvement after one cycle of line search along \ncoordinate\u00a0...", 
            "htmlSnippet": "It can be shown that this sequence has similar convergence properties as <br>\n<b>steepest descent</b>. No improvement after one cycle of line search along <br>\ncoordinate&nbsp;...", 
            "link": "https://en.wikipedia.org/wiki/Coordinate_descent", 
            "cacheId": "uv9inQcvb78J"
        }, 
        {
            "kind": "customsearch#result", 
            "title": "Talk:Method of steepest descent - Wikipedia, the free encyclopedia", 
            "displayLink": "en.wikipedia.org", 
            "htmlTitle": "Talk:Method of <b>steepest descent</b> - Wikipedia, the free encyclopedia", 
            "formattedUrl": "https://en.wikipedia.org/wiki/Talk%3AMethod_of_steepest_descent", 
            "htmlFormattedUrl": "https://en.wikipedia.org/wiki/Talk%3AMethod_of_<b>steepest</b>_<b>descent</b>", 
            "snippet": "Laplace's method article still retains some text and formulas that are specific to \nthe Method of steepest descent, that, i believe, would be better placed within this\n\u00a0...", 
            "htmlSnippet": "Laplace&#39;s method article still retains some text and formulas that are specific to <br>\nthe Method of <b>steepest descent</b>, that, i believe, would be better placed within this<br>\n&nbsp;...", 
            "link": "https://en.wikipedia.org/wiki/Talk%3AMethod_of_steepest_descent", 
            "cacheId": "uy3UtBhMqXEJ"
        }, 
        {
            "kind": "customsearch#result", 
            "title": "Subgradient method - Wikipedia, the free encyclopedia", 
            "displayLink": "en.wikipedia.org", 
            "htmlTitle": "Subgradient method - Wikipedia, the free encyclopedia", 
            "formattedUrl": "https://en.wikipedia.org/wiki/Subgradient_method", 
            "htmlFormattedUrl": "https://en.wikipedia.org/wiki/Sub<b>gradient</b>_method", 
            "snippet": "When the objective function is differentiable, subgradient methods for \nunconstrained problems use the same search direction as the method of steepest \ndescent.", 
            "htmlSnippet": "When the objective function is differentiable, subgradient methods for <br>\nunconstrained problems use the same search direction as the method of <b>steepest</b> <br>\n<b>descent</b>.", 
            "link": "https://en.wikipedia.org/wiki/Subgradient_method", 
            "cacheId": "XwEyV9Da02oJ"
        }, 
        {
            "kind": "customsearch#result", 
            "title": "Backpropagation - Wikipedia, the free encyclopedia", 
            "displayLink": "en.wikipedia.org", 
            "htmlTitle": "Backpropagation - Wikipedia, the free encyclopedia", 
            "formattedUrl": "https://en.wikipedia.org/wiki/Backpropagation", 
            "htmlFormattedUrl": "https://en.wikipedia.org/wiki/Backpropagation", 
            "snippet": "4.1 Learning as an optimization problem; 4.2 An analogy for understanding \ngradient descent. 5 Derivation. 5.1 Finding the derivative of the error. 6 Modes of\n\u00a0...", 
            "htmlSnippet": "4.1 Learning as an optimization problem; 4.2 An analogy for understanding <br>\n<b>gradient descent</b>. 5 Derivation. 5.1 Finding the derivative of the error. 6 Modes of<br>\n&nbsp;...", 
            "link": "https://en.wikipedia.org/wiki/Backpropagation", 
            "cacheId": "DsCFcZjT_mcJ"
        }, 
        {
            "kind": "customsearch#result", 
            "title": "Delta rule - Wikipedia, the free encyclopedia", 
            "displayLink": "en.wikipedia.org", 
            "htmlTitle": "Delta rule - Wikipedia, the free encyclopedia", 
            "formattedUrl": "https://en.wikipedia.org/wiki/Delta_rule", 
            "htmlFormattedUrl": "https://en.wikipedia.org/wiki/Delta_rule", 
            "snippet": "In machine learning, the delta rule is a gradient descent learning rule for \nupdating the weights of the inputs to artificial neurons in a single-layer neural \nnetwork.", 
            "htmlSnippet": "In machine learning, the delta rule is a <b>gradient descent</b> learning rule for <br>\nupdating the weights of the inputs to artificial neurons in a single-layer neural <br>\nnetwork.", 
            "link": "https://en.wikipedia.org/wiki/Delta_rule", 
            "cacheId": "LjnltUBKML0J"
        }
    ], 
    "context": {
        "title": "Algo Search"
    }, 
    "queries": {
        "request": [
            {
                "count": 10, 
                "outputEncoding": "utf8", 
                "title": "Google Custom Search - Steepest descent", 
                "safe": "off", 
                "searchTerms": "Steepest descent", 
                "startIndex": 1, 
                "cx": "016575028638610389147:5wbwpqg1878", 
                "inputEncoding": "utf8", 
                "totalResults": "696"
            }
        ], 
        "nextPage": [
            {
                "count": 10, 
                "outputEncoding": "utf8", 
                "title": "Google Custom Search - Steepest descent", 
                "safe": "off", 
                "searchTerms": "Steepest descent", 
                "startIndex": 11, 
                "cx": "016575028638610389147:5wbwpqg1878", 
                "inputEncoding": "utf8", 
                "totalResults": "696"
            }
        ]
    }, 
    "searchInformation": {
        "formattedSearchTime": "0.18", 
        "formattedTotalResults": "696", 
        "totalResults": "696", 
        "searchTime": 0.180591
    }
}
{"summary": "In mathematics and computer science, an algorithm (/\u02c8\u00e6l\u0261\u0259r\u026a\u00f0\u0259m/ AL-g\u0259-ri-dh\u0259m) is a self-contained step-by-step set of operations to be performed. Algorithms exist that perform calculation, data processing, and automated reasoning.\nAn algorithm is an effective method that can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.\nThe concept of algorithm has existed for centuries, however a partial formalization of what would become the modern algorithm began with attempts to solve the Entscheidungsproblem (the \"decision problem\") posed by David Hilbert in 1928. Subsequent formalizations were framed as attempts to define \"effective calculability\" or \"effective method\"; those formalizations included the G\u00f6del\u2013Herbrand\u2013Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's \"Formulation 1\" of 1936, and Alan Turing's Turing machines of 1936\u20137 and 1939. Giving a formal definition of algorithms, corresponding to the intuitive notion, remains a challenging problem.", "links": ["7400 series", "A. A. Markov", "A. M. Turing", "Abacus", "Abstract machine", "Ada Lovelace", "Al-Jabr", "Al-Khw\u0101rizm\u012b", "Alan Turing", "Alfred North Whitehead", "Algebra", "Algorism", "Algorithm", "Algorithm (disambiguation)", "Algorithm analysis", "Algorithm characterizations", "Algorithm design", "Algorithm engineering", "Algorithmic composition", "Algorithmic efficiency", "Algorithmic synthesis", "Algorithmic trading", "Alonzo Church", "Analog computer", "Analysis of algorithms", "Analytical engine", "Andreas Blass", "Approximation algorithm", "Ariane V", "Arithmetic", "Arithmetica", "Artificial intelligence", "Assembly code", "Assignment operation", "Association for Computing Machinery", "Astronomer", "Asymptotically optimal", "Automata theory", "Automated reasoning", "Axiom", "Babbage", "Backtracking", "Baghdad", "Baudot code", "Benchmark (computing)", "Bertrand Russell", "Big O notation", "Binary search", "Binary search algorithm", "Bletchley Park", "Boolean equation", "Brahmagupta", "Branch and bound", "Brute-force search", "Brute force search", "Burali-Forti paradox", "Busy beaver", "Calculation", "Calculator", "Calculus ratiocinator", "Carl B. Boyer", "Charles Babbage", "Chess", "Church thesis", "Church\u2013Turing thesis", "Claude Shannon", "Clock", "Combinatorial", "Communications of the ACM", "Complexity class", "Computability", "Computability theory", "Computation", "Computational complexity theory", "Computational geometry", "Computer Network", "Computer programs", "Computer science", "Computer systems", "Control flow", "Control table", "Convex polytope", "Cris Moore", "Cryptography", "DMOZ", "DRAKON", "Daniel Dennett", "Data compression", "Data processing", "Data structure", "David Fowler (mathematician)", "David Hilbert", "Decidability (logic)", "Deductive reasoning", "Deterministic", "Deterministic algorithm", "Diamond v. Diehr", "Dictionary of Algorithms and Data Structures", "Difference engine", "Differential equation", "Digital object identifier", "Dilson", "Diophantine equation", "Diophantus", "Distributed algorithms", "Divide and conquer algorithm", "Domain (mathematics)", "Donald Knuth", "Dynamic programming", "Effective calculability", "Effective method", "Electrical circuit", "Emil Post", "Empirical algorithmics", "Encyclopedia of Mathematics", "Entscheidungsproblem", "Eric W. Weisstein", "Euclid", "Euclid's Elements", "Euclid's algorithm", "Euclid algorithm", "Euclidean algorithm", "Eudoxus of Cnidus", "Execution (computing)", "Export of cryptography", "Extended Euclidean algorithm", "Fast Fourier transform", "Feedback", "Finite state machine", "Flow chart", "Flowchart", "Floyd\u2013Warshall algorithm", "Formal semantics of programming languages", "Formal system", "Formulation 1", "Foundations of mathematics", "Function (mathematics)", "Functional programming", "GOTO", "Garbage in, garbage out", "Genetic algorithm", "Geographer", "Georg Cantor", "George B. Dantzig", "George Boole", "George Boolos", "George Stibitz", "Giuseppe Peano", "Google+", "Gottfried Leibniz", "Gottlob Frege", "Gottschalk v. Benson", "Graph (mathematics)", "Graph exploration algorithm", "Graph theory", "Graphics Interchange Format", "Greater Iran", "Greatest common divisor", "Greedy algorithm", "Greek mathematics", "G\u00f6del", "Halting problem", "Heuristic", "Heuristic algorithm", "Heuristics", "Hindu numeral system", "Hollerith cards", "House of Wisdom", "Howard Aiken", "Huffman coding", "Human brain", "Hypercomputation", "Imperative programming", "Inductive reasoning", "Instance (computer science)", "Integer", "Integer programming", "Integrated Authority File", "International Standard Book Number", "Interpreter (computing)", "Introduction to Algorithms", "Introduction to Arithmetic", "Islamic astronomy", "Islamic geography", "Islamic mathematics", "Iteration", "J.B. Rosser", "J. Barkley Rosser", "JSTOR", "Jacquard loom", "Jacques Herbrand", "John G. Kemeny", "John Venn", "John von Neumann", "Joseph-Marie Jacquard", "Karnaugh map", "Khwarezm", "Kleene", "Kruskal's algorithm", "Kurt G\u00f6del", "Lambda calculus", "Las Vegas algorithm", "Latin", "Linear programming", "List of Indian mathematicians", "List of algorithm general topics", "List of algorithms", "List of important publications in theoretical computer science", "Local optimum", "Local search (optimization)", "Logic", "Logic programming", "Logical NAND", "Lookup table", "Machine code", "Machine learning", "Martin Davis", "Marvin Minsky", "MathWorld", "Mathematical induction", "Mathematical table", "Mathematics", "Max-Planck-Institut f\u00fcr Informatik", "Maximum flow problem", "Medical algorithm", "Memoization", "Memory", "Merge algorithm", "Mergesort", "Methods of computing square roots", "Minsky machine", "Monte Carlo algorithm", "Muhammad ibn M\u016bs\u0101 al-Khw\u0101rizm\u012b", "Mu\u1e25ammad ibn M\u016bs\u0101 al-Khw\u0101rizm\u012b", "National Diet Library", "National Institute of Standards and Technology", "Natural language", "Neural network", "Nicomachus", "Non-deterministic algorithm", "Null string", "Numerical Mathematics Consortium", "Numerical analysis", "Oak Ridge National Laboratory", "Optimal substructure", "Optimization problem", "Overlapping subproblems", "P (complexity)", "P versus NP problem", "Parallel algorithm", "Parameterized complexity", "Parsing", "Partial function", "Persian language", "Persian people", "Philosophy of mind", "Pidgin code", "Piotr Indyk", "Polynomial time", "Post\u2013Turing machine", "Praeger Publishers", "Prim's algorithm", "Principia Mathematica", "Proceedings of the London Mathematical Society", "Profiling (computer programming)", "Program loops", "Program optimization", "Programming language", "Proof of correctness", "Pseudocode", "Quadratic equations", "Quantity", "Quantum algorithm", "Quantum computation", "Quantum entanglement", "Quantum superposition", "Quicksort", "RP (complexity)", "Randomized algorithm", "Randomized algorithms", "Randomness", "Recursion", "Recursion (computer science)", "Recursive algorithm", "Reductio ad absurdum", "Reduction (complexity)", "Relatively prime", "Relay", "Richard Jeffrey", "Richard Paradox", "Robert Kowalski", "Robert N. Bellah", "Roman numerals", "Russell paradox", "S. C. Kleene", "Search algorithm", "Selection algorithm", "Sieve of Eratosthenes", "Simplex algorithm", "Simulated annealing", "Software patent debate", "Software patents", "Sollin's algorithm", "Sorting algorithm", "Spaghetti code", "Springer Science+Business Media", "Stack (data structure)", "Stanford University", "Stanley Jevons", "State University of New York at Stony Brook", "State diagram", "State transition table", "Stephen C. Kleene", "Stephen Cole Kleene", "Stephen Kleene", "String algorithms", "Structured program theorem", "Syllogism", "Synthetic rubber", "Tabu search", "Telegraph", "Teleprinter", "The Compendious Book on Calculation by Completion and Balancing", "Theory of computation", "Thomas E. Kurtz", "Ticker tape", "Time complexity", "Total function", "Towers of Hanoi", "Turing-complete", "Turing complete", "Turing completeness", "Turing machine", "Turing machines", "Unary numeral system", "Unisys", "United States Patent and Trademark Office", "University of Illinois", "University of Indianapolis", "University of Iowa", "University of Tennessee", "Uzbekistan", "Van Heijenoort", "Verge escapement", "Volume", "Wikiversity", "Yuri Gurevich", "Zero", "Zero-error Probabilistic Polynomial time", "\u039b-calculus"], "categories": ["Algorithms", "Articles containing Persian-language text", "Articles including recorded pronunciations", "Articles with DMOZ links", "Articles with example pseudocode", "Mathematical logic", "Pages using duplicate arguments in template calls", "Theoretical computer science", "Use mdy dates from June 2013", "Wikipedia articles with GND identifiers"], "title": "Algorithm"}
{"summary": "This is a list of algorithm general topics.\nAnalysis of algorithms\nAnt colony algorithm\nApproximation algorithm\nBest and worst cases\nBig O notation\nCombinatorial search\nCompetitive analysis\nComputability theory\nComputational complexity theory\nEmbarrassingly parallel problem\nEmergent algorithm\nEvolutionary algorithm\nFast Fourier transform\nGenetic algorithm\nGraph exploration algorithm\nHeuristic\nHill climbing\nImplementation\nLas Vegas algorithm\nLock-free and wait-free algorithms\nMonte Carlo algorithm\nNumerical analysis\nOnline algorithm\nPolynomial time approximation scheme\nProblem size\nPseudorandom number generator\nQuantum algorithm\nRandom-restart hill climbing\nRandomized algorithm\nRunning time\nSorting algorithm\nSearch algorithm\nStable algorithm (disambiguation)\nSuper-recursive algorithm\nTree search algorithm", "links": ["Algorithm", "Analysis of algorithms", "Ant colony algorithm", "Approximation algorithm", "Best and worst cases", "Big O notation", "Combinatorial search", "Competitive analysis (online algorithm)", "Complexity class", "Computability theory", "Computational complexity theory", "Embarrassingly parallel", "Emergent algorithm", "Evolutionary algorithm", "Fast Fourier transform", "Genetic algorithm", "Graph exploration algorithm", "Heuristic", "Hill climbing", "Implementation", "Las Vegas algorithm", "List of algorithms", "List of complexity classes", "List of computability and complexity topics", "List of data structures", "Lock-free and wait-free algorithms", "Monte Carlo algorithm", "Numerical analysis", "Online algorithm", "Polynomial time approximation scheme", "Problem size", "Pseudorandom number generator", "Quantum algorithm", "Random-restart hill climbing", "Randomized algorithm", "Running time", "Search algorithm", "Sorting algorithm", "Stable algorithm (disambiguation)", "Super-recursive algorithm", "Tree search algorithm"], "categories": ["Algorithms", "Mathematics-related lists"], "title": "List of algorithm general topics"}
{"summary": "The following is a list of algorithms along with one-line descriptions for each.\n\n", "links": ["3Dc", "A*", "A-law algorithm", "AC-3 algorithm", "AKS primality test", "ALOPEX", "Abstract algebra", "AdaBoost", "Adaptive-additive algorithm", "Adaptive Huffman coding", "Adaptive coding", "Adaptive histogram equalization", "Adaptive replacement cache", "Addition-chain exponentiation", "Adler-32", "Advanced Encryption Standard", "Affine transformation", "Aho\u2013Corasick string matching algorithm", "Algorithm", "Algorithm X", "Algorithms for Recovery and Isolation Exploiting Semantics", "Algorithms for calculating variance", "All pairs shortest path", "Alpha-beta pruning", "Alpha max plus beta min algorithm", "Ambient occlusion", "Analytical hierarchy", "Ant colony optimization", "Antiderivatives", "Antipodal point", "Approximate counting algorithm", "Apriori algorithm", "Arbitrary-precision arithmetic", "Arithmetic coding", "Arithmetical hierarchy", "Arnoldi iteration", "Artificial neural network", "Assignment problem", "Association rule learning", "Astronomical algorithms", "Asymmetric key algorithm", "Audio data compression", "B*", "B-spline", "BCH Code", "BCJR algorithm", "BFGS method", "BKM algorithm", "Baby-step giant-step", "Backpropagation", "Backtracking", "Backward Euler method", "Bailey\u2013Borwein\u2013Plouffe formula", "Baillie-PSW primality test", "Bandwidth (matrix theory)", "Banker's algorithm", "Barnes\u2013Hut simulation", "Basic Local Alignment Search Tool", "Basis function", "Baum\u2013Welch algorithm", "Bayesian statistics", "Bead sort", "Beam search", "Beam stack search", "Beam tracing", "Bees algorithm", "Bellman\u2013Ford algorithm", "Benson's algorithm", "Bentley\u2013Ottmann algorithm", "Berkeley algorithm", "Berlekamp\u2013Massey algorithm", "Best-first search", "Best Bin First", "Biconjugate gradient method", "Bicubic interpolation", "Bidirectional search", "Bilinear interpolation", "Binary GCD algorithm", "Binary search algorithm", "Binary splitting", "Bioinformatics", "Bionics", "Birkhoff interpolation", "Bisection method", "Bitap algorithm", "Bitonic sorter", "Block Truncation Coding", "Block nested loop", "Bloom filter", "Blowfish (cipher)", "Bluestein's FFT algorithm", "Blum Blum Shub", "Boehm garbage collector", "Bogosort", "Boosting (meta-algorithm)", "Booth's multiplication algorithm", "Bootstrap aggregating", "Borwein's algorithm", "Bor\u016fvka's algorithm", "Bowyer\u2013Watson algorithm", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "Branch and bound", "Branch and cut", "Breadth-first search", "Bresenham's line algorithm", "Bron\u2013Kerbosch algorithm", "BrownBoost", "Bruss algorithm", "Bruun's FFT algorithm", "Bubble sort", "Buchberger's algorithm", "Bucket sort", "Buddy memory allocation", "Bully algorithm", "Burrows\u2013Wheeler transform", "Burst trie", "Burstsort", "Buzen's algorithm", "Byte pair encoding", "Byzantine fault tolerance", "B\u00e9zier curve", "C3 linearization", "C4.5 algorithm", "CHS conversion", "CNF-SAT", "CORDIC", "CYK algorithm", "Cache algorithms", "Calculus", "Cannon's algorithm", "Canny edge detector", "Canonical LR parser", "Canopy clustering algorithm", "Cantor\u2013Zassenhaus algorithm", "Causality", "Chaff algorithm", "Chain matrix multiplication", "Chaitin's algorithm", "Chakravala method", "Chan's algorithm", "Cheney's algorithm", "Chew's second algorithm", "Chien search", "Cholesky decomposition", "Chomsky normal form", "Christofides algorithm", "Clipping (computer graphics)", "Clock synchronization", "Clock with Adaptive Replacement", "Closest pair problem", "Cocktail sort", "Code-excited linear prediction", "Coding theory", "Cohen\u2013Sutherland", "Collision detection", "Coloring algorithm", "Comb sort", "Combinatorial optimization", "Combinatorics", "Complete-linkage clustering", "Computational complexity theory", "Computational geometry", "Computational linguistics", "Computational mathematics", "Computational physics", "Computational science", "Computational statistics", "Computer algebra", "Computer architecture", "Computer graphics", "Computer science", "Computing \u03c0", "Computus", "Cone algorithm", "Cone tracing", "Congruence of squares", "Conjugate gradient", "Conjunctive normal form", "Connected-component labeling", "Constrained Delaunay triangulation", "Constraint algorithm", "Constraint satisfaction", "Context-free grammar", "Context tree weighting", "Contour line", "Convex hull", "Convex hull algorithms", "Convex polygon", "Convex set", "Cooley\u2013Tukey FFT algorithm", "Coppersmith\u2013Winograd algorithm", "Coset", "Counting sort", "Crank-Nicolson method", "Cristian's algorithm", "Cross-entropy method", "Cryptographic hash function", "Cryptographically secure pseudo-random number generator", "Cryptography", "Cubic interpolation", "Cuthill\u2013McKee algorithm", "Cutting-plane method", "Cycle detection", "Cycle sort", "Cyclic redundancy check", "Cyrus\u2013Beck", "D*", "DBSCAN", "DEFLATE (algorithm)", "DPLL algorithm", "DTMF", "Daitch\u2013Mokotoff Soundex", "Damerau\u2013Levenshtein distance", "Damm algorithm", "Dancing Links", "Dantzig\u2013Wolfe decomposition", "Data Encryption Standard", "Data clustering", "Data mining", "Database", "Davis\u2013Putnam algorithm", "De Boor algorithm", "De Bruijn graph", "De Casteljau's algorithm", "Decision tree learning", "Dekker's algorithm", "Delaunay triangulation", "Delayed column generation", "Delta encoding", "Demon algorithm", "Depth-first search", "Deterministic automaton", "Deutsch-Jozsa algorithm", "Dice's coefficient", "Dictionary coder", "Difference-map algorithm", "Difference map algorithm", "Differential equation", "Differential evolution", "Diffie\u2013Hellman key exchange", "Digital Differential Analyzer (graphics algorithm)", "Digital Signature Algorithm", "Digital signal processing", "Dijkstra's algorithm", "Dijkstra-Scholten algorithm", "Dinic's algorithm", "Discrete Fourier transform", "Discrete Green's Theorem", "Discrete logarithm", "Disk scheduling", "Distributed algorithm", "Distributed systems", "Dithering", "Divide and conquer algorithm", "Division algorithm", "Dixon's algorithm", "Doomsday algorithm", "Double Metaphone", "Double dabble", "Duality (mathematics)", "Dynamic Markov compression", "Dynamic Programming", "Dynamic time warping", "Dynamical system", "ESC algorithm", "Earley parser", "Earliest deadline first scheduling", "Edge detection", "Edmonds's algorithm", "Edmonds\u2013Karp algorithm", "Eigenvalue algorithm", "ElGamal encryption", "Elementary function (differential algebra)", "Elevator algorithm", "Elias delta coding", "Elias gamma coding", "Elias omega coding", "Ellipsoid method", "Elliptic curve cryptography", "Embedded Zerotree Wavelet", "Entropy", "Entropy encoding", "Error detection and correction", "Error diffusion", "Espresso heuristic logic minimizer", "Estimation theory", "Euclidean algorithm", "Euclidean distance map", "Euclidean minimum spanning tree", "Euclidean shortest path problem", "Euler integration", "Euler method", "Evolution strategy", "Evolutionary computation", "Exact cover", "Expectation-maximization algorithm", "Exponential-Golomb coding", "Exponential backoff", "Exponential function", "Exponentiating by squaring", "Extended Euclidean algorithm", "Extrapolation", "FELICS", "FLAME clustering", "FNN algorithm", "Fair-share scheduling", "False position method", "Fast Cosine Transform", "Fast Fourier transform", "Fast clipping", "Fast folding algorithm", "Fast multipole method", "Fatigue (material)", "Faug\u00e8re F4 algorithm", "Fault-tolerant system", "Featherstone's algorithm", "Feature detection (computer vision)", "Feature space", "Fermat's factorization method", "Fermat primality test", "Fibonacci coding", "Fibonacci numbers", "Fibonacci search technique", "Finite difference method", "Fisher\u2013Yates shuffle", "Fitness proportionate selection", "Flashsort", "Fletcher's checksum", "Flood fill", "Flow network", "Floyd's cycle-finding algorithm", "Floyd\u2013Steinberg dithering", "Floyd\u2013Warshall algorithm", "Force-based algorithms (graph drawing)", "Ford\u2013Fulkerson algorithm", "Formal grammar", "Fortuna (PRNG)", "Fortune's Algorithm", "Forward-backward algorithm", "Forward error correction", "Fowler\u2013Noll\u2013Vo hash function", "Fractal compression", "Fractal dimension", "Freivalds' algorithm", "Fuzzy clustering", "F\u00fcrer's algorithm", "GLR parser", "Garbage collection (computer science)", "Gaussian elimination", "Gauss\u2013Jordan elimination", "Gauss\u2013Legendre algorithm", "Gauss\u2013Newton algorithm", "Gauss\u2013Seidel method", "Gene expression programming", "General Problem Solver", "General number field sieve", "Generalised Hough transform", "Genetic algorithms", "Geometric hashing", "Geoscience", "Gerchberg\u2013Saxton algorithm", "Gibbs sampling", "Gift wrapping algorithm", "Gilbert\u2013Johnson\u2013Keerthi distance algorithm", "Girvan\u2013Newman algorithm", "Global illumination", "Gnome sort", "Goertzel algorithm", "Golden section search", "Goldschmidt division", "Golomb coding", "Gordon\u2013Newell theorem", "Gosper's algorithm", "Gouraud shading", "Gradient descent", "Graham scan", "Gram\u2013Schmidt process", "Graph drawing", "Graph search algorithm", "Graph theory", "Gray code", "Greatest common divisor", "Greedy randomized adaptive search procedure", "Ground state", "Grover's algorithm", "GrowCut algorithm", "Gr\u00f6bner basis", "Half-toning", "Halley's method", "Hamming(7,4)", "Hamming code", "Hamming distance", "Hamming weight", "Harmony search", "Hash Function", "Hash join", "Hash tree (persistent data structure)", "Heap's algorithm", "Heapsort", "Hermite interpolation", "Heuristic", "Hidden Markov model", "Hidden markov model", "Hidden surface determination", "Hindley-Milner type inference", "Hirschberg's algorithm", "Histogram equalization", "Hopcroft\u2013Karp algorithm", "Hopfield net", "Hough transform", "Huang's algorithm", "Hubs and authorities", "Huffman coding", "Hungarian algorithm", "Hungarian method", "Hyperlink-Induced Topic Search", "ID3 algorithm", "Image-based lighting", "Image Compression", "Image compression", "Image processing", "Importance sampling", "Incremental encoding", "Incremental heuristic search", "Index calculus algorithm", "Information theory", "Insertion sort", "Inside-outside algorithm", "Integer factorization", "Integer linear programming", "Interior point method", "International Data Encryption Algorithm", "Interpolation", "Interpolation search", "Intersection algorithm", "Introsort", "Inverse iteration", "Isosurface", "Iterative deepening depth-first search", "Jaccard index", "Jacobi eigenvalue algorithm", "Jaro\u2013Winkler distance", "Johnson algorithm", "Join (SQL)", "Jump-and-Walk algorithm", "Jump point search", "Jump search", "K-means++", "K-means clustering", "K-medoids", "K-nearest neighbors", "Kabsch algorithm", "Kadane's algorithm", "Kahan summation algorithm", "Kalman filter", "Karatsuba algorithm", "Karger's algorithm", "Karmarkar's algorithm", "Karn's Algorithm", "Karplus-Strong string synthesis", "Keyed-hash message authentication code", "Kirkpatrick\u2013Seidel algorithm", "Knight's Tour", "Knuth\u2013Bendix completion algorithm", "Knuth\u2013Morris\u2013Pratt algorithm", "Kosaraju's algorithm", "Kruskal's algorithm", "LL parser", "LPBoost", "LR parser", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX", "Lagged Fibonacci generator", "Lagrange interpolation", "Lagrange polynomial", "Lamport's Bakery algorithm", "Lamport's Distributed Mutual Exclusion Algorithm", "Lamport ordering", "Lanczos iteration", "Lanczos resampling", "Laplacian smoothing", "Lax\u2013Wendroff method", "Least slack time scheduling", "Least squares", "Lempel\u2013Ziv", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Lenstra elliptic curve factorization", "Lesk algorithm", "Level set method", "Levenberg\u2013Marquardt algorithm", "Levenshtein coding", "Levenshtein distance", "Levinson recursion", "Lexical analysis", "Lexicographic breadth-first search", "Liang\u2013Barsky", "Library sort", "Linde\u2013Buzo\u2013Gray algorithm", "Line clipping", "Line drawing algorithm", "Line search", "Line segment intersection", "Linear-time", "Linear classifier", "Linear congruential generator", "Linear feedback shift register", "Linear interpolation", "Linear multistep method", "Linear predictive coding", "Linear programming", "Linear search", "Linear time", "List of algorithm general topics", "List of algorithms", "List of data structures", "List of machine learning algorithms", "List of numerical analysis topics", "List of terms relating to algorithms and data structures", "List scheduling", "Lloyd's algorithm", "Local search (optimization)", "Locality-sensitive hashing", "Logistic regression", "LogitBoost", "Long division", "Longest common subsequence problem", "Longest common substring problem", "Longest increasing subsequence problem", "Longest path problem", "Longitudinal redundancy check", "Look-ahead LR parser", "Lossless data compression", "Lowest common ancestor", "Lucas primality test", "Luhn algorithm", "Luhn mod N algorithm", "Lule\u00e5 algorithm", "MD5", "MISER algorithm", "Machine Learning", "Machine learning", "Maekawa's Algorithm", "Magnitude (mathematics)", "Manning Criteria", "Marching cubes", "Marching squares", "Marching tetrahedrons", "Marching triangles", "Mark-compact algorithm", "Mark and sweep", "Markov decision process", "Marr\u2013Hildreth algorithm", "Marzullo's algorithm", "Masaru Tomita", "Match Rating Approach", "Mathematical constant", "Mathematical optimization", "Matrix multiplication", "Matrix multiplication algorithm", "Maximal clique", "Maximum a posteriori", "Maximum cardinality matching", "Maximum flow", "Maximum flow problem", "Maximum likelihood", "Maximum parsimony (phylogenetics)", "Medical algorithms", "Medical imaging", "Medoid", "Memetic algorithm", "Merge algorithm", "Merge sort", "Mersenne twister", "Metaheuristic", "Metaphone", "Methods of computing square roots", "Metric space", "Metropolis light transport", "Metropolis\u2013Hastings algorithm", "Microcanonical ensemble", "Midpoint circle algorithm", "Miller\u2013Rabin primality test", "Min conflicts algorithm", "Minimax", "Minimum bounding box", "Minimum bounding box algorithms", "Minimum cut", "Minimum degree algorithm", "Minimum spanning tree", "Modular arithmetic", "Monotone cubic interpolation", "Monte Carlo method", "Monte Carlo simulation", "Montgomery reduction", "Mu-law algorithm", "Muller's method", "Multi level feedback queue", "Multigrid methods", "Multiplication algorithm", "Multiplicative inverse", "Multivariate division algorithm", "Multivariate interpolation", "Mutual exclusion", "N-body problem", "NIST", "NTRUEncrypt", "Nagle's algorithm", "Naimi-Trehel's log(n) Algorithm", "Natural language processing", "Nearest-neighbor interpolation", "Nearest neighbor search", "Nearest neighbour algorithm", "Needleman\u2013Wunsch algorithm", "Nelder\u2013Mead method", "Nested loop join", "Nested sampling algorithm", "Network congestion", "Network scheduler", "Network theory", "Neural network", "Neville's algorithm", "New York State Identification and Intelligence System", "Newell's algorithm", "Newton's method", "Newton's method in optimization", "Newton\u2013Raphson division", "Nicholl\u2013Lee\u2013Nicholl", "Non-deterministic algorithm", "Non-restoring division", "Nonblocking Minimal Spanning Switch", "Nondeterministic algorithm", "Nonlinear optimization", "Normal mapping", "Nth root algorithm", "Number theory", "Numerical analysis", "Numerical integration", "Numerical linear algebra", "OPTICS algorithm", "Odd-even sort", "Odds algorithm", "Odlyzko\u2013Sch\u00f6nhage algorithm", "One-attribute rule", "Online algorithm", "Operating systems", "Operator-precedence parser", "Optimal substructure", "Ordered dithering", "Ordered subset expectation maximization", "Overlapping subproblem", "PPM compression algorithm", "Package-merge algorithm", "Packrat parser", "PageRank", "Page replacement algorithms", "Painter's algorithm", "Pancake sorting", "Pareto distribution", "Pareto interpolation", "Parity bit", "Parsing", "Parsing expression grammar", "Partial differential equation", "Partial least squares regression", "Partial order", "Partial ordering", "Particle swarm optimization", "Path-based strong component algorithm", "Path tracing", "Patience sorting", "Paxos algorithm", "Pearson hashing", "Pell's equation", "Perceptron", "Perfect matching", "Permutation group", "Permutations", "Peterson's algorithm", "Peterson\u2013Gorenstein\u2013Zierler algorithm", "Petrick's method", "Phonetic algorithm", "Phong shading", "Photon mapping", "Pi", "Pigeonhole sort", "Pohlig\u2013Hellman algorithm", "Point cloud", "Point in polygon", "Point set registration", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Polygon triangulation", "Polynomial", "Polynomial interpolation", "Polynomial long division", "Polynomial time", "Positron emission tomography", "Postman sort", "Power iteration", "Powerset construction", "Pratt parser", "Prim's algorithm", "Primality test", "Prime-factor FFT algorithm", "Prime factorization algorithm", "Prime number", "Priority queue", "Probabilistic context-free grammar", "Probability distribution", "Process scheduler", "Process synchronization", "Programming language theory", "Pr\u00fcfer sequence", "Pseudorandom number generator", "Pulmonary embolism", "Pulse-coupled neural networks", "Push\u2013relabel algorithm", "Q-learning", "QR algorithm", "Quadratic sieve", "Quantum algorithm", "Quasitriangulation", "Queuing theory", "Quickhull", "Quicksort", "Quine\u2013McCluskey algorithm", "RANSAC", "RC4 (cipher)", "RIPEMD-160", "RMSD", "RSA (algorithm)", "RTR0", "Rabin\u2013Karp string search algorithm", "Rader's FFT algorithm", "Radial basis function network", "Radiosity (3D computer graphics)", "Radix sort", "Radon transform", "Rainflow-counting algorithm", "Ramer\u2013Douglas\u2013Peucker algorithm", "Random-restart hill climbing", "Random forest", "Random walker algorithm", "Range encoding", "Rate-monotonic scheduling", "Ray tracing (graphics)", "Rayleigh quotient iteration", "Raymond's Algorithm", "Recurrent neural network", "Recursive descent parser", "Redundancy check", "Reed\u2013Solomon error correction", "Reference counting", "Region growing", "Reinforcement Learning", "Relevance Vector Machine", "Restoring division", "Rete algorithm", "Reverse-delete algorithm", "Rewriting", "Ricart-Agrawala Algorithm", "Rice coding", "Richardson\u2013Lucy deconvolution", "Ridder's method", "Riemann zeta function", "Riemersma dithering", "Rijndael", "Risch algorithm", "Ritz method", "Root-finding algorithm", "Rotating calipers", "Round-robin scheduling", "Rounding functions", "Run-length encoding", "Runge's phenomenon", "Runge\u2013Kutta methods", "Ruppert's algorithm", "SEQUITUR algorithm", "SHA-1", "SHA-2", "SRT division", "SSS*", "SUBCLU", "SURF", "SURF (Speeded Up Robust Features)", "Samplesort", "Scale-invariant feature transform", "Scanline rendering", "Scheduling (computing)", "Schensted algorithm", "Schreier\u2013Sims algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Scoring algorithm", "Seam carving", "Secant method", "Secret sharing", "Seek time", "Segmentation (image processing)", "Selection algorithm", "Selection sort", "Self-organizing map", "Semi-space collector", "Sequence alignment", "Sequence assembly", "Sequences", "Set (mathematics)", "Set Partitioning in Hierarchical Trees", "Sethi-Ullman algorithm", "Shading", "Shamir's Secret Sharing", "Shamos\u2013Hoey algorithm", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Shellsort", "Shifting nth-root algorithm", "Shoelace algorithm", "Shor's algorithm", "Shortest common supersequence", "Shortest job next", "Shortest path problem", "Shortest remaining time", "Shortest seek first", "Shunting yard algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Signal processing", "Simon's algorithm", "Simple LR parser", "Simple precedence parser", "Simplex algorithm", "Simulated annealing", "Single-linkage clustering", "Single photon emission computed tomography", "Slerp", "Smith\u2013Waterman algorithm", "Smoothsort", "Snapshot algorithm", "Software engineering", "Sort-Merge Join", "Sorted list", "Sorting algorithm", "Sorting algorithms", "Sorting by signed reversals", "Soundex", "Spaghetti sort", "Sparse matrix", "Special functions", "Special number field sieve", "Spectral envelope", "Spectral layout", "Speech encoding", "Spigot algorithm", "Spline interpolation", "Stable marriage problem", "State-Action-Reward-State-Action", "State space search", "Statistical classification", "Steinhaus\u2013Johnson\u2013Trotter algorithm", "Stemming", "Stochastic tunneling", "Stochastic universal sampling", "Stone's method", "Stooge sort", "Strand sort", "Strassen algorithm", "Stress (physics)", "String metrics", "Strong generating set", "Strongly connected components", "Strongly polynomial", "Structured SVM", "Sublinear", "Subsequence", "Subset sum problem", "Substring", "Substring search", "Successive over-relaxation", "Suffix tree", "Sukhotin's algorithm", "Summed area table", "Supervised learning", "Support Vector Machines", "Sutherland\u2013Hodgman", "Swarm intelligence", "Sweep and prune", "Sweep line algorithm", "Symbolic Cholesky decomposition", "Symmetric key algorithm", "Symmetric matrices", "System of linear equations", "Tabu search", "Tarjan's off-line least common ancestors algorithm", "Tarjan's strongly connected components algorithm", "Tarski\u2013Kuratowski algorithm", "Telephone exchange", "Temporal difference learning", "Ternary search", "Texas Medication Algorithm Project", "Theory of computation", "Threefish", "Tiger (hash)", "Timsort", "Tiny Encryption Algorithm", "Todd\u2013Coxeter algorithm", "Toeplitz matrix", "Tomasulo algorithm", "Toom\u2013Cook multiplication", "Top-down parsing", "Top-nodes algorithm", "Topics in cryptography", "Topological sorting", "Tournament selection", "Transaction (database)", "Transform coding", "Transitive closure", "Transposition table", "Trapezoidal rule (differential equations)", "Traveling salesman problem", "Tree sort", "Tree traversal", "Trial division", "Triangulation (geometry)", "Tricubic interpolation", "Tridiagonal matrix algorithm", "Trie", "Trigonometric interpolation", "Trigram search", "Truncated binary encoding", "Truncated binary exponential backoff", "Truncation selection", "TrustRank", "Twofish", "UPGMA", "Ukkonen's algorithm", "Unary coding", "Unicode Collation Algorithm", "Uniform-cost search", "Uniform binary search", "Universal code (data compression)", "VEGAS algorithm", "Variational method", "Vatti clipping algorithm", "Vector clocks", "Vector optimization", "Vector quantization", "Velvet (algorithm)", "Verhoeff algorithm", "Verlet integration", "Video compression", "Vincenty's formulae", "Visual cortex", "Viterbi algorithm", "Voronoi diagram", "WHIRLPOOL", "Wang and Landau algorithm", "Ward's method", "Warnock algorithm", "Warped Linear Predictive Coding", "Watershed (algorithm)", "Wavelet compression", "Weiler\u2013Atherton", "Winnow algorithm", "X-ray", "X-ray crystallography", "Xiaolin Wu's line algorithm", "Xor swap algorithm", "Yamartino method", "Yarrow algorithm", "Young tableaux", "Zeller's congruence", "Zhu\u2013Takaoka string matching algorithm", "Ziggurat algorithm", "Zobrist hashing"], "categories": ["Algorithms", "All articles needing additional references", "Articles contradicting other articles", "Articles needing additional references from April 2014", "Mathematics-related lists"], "title": "List of algorithms"}
{"summary": "An adaptive algorithm is an algorithm that changes its behavior based on information available at the time it is run. This might be information about computational resources available, or the history of data recently received.\nFor example, stable partition, using no additional memory is O(n lg n) but given O(n) memory, it can be O(n) in time. As implemented by the C++ Standard Library, stable_partition is adaptive and so it acquires as much memory as it can get (up to what it would need at most) and applies the algorithm using that available memory. Another example is adaptive sort, whose behaviour changes upon the presortedness of its input.\nAn example of an adaptive algorithm in radar systems is the constant false alarm rate (CFAR) detector.\nIn machine learning and optimization, many algorithms are adaptive or have adaptive variants, which usually means that the algorithm parameters are automatically adjusted according to statistics about the optimisation thus far (e.g. the rate of convergence). Examples include adaptive simulated annealing, adaptive coordinate descent, AdaBoost, and adaptive quadrature.\nIn data compression, adaptive coding algorithms such as Adaptive Huffman coding or Prediction by partial matching can take a stream of data as input, and adapt their compression technique based on the symbols that they have already encountered.\nIn signal processing, the Adaptive Transform Acoustic Coding (ATRAC) codec used in MiniDisc recorders is called \"adaptive\" because the window length (the size of an audio \"chunk\") can change according to the nature of the sound being compressed, to try and achieve the best-sounding compression strategy.", "links": ["AdaBoost", "Adaptive Huffman coding", "Adaptive Transform Acoustic Coding", "Adaptive coding", "Adaptive coordinate descent", "Adaptive filter", "Adaptive quadrature", "Adaptive simulated annealing", "Adaptive sort", "Algorithm", "C++ Standard Library", "Constant false alarm rate", "Data compression", "Machine learning", "MiniDisc", "Optimization", "Prediction by partial matching", "Radar", "Signal processing", "Software engineering", "Stable partition"], "categories": ["Algorithms", "All stub articles", "Software engineering stubs"], "title": "Adaptive algorithm"}
{"summary": "Adaptive dimensional search algorithms differ from nature-inspired metaheuristic techniques in the sense that they do not use any metaphor as an underlying principle for implementation. Rather, they utilize a simple, performance-oriented methodology based on the update of the search dimensionality ratio (SDR) parameter at each iteration.\nMany robust metaheuristic techniques, such as simulated annealing, evolutionary algorithms, particle swarm optimization, and ant colony optimization, have been introduced by researchers in the last few decades through clearly identifying and formulating similarities between algorithms and the processes they are modeled on. However, over time this trend of developing new search methods has made researchers feel obligated to associate their innovative ideas with some natural event to provide a basis for justification of their thoughts and the originality of their algorithms. As a result, literature has abounded with metaheuristic algorithms that have weak or no similarities to the natural processes which they are purported to derive from.", "links": ["Algorithm", "Ant colony optimization algorithms", "Evolutionary algorithm", "Metaheuristic", "Metaphor", "Methodology", "Nature", "Particle swarm optimization", "Simulated annealing"], "categories": ["Algorithms", "All articles needing expert attention", "All articles that are too technical", "Articles needing expert attention from October 2015", "Computer science", "Mathematical optimization", "Optimization algorithms and methods", "Wikipedia articles that are too technical from October 2015"], "title": "Adaptive dimensional search"}
{"summary": "The Algorithm Auction is the world\u2019s first auction of computer algorithms. Created by Ruse Laboratories, the initial auction featured seven lots and was held at the Cooper Hewitt, Smithsonian Design Museum on March 27, 2015.\nFive lots were physical representations of famous code or algorithms, including a signed, handwritten copy of the original Hello, World! C program by its creator Brian Kernighan on dot-matrix printer paper, a printed copy of 5,000 lines of Assembly code comprising the earliest known version of Turtle Graphics, signed by its creator Hal Abelson, a necktie containing the six-line qrpff algorithm capable of decrypting content on a commercially produced DVD video disc, and a pair of drawings representing OKCupid\u2019s original Compatibility Calculation algorithm, signed by the company founders. The qrpff lot sold for $2,500.\nTwo other lots were \u201cliving algorithms,\u201d including a set of JavaScript tools for building applications that are accessible to the visually impaired and the other is for a program that converts lines of software code into music. Winning bidders received, along with artifacts related to the algorithms, a full intellectual property license to use, modify, or open-source the code. All lots were sold, with Hello World receiving the most bids.\nExhibited alongside the auction lots were a facsimile of the Plimpton 322 tablet on loan from Columbia University, and Nigella, an art-world facing computer virus named after Nigella Lawson and created by cypherpunk and hacktavist Richard Jones.\nSebastian Chan, Director of Digital & Emerging Media at the Cooper\u2013Hewitt, attended the event remotely from Milan, Italy via a Beam Pro telepresence robot.", "links": ["Assembly language", "Brian Kernighan", "C (programming language)", "Columbia University", "Computer algorithms", "Cooper Hewitt, Smithsonian Design Museum", "DVD", "Hal Abelson", "Hello, World!", "Intellectual property", "JavaScript", "Milan, Italy", "Museum of Modern Art", "Nigella Lawson", "OKCupid", "Plimpton 322", "Qrpff", "Robot", "Turtle Graphics", "Visually impaired"], "categories": ["2015 in computer science", "Algorithms", "Visual arts"], "title": "The Algorithm Auction"}
{"summary": "Algorithm characterizations are attempts to formalize the word algorithm. Algorithm does not have a generally accepted formal definition. Researchers are actively working on this problem. This article will present some of the \"characterizations\" of the notion of \"algorithm\" in more detail.\nThis article is a supplement to the article Algorithm.\n^ cf [164] Andreas Blass and Yuri Gurevich \"Algorithms: A Quest for Absolute Definitions\" Bulletin of the European Association for Theoretical Computer Science Number 81 (October 2003), pages 195\u2013225. Reprinted in Chapter on Logic in Computer Science Current Trends in Theoretical Computer Science World Scientific, 2004, pages 283\u2013311 Reprinted in Church's Thesis After 70 Years Ontos Verlag, 2006, 24\u201357}, or http://math.ucsd.edu/~sbuss/ResearchWeb/FutureOfLogic/paper.pdf (cited in a 2007 Dershowitz\u2013Gurevich paper): Samual R. Buss, Alexander S. Kechris, Anand Pillay, and Richard A. Shore, \u201cThe Prospects for Mathematical Logic in the Twenty-first Century\u201d.", "links": ["A. A. Markov", "Abstract machine", "Ackermann function", "Alan Turing", "Alexander S. Kechris", "Algorithm", "Algorithm examples", "Alonzo Church", "Andreas Blass", "Boolean equation", "C preprocessor", "Cantor's diagonal argument", "Chinese room", "Chomsky hierarchy", "Christos H. Papadimitriou", "Church's thesis", "Church Thesis", "Church\u2013Turing Thesis", "Complexity class", "Computable function", "Computational theory of mind", "Computer", "Constructive mathematics", "Containment hierarchy", "Counter machine", "Counter machine models", "Daniel Dennett", "Darwin's Dangerous Idea", "Data flow diagram", "David Berlinski", "Digital object identifier", "Donald Knuth", "Dynamic semantics", "Emil Post", "Euclidean algorithm", "Formal grammar", "Formal language", "Formal system", "George Boolos", "Greatest common divisor", "G\u00f6del", "Harry R. Lewis", "Hartley Rogers, Jr", "Ian Stewart (mathematician)", "International Standard Book Number", "Intrinsic", "Intuitionism", "J. Barkley Rosser", "JSTOR", "Jan van Leeuwen", "John P. Burgess", "John Searle", "John Venn", "Kleene", "Kurt G\u00f6del", "Lambda calculus", "Logical form", "M4 (computer language)", "MIX", "Machine language", "Martin Davis", "Marvin Minsky", "Michael Sipser", "Mind", "Mu-operator", "Mu recursive function", "Natural number", "Peter van Emde Boas", "Pointer machine", "Post\u2013Turing machine", "Primitive recursive", "Primitive recursive function", "Primitive recursive functions", "Programming language", "Random Access Machine", "Random access machine", "Random access stored program machine", "Recursion", "Register machine", "Richard Jeffrey", "Robert Soare", "Robin Gandy", "R\u00f3zsa P\u00e9ter", "Semantic", "Sic", "State diagram", "Stephen Kleene", "Super-exponentiation", "Syllogism", "Syntax", "Taxonomy (general)", "Tree diagram", "Turing", "Turing's Thesis", "Turing complete", "Turing machine", "Turing machine equivalents", "Turing machines", "Universal Turing machine", "W. Stanley Jevons", "Wilhelm Ackermann", "Yuri Gurevich"], "categories": ["Algorithms", "All articles to be expanded", "All articles with empty sections", "All articles with specifically marked weasel-worded phrases", "Articles to be expanded from January 2014", "Articles to be expanded from June 2008", "Articles with empty sections from January 2014", "Articles with specifically marked weasel-worded phrases from February 2013", "Computability theory", "Formal methods", "Models of computation", "Pages using duplicate arguments in template calls"], "title": "Algorithm characterizations"}
{"summary": "Algorithm design is a specific method to create a mathematical process in solving problems. Applied algorithm design is algorithm engineering.\nAlgorithm design is identified and incorporated into many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are algorithm design patterns, such as template method pattern and decorator pattern, and uses of data structures, and name and sort lists. Some current day uses of algorithm design can be found in internet retrieval processes of web crawling, packet routing and caching.\nMainframe programming languages such as ALGOL (for Algorithmic language), FORTRAN, COBOL, PL/I, SAIL, and SNOBOL are computing tools to implement an \"algorithm design\"... but, an \"algorithm design\" (a/d) is not a language. An a/d can be a hand written process, e.g. set of equations, a series of mechanical processes done by hand, an analog piece of equipment, or a digital process and/or processor.\nOne of the most important aspects of algorithm design is creating an algorithm that has an efficient runtime, also known as its big Oh.\nSteps in development of Algorithms\nProblem definition\nDevelopment of a model\nSpecification of Algorithm\nDesigning an Algorithm\nChecking the correctness of Algorithm\nAnalysis of Algorithm\nImplementation of Algorithm\nProgram testing\nDocumentation Preparation", "links": ["ACM Computing Classification System", "AIGA", "ALGOL", "Activity-centered design", "Advertising", "Aesthetics", "Algorithm", "Algorithm engineering", "Analysis of algorithms", "Application security", "Applied arts", "Architect-led design\u2013build", "Architectural lighting design", "Architectural model", "Architecture", "Artificial intelligence", "Automata theory", "Automated planning and scheduling", "Automotive design", "Back tracking", "Big Oh", "Blueprint", "Book design", "Building design", "C-K theory", "COBOL", "Ceramic art", "Chartered Society of Designers", "Co-design", "Communication design", "Community design", "Compiler construction", "Comprehensive layout", "Computational biology", "Computational chemistry", "Computational complexity theory", "Computational engineering", "Computational geometry", "Computational mathematics", "Computational physics", "Computational social science", "Computer-aided design", "Computer accessibility", "Computer animation", "Computer architecture", "Computer data storage", "Computer graphics", "Computer hardware", "Computer network", "Computer programming", "Computer science", "Computer security", "Computer security compromised by hardware failure", "Computer vision", "Computing platform", "Concept art", "Concurrency (computer science)", "Concurrent computing", "Configuration design", "Contextual design", "Control theory", "Corporate design", "Costume design", "Cradle-to-cradle design", "Creative industries", "Creative problem-solving", "Creativity techniques", "Critical design", "Cross-validation (statistics)", "Cryptography", "Cultural icon", "Cyberwarfare", "Data mining", "Database design", "Database management system", "Decision support system", "Decorator pattern", "Dependability", "Design", "Design Council", "Design Research Society", "Design and Industries Association", "Design around", "Design brief", "Design by committee", "Design by contract", "Design closure", "Design controls", "Design director", "Design education", "Design elements and principles", "Design engineer", "Design firm", "Design flow (EDA)", "Design for Six Sigma", "Design for X", "Design for assembly", "Design for manufacturability", "Design for testing", "Design history", "Design knowledge", "Design language", "Design leadership", "Design life", "Design load", "Design management", "Design methods", "Design museum", "Design of experiments", "Design paradigm", "Design patent", "Design pattern", "Design rationale", "Design research", "Design review", "Design science", "Design specification", "Design strategy", "Design studio", "Design technology", "Design thinking", "Design tool", "Designer", "Design\u2013bid\u2013build", "Design\u2013build", "Digital art", "Digital library", "Digital marketing", "Discrete mathematics", "Distributed artificial intelligence", "Distributed computing", "Divide and conquer algorithm", "Document management system", "Domain-specific language", "Drug design", "Dynamic programming", "E-commerce", "Ecodesign", "Ecological design", "Educational technology", "Electrical system design", "Electronic design automation", "Electronic publishing", "Electronic voting", "Embedded system", "Empathic design", "Engineering", "Engineering design process", "Enterprise architecture", "Enterprise information system", "Enterprise software", "Environmental design", "Environmental graphic design", "Environmental impact design", "Error-tolerant design", "European Design Award", "Exhibit design", "Experience design", "FORTRAN", "Fashion design", "Fault-tolerant design", "Floral design", "Flowchart", "Formal language", "Formal methods", "Functional design", "Furniture", "Futures studies", "Game art design", "Game design", "Garden design", "Geographic information system", "German Design Award", "Geschmacksmuster", "Glass art", "Good Design Award (Chicago)", "Good Design Award (Japan)", "Graphex", "Graphic design", "Graphics processing unit", "Greedy algorithm", "Green computing", "Health informatics", "Human\u2013computer interaction", "IF product design award", "Icon design", "Illustration", "Image compression", "Immersive design", "Industrial design", "Industrial design right", "Industrial design rights in the European Union", "Information design", "Information retrieval", "Information security", "Information system", "Information theory", "Instructional design", "Integrated circuit", "Integrated circuit design", "Integrated development environment", "Intelligent design", "Interaction design", "Interior architecture", "Interior design", "International Forum Design", "International Standard Book Number", "Interpreter (computing)", "Intrusion detection system", "James Dyson Award", "Jewelry design", "Job design", "KISS principle", "Keyline design", "Knowledge representation and reasoning", "Landscape architecture", "Landscape design", "Level design", "Library (computing)", "Lighting designer", "List of machine learning concepts", "Logic in computer science", "Machine learning", "Mathematical analysis", "Mathematical optimization", "Mathematical software", "Metadesign", "Michael T. Goodrich", "Middleware", "Mind map", "Mixed reality", "Mockup", "Model of computation", "Modeling language", "Modular design", "Motion graphic design", "Multi-task learning", "Multimedia database", "Multiprocessing", "Multithreading (computer architecture)", "Natural language processing", "Network architecture", "Network performance", "Network protocol", "Network scheduler", "Network security", "Network service", "Networking hardware", "New product development", "News design", "Nuclear weapon design", "Nucleic acid design", "Numerical analysis", "Open-source software", "Open design", "Operating system", "Operation research", "Operations research", "Organization design", "Outline of design", "Packaging and labeling", "Parallel computing", "Parametric design", "Participatory design", "Passive solar building design", "Peripheral", "Philosophy of artificial intelligence", "Philosophy of design", "Photo manipulation", "Photography", "Platform-based design", "Prince Philip Designers Prize", "Printed circuit board", "Probabilistic design", "Probability", "Process control", "Process design", "Process simulation", "Product design", "Product design specification", "Production design", "Programming language", "Programming language theory", "Programming paradigm", "Programming team", "Programming tool", "Protein design", "Prototype", "Public art", "Randomized algorithm", "Rational design", "Real-time computing", "Regenerative design", "Reinforcement learning", "Reliability engineering", "Rendering (computer graphics)", "Requirements analysis", "Research design", "Retail design", "Roberto Tamassia", "SAIL programming language", "STEAM fields", "Safe-life design", "Scenic design", "Security service (telecommunication)", "Semantics (computer science)", "Sensory design", "Service design", "Signage", "Sketch (drawing)", "Slow design", "Social computing", "Social design", "Social software", "Software configuration management", "Software design", "Software development", "Software development process", "Software framework", "Software maintenance", "Software quality", "Software repository", "Software verification and validation", "Solid modeling", "Sonic interaction design", "Sound design", "Statistics", "Steven Skiena", "Storyboard", "Supervised learning", "Sustainable design", "Sustainable graphic design", "Sustainable landscape architecture", "Systems design", "TRIZ", "Technical drawing", "Template method pattern", "Textile design", "The Design Society", "Theory of computation", "Theory of constraints", "Tools for Ideas", "Top-down and bottom-up design", "Traffic sign design", "Transformation design", "Transgenerational design", "Typography", "Ubiquitous computing", "Universal design", "Unsupervised learning", "Urban design", "Usage-centered design", "Use-centered design", "User-centered design", "User experience design", "User innovation", "User interface design", "Very-large-scale integration", "Video design", "Video game", "Video game design", "Virtual machine", "Virtual reality", "Visual merchandising", "Visualization (computer graphics)", "Web design", "Website wireframe", "Wicked problem", "Word processor", "World Wide Web"], "categories": ["Algorithms", "All stub articles", "Mathematical analysis stubs", "Operations research"], "title": "Algorithm design"}
{"summary": "Algorithm Engineering focuses on the design, analysis, implementation, optimization, profiling and experimental evaluation of computer algorithms, bridging the gap between algorithm theory and practical applications of algorithms in software engineering. It is a general methodology for algorithmic research.", "links": ["Algorithms", "Center for Discrete Mathematics and Theoretical Computer Science", "DIMACS", "Experimental algorithmics", "Library of Efficient Data types and Algorithms", "National Science Foundation", "Peter Sanders (computer scientist)", "Rutgers University", "SIGACT", "Society for Industrial and Applied Mathematics", "Software engineering"], "categories": ["Algorithms", "Theoretical computer science"], "title": "Algorithm engineering"}
{"summary": "Algorithmic logic is a calculus of programs which allows the expression of semantical properties of programs by appropriate logical formulas. It provides a framework that enables proving the formulas from the axioms of program constructs such as assignment, iteration and composition instructions and from the axioms of the data structures in question see Mirkowska & Salwicki (1987), Banachowski et al. (1977).\nThe following diagram helps to locate algorithmic logic among other logics. \nThe formalized language of algorithmic logic (and of algorithmic theories of various data structures) contains three types of well formed expressions: terms - i.e. expressions denoting operations on elements of data structures, formulas - i.e. expressions denoting the relations among elements of data structures, programs - i.e. algorithms - these expressions describe the computations. For semantics of terms and formulas consult pages on first order logic and Tarski's semantic. The meaning of a program  is the set of possible computations of the program.\n\nAlgorithmic logic is one of many logics of programs. Another logic of programs is dynamic logic, see dynamic logic, Harel, Kozen & Tiuryn (2000).", "links": ["Dynamic logic (modal logic)", "First-order logic", "International Standard Book Number", "Mathematical logic"], "categories": ["Algorithms", "All articles covered by WikiProject Wikify", "All articles with too few wikilinks", "All orphaned articles", "All stub articles", "Articles covered by WikiProject Wikify from June 2015", "Articles with too few wikilinks from June 2015", "Mathematical logic stubs", "Orphaned articles from June 2015", "Theoretical computer science"], "title": "Algorithmic logic"}
{"summary": "Algorithmics is the science of algorithms. It includes algorithm design, the art of building a procedure which can solve efficiently a specific problem or a class of problem, algorithmic complexity theory, the study of estimating the hardness of problems by studying the properties of algorithm that solves them, or algorithm analysis, the science of studying the properties of a problem, such as quantifying resources in time and memory space needed by this algorithm to solve this problem. Algorithmic trading encompasses trading systems that are heavily reliant on complex mathematical formulas and high-speed, computer programs to determine trading strategies.", "links": ["ACM Computing Classification System", "Algorithm", "Algorithm analysis", "Algorithm design", "Algorithmic complexity theory", "Algorithmic trading", "Analysis of algorithms", "Application security", "Artificial intelligence", "Automata theory", "Automated planning and scheduling", "Compiler construction", "Computational biology", "Computational chemistry", "Computational complexity theory", "Computational engineering", "Computational geometry", "Computational mathematics", "Computational physics", "Computational social science", "Computer accessibility", "Computer animation", "Computer architecture", "Computer data storage", "Computer graphics", "Computer hardware", "Computer network", "Computer programming", "Computer science", "Computer security", "Computer security compromised by hardware failure", "Computer vision", "Computing platform", "Concurrency (computer science)", "Concurrent computing", "Control theory", "Cross-validation (statistics)", "Cryptography", "Cyberwarfare", "Data mining", "Database management system", "Decision support system", "Dependability", "Digital art", "Digital library", "Digital marketing", "Discrete mathematics", "Distributed artificial intelligence", "Distributed computing", "Document management system", "Domain-specific language", "E-commerce", "Educational technology", "Electronic design automation", "Electronic publishing", "Electronic voting", "Embedded system", "Enterprise information system", "Enterprise software", "Formal language", "Formal methods", "Geographic information system", "Graphics processing unit", "Green computing", "Health informatics", "Human\u2013computer interaction", "Image compression", "Information retrieval", "Information security", "Information system", "Information theory", "Integrated circuit", "Integrated development environment", "Interaction design", "Interpreter (computing)", "Intrusion detection system", "Knowledge representation and reasoning", "Library (computing)", "List of machine learning concepts", "Logic in computer science", "Machine learning", "Mathematical analysis", "Mathematical optimization", "Mathematical software", "Middleware", "Mixed reality", "Model of computation", "Modeling language", "Multi-task learning", "Multimedia database", "Multiprocessing", "Multithreading (computer architecture)", "Natural language processing", "Network architecture", "Network performance", "Network protocol", "Network scheduler", "Network security", "Network service", "Networking hardware", "Numerical analysis", "Open-source software", "Operating system", "Operations research", "Parallel computing", "Peripheral", "Philosophy of artificial intelligence", "Photo manipulation", "Printed circuit board", "Probability", "Process control", "Programming language", "Programming language theory", "Programming paradigm", "Programming team", "Programming tool", "Randomized algorithm", "Real-time computing", "Reinforcement learning", "Rendering (computer graphics)", "Requirements analysis", "Security service (telecommunication)", "Semantics (computer science)", "Social computing", "Social software", "Software configuration management", "Software design", "Software development", "Software development process", "Software framework", "Software maintenance", "Software quality", "Software repository", "Software verification and validation", "Solid modeling", "Statistics", "Supervised learning", "Theory of computation", "Ubiquitous computing", "Unsupervised learning", "Very-large-scale integration", "Video game", "Virtual machine", "Virtual reality", "Visualization (computer graphics)", "Word processor", "World Wide Web"], "categories": ["Algorithms"], "title": "Algorithmics"}
{"summary": "The Art of Computer Programming (sometimes known by its initials TAOCP) is a comprehensive monograph written by Donald Knuth that covers many kinds of programming algorithms and their analysis.\nKnuth began the project, originally conceived as a single book with twelve chapters, in 1962. The first three of what was then expected to be a seven-volume set were published in 1968, 1969, and 1973. The first installment of Volume 4 (a paperback fascicle) was published in 2005. The hardback volume 4A was published in 2011. Additional fascicle installments are planned for release approximately biannually.", "links": ["-yllion", "AMS Euler", "Addison-Wesley", "Algorithm", "American Scientist", "Analysis of algorithms", "Approximation algorithm", "Arbitrary-precision arithmetic", "Arithmetic", "Assembly language", "Asymptotic analysis", "Big O notation", "Bill Gates", "Binary decision diagram", "Binary numeral system", "Binomial coefficient", "Bitwise operation", "Boolean function", "Branch-and-bound", "CWEB", "Caltech", "Case Institute of Technology", "Case Western Reserve University", "Charles Babbage Institute", "Combination", "Combinatorics", "Compiler", "Compilers", "Computer Modern", "Computer programming", "Computers and Typesetting", "Concrete Mathematics", "Concrete Roman", "Context-free language", "Coroutine", "Dancing Links", "Data compression", "Data structure", "Decimal", "Dennis Shasha", "Digital object identifier", "Dijkstra's algorithm", "Donald Knuth", "Dynamic memory allocation", "Dynamic programming", "Euler's summation formula", "External Sorting", "Factorial", "Fascicle (book)", "Fibonacci number", "Fisher\u2013Yates shuffle", "Floating point", "Font", "GNU MDK", "GNU MIX Development Kit", "Generating function", "Gerald L. Alexanderson", "Hardcover", "Harmonic number", "Hash table", "Hexadecimal", "Hot metal typesetting", "IBM 650", "Internal sort", "International Standard Book Number", "Introduction to Algorithms", "John Todd (computer scientist)", "Knuth's Algorithm X", "Knuth's Simpath algorithm", "Knuth Prize", "Knuth reward check", "Knuth\u2013Bendix completion algorithm", "Knuth\u2013Morris\u2013Pratt algorithm", "Lexical analysis", "Literate programming", "METAFONT", "MIX", "MMIX", "Man or boy test", "Mathematical induction", "Matroid", "Monograph", "NP-hard", "Non-fiction", "Olga Taussky-Todd", "Parsing", "Partition (number theory)", "Partition of a set", "Permutation", "Philip Morrison", "Polynomial", "Positional notation", "Potrzebie", "Power Series", "Pseudo-random number generator", "Pseudo-random number sampling", "Quater-imaginary base", "RISC", "Radix", "Random Sequence", "Rational number", "Recursion", "Richard S. Varga", "Robert Floyd", "Robinson\u2013Schensted\u2013Knuth correspondence", "Search algorithm", "Search algorithms", "Selected papers series of Knuth", "Software", "Sorting algorithm", "Statistical randomness", "String searching algorithm", "Structured programming", "Surreal Numbers (book)", "TeX", "The Complexity of Songs", "The New York Times", "Things a Computer Scientist Rarely Talks About", "Trabb Pardo\u2013Knuth algorithm", "Tree (data structure)", "Tree (graph theory)", "Tuple", "Two-element Boolean algebra", "Typesetting", "Unique key", "WEB"], "categories": ["1968 books", "1969 books", "1973 books", "1981 books", "2011 books", "Addison-Wesley books", "Algorithms", "All articles containing potentially dated statements", "All articles with unsourced statements", "Analysis of algorithms", "Articles containing potentially dated statements from 2011", "Articles with inconsistent citation formats", "Articles with unsourced statements from June 2012", "Books by Donald Knuth", "Computer programming books", "Computer science books", "Monographs"], "title": "The Art of Computer Programming"}
{"summary": "AVT Statistical filtering algorithm is an approach to improving quality of raw data collected from various sources. One way to improve signal/noise ratio is to implement filtering to separate useful signal from noise. In ideal situation the useful signal has different frequency then noise and noise is separated/filtered out by frequency discrimination using various filters. Frequency discrimination filtering is done using Low Pass, High Pass and Band Pass filtering which refers to relative frequency filtering criteria target for such configuration. Filters are created using passive and active components and sometimes are implemented using software algorithms based on FFT.\nSometimes signal frequency coincides with noise frequency in real life. In this situations frequency discrimination filtering does not work since the noise and useful signal are indistinguishable. To achieve filtering in such conditions there are several algorithms available which is described below in more detail.", "links": ["Band-pass filter", "Digital filter", "High-pass filter", "Kepler (spacecraft)", "Low-pass filter", "Plot (graphics)", "Search for extraterrestrial intelligence", "Smoothness"], "categories": ["Algorithms"], "title": "AVT Statistical filtering algorithm"}
{"summary": "The Boyer-Moore Vote Algorithm solves the majority vote problem in linear time [O(n)] and constant memory [O(1)]. The majority vote problem is to determine in any given sequence of choices whether there is a choice with more occurrences than all the others, and if so, to determine this choice. Mathematically, given a finite sequence (length n) of numbers, the object is to find the majority number defined as the number that appears more than \u230a n/2 \u230b times.", "links": ["Algorithm", "Candidate", "Choice", "Counter (digital)", "Element (mathematics)", "Increment and decrement operators", "Iteration", "Linear time", "Majority", "Mathematics", "Matrix (mathematics)", "Sequence"], "categories": ["Algorithms", "All orphaned articles", "Orphaned articles from December 2014"], "title": "Boyer\u2013Moore majority vote algorithm"}
{"summary": "The British Museum algorithm is a general approach to find a solution by checking all possibilities one by one, beginning with the smallest. The term refers to a conceptual, not a practical, technique where the number of possibilities is enormous.\nFor instance, one may, in theory, find the smallest program that solves a particular problem in the following way: Generate all possible source codes of length one character. Check each one to see if it solves the problem. (Note: the halting problem makes this check troublesome.) If not, generate and check all programs of two characters, three characters, etc. Conceptually, this finds the smallest program, but in practice it tends to take an unacceptable amount of time (more than the lifetime of the universe, in many instances).\nSimilar arguments can be made to show that optimizations, theorem proving, language recognition, etc. are possible or impossible.\nNewell, Shaw, and Simon called this procedure the British Museum algorithm\n\"... since it seemed to them as sensible as placing monkeys in front of typewriters in order to reproduce all the books in the British Museum.\"", "links": ["A* search algorithm", "Allen Newell", "Alpha\u2013beta pruning", "American Psychological Association", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum", "Brute-force search", "Cliff Shaw", "D*", "Depth-first search", "Depth-limited search", "Dictionary of Algorithms and Data Structures", "Digital object identifier", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph traversal", "Halting problem", "Herbert A. Simon", "Hill climbing", "Infinite monkey theorem", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "National Institute of Standards and Technology", "Prim's algorithm", "Psychological Review", "SMA*", "Search game", "Tree traversal"], "categories": ["Algorithms", "All articles with unsourced statements", "Articles with unsourced statements from August 2013", "British Museum", "Use British English from August 2015", "Use dmy dates from August 2015"], "title": "British Museum algorithm"}
{"summary": "K. Mani Chandy, Jayadev Misra and Laura M Haas devise Chandy Misra Haas algorithm for Resource model. It checks whether there is any deadlock in a distributed system or not.", "links": ["Computational complexity theory", "Deadlock", "Digital object identifier", "Distributed system", "Linear time"], "categories": ["Algorithms"], "title": "Chandy-Misra-Haas Algorithm:Resource Model"}
{"summary": "Chinese Whispers is a clustering method used in network science named after the famous whispering game. Clustering methods are basically used to identify communities of nodes or links in a given network. This algorithm was designed by Chris Biemann and Sven Teresinak in 2005. The name comes from the fact that the process can be modeled as a separation of communities where the nodes send the same type of information to each other.\nChinese Whispers is a hard partitioning, randomized, flat clustering (no hierarchical relations between clusters) method. The random property means that running the process on the same network several times can lead to different results, while because of hard partitioning one node can only belong to one cluster at a given moment. The original algorithm is applicable to undirected, weighted and unweighted graphs. Chinese Whispers is time linear which means that it is extremely fast even if the number of nodes and links are very high in the network.", "links": ["Chinese Whispers", "Chris Biemann", "Gephi", "Hierarchical clustering", "Natural Language Processing", "Open source", "Small-world experiment", "Sven Teresinak"], "categories": ["Algorithms", "All orphaned articles", "Orphaned articles from June 2015"], "title": "Chinese Whispers (clustering method)"}
{"summary": "Collaborative Diffusion is a type of pathfinding algorithm which uses the concept of antiobjects, objects within a computer program that function opposite to what would be conventionally expected. Collaborative Diffusion is typically used in video games, when multiple agents must path towards a single target agent. For example, the ghosts in Pac-Man. In this case, the background tiles serve as antiobjects, carrying out the necessary calculations for creating a path and having the foreground objects react accordingly, whereas having foreground objects be responsible for their own pathing would be conventionally expected.\nCollaborative Diffusion is favored for its efficiency over other pathfinding algorithms, such as A*, when handling multiple agents. Also, this method allows elements of competition and teamwork to easily be incorporated between tracking agents. Notably, the time taken to calculate paths remains constant as the number of agents increases.", "links": ["A* search algorithm", "Pac-Man", "Pathfinding"], "categories": ["Algorithms"], "title": "Collaborative diffusion"}
{"summary": "The Design Analysis Kit for Optimization and Terascale Applications (DAKOTA) is a software toolkit developed by engineers at Sandia National Laboratories to provide a flexible, extensible interface between analysis codes and iterative systems analysis methods. DAKOTA contains optimization algorithms using gradient and nongradient-based methods, parameter estimation with nonlinear least squares methods, uncertainty quantification with sampling, reliability, and stochastic finite element methods, and sensitivity/variance analysis with design of experiments and parameter study capabilities.", "links": ["Algorithm", "Dakota (disambiguation)", "Finite element method", "Gradient", "Iterative", "Least squares", "Nonlinear", "Programming tool", "Sandia National Laboratories", "Software", "Stochastic", "U.S. state", "United States"], "categories": ["Algorithms", "All articles lacking sources", "All articles with topics of unclear notability", "All orphaned articles", "All stub articles", "Articles lacking sources from February 2015", "Articles with topics of unclear notability from February 2015", "Computer programming tool stubs", "Orphaned articles from February 2009", "Software", "United States government stubs"], "title": "DAKOTA"}
{"summary": "In computer science, divide and conquer (D&C) is an algorithm design paradigm based on multi-branched recursion. A divide and conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same (or related) type (divide), until these become simple enough to be solved directly (conquer). The solutions to the sub-problems are then combined to give a solution to the original problem.\nThis divide and conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. Karatsuba), syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFTs).\nUnderstanding and designing D&C algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization. These D&C complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion.\nThe correctness of a divide and conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.", "links": ["Akra\u2013Bazzi method", "Algorithm", "Algorithm design", "Analysis of algorithms", "Anatolii Alexeevitch Karatsuba", "Andrey Kolmogorov", "Arm's-length recursion", "Asymptotic complexity", "Babylonia", "Big O notation", "Binary search", "Bisection algorithm", "Bottom-up design", "Branch and bound", "Breadth first recursion", "Cache-oblivious algorithm", "Call stack", "Carl Friedrich Gauss", "Chart parsing", "Computer science", "Conditional (programming)", "Cooley-Tukey FFT algorithm", "Digital object identifier", "Discrete Fourier transform", "Divide and rule", "Doklady Akademii Nauk SSSR", "Donald Knuth", "Dynamic programming", "Euclidean algorithm", "Fast Fourier transform", "Fibonacci number", "Floating point", "Fork\u2013join model", "Geometric series", "Greatest common divisor", "Heuristic (computer science)", "Hybrid algorithm", "IBM 80 series Card Sorters", "Insertion sort", "International Standard Book Number", "John Mauchly", "John von Neumann", "Karatsuba algorithm", "Loop (computing)", "Loop nest optimization", "Loop unwinding", "MapReduce", "Master theorem", "Mathematical induction", "Memoization", "Memory cache", "Merge sort", "Multiplication algorithm", "Non-Uniform Memory Access", "Numerical algorithm", "Pairwise summation", "Paradigm", "Partial evaluation", "Post office", "Priority queue", "Prune and search", "Queue (data structure)", "Quicksort", "Radix sort", "Recurrence relation", "Recursion", "Recursion (computer science)", "Root-finding algorithm", "Sorting algorithm", "Source code generation", "Stack (data structure)", "Stack overflow", "Strassen algorithm", "Subroutine", "Syntactic analysis", "Tail recursion", "The Art of Computer Programming", "Theorem", "Top-down parser", "Tower of Hanoi", "Virtual memory", "Yuri Petrovich Ofman"], "categories": ["Algorithms", "Operations research", "Optimization algorithms and methods", "Pages with citations lacking titles"], "title": "Divide and conquer algorithms"}
{"summary": "In applied mathematics, the devex algorithm is a pivot rule for the simplex method developed by Harris. It identifies the steepest-edge approximately in its search for the optimal solution.", "links": ["Algorithm", "Data structure", "Simplex method"], "categories": ["Algorithms", "Algorithms and data structures stubs", "All articles needing additional references", "All stub articles", "Articles needing additional references from August 2013", "Computer science stubs"], "title": "Devex algorithm"}
{"summary": "In computer science, divide and conquer (D&C) is an algorithm design paradigm based on multi-branched recursion. A divide and conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same (or related) type (divide), until these become simple enough to be solved directly (conquer). The solutions to the sub-problems are then combined to give a solution to the original problem.\nThis divide and conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. Karatsuba), syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFTs).\nUnderstanding and designing D&C algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization. These D&C complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion.\nThe correctness of a divide and conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.", "links": ["Akra\u2013Bazzi method", "Algorithm", "Algorithm design", "Analysis of algorithms", "Anatolii Alexeevitch Karatsuba", "Andrey Kolmogorov", "Arm's-length recursion", "Asymptotic complexity", "Babylonia", "Big O notation", "Binary search", "Bisection algorithm", "Bottom-up design", "Branch and bound", "Breadth first recursion", "Cache-oblivious algorithm", "Call stack", "Carl Friedrich Gauss", "Chart parsing", "Computer science", "Conditional (programming)", "Cooley-Tukey FFT algorithm", "Digital object identifier", "Discrete Fourier transform", "Divide and rule", "Doklady Akademii Nauk SSSR", "Donald Knuth", "Dynamic programming", "Euclidean algorithm", "Fast Fourier transform", "Fibonacci number", "Floating point", "Fork\u2013join model", "Geometric series", "Greatest common divisor", "Heuristic (computer science)", "Hybrid algorithm", "IBM 80 series Card Sorters", "Insertion sort", "International Standard Book Number", "John Mauchly", "John von Neumann", "Karatsuba algorithm", "Loop (computing)", "Loop nest optimization", "Loop unwinding", "MapReduce", "Master theorem", "Mathematical induction", "Memoization", "Memory cache", "Merge sort", "Multiplication algorithm", "Non-Uniform Memory Access", "Numerical algorithm", "Pairwise summation", "Paradigm", "Partial evaluation", "Post office", "Priority queue", "Prune and search", "Queue (data structure)", "Quicksort", "Radix sort", "Recurrence relation", "Recursion", "Recursion (computer science)", "Root-finding algorithm", "Sorting algorithm", "Source code generation", "Stack (data structure)", "Stack overflow", "Strassen algorithm", "Subroutine", "Syntactic analysis", "Tail recursion", "The Art of Computer Programming", "Theorem", "Top-down parser", "Tower of Hanoi", "Virtual memory", "Yuri Petrovich Ofman"], "categories": ["Algorithms", "Operations research", "Optimization algorithms and methods", "Pages with citations lacking titles"], "title": "Divide and conquer algorithms"}
{"summary": "Domain reduction algorithms are algorithms used to reduce constraints and degrees of freedom in order to provide solutions for partial differential equations.", "links": ["Computer programming", "International Standard Book Number", "Partial differential equations"], "categories": ["Algorithms", "All stub articles", "Computer programming stubs"], "title": "Domain reduction algorithm"}
{"summary": "The driver scheduling problem (DSP) is type of problem in operations research and theoretical computer science.\nThe DSP consists of selecting a set of duties (assignments) for the drivers or pilots of vehicles (e.g., buses, trains, boats, or planes) involved in the transportation of passengers or goods.\nThis very complex problem involves several constraints related to labour and company rules and also different evaluation criteria and objectives. Being able to solve this problem efficiently can have a great impact on costs and quality of service for public transportation companies. There is a large number of different rules that a feasible duty might be required to satisfy, such as\nMinimum and maximum stretch duration\nMinimum and maximum break duration\nMinimum and maximum work duration\nMinimum and maximum total duration\nMaximum extra work duration\nMaximum number of vehicle changes\nMinimum driving duration of a particular vehicle\nOperations research has provided optimization models and algorithms that lead to efficient solutions for this problem. Among the most common models proposed to solve the DSP are the Set Covering and Set Partitioning Models (SPP/SCP). In the SPP model, each work piece (task) is covered by only one duty. In the SCP model, it is possible to have more than one duty covering a given work piece. In both models, the set of work pieces that needs to be covered is laid out in rows, and the set of previously defined feasible duties available for covering specific work pieces is arranged in columns. The DSP resolution, based on either of these models, is the selection of the set of feasible duties that guarantees that there is one (SPP) or more (SCP) duties covering each work piece while minimizing the total cost of the final schedule.", "links": ["Algorithm", "Digital object identifier", "International Standard Book Number", "Operations research", "Optimization", "Set covering", "Theoretical computer science"], "categories": ["Algorithms"], "title": "Driver scheduling problem"}
{"summary": "EdgeRank is the name commonly given to the algorithm that Facebook uses to determine what articles should be displayed in a user's News Feed. As of 2011, Facebook has stopped using the \"EdgeRank\" term internally to refer to its News Feed ranking algorithm, and in 2013, uses an algorithm that takes more than 100,000 factors into account in addition to EdgeRank's three. In 2010, the EdgeRank algorithm was described as:\n\nwhere:\n is user affinity\n is how the content is weighted\n is a time-based decay parameter.\nSome of the methods that Facebook uses to adjust the parameters are proprietary and not available to the public.\n^ McGee, Matt (Aug 16, 2013). \"EdgeRank Is Dead: Facebook\u2019s News Feed Algorithm Now Has Close To 100K Weight Factors\". Retrieved 28 May 2014. \n^ \"EdgeRank: The Secret Sauce That Makes Facebook's News Feed Tick\". Techcrunch. 2010-04-22. Retrieved 2012-12-08.", "links": ["Activity stream", "Adam D'Angelo", "Algorithm", "Andrei Alexandrescu", "Andrew McCollum", "Apache Cassandra", "Apache Hive", "Apache Thrift", "Blake Ross", "Bret Taylor", "Censorship of Facebook", "Charlie Cheever", "Chris Cox (Facebook)", "Chris Hughes", "Chris Kelly (entrepreneur)", "Criticism of Facebook", "David Ebersman", "David Wehner", "Donald E. Graham", "Dustin Moskovitz", "Eduardo Saverin", "Elliot Schrage", "Erskine Bowles", "Facebook", "Facebook Beacon", "Facebook Credits", "Facebook F8", "Facebook Graph Search", "Facebook Home", "Facebook Messenger", "Facebook Paper", "Facebook Platform", "Facebook Query Language", "Facebook diplomacy", "Facebook features", "Facebook like button", "Fan-gating", "FriendFeed", "Friending", "George Hotz", "Gideon Yu", "HTC First", "Hack (programming language)", "HipHop for PHP", "History of Facebook", "Initial public offering of Facebook", "Instagram", "Jim Breyer", "Joe Lockhart", "Lars Rasmussen (software developer)", "Like button", "List of mergers and acquisitions by Facebook", "Marc Andreessen", "Mark Zuckerberg", "Matt Cohler", "Mike Schroepfer", "News Feed (Facebook)", "Open Compute Project", "Owen Van Natta", "PageRank", "Peter Thiel", "Randi Zuckerberg", "React (JavaScript library)", "Reblogging", "Reed Hastings", "Scribe (log server)", "Sean Parker", "Sheryl Sandberg", "Social graph", "Susan Desmond-Hellmann", "Ted Ullyot", "The Accidental Billionaires", "The Facebook Effect", "The Social Network", "Timeline of Facebook", "WhatsApp", "Wirehog", "World Wide Web", "Yishan Wong"], "categories": ["Algorithms", "All stub articles", "Facebook", "World Wide Web stubs"], "title": "EdgeRank"}
{"summary": "The Flajolet\u2013Martin algorithm is an algorithm for approximating the number of distinct elements in a stream with a single pass and space-consumption which is logarithmic in the maximum number of possible distinct elements in the stream. The algorithm was introduced by Philippe Flajolet and G. Nigel Martin in their 1984 paper \"Probabilistic Counting Algorithms for Data Base Applications\". Later it has been refined in the papers \"LogLog counting of large cardinalities\" by Marianne Durand and Philippe Flajolet, and \"HyperLogLog: The analysis of a near-optimal cardinality estimation algorithm\" by Philippe Flajolet et al.\nIn their 2010 paper \"An optimal algorithm for the distinct elements problem\", Daniel M. Kane, Jelani Nelson and David P. Woodruff gives an improved algorithm which uses nearly optimal space, and has optimal O(1) update and reporting times.\n\n", "links": ["Algorithm", "Digital object identifier", "Discrete uniform distribution", "G. Nigel Martin", "Hash function", "International Standard Book Number", "Marianne Durand", "Multiset", "Philippe Flajolet", "Streaming algorithm"], "categories": ["Algorithms", "All orphaned articles", "Orphaned articles from November 2014"], "title": "Flajolet\u2013Martin algorithm"}
{"summary": "The generalized distributive law (GDL) is a general message passing algorithm devised by Srinivas M. Aji and Robert J. McEliece. It is a synthesis of the work of many authors in the information theory, digital communications, signal processing, statistics, and artificial intelligence communities. This article is based on a semi-tutorial by Srinivas M. Aji and Robert J. McEliece with the same title.", "links": ["Artificial intelligence", "Belief propagation", "Bucket elimination", "Channel (communications)", "Commutative monoid", "Commutative semiring", "Convolutional codes", "Degree (graph theory)", "Digital communications", "Digital object identifier", "Fast Fourier transform", "Fast algorithms", "Forward-backward algorithm", "Hadamard transform", "Information theory", "Junction tree", "Linear code", "Markov chain", "Matrix chain multiplication", "Maximum likelihood decoding", "Message passing", "Robert McEliece", "Signal processing", "Statistics", "Tanner graph", "Tree (graph theory)", "Trellis (graph)", "Viterbi algorithm"], "categories": ["Algorithms", "All articles lacking in-text citations", "All articles needing additional references", "Articles lacking in-text citations from June 2012", "Articles needing additional references from June 2012", "Artificial intelligence", "Digital signal processing", "Graphical models", "Information theory", "Wikipedia articles needing clarification from June 2015"], "title": "Generalized distributive law"}
{"summary": "The Grey Wolf Optimizer (GWO) is a recently proposed swarm-based meta-heuristic. This algorithm mimics the social leadership and hunting behaviour of gray wolves in nature. The main phases of hunt in a pack of wolves have been mathematically modeled to solve optimization problems.", "links": ["Gray wolves", "Multilayer perceptron", "Yellowstone National Park"], "categories": ["Algorithms"], "title": "Grey wolf optimizer"}
{"summary": "HAKMEM, alternatively known as AI Memo 239, is a February 1972 \"memo\" (technical report) of the MIT AI Lab containing a wide variety of hacks, including useful and clever algorithms for mathematical computation, some number theory and schematic diagrams for hardware \u2014 in Guy L. Steele's words, \"a bizarre and eclectic potpourri of technical trivia\". Contributors included about two dozen members and associates of the AI Lab. The title of the report is short for \"hacks memo\", abbreviated to six upper case characters that would fit in a single PDP-10 machine word (using a six-bit character set).\n^ a b Steele's foreword to Hank S. Warren (2012). Hacker's Delight (PDF). Addison\u2013Wesley. p. xi.", "links": ["AI Memo", "Algorithm", "Bill Gosper", "Continued fraction", "Guy L. Steele", "Hack (technology slang)", "Hacker (programmer subculture)", "MIT AI Lab", "Number theory", "PDP-10", "Richard Schroeppel", "Schematic diagram", "Technical report"], "categories": ["1972 in Massachusetts", "Algorithms", "All articles lacking reliable references", "All articles with unsourced statements", "Articles lacking reliable references from July 2015", "Articles with unsourced statements from August 2014", "Computer science papers", "Memoranda"], "title": "HAKMEM"}
{"summary": "The Havel\u2013Hakimi algorithm is an algorithm in graph theory solving the graph realization problem, i.e. the question if there exists for a finite list of nonnegative integers a simple graph such that its degree sequence is exactly this list. For a positive answer the list of integers is called graphic. The algorithm constructs a special solution if one exists or proves that one cannot find a positive answer. This construction is based on a recursive algorithm. The algorithm was published by Havel (1955), and later by Hakimi (1962).", "links": ["Degree (graph theory)", "Enumeration", "Erd\u0151s\u2013Gallai theorem", "Graph (mathematics)", "Graph realization problem", "Graph theory", "Integer", "Mathematical Reviews", "Nonincreasing", "Recursion (computer science)", "S. L. Hakimi", "Society for Industrial and Applied Mathematics", "V. J. Havel"], "categories": ["Algorithms", "CS1 Czech-language sources (cs)", "Graph theory"], "title": "Havel\u2013Hakimi algorithm"}
{"summary": "The HCS (Highly Connected Subgraphs) clustering algorithm (also known as the HCS algorithm , and other names such as Highly Connected Clusters/Components/Kernels) is an algorithm based on graph connectivity for Cluster analysis, by first representing the similarity data in a similarity graph, and afterwards finding all the highly connected subgraphs as clusters. The algorithm does not make any prior assumptions on the number of the clusters. This algorithm was published by Erez Hartuv (erez dot hartuv at gmail dot com) and Ron Shamir in 1998.\nThe HCS algorithm gives clustering solution, which is inherently meaningful in the application domain, since each solution cluster must have diameter 2 while a union of two solution clusters will have diameter 3.", "links": ["Best, worst and average case", "Clique problem", "Cluster analysis", "Connectivity (graph theory)", "Digital object identifier", "Distance (graph theory)", "Graph (data structure)", "Highly Connected Subgraph", "Karger's algorithm", "Minimum cut", "Ron Shamir", "Similarity graph"], "categories": ["Algorithms"], "title": "HCS clustering algorithm"}
{"summary": "In computer science, a holographic algorithm is an algorithm that uses a holographic reduction. A holographic reduction is a constant-time reduction that maps solution fragments many-to-many such that the sum of the solution fragments remains unchanged. These concepts were introduced by Leslie Valiant, who called them holographic because \"their effect can be viewed as that of producing interference patterns among the solution fragments\". The algorithms are unrelated to laser holography, except metaphorically. Their power comes from the mutual cancellation of many contributions to a sum, analogous to the interference patterns in a hologram.\nHolographic algorithms have been used to find polynomial-time solutions to problems without such previously known solutions for special cases of satisfiability, vertex cover, and other graph problems. They have received notable coverage due to speculation that they are relevant to the P versus NP problem and their impact on computational complexity theory. Although some of the general problems are #P-hard problems, the special cases solved are not themselves #P-hard, and thus do not prove FP = #P.\nHolographic algorithms have some similarities with quantum computation, but are completely classical.", "links": ["American Scientist", "Basis (linear algebra)", "Bipartite graph", "Boolean satisfiability problem", "Brian Hayes (scientist)", "Chinese remainder theorem", "Complement (set theory)", "Computational complexity theory", "Computer science", "Conjunctive normal form", "Constraint graph", "Constraint satisfaction problem", "Digital object identifier", "FKT algorithm", "Fibonacci number", "Graph theory", "Holographic data storage", "Holography", "Hypergraph", "Independent set (graph theory)", "International Standard Book Number", "International Standard Serial Number", "Invertible matrix", "Leslie Valiant", "Many-one reduction", "Matchgates", "Mersenne number", "Modular arithmetic", "Modulo operation", "Monotonic function", "P (complexity)", "P versus NP problem", "Perfect matching", "Planar graph", "Quantum computation", "Recurrence relation", "Reduction (complexity)", "Regular graph", "Sharp-P", "Symmetric function", "Tensor product", "Truth table", "Vertex cover"], "categories": ["Algorithms"], "title": "Holographic algorithm"}
{"summary": "A hybrid algorithm is an algorithm that combines two or more other algorithms that solve the same problem, either choosing one (depending on the data), or switching between them over the course of the algorithm. This is generally done to combine desired features of each, so that the overall algorithm is better than the individual components.\n\"Hybrid algorithm\" does not refer to simply combining multiple algorithms to solve a different problem \u2013 many algorithms can be considered as combinations of simpler pieces \u2013 but only to combining algorithms that solve the same problem, but differ in other characteristics, notably performance.", "links": ["Algorithm", "Arm's-length recursion", "Binary search", "Bucket sort", "Computer science", "Decrease and conquer", "Distributed algorithm", "Distribution sort", "Divide and conquer algorithm", "External sorting", "Flashsort", "Heap sort", "Hybrid algorithm (constraint satisfaction)", "Hybrid genetic algorithm", "Hybrid input output (HIO) algorithm for phase retrieval", "Introselect", "Introsort", "MapReduce", "Median of medians", "Merge sort", "Quickselect", "Quicksort", "Recursive algorithm", "Sorting algorithm", "Timsort"], "categories": ["Algorithms", "All articles lacking sources", "Articles lacking sources from May 2014"], "title": "Hybrid algorithm"}
{"summary": "A hyphenation algorithm is a set of rules (especially one codified for implementation in a computer program) that decides at which points a word can be broken over two lines with a hyphen. For example, a hyphenation algorithm might decide that impeachment can be broken as impeach-ment or im-peachment, but not, say, as impe-achment.\nOne of the reasons for the complexity of the rules of word-breaking is that different \"dialects\" of English tend to differ on the rule: American English tends to work on sound, while British English tends to look to the origins of the word and then to sound. There are also a large number of exceptions, which further complicates matters.\nSome rules of thumb can be found in the reference \"On Hyphenation \u2013 Anarchy of Pedantry\". Among algorithmic approaches to hyphenation, the one implemented in the TeX typesetting system is widely used. It is thoroughly documented in the first two volumes of Computers and Typesetting and in Frank Liang's dissertation. Contrary to the belief that TeX relies on a large dictionary of exceptions, the point of Liang's work was to get the algorithm as accurate as he practically could and keep any exception dictionary small. In TeX's original hyphenation patterns for US English, the exception list contains fourteen words.", "links": ["ASCII", "Algorithmic", "American English", "British English", "Computer Science", "Computers and Typesetting", "Frank Liang", "Haskell (programming language)", "Hyphen", "JavaScript", "LaTeX", "Lyrics", "Perl", "PostScript", "Python (programming language)", "Ruby (programming language)", "Stanford University", "TeX"], "categories": ["Algorithms", "Articles needing more viewpoints from December 2013", "Digital typography"], "title": "Hyphenation algorithm"}
{"summary": "In computer science, an in-place algorithm is an algorithm which transforms input using a data structure with a small amount of extra storage space. The input is usually overwritten by the output as the algorithm executes. An algorithm which is not in-place is sometimes called not-in-place or out-of-place.\nIn-place can have slightly different meanings. In its strictest form, the algorithm can only have a constant amount of extra space, counting everything including function calls and pointers. However, this form is very limited as simply having an index to a length n array requires O(log n) bits. More broadly, in-place means that the algorithm does not use extra space for manipulating the input but may require a small though nonconstant extra space for its operation. Usually, this space is O(log n), though sometimes anything in o(n) is allowed. Note that space complexity also has varied choices in whether or not to count the index lengths as part of the space used. Often, the space complexity is given in terms of the number of indices or pointers needed, ignoring their length. In this article, we refer to total space complexity (DSPACE), counting pointer lengths. Therefore, the space requirements here have an extra log n factor compared to an analysis that ignores the length of indices and pointers.\nAn algorithm may or may not count the output as part of its space usage. Since in-place algorithms usually overwrite their input with output, no additional space is needed. When writing the output to write-only memory or a stream, it may make be more appropriate to only consider the working space of the algorithm. In theory applications such as log-space reductions, it is more typical to always ignore output space (in these cases it is more essential that the output is write-only).", "links": ["Algorithm", "Array data structure", "BPL (complexity)", "Big O notation", "Bipartite graph", "Bubble sort", "Comb sort", "Computational complexity theory", "Computer science", "Connected component (graph theory)", "Data structure", "Depth-first search", "Deterministic space", "Divide and conquer algorithm", "Execute in place", "Functional programming", "Heapsort", "In-place", "Insertion sort", "L (complexity)", "Log-space reduction", "Miller-Rabin primality test", "Pollard's rho algorithm", "Purely functional data structure", "Quicksort", "RL (complexity)", "Random walk", "Randomized algorithm", "Regular language", "SL (complexity)", "Selection algorithm", "Selection sort", "Shell sort", "Side effect (computer science)", "Sorting algorithm", "Trim (programming)", "Undirected graph"], "categories": ["Algorithms", "All articles needing additional references", "Articles needing additional references from January 2015"], "title": "In-place algorithm"}
{"summary": "Jump-and-Walk is an algorithm for point location in triangulations (though most of the theoretical analysis were performed in 2D and 3D random Delaunay triangulations). Surprisingly, the algorithm does not need any preprocessing or complex data structures except some simple representation of the triangulation itself. The predecessor of Jump-and-Walk was due to Lawson (1977) and Green and Sibson (1978), which picks a random starting point S and then walks from S toward the query point Q one triangle at a time. But no theoretical analysis was known for these predecessors until after mid-1990s.\nJump-and-Walk picks a small group of sample points and starts the walk from the sample point which is the closest to Q until the simplex containing Q is found. The algorithm was a folklore in practice for some time, and the formal presentation of the algorithm and the analysis of its performance on 2D random Delaunay triangulation was done by Devroye, Mucke and Zhu in mid-1990s (the paper appeared in Algorithmica, 1998). The analysis on 3D random Delaunay triangulation was done by Mucke, Saias and Zhu (ACM Symposium of Computational Geometry, 1996). In both cases, a boundary condition was assumed, namely, Q must be slightly away from the boundary of the convex domain where the vertices of the random Delaunay triangulation are drawn. In 2004, Devroye, Lemaire and Moreau showed that in 2D the boundary condition can be withdrawn (the paper appeared in Computational Geometry: Theory and Applications, 2004).\nJump-and-Walk has been used in many famous software packages, e.g., QHULL, Triangle and CGAL.", "links": ["Algorithm", "John R. Rice (professor)", "Point location", "Triangulation"], "categories": ["Algorithms", "Triangulation (geometry)"], "title": "Jump-and-Walk algorithm"}
{"summary": "In robotics and motion planning, kinodynamic planning is a class of problems for which velocity, acceleration, and force/torque bounds must be satisfied, together with kinematic constraints such as avoiding obstacles. The term was coined by Bruce Donald, Pat Xavier, John Canny, and John Reif. Donald et al. developed the first polynomial-time approximation schemes (PTAS) for the problem. By providing a provably polynomial-time \u03b5-approximation algorithm, they resolved a long-standing open problem in optimal control. Their first paper considered time-optimal control (\"fastest path\") of a point mass under Newtonian dynamics, amidst polygonal (2D) or polyhedral (3D) obstacles, subject to state bounds on position, velocity, and acceleration. Later they extended the technique to many other cases, for example, to 3D open-chain kinematic robots under full Lagrangian dynamics. More recently, many practical heuristic algorithms based on stochastic optimization and iterative sampling were developed, by a wide range of authors, to address the kinodynamic planning problem. These techniques for kinodynamic planning have been shown to work well in practice. However, none of these heuristic techniques can guarantee the optimality of the computed solution (i.e., they have no performance guarantees), and none can be mathematically proven to be faster than the original PTAS algorithms (i.e., none have a provably lower computational complexity).", "links": ["Acceleration", "Bruce Donald", "Digital object identifier", "Heuristic algorithm", "John Canny", "John Reif", "Motion planning", "Polynomial-time", "Polynomial-time approximation scheme", "Robotics", "Velocity", "\u0395-approximation algorithm"], "categories": ["Algorithms", "Automated planning and scheduling", "Robot control", "Robot kinematics"], "title": "Kinodynamic planning"}
{"summary": "The Kinetic Simulation Algorithm Ontology (KiSAO) supplies information about existing algorithms available for the simulation of systems biology models, their characterization and interrelationships. KiSAO is part of the BioModels.net project and of the COMBINE initiative.", "links": ["Artistic License", "BioModels.net", "COMBINE", "Computer software", "Digital object identifier", "European Bioinformatics Institute", "File format", "MIRIAM", "PubMed Central", "PubMed Identifier", "Research center", "SED-ML", "Software license", "Systems Biology Ontology", "Systems biology", "TErminology for the Description of DYnamics", "Web Ontology Language"], "categories": ["Algorithms", "Bioinformatics", "Biological databases", "Free science software", "Systems biology"], "title": "KiSAO"}
{"summary": "In theoretical computer science, in particular in formal language theory, Kleene's algorithm transforms a given deterministic finite automaton (DFA) into a regular expression. Together with other conversion algorithms, it establishes the equivalence of several description formats for regular languages.", "links": ["Deterministic finite automaton", "Floyd-Warshall algorithm", "Formal language theory", "Generalized star height problem", "International Standard Book Number", "Kleene", "Kleene algebra", "Regular expression", "Regular language", "Star height", "Star height problem", "Theoretical computer science", "Thompson's construction algorithm"], "categories": ["Algorithms", "Finite automata", "Regular expressions"], "title": "Kleene's algorithm"}
{"summary": "The Kleitman\u2013Wang algorithms are two different algorithms in graph theory solving the digraph realization problem, i.e. the question if there exists for a finite list of nonnegative integer pairs a simple directed graph such that its degree sequence is exactly this list. For a positive answer the list of integer pairs is called digraphic. Both algorithms construct a special solution if one exists or prove that one cannot find a positive answer. These constructions are based on recursive algorithms. Kleitman and Wang  gave these algorithms in 1973.", "links": ["Digital object identifier", "Digraph realization problem", "Directed graph", "Enumeration", "Fulkerson\u2013Chen\u2013Anstee theorem", "Graph theory", "Integer", "Lexicographical order", "List (abstract data type)", "Nonincreasing", "Recursion (computer science)"], "categories": ["Algorithms", "Graph theory"], "title": "Kleitman\u2013Wang algorithms"}
{"summary": "Within complex networks, real networks tend to have community structure. Label propagation is an algorithm  for finding communities. In comparison with other algorithms label propagation has advantage in its running time, amount of a priori information needed about the network structure (no parameter is required to be known beforehand). Disadvantage is that it produces no unique solution, but an aggregate of many solutions.", "links": ["Community structure", "Complex networks", "Jaccard index"], "categories": ["Algorithms", "All articles needing expert attention", "All articles that are too technical", "All orphaned articles", "Articles needing expert attention from June 2015", "Networks", "Orphaned articles from June 2015", "Wikipedia articles that are too technical from June 2015"], "title": "Label Propagation Algorithm"}
{"summary": "Lancichinetti-Fortunato-Radicchi (LFR) benchmark is an algorithm that generates benchmark networks (artificial networks that resemble real-world networks). They have a priori known communities and are used to compare different community detection methods. The advantage of LFR over other methods is that it accounts for the heterogeneity in the distributions of node degrees and of community sizes.", "links": ["Benchmark networks", "Homogeneity (statistics)", "Power law distribution"], "categories": ["Algorithms", "All articles covered by WikiProject Wikify", "All articles needing expert attention", "All articles that are too technical", "All articles with too few wikilinks", "All orphaned articles", "Articles covered by WikiProject Wikify from June 2015", "Articles needing expert attention from June 2015", "Articles with too few wikilinks from June 2015", "Computer benchmarks", "Orphaned articles from June 2015", "Statistical models", "Wikipedia articles that are too technical from June 2015"], "title": "Lancichinetti-Fortunato-Radicchi Benchmark"}
{"summary": "The lossy count algorithm is an algorithm to identify elements in a data stream whose frequency count exceed a user-given threshold. The frequency computed by this algorithm is not always accurate, but has an error threshold that can be specified by the user. The run time space required by the algorithm is inversely proportional to the specified error threshold, hence larger the error, the smaller the footprint. It was created by eminent computer scientists Rajeev Motwani and Gurmeet Singh Manku. This algorithm finds huge application in computations where data takes the form of a continuous data stream instead of a finite data set, for e.g. network traffic measurements, web server logs, clickstreams.", "links": ["Algorithm", "Clickstream", "Data set", "Data stream", "Frequency", "Gurmeet Singh Manku", "Rajeev Motwani"], "categories": ["Algorithms"], "title": "Lossy Count Algorithm"}
{"summary": "The Manhattan address algorithm is used to estimate the number of the closest cross street for a given building number on the island of Manhattan. The algorithm is given in any print telephone directory  as well as on numerous web pages and in New York City guide books.", "links": ["110th Street (Manhattan)", "112th Street (Manhattan)", "116th Street (Manhattan)", "120th Street (Manhattan)", "122nd Street (Manhattan)", "125th Street (Manhattan)", "132nd Street (Manhattan)", "135th Street (Manhattan)", "145th Street (Manhattan)", "14th Street (Manhattan)", "155th Street (Manhattan)", "181st Street (Manhattan)", "187th Street (Manhattan)", "23rd Street (Manhattan)", "34th Street (Manhattan)", "42nd Street (Manhattan)", "47th Street (Manhattan)", "4th Street (Manhattan)", "50th Street (Manhattan)", "51st Street (Manhattan)", "52nd Street (Manhattan)", "53rd Street (Manhattan)", "54th Street (Manhattan)", "55th Street (Manhattan)", "57th Street (Manhattan)", "59th Street (Manhattan)", "66th Street (Manhattan)", "6\u00bd Avenue", "72nd Street (Manhattan)", "74th Street (Manhattan)", "79th Street (Manhattan)", "85th Street (Manhattan)", "86th Street (Manhattan)", "89th Street (Manhattan)", "8th Street / St. Mark's Place (Manhattan)", "93rd Street (Manhattan)", "95th Street (Manhattan)", "96th Street (Manhattan)", "Allen Street", "Asser Levy Place", "Astor Place", "Astor Row", "Audubon Avenue", "Avenue A (Manhattan)", "Avenue B (Manhattan)", "Avenue C (Manhattan)", "Avenue D (Manhattan)", "Bank Street (Manhattan)", "Baxter Street", "Beach Street (Manhattan)", "Beekman Place", "Bleecker Street", "Bogardus Place", "Bowery", "Bridge Street (Manhattan)", "Broad Street (Manhattan)", "Broadway (Manhattan)", "Broome Street", "Building number", "Cabrini Boulevard", "Canal Street (Manhattan)", "Center Drive (Manhattan)", "Central Park West", "Centre Market Place", "Centre Street (Manhattan)", "Chambers Street (Manhattan)", "Chatham Square", "Cherry Street (Manhattan)", "Christopher Street", "Chrystie Street", "Church Street (Manhattan)", "Claremont Avenue", "Coenties Slip", "Columbus Circle", "Commissioners' Plan of 1811", "Convent Avenue", "Cooper Square", "Delancey Street", "Division Street (Manhattan)", "Doyers Street", "Duarte Square", "Duffy Square", "Duke Ellington Circle", "Dyckman Street", "Dyer Avenue", "East Broadway (Manhattan)", "East Drive (Manhattan)", "East Side (Manhattan)", "Edgecombe Avenue", "Eighth Avenue (Manhattan)", "Eleventh Avenue (Manhattan)", "Elizabeth Street (Manhattan)", "Essex Street", "Fifth Avenue", "Fifth Avenue (Manhattan)", "Financial District, Manhattan", "First Avenue (Manhattan)", "Foley Square", "Forsyth Street", "Fort Washington Avenue", "Fort Washington Avenue (Manhattan)", "Fourth Avenue (Manhattan)", "Franklin D. Roosevelt East River Drive", "Frederick Douglass Circle", "Fulton Street (Manhattan)", "Gay Street (Manhattan)", "George Abbott Way", "Gramercy Park", "Grand Army Plaza (Manhattan)", "Grand Street (Manhattan)", "Great Jones Street", "Greenwich Avenue", "Greenwich Street", "Guide book", "Hanover Square (Manhattan)", "Henry Street (Manhattan)", "Herald Square", "Hester Street (Manhattan)", "Houston Street", "Hudson Park and Boulevard", "Hudson Street (Manhattan)", "Jackson Square Park", "Jones Street", "Lafayette Street", "Lenox Avenue", "Lexington Avenue", "Liberty Street (Manhattan)", "Lincoln Square, Manhattan", "Lincoln Tunnel Expressway", "List of eponymous streets in New York City", "List of numbered streets in Manhattan", "List of streets in Manhattan", "Lower East Side", "Lower Manhattan", "Lower West Side, Manhattan", "Ludlow Street (Manhattan)", "Macdougal Street", "Madison Avenue", "Madison Square", "Madison Street (Manhattan)", "Maiden Lane (Manhattan)", "Manhattan", "Manhattan Avenue", "Marketfield Street", "Midtown Manhattan", "Morningside Drive (Manhattan)", "Mother Hale Way", "Mott Street", "Mulberry Street (Manhattan)", "Mulry Square", "Nassau Street (Manhattan)", "New York City", "Ninth Avenue (Manhattan)", "North Moore Street", "Numbered street", "Orchard Street (Manhattan)", "Park Avenue", "Park Avenue Tunnel (roadway)", "Park Avenue Viaduct", "Park Row (Manhattan)", "Patchin Place", "Pearl Street (Manhattan)", "Pershing Square, Manhattan", "Petrosino Square", "Pleasant Avenue", "Riverside Drive (Manhattan)", "Rivington Street", "Rockefeller Center", "Roosevelt Street", "Saint Nicholas Avenue", "Second Avenue (Manhattan)", "Seminary Row", "Seventh Avenue (Manhattan)", "Sherman Square", "Shubert Alley", "Sixth Avenue (Manhattan)", "South Street (Manhattan)", "Spring Street (Manhattan)", "St. John's Park", "St. Nicholas Avenue", "St. Nicholas Historic District", "Stanton Street", "Stone Street (Manhattan)", "Stuyvesant Square", "Stuyvesant Street", "Sullivan Street", "Sylvan Place", "Taras Shevchenko Place", "Telephone directory", "Tenth Avenue (Manhattan)", "The New York Times", "Third Avenue", "Thirteenth Avenue (Manhattan)", "Thompson Street (Manhattan)", "Times Square", "Tompkins Square Park", "Union Square, Manhattan", "University Place (Manhattan)", "Upper Manhattan", "Vanderbilt Avenue", "Varick Street", "Verdi Square", "Vesey Street", "Wadsworth Avenue", "Wall Street", "Washington Mews", "Washington Square Park", "Washington Street (Manhattan)", "Waverly Place", "Weehawken Street", "West Broadway", "West Drive (Manhattan)", "West End Avenue", "West Side (Manhattan)", "West Side Highway", "Whitehall Street", "William Street (Manhattan)", "Worth Street", "York Avenue", "York Avenue / Sutton Place", "Zuccotti Park"], "categories": ["Algorithms", "Streets in Manhattan"], "title": "Manhattan address algorithm"}
{"summary": "A matching engine is an algorithm that operates on an order book and matches and determines prices at which orders are matched. A theoretical study of matching is proposed by Jean-Fran\u00e7ois Mertens's limit price mechanism.\nMatching engine term is also used to describe the technology to cross buyer and seller trades in regulated, dark pools and OTC markets.\nThe underlying technology need to support different types of orders (limit, market, stop loss) and implement several Matching rules. Some technology providers such as List, SmartTrade Technologies or Cinnober are specialised in providing matching Engine.", "links": ["Dark pool", "Exchange (organized market)", "Jean-Fran\u00e7ois Mertens", "Order (exchange)", "Order book", "Order book (trading)", "Over-the-counter (finance)", "SmartTrade Technologies"], "categories": ["Algorithms", "All articles to be merged", "All articles with topics of unclear notability", "Articles to be merged from February 2013", "Articles with topics of unclear notability from May 2012"], "title": "Matching engine"}
{"summary": "Maze generation algorithms are automated methods for the creation of mazes.", "links": ["Algorithm", "Amortized time", "Backtracking", "Binary tree", "Cellular automata", "Commodore 64", "Connected graph", "Conway's Game of Life", "Depth-first search", "Deterministic", "Disjoint-set data structure", "Dual graph", "Kruskal's algorithm", "Maze", "Maze solving algorithm", "Moore neighbourhood", "PETSCII", "Planar graph", "Prim's algorithm", "Recursion", "Spanning tree (mathematics)"], "categories": ["Algorithms", "All articles needing additional references", "Articles containing video clips", "Articles needing additional references from August 2012", "Articles with example Python code", "Mazes", "Random graphs", "Wikipedia articles needing clarification from October 2014"], "title": "Maze generation algorithm"}
{"summary": "There are a number of different maze solving algorithms, that is, automated methods for the solving of mazes. The random mouse, wall follower, Pledge, and Tr\u00e9maux's algorithms are designed to be used inside the maze by a traveler with no prior knowledge of the maze, whereas the dead-end filling and shortest path algorithms are designed to be used by a person or computer program that can see the whole maze at once.\nMazes containing no loops are known as \"simply connected\", or \"perfect\" mazes, and are equivalent to a tree in graph theory. Thus many maze solving algorithms are closely related to graph theory. Intuitively, if one pulled and stretched out the paths in the maze in the proper way, the result could be made to resemble a tree.", "links": ["A* algorithm", "Algorithm", "Algorithms", "Breadth-first search", "Charles Pierre Tr\u00e9maux", "Cul-de-sac", "Degree (angle)", "Depth-first search", "Digital object identifier", "Exeter", "Glossary of graph theory", "Graph theory", "Heuristic", "International Standard Book Number", "Java (programming language)", "Jean Pelletier-Thibert", "Jon Pledge", "Las Vegas algorithm", "Lisa Simpson", "Maze", "Maze generation algorithm", "Mazes", "Queue (data structure)", "Robot", "Shimon Even", "Shortest path algorithm", "Simply connected space", "Stop or My Dog Will Shoot", "The Simpsons", "Tree (graph theory)", "YouTube"], "categories": ["Algorithms", "Mazes"], "title": "Maze solving algorithm"}
{"summary": "A medical algorithm is any computation, formula, statistical survey, nomogram, or look-up table, useful in healthcare. Medical algorithms include decision tree approaches to healthcare treatment (e.g., if symptoms A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty.", "links": ["Algorithm", "Arden syntax", "Artificial neural network", "Automatic control", "Automation", "Body mass index", "Calculation", "Calculators", "Chest pain", "Clinical decision support system", "Clinical trial protocol", "Clinician", "Computation", "Computerized health diagnostics", "Consensus (medical)", "Decision-making", "Decision making", "Decision tree", "Diagnosis", "Digital object identifier", "Etiology", "Evidence-based medicine", "Flowcharts", "Food energy", "Formula", "Guideline (medical)", "Healthcare", "ICU scoring systems", "Journal club", "Journal of the American Medical Informatics Association", "Logic", "Look-up table", "Medical", "Medical care", "Medical equipment", "Medical guideline", "Medical informatics", "Medical logic module", "Nomogram", "Obesity", "Odds algorithm", "Overweight", "Physician", "Prediction", "Prognosis", "PubMed Central", "Stanford University", "Statistical survey", "Symptom", "Texas Medication Algorithm Project", "Treatment Guidelines from The Medical Letter"], "categories": ["Algorithms", "All articles that may contain original research", "All articles to be expanded", "Articles needing translation from Russian Wikipedia", "Articles that may contain original research from October 2007", "Articles to be expanded from September 2015", "Health informatics", "Knowledge representation"], "title": "Medical algorithm"}
{"summary": "METIS is a software package for graph partitioning that implements various multilevel algorithms.\nMETIS' multilevel approach has three phases and comes with several algorithms for each phase:\nCoarsen the graph by generating a sequence of graphs G0, G1, ..., GN, where G0 is the original graph and for each 0 \u2264 i \u2264 j \u2264 N, the number of vertices in Gi is greater than the number of vertices in Gj.\nCompute a partition of GN\nProject the partition back through the sequence in the order of GN, ..., G0, refining it with respect to each graph.\nThe final partition computed during the third phase (the refined partition projected onto G0) is a partition of the original graph.", "links": ["Algorithm", "Data structure", "Digital object identifier", "Graph partition", "Graph partitioning"], "categories": ["Algorithms", "Algorithms and data structures stubs", "All articles covered by WikiProject Wikify", "All articles with too few wikilinks", "All stub articles", "Articles covered by WikiProject Wikify from December 2013", "Articles with too few wikilinks from December 2013", "Computational problems in graph theory", "Computer science stubs"], "title": "METIS"}
{"summary": "In computing, a one-pass algorithm is one which reads its input exactly once, in order, without unbounded buffering. A one-pass algorithm generally requires O(n) (see 'big O' notation) time and less than O(n) storage (typically O(1)), where n is the size of the input.\nBasically one-pass algorithm operates as follows: (1) the object descriptions are processed serially; (2) the first object becomes the cluster representative of the first cluster; (3) each subsequent object is matched against all cluster representatives existing at its processing time; (4) a given object is assigned to one cluster (or more if overlap is allowed) according to some condition on the matching function; (5) when an object is assigned to a cluster the representative for that cluster is recomputed; (6) if an object fails a certain test it becomes the cluster representative of a new cluster", "links": ["Big O Notation", "Buffer (computer science)", "Mean", "Median", "Mode (statistics)", "Standard deviation", "Summation", "Variance"], "categories": ["Algorithms", "All articles lacking sources", "Articles lacking sources from January 2007"], "title": "One-pass algorithm"}
{"summary": "Out-of-core or external memory algorithms are algorithms that are designed to process data that is too large to fit into a computer's main memory at one time. Such algorithms must be optimized to efficiently fetch and access data stored in slow bulk memory such as hard drives or tape drives.\nA typical example is geographic information systems, especially digital elevation models, where the full data set easily exceeds several gigabytes or even terabytes of data.\nThis notion naturally extends to a network connecting a data server to a treatment or visualization workstation. Popular mass-of-data based web applications such as google-Map or google-Earth enter this topic.\nThis extends beyond general purpose CPUs, and also includes GPU computing as well as classical digital signal processing. In GPGPU based computing where powerful graphics cards (GPUs) with little memory (compared to the more familiar system memory which is most often referred to simply as RAM) and slow CPU to GPU memory transfer (when compared to computation bandwidth).", "links": ["Algorithm", "Algorithms", "Central processing unit", "Data structure", "Digital elevation model", "Digital object identifier", "Digital signal processing", "External sorting", "General-purpose computing on graphics processing units", "Geographic information system", "Gigabytes", "Graphics processing unit", "Main memory", "Online algorithm", "RAM", "Streaming algorithm", "Terabytes"], "categories": ["Algorithms", "Algorithms and data structures stubs", "All stub articles", "Computer science stubs"], "title": "Out-of-core algorithm"}
{"summary": "Algorithms said to employ a Ping-Pong scheme exist in different fields of Software Engineering. They are characterized by an alternation between two entities. In the examples described below, these entities are communication partners, network paths or file blocks.", "links": ["Block (data storage)", "Crash (computing)", "DBMS", "Database transaction", "Durability (database systems)", "International Standard Book Number", "Internet Control Message Protocol", "Ping (networking utility)", "Python (programming language)", "Routing", "Software Engineering", "Transaction log"], "categories": ["Algorithms", "All articles needing additional references", "Articles needing additional references from June 2010"], "title": "Ping-pong scheme"}
{"summary": "Pointer jumping or path doubling is a design technique for parallel algorithms that operate on pointer structures, such as linked lists and directed graphs. It can be used to find the roots of a forest of rooted trees, and can also be applied to parallelize many other graph algorithms including connected components, minimum spanning trees, and biconnected components.", "links": ["Algorithm design", "Amdahl's law", "Analysis of parallel algorithms", "Application checkpointing", "Application programming interface", "Asymmetric multiprocessing", "Ateji PX", "Barrier (computer science)", "Beowulf cluster", "Biconnected components", "Bit-level parallelism", "Boost (C++ libraries)", "C++ AMP", "CUDA", "Cache-only memory architecture", "Cache coherence", "Cache invalidation", "Charles E. Leiserson", "Charm++", "Cilk", "Clifford Stein", "Cloud computing", "Coarray Fortran", "Computer cluster", "Computer hardware", "Computer programming", "Concurrency (computer science)", "Connected components", "Cost efficiency", "Data parallelism", "Deadlock", "Deterministic algorithm", "Directed graph", "Distributed computing", "Distributed memory", "Distributed shared memory", "Dryad (programming)", "Embarrassingly parallel", "Explicit parallelism", "Fiber (computer science)", "Flynn's taxonomy", "Forest (graph theory)", "Global Arrays", "Graph (mathematics)", "Graph algorithm", "Grid computing", "Gustafson's law", "High-performance computing", "Hyper-threading", "Implicit parallelism", "Instruction-level parallelism", "Instruction window", "International Standard Book Number", "Introduction to Algorithms", "Karp\u2013Flatt metric", "Linked list", "Logarithmic time", "MIMD", "MISD", "Massively parallel (computing)", "Memory-level parallelism", "Memory coherence", "Message Passing Interface", "Minimum spanning trees", "Multiprocessing", "Multiprocessor", "Multithreading (computer architecture)", "Non-blocking algorithm", "Non-uniform memory access", "Null pointer", "OpenACC", "OpenCL", "OpenHMPP", "OpenMP", "POSIX Threads", "Parallel Extensions", "Parallel LINQ", "Parallel Virtual Machine", "Parallel algorithm", "Parallel computing", "Parallel programming model", "Parallel random-access machine", "Parallel random access machine", "Parallel slowdown", "Path (graph theory)", "Process (computing)", "Pseudocode", "Race condition", "Resource starvation", "Ron Rivest", "Rooted tree", "SIMD", "SISD", "SPMD", "Scalability", "Semiconductor memory", "Shared memory", "Simultaneous multithreading", "Software lockout", "Speedup", "Supercomputer", "Superscalar", "Symmetric multiprocessing", "Synchronization (computer science)", "Task parallelism", "Temporal multithreading", "Thomas H. Cormen", "Thread (computing)", "Threading Building Blocks", "Unified Parallel C", "Uniform memory access", "Vector processor", "Vertex (graph theory)"], "categories": ["Algorithms", "Parallel computing"], "title": "Pointer jumping"}
{"summary": "In mathematics, particularly numerical analysis, a predictor\u2013corrector method is an algorithm that proceeds in two steps. First, the prediction step calculates a rough approximation of the desired quantity. Second, the corrector step refines the initial approximation using another means.", "links": ["Algorithm", "Backward differentiation formula", "Beeman's algorithm", "Eric W. Weisstein", "Euler method", "Explicit and implicit methods", "Heun's method", "International Standard Book Number", "John C. Butcher", "John Wiley & Sons", "MathWorld", "Mathematics", "Mehrotra predictor\u2013corrector method", "Numerical analysis", "Numerical continuation", "Numerical methods for ordinary differential equations", "Trapezoidal rule (differential equations)"], "categories": ["Algorithms", "Numerical analysis"], "title": "Predictor\u2013corrector method"}
{"summary": "In computer science, a randomization function or randomizing function is an algorithm or procedure that implements a randomly chosen function between two specific sets, suitable for use in a randomized algorithm.\nRandomizing functions are related to random number generators and hash functions, but have somewhat different requirements and uses, and often need specific algorithms.", "links": ["Algorithm", "Computer science", "Expected value", "Function (mathematics)", "Hash function", "Pseudo-random number generator", "Quicksort", "Random", "Random number generator", "Random seed", "Randomized algorithm", "Set (mathematics)", "Sorting algorithm", "Subroutine"], "categories": ["Algorithms", "All articles lacking sources", "All stub articles", "Articles lacking sources from April 2009", "Computer science stubs"], "title": "Randomization function"}
{"summary": "Within computer science and operations research, many combinatorial optimization problems are computationally intractable to solve exactly (to optimality). Many such problems do admit fast (polynomial time) approximation algorithms\u2014that is, algorithms that are guaranteed to return an approximately optimal solution given any input.\nRandomized rounding (Raghavan & Tompson 1987) is a widely used approach for designing and analyzing such approximation algorithms. The basic idea is to use the probabilistic method to convert an optimal solution of a relaxation of the problem into an approximately optimal solution to the original problem.", "links": ["Algorithm", "Analysis of algorithms", "Approximation algorithm", "Approximation algorithms", "Association for Computing Machinery", "Cambridge University Press", "Combinatorial optimization", "Combinatorica", "Computer science", "Derandomization", "Digital object identifier", "Expected value", "Graph (mathematics)", "Independent set (graph theory)", "Integer linear program", "International Standard Book Number", "Intractability (complexity)", "Journal of Computer and System Sciences", "Linear programming", "Linear programming relaxation", "Markov's inequality", "Mathematical Reviews", "Method of conditional probabilities", "Naive union bound", "Operations research", "Pessimistic estimator", "Polynomial time", "Prabhakar Raghavan", "Probabilistic method", "Rajeev Motwani", "Semi-definite programming", "Set Cover", "Set cover", "Springer Verlag", "Tur\u00e1n's theorem", "Vijay Vazirani"], "categories": ["Algorithms", "All Wikipedia articles needing clarification", "Pages using duplicate arguments in template calls", "Probabilistic arguments", "Wikipedia articles needing clarification from May 2010"], "title": "Randomized rounding"}
{"summary": "Rendezvous or Highest Random Weight (HRW) hashing is an algorithm that allows clients to achieve distributed agreement on a set of k options out of a possible set of n options. A typical application is when clients need to agree on which sites (or proxies) objects are to assigned to. When k is 1, it accomplishes goals similar to consistent hashing, using an entirely different method.", "links": ["Cache Array Routing Protocol", "Consistent hashing", "Digital object identifier", "Distributed hash table", "Hash table", "Hit rate", "Key establishment", "Load balancing (computing)", "MBONE", "Protocol Independent Multicast"], "categories": ["Algorithms", "Hashing"], "title": "Rendezvous hashing"}
{"summary": "Reservoir sampling is a family of randomized algorithms for randomly choosing a sample of k items from a list S containing n items, where n is either a very large or unknown number. Typically n is large enough that the list doesn't fit into main memory.", "links": ["Cube sampling", "Digital object identifier", "Fisher-Yates shuffle", "Jeffrey Vitter", "Main memory", "Mathematical induction", "Moving average", "Python (programming language)", "Randomized algorithm", "Reservoir sampling", "Sampling (statistics)"], "categories": ["Algorithms", "All Wikipedia articles needing clarification", "All articles needing expert attention", "Analysis of algorithms", "Articles needing expert attention from February 2010", "Articles needing expert attention with no reason or talk parameter", "Computing articles needing expert attention", "Probabilistic complexity theory", "Wikipedia articles needing clarification from December 2009"], "title": "Reservoir sampling"}
{"summary": "Rna22 is a pattern-based algorithm for the discovery of microRNA target sites and the corresponding heteroduplexes.\nThe algorithm is conceptually distinct from other methods for predicting microRNA:mRNA heteroduplexes in that it does not use experimentally validated heteroduplexes for training, instead relying only on the sequences of known mature miRNAs that are found in the public databases. The key idea of rna22 is that the reverse complement of any salient sequence features that one can identify in mature microRNA sequences (using pattern discovery techniques) should allow one to identify candidate microRNA target sites in a sequence of interest: rna22 makes use of the Teiresias algorithm to discover such salient features. Once a candidate microRNA target site has been located, the targeting microRNA can be identified with the help of any of several algorithms able to compute RNA:RNA heteroduplexes. A new version (v2.0) of the algorithm is now available: v2.0-beta adds probability estimates to each prediction, gives users the ability to choose the sensitivity/specificity settings on-the-fly, is significantly faster than the original, and can be accessed through http://cm.jefferson.edu/rna22/Interactive/.\nRna22 neither relies on nor imposes any cross-organism conservation constraints to filter out unlikely candidates; this gives it the ability to discover microRNA binding sites that may not be conserved in phylogenetically proximal organisms. Also, as mentioned above, rna22 can identify putative microRNA binding sites without needing to know the identity of the targeting microRNA. A notable property of rna22 is that it does not require the presence of the exact reverse complement of a microRNA's seed in a putative target permitting bulges and G:U wobbles in the seed region of the heteroduplex. Lastly, the algorithm has been shown to achieve high signal-to-noise ratio.\nUse of rna22 led to the discovery of \"non-canonical\" microRNA targets in the coding regions of the mouse Nanog, Oct4 and Sox2. Most of these targets are not conserved in the human orthologues of these three transcription factors even though they reside in the coding region of the corresponding mRNAs. Moreover, most of these targets contain G:U wobbles, one or more bulges, or both, in the seed region of the heteroduplex. In addition to coding regions, rna22 has helped discover non-canonical targets in 3'UTRs.\nA recent study examined the problem of non-canonical miRNA targets using molecular dynamics simulations of the crystal structure of the Argonaute-miRNA:mRNA ternary complex. The study found that several kinds of modifications, including combinations of multiple G:U wobbles and mismatches in the seed region, are admissible and result in only minor structural fluctuations that do not affect the stability of the ternary complex. The study also showed that the findings of the molecular dynamics simulation are supported by HITS-CLIP (CLIP-seq) data. These results suggest that bona fide miRNA targets transcend the canonical seed-model in turn making target prediction tools like rna22 an ideal choice for exploring the newly augmented spectrum of miRNA targets.", "links": ["Digital object identifier", "List of RNA structure prediction software", "MicroRNA", "PubMed Central", "PubMed Identifier", "Teiresias algorithm"], "categories": ["Algorithms", "MicroRNA", "Pattern matching", "RNA"], "title": "RNA22"}
{"summary": "In computer science, run-time algorithm specialization is a methodology for creating efficient algorithms for costly computation tasks of certain kinds. The methodology originates in the field of automated theorem proving and, more specifically, in the Vampire theorem prover project.\nThe idea is inspired by the use of partial evaluation in optimising program translation. Many core operations in theorem provers exhibit the following pattern. Suppose that we need to execute some algorithm  in a situation where a value of  is fixed for potentially many different values of . In order to do this efficiently, we can try to find a specialization of  for every fixed , i.e., such an algorithm , that executing  is equivalent to executing .\nThe specialized algorithm may be more efficient than the generic one, since it can exploit some particular properties of the fixed value . Typically,  can avoid some operations that  would have to perform, if they are known to be redundant for this particular parameter . In particular, we can often identify some tests that are true or false for , unroll loops and recursion, etc.", "links": ["Abstract machine", "Automated theorem proving", "C (programming language)", "Computer science", "Multi-stage programming", "Partial evaluation", "Psyco", "Python (programming language)", "Vampire theorem prover"], "categories": ["Algorithms", "Software optimization"], "title": "Run-time algorithm specialisation"}
{"summary": "In coding theory, the Sardinas\u2013Patterson algorithm is a classical algorithm for determining in polynomial time whether a given variable-length code is uniquely decodable, named after August Albert Sardinas and George W. Patterson, who published it in 1953. The algorithm carries out a systematic search for a string which admits two different decompositions into codewords. As Knuth reports, the algorithm was rediscovered about ten years later in 1963 by Floyd, despite the fact that it was at the time already well known in coding theory.", "links": ["Arto Salomaa", "Block code", "Cambridge University Press", "Coding theory", "Correctness (computer science)", "Digital object identifier", "Donald Knuth", "Empty word", "Formal language", "Immerman\u2013Szelepcs\u00e9nyi theorem", "Institute of Radio Engineers", "International Standard Book Number", "Kraft's inequality", "Mathematical Reviews", "NL-complete", "NL (complexity)", "Nondeterministic turing machine", "Polynomial time", "Prefix (computer science)", "Prefix code", "Recursive definition", "Robert Floyd", "Robert G. Gallager", "Suffix (computer science)", "Suffix tree", "Timeline of information theory", "Variable-length code", "Zentralblatt MATH"], "categories": ["Algorithms", "Coding theory", "Data compression"], "title": "Sardinas\u2013Patterson algorithm"}
{"summary": "In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially \u2013 once through, from start to finish, without other processing executing \u2013 as opposed to concurrently or in parallel. The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap \u2013 many distributed algorithms are both concurrent and parallel \u2013 and thus \"sequential\" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.\n\"Sequential algorithm\" may also refer specifically to an algorithm for decoding a convolutional code.", "links": ["Algorithm", "Computer science", "Concurrent algorithm", "Concurrent computing", "Convolutional code", "Data structure", "Distributed algorithm", "Online algorithm", "Parallel algorithm", "Parallel computing", "Streaming algorithm"], "categories": ["Algorithms", "Algorithms and data structures stubs", "All stub articles", "Computer science stubs"], "title": "Sequential algorithm"}
{"summary": "In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially \u2013 once through, from start to finish, without other processing executing \u2013 as opposed to concurrently or in parallel. The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap \u2013 many distributed algorithms are both concurrent and parallel \u2013 and thus \"sequential\" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.\n\"Sequential algorithm\" may also refer specifically to an algorithm for decoding a convolutional code.", "links": ["Algorithm", "Computer science", "Concurrent algorithm", "Concurrent computing", "Convolutional code", "Data structure", "Distributed algorithm", "Online algorithm", "Parallel algorithm", "Parallel computing", "Streaming algorithm"], "categories": ["Algorithms", "Algorithms and data structures stubs", "All stub articles", "Computer science stubs"], "title": "Sequential algorithm"}
{"summary": "In mathematics, the sieve of Eratosthenes (Ancient Greek: \u03ba\u03cc\u03c3\u03ba\u03b9\u03bd\u03bf\u03bd \u1f18\u03c1\u03b1\u03c4\u03bf\u03c3\u03b8\u03ad\u03bd\u03bf\u03c5\u03c2, k\u00f3skinon Eratosth\u00e9nous), one of a number of prime number sieves, is a simple, ancient algorithm for finding all prime numbers up to any given limit. It does so by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting with the multiples of 2.\nThe multiples of a given prime are generated as a sequence of numbers starting from that prime, with constant difference between them that is equal to that prime. This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime.\nThe sieve of Eratosthenes is one of the most efficient ways to find all of the smaller primes. It is named after Eratosthenes of Cyrene, a Greek mathematician; although none of his works have survived, the sieve was described and attributed to Eratosthenes in the Introduction to Arithmetic by Nicomachus.\nThe sieve may be used to find primes in arithmetic progressions.", "links": ["1 (number)", "AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Analysis of algorithms", "Ancient Egyptian multiplication", "Ancient Greek", "Arithmetic progression", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Big O Notation", "Binary GCD algorithm", "Bit complexity", "CPU cache", "Chakravala method", "Cipolla's algorithm", "Composite number", "Continued fraction factorization", "Coprime", "Cornacchia's algorithm", "David Turner (computer scientist)", "Digital object identifier", "Discrete logarithm", "Divisor", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Eratosthenes", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "Functional programming", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Greek mathematics", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Introduction to Arithmetic", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "List (computing)", "Locality of reference", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Mathematics", "Meissel\u2013Mertens constant", "Mertens' theorems", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Natural number", "Nicomachus", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Prime-counting function", "Prime harmonic series", "Prime number", "Prime number theorem", "Proof of the Euler product formula for the Riemann zeta function", "Proth's theorem", "Pseudo-polynomial time", "Pseudocode", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Random access machine", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Sorenson", "Sieve of Sundaram", "Sieve theory", "Solovay\u2013Strassen primality test", "Special number field sieve", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm", "Wolfram Demonstrations Project"], "categories": ["Algorithms", "All articles needing additional references", "Articles containing Ancient Greek-language text", "Articles containing Greek-language text", "Articles needing additional references from June 2015", "Articles with example pseudocode", "Pages using citations with accessdate and no URL", "Primality tests", "Sieve theory"], "title": "Sieve of Eratosthenes"}
{"summary": "Given an atomic DEVS model, simulation algorithms are methods to generate the model's legal behaviors which are trajectories not to reach to illegal states. (see Behavior of DEVS). [Zeigler84] originally introduced the algorithms that handle time variables related to lifespan  and elapsed time  by introducing two other time variables, last event time, , and next event time  with the following relations:\n\nand\n\nwhere  denotes the current time. And the remaining time,\n\nis equivalently computed as\n\n, apparently .\n\nSince the behavior of a given atomic DEVS model can be defined in two different views depending on the total state and the external transition function (refer to Behavior of DEVS), the simulation algorithms are also introduced in two different views as below.", "links": ["Behavior of Atomic DEVS", "Behavior of DEVS", "Behavior of atomic DEVS", "DEVS", "Simulation Algorithms for Atomic DEVS", "Simulation algorithms for coupled DEVS"], "categories": ["Algorithms"], "title": "Simulation algorithms for atomic DEVS"}
{"summary": "Given a coupled DEVS model, simulation algorithms are methods to generate the model's legal behaviors, which are a set of trajectories not to reach illegal states. (see behavior of a Coupled DEVS model.) [Zeigler84] originally introduced the algorithms that handle time variables related to lifespan  and elapsed time  by introducing two other time variables, last event time, , and next event time  with the following relations:\n\nand\n\nwhere  denotes the current time. And the remaining time,\n\nis equivalently computed as\n\n, apparently .\nBased on these relationships, the algorithms to simulate the behavior of a given Coupled DEVS are written as follows.", "links": ["Behavior of Coupled DEVS", "DEVS", "Simulation Algorithms for Atomic DEVS", "Simulation Algorithms for Coupled DEVS"], "categories": ["Algorithms"], "title": "Simulation algorithms for coupled DEVS"}
{"summary": "In computer science, streaming algorithms are algorithms for processing data streams in which the input is presented as a sequence of items and can be examined in only a few passes (typically just one). These algorithms have limited memory available to them (much less than the input size) and also limited processing time per item.\nThese constraints may mean that an algorithm produces an approximate answer based on a summary or \"sketch\" of the data stream in memory.", "links": ["Adjacency list", "Adjacency matrix", "Andrew McGregor", "Anna C. Gilbert", "Bloom filter", "Christos Papadimitriou", "Communication complexity", "Computer network", "Computer science", "Count-Min sketch", "Data stream", "Data stream clustering", "Data stream mining", "Digital object identifier", "Elephant flow", "Feature hashing", "Gini coefficient", "G\u00f6del Prize", "Hans-Peter Kriegel", "International Standard Book Number", "International Standard Serial Number", "Jennifer Widom", "Join (SQL)", "Journal of Computer and System Sciences", "Lossy Count Algorithm", "Mario Szegedy", "Moving average", "Noga Alon", "Online algorithm", "Online algorithms", "Online machine learning", "Piotr Indyk", "Rajeev Motwani", "Richard Karp", "S. Muthu Muthukrishnan", "Scott Shenker", "Semi-streaming algorithm", "Sequential algorithm", "Statistical classification", "Stochastic gradient descent", "Stream processing", "Symposium on Theory of Computing", "Yossi Matias"], "categories": ["Algorithms", "All articles with unsourced statements", "Articles with unsourced statements from March 2013", "Pages using duplicate arguments in template calls"], "title": "Streaming algorithm"}
{"summary": "In computability theory, super-recursive algorithms are a generalization of ordinary algorithms that are more powerful, that is, compute more than Turing machines. The term was introduced by Mark Burgin, whose book \"Super-recursive algorithms\" develops their theory and presents several mathematical models. Turing machines and other mathematical models of conventional algorithms allow researchers to find properties of recursive algorithms and their computations. In a similar way, mathematical models of super-recursive algorithms, such as inductive Turing machines, allow researchers to find properties of super-recursive algorithms and their computations.\nBurgin, as well as other researchers (including Selim Akl, Eugene Eberbach, Peter Kugel, Jan van Leeuwen, Hava Siegelmann, Peter Wegner, and Ji\u0159\u00ed Wiedermann) who studied different kinds of super-recursive algorithms and contributed to the theory of super-recursive algorithms, have argued that super-recursive algorithms can be used to disprove the Church-Turing thesis, but this point of view has been criticized within the mathematical community and is not widely accepted.", "links": ["Academy of Sciences of the Czech Republic", "Algorithm", "Arithmetical hierarchy", "BioSystems", "Birkh\u00e4user", "Brown University", "Bruce Edmonds", "Bulletin of Symbolic Logic", "Carlos Gershenson", "Church-Turing thesis", "Church\u2013Turing thesis", "Communications of the ACM", "Computability theory", "Computing Reviews", "European Association for Theoretical Computer Science", "FOLDOC", "Halting problem", "Hava Siegelmann", "Hypercomputation", "Information and Control", "International Journal of Foundations of Computer Science", "J. Symb. Logic", "J. Symbolic Logic", "Jan van Leeuwen", "Juraj Hromkovi\u010d", "J\u00fcrgen Schmidhuber", "Kurt G\u00f6del", "Leningrad State University", "Martin Davis", "MathSciNet", "Minds and Machines", "Monatshefte f\u00fcr Mathematik und Physik", "North-Holland Publishing Company", "Odense University", "Principia Mathematica", "Proc. Lond. Math. Soc.", "Quantum algorithms", "Russian Academy of Sciences", "Selim Akl", "Springer Publishing", "Stanford University", "Stephen C. Kleene", "Supertask", "The Computer Journal", "Theoretical Computer Science (journal)", "Theory of everything", "Turing machine", "Turing machines", "Universal Turing machine", "World Scientific", "Zentralblatt MATH"], "categories": ["Algorithms", "Hypercomputation", "Pages using duplicate arguments in template calls", "Theory of computation"], "title": "Super-recursive algorithm"}
{"summary": "The following timeline outlines the development of algorithms (mainly \"mathematical recipes\") since their inception.", "links": ["A* search algorithm", "AKS primality test", "AVL tree", "Abraham Lempel", "Abstract machine", "Achi Brandt", "AdaBoost", "Adi Shamir", "Ahmad al-Qalqashandi", "Aho\u2013Corasick string matching algorithm", "Al-Khawarizmi", "Al-Kindi", "Alan Turing", "Alfred V. Aho", "Algorism", "Algorithm", "Andrew Tridgell", "Andrew Viterbi", "Antoon Bosselaers", "Apriori algorithm", "Arabic numerals", "B-tree", "BFGS method", "Babylonia", "Bart Preneel", "Bellman\u2013Ford algorithm", "Bertram Raphael", "Blum Blum Shub", "Bootstrap aggregating", "Boris Delaunay", "Bor\u016fvka's algorithm", "Boyer\u2013Moore string search algorithm", "Brahmagupta", "Bresenham's line algorithm", "Bruce Schneier", "Bruno Buchberger", "Bruun's FFT algorithm", "Buchberger's algorithm", "Burrows\u2013Wheeler transform", "C. A. R. Hoare", "C. D. Gelatt", "C4.5 algorithm", "CYK algorithm", "Carl Friedrich Gauss", "Carl Pomerance", "Carle David Tolm\u00e9 Runge", "Chakravala method", "Chronology", "Cipher", "Classification and regression tree", "Clifford Cocks", "Cooley\u2013Tukey FFT algorithm", "Corinna Cortes", "Cornelius Lanczos", "Cryptanalysis", "Cylindrical algebraic decomposition", "D. Deutsch", "D. R. Fulkerson", "Daniel Dominic Sleator", "Daniel H. Younger", "Dantzig algorithm", "David A. Huffman", "David Wheeler (computer scientist)", "De Casteljau's algorithm", "Delaunay triangulation", "Deutsch\u2013Jozsa algorithm", "Dijkstra's algorithm", "Donald Knuth", "Donald L. Shell", "Edsger Dijkstra", "Ellipsoid method", "Encryption", "Euclidean algorithm", "Eugene Salamin (mathematician)", "Exponentiation", "Factorization", "Fast Fourier Transform", "Fast multipole method", "Ford\u2013Fulkerson algorithm", "Frequency analysis", "G.C. Danielson", "Gaussian elimination", "General number field sieve", "Genetic algorithm", "Georg Bruun", "George Dantzig", "George E. Collins", "Gradient boosting", "Graham scan", "Grover's algorithm", "Gr\u00f6bner basis", "H. Murakami", "Hans Dobbertin", "Harold H. Seward", "Heapsort", "Hendrik Lenstra", "Huffman coding", "ID3 algorithm", "Ibn al-Haytham", "Igor Pavlov (programmer)", "Integral", "Isaac Newton", "J. H. Morris", "J. W. J. Williams", "Jack E. Bresenham", "Jacob Ziv", "James Cooley", "Jarvis march", "Jerome H. Friedman", "Joe Buhler", "John Henry Holland", "John Kelsey (cryptanalyst)", "John Machin", "John Napier", "John Pollard (mathematician)", "John Tukey", "John von Neumann", "Joseph Kruskal", "Joseph Raphson", "Jurij Vega", "Karatsuba multiplication", "Karmarkar's interior-point algorithm", "Knuth\u2013Bendix completion algorithm", "Knuth\u2013Morris\u2013Pratt algorithm", "Kruskal's algorithm", "L. Blum", "L. R. Ford, Jr.", "LZ77", "LZ78", "LZW (algorithm)", "Larry Page", "Latin", "Latin translations of the 12th century", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Len Adleman", "Leo Breiman", "Leonard Adleman", "Leonid Khachiyan", "Leslie Greengard", "Letter frequencies", "Levenshtein distance", "Linear equation", "Liu Hui", "Lock-free and wait-free algorithms", "Logarithm", "Lov K. Grover", "M. Blum", "M. P. Vecchi", "M. Shub", "Manindra Agrawal", "Margaret J. Corasick", "Maurice Herlihy", "Merge sort", "Michael Burrows", "Multigrid methods", "Narendra Karmarkar", "Neeraj Kayal", "Newton's method", "Nicholas Metropolis", "Niels Ferguson", "Nils Nilsson (researcher)", "Nitin Saxena", "PageRank", "Paul de Casteljau", "Peter B. Bendix", "Peter E. Hart", "Peter Shor", "Plaintext", "Pollard's p - 1 algorithm", "Pollard's rho algorithm", "Prim's algorithm", "Quadratic equation", "Quadratic sieve", "Quasi-Newton method", "Quicksort", "R. A. Jarvis", "R. P. Fedorenko", "RIPEMD-160", "RSA (algorithm)", "Radix sort", "Recipes", "Red-black tree", "Richard Brent (scientist)", "Richard E. Bellman", "Richard Jozsa", "Risch algorithm", "Robert Endre Tarjan", "Robert Henry Risch", "Robert Prim", "Robert Schapire", "Ron Rivest", "Ronald Graham", "Ross Quinlan", "Rsync algorithm", "S. Kirkpatrick", "Salamin\u2013Brent algorithm", "Shell sort", "Shor's algorithm", "Sieve of Eratosthenes", "Simon Singh", "Simplex algorithm", "Simulated annealing", "Special number field sieve", "Splay tree", "Square root", "Strassen algorithm", "Substitution cipher", "Support vector machine", "Tadao Kasami", "Terry Welch", "The Code Book", "The Compendious Book on Calculation by Completion and Balancing", "Transposition cipher", "Turing machine", "Ukkonen's algorithm", "V. Cerny", "Vaughan Pratt", "Viola\u2013Jones object detection framework", "Viterbi algorithm", "Vladimir Levenshtein", "Vladimir Rokhlin (American scientist)", "Vladimir Vapnik", "Volker Strassen", "Wolfgang Hackbusch", "Writing", "Yarrow algorithm", "Yoav Freund"], "categories": ["Algorithms", "Computing timelines", "Mathematics timelines"], "title": "Timeline of algorithms"}
{"summary": "Tomasulo\u2019s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution, designed to efficiently utilize multiple execution units. It was developed by Robert Tomasulo at IBM in 1967, and first implemented in the IBM System/360 Model 91\u2019s floating point unit.\nThe major innovations of Tomasulo\u2019s algorithm include register renaming in hardware, reservation stations for all execution units, and a common data bus (CDB) on which computed values broadcast to all reservation stations that may need them. These developments allow for improved parallel execution of instructions that would otherwise stall under the use of scoreboarding or other earlier algorithms.\nRobert Tomasulo received the Eckert-Mauchly Award in 1997 for his work on the algorithm.", "links": ["Algorithm", "Arithmetic logic unit", "Computer Architecture: A Quantitative Approach", "Computer architecture", "Digital object identifier", "Eckert-Mauchly Award", "Elsevier", "Exception (computing)", "Floating point unit", "Hazard (computer architecture)", "IBM", "IBM System/360", "Instruction level parallelism", "International Standard Book Number", "International Standard Serial Number", "Out-of-order execution", "Parallel computing", "Re-order buffer", "Register renaming", "Reservation station", "Robert Tomasulo", "Scoreboarding"], "categories": ["Algorithms", "Instruction processing", "Wikipedia articles needing clarification from March 2015"], "title": "Tomasulo algorithm"}
{"summary": "In type theory and functional programming, Hindley\u2013Milner (HM) (also known as Damas\u2013Milner or Damas\u2013Hindley\u2013Milner) is a classical type system for the lambda calculus with parametric polymorphism, first described by J. Roger Hindley and later rediscovered by Robin Milner. Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.\nAmong HM's more notable properties is completeness and its ability to deduce the most general type of a given program without the need of any type annotations or other hints supplied by the programmer. Algorithm W is a fast algorithm, performing type inference in almost linear time with respect to the size of the source, making it practically usable to type large programs. HM is preferably used for functional languages. It was first implemented as part of the type system of the programming language ML. Since then, HM has been extended in various ways, most notably by constrained types as used in Haskell.", "links": ["Alpha-renaming", "Anonymous function", "Assignment (mathematical logic)", "Attribute grammar", "Bounded types", "CiteSeer", "Completeness (logic)", "Consistency", "DEXPTIME", "Deductive system", "Digital object identifier", "Disjoint-set data structure", "Equivalence class", "Fixed point combinator", "Formal system", "Free variables and bound variables", "Functional language", "Functional programming", "Harry Mairson", "Haskell (programming language)", "Identity function", "International Standard Book Number", "J. Roger Hindley", "JSTOR", "John Alan Robinson", "Judgment (mathematical logic)", "Lambda calculus", "Linear time", "ML (programming language)", "NP-hard", "Occurs check", "Parametric polymorphism", "Partial order", "Principal type", "Procedure (computer science)", "Prolog", "Proof system", "Robin Milner", "Rule of inference", "Side effect (computer science)", "Syntax", "Syntax-directed", "Term (logic)", "Type annotation", "Type inference", "Type rules", "Type system", "Type theory", "Type variable", "Undecidable problem", "Unification (computer science)", "Unification (computing)"], "categories": ["1969 in computer science", "1978 in computer science", "1985 in computer science", "Algorithms", "All accuracy disputes", "Articles with disputed statements from October 2013", "Formal methods", "Lambda calculus", "Theoretical computer science", "Type inference", "Type systems", "Type theory"], "title": "Hindley\u2013Milner type system"}
{"summary": "In computer programming, the XOR swap is an algorithm that uses the XOR bitwise operation to swap values of distinct variables having the same data type without using a temporary variable. \"Distinct\" means that the variables are stored at different memory addresses; the actual values of the variables do not have to be different.", "links": ["Abelian group", "Algorithm", "Aliasing (computing)", "Amiga CD32", "Associativity", "Bignum", "Binary operation", "Bitwise operation", "Block matrices", "CPU architecture", "C (programming language)", "Call by name", "Commutative operation", "Computer programming", "Cyclic group", "Data type", "Elementary matrix", "Exclusive disjunction", "Feistel cipher", "Identity element", "Instruction-level parallelism", "Instruction pipeline", "Integer overflow", "Inverse element", "Jensen's Device", "Machine code", "Microcontrollers", "Modular arithmetic", "Nibble", "Processor register", "Register allocator", "Register pressure", "Shear mapping", "Swap (computer science)", "Symmetric difference", "System/370", "Variable (programming)", "XOR linked list"], "categories": ["Algorithms", "All articles needing additional references", "Articles needing additional references from February 2012", "Articles with example C code", "Binary arithmetic"], "title": "XOR swap algorithm"}
{"summary": "Xulvi-Brunet and Sokolov\u2019s algorithm generates networks with chosen degree correlations. This method is based on link rewiring, in which the desired degree is governed by parameter \u03c1. By varying this single parameter it is possible to generate networks from random (when \u03c1 = 0) to perfectly assortative or disassortative (when \u03c1 = 1). This algorithm allows to keep network\u2019s degree distribution unchanged when changing the value of \u03c1.", "links": ["Assortativity", "Degree distribution", "Network science"], "categories": ["Algorithms", "All orphaned articles", "Network theory", "Orphaned articles from June 2015"], "title": "Xulvi-Brunet - Sokolov algorithm"}
{"summary": "In mathematics, the Zassenhaus algorithm is a method to calculate a basis for the intersection and sum of two subspaces of a vector space. It is named after Hans Zassenhaus, but no publication of this algorithm by him is known. It is used in computer algebra systems.", "links": ["Basis (linear algebra)", "Block matrix", "Computer algebra system", "Digital object identifier", "Elementary row operations", "Eugene M. Luks", "Hans Julius Zassenhaus", "International Standard Book Number", "Kernel (linear algebra)", "Linear independence", "Linear subspace", "Restriction (mathematics)", "Row echelon form", "Spanning set", "Springer Vieweg Verlag", "Standard basis", "Vector space"], "categories": ["Algorithms", "CS1 German-language sources (de)", "Linear algebra"], "title": "Zassenhaus algorithm"}
{"summary": "In computer science, divide and conquer (D&C) is an algorithm design paradigm based on multi-branched recursion. A divide and conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same (or related) type (divide), until these become simple enough to be solved directly (conquer). The solutions to the sub-problems are then combined to give a solution to the original problem.\nThis divide and conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. Karatsuba), syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFTs).\nUnderstanding and designing D&C algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization. These D&C complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion.\nThe correctness of a divide and conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.", "links": ["Akra\u2013Bazzi method", "Algorithm", "Algorithm design", "Analysis of algorithms", "Anatolii Alexeevitch Karatsuba", "Andrey Kolmogorov", "Arm's-length recursion", "Asymptotic complexity", "Babylonia", "Big O notation", "Binary search", "Bisection algorithm", "Bottom-up design", "Branch and bound", "Breadth first recursion", "Cache-oblivious algorithm", "Call stack", "Carl Friedrich Gauss", "Chart parsing", "Computer science", "Conditional (programming)", "Cooley-Tukey FFT algorithm", "Digital object identifier", "Discrete Fourier transform", "Divide and rule", "Doklady Akademii Nauk SSSR", "Donald Knuth", "Dynamic programming", "Euclidean algorithm", "Fast Fourier transform", "Fibonacci number", "Floating point", "Fork\u2013join model", "Geometric series", "Greatest common divisor", "Heuristic (computer science)", "Hybrid algorithm", "IBM 80 series Card Sorters", "Insertion sort", "International Standard Book Number", "John Mauchly", "John von Neumann", "Karatsuba algorithm", "Loop (computing)", "Loop nest optimization", "Loop unwinding", "MapReduce", "Master theorem", "Mathematical induction", "Memoization", "Memory cache", "Merge sort", "Multiplication algorithm", "Non-Uniform Memory Access", "Numerical algorithm", "Pairwise summation", "Paradigm", "Partial evaluation", "Post office", "Priority queue", "Prune and search", "Queue (data structure)", "Quicksort", "Radix sort", "Recurrence relation", "Recursion", "Recursion (computer science)", "Root-finding algorithm", "Sorting algorithm", "Source code generation", "Stack (data structure)", "Stack overflow", "Strassen algorithm", "Subroutine", "Syntactic analysis", "Tail recursion", "The Art of Computer Programming", "Theorem", "Top-down parser", "Tower of Hanoi", "Virtual memory", "Yuri Petrovich Ofman"], "categories": ["Algorithms", "Operations research", "Optimization algorithms and methods", "Pages with citations lacking titles"], "title": "Divide and conquer algorithms"}
{"summary": "In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially \u2013 once through, from start to finish, without other processing executing \u2013 as opposed to concurrently or in parallel. The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap \u2013 many distributed algorithms are both concurrent and parallel \u2013 and thus \"sequential\" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.\n\"Sequential algorithm\" may also refer specifically to an algorithm for decoding a convolutional code.", "links": ["Algorithm", "Computer science", "Concurrent algorithm", "Concurrent computing", "Convolutional code", "Data structure", "Distributed algorithm", "Online algorithm", "Parallel algorithm", "Parallel computing", "Streaming algorithm"], "categories": ["Algorithms", "Algorithms and data structures stubs", "All stub articles", "Computer science stubs"], "title": "Sequential algorithm"}
{"summary": "In mathematics, the Bareiss algorithm, named after Erwin Bareiss, is an algorithm to calculate the determinant or the echelon form of a matrix with integer entries using only integer arithmetic; any divisions that are performed are guaranteed to be exact (there is no remainder). The method can also be used to compute the determinant of matrices with (approximated) real entries, avoiding the introduction any round-off errors beyond those already present in the input.\nDuring the execution of Bareiss algorithm, every integer that is computed is the determinant of a submatrix of the input matrix. This allows, using Hadamard inequality, to bound the size of these integers. Otherwise, Bareiss algorithm may be viewed as a variant of Gaussian elimination and needs roughly the same number of arithmetic operations.\nIt follows that, for an n \u00d7 n matrix of maximum (absolute) value 2L for each entry, the Bareiss algorithm runs in O(n3) elementary operations with an O(n n/2 2nL) bound on the absolute value of intermediate values needed. Its computational complexity is thus O(n5L2 (log(n)2 + L2)) when using elementary arithmetic or O(n4L (log(n) + L) log(log(n) + L))) by using fast multiplication.\nThe general Bareiss algorithm is distinct from the Bareiss algorithm for Toeplitz matrices.", "links": ["Algorithm", "Analysis of algorithms", "Basic Linear Algebra Subprograms", "Big O notation", "CPU cache", "Cache-oblivious algorithm", "Comparison of linear algebra libraries", "Comparison of numerical analysis software", "Data structure", "Determinant", "Digital object identifier", "Division (mathematics)", "Echelon form", "Erwin Bareiss", "Fast multiplication", "Floating point", "Gaussian elimination", "Hadamard inequality", "Integer", "Mathematics of Computation", "Matrix (mathematics)", "Matrix decomposition", "Matrix multiplication", "Matrix multiplication algorithm", "Multiprocessing", "Numerical linear algebra", "Numerical stability", "Real number", "Remainder", "SIMD", "Sparse matrix", "System of linear equations", "Toeplitz matrix", "Translation lookaside buffer"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer algebra", "Computer science stubs", "Determinants", "Exchange algorithms", "Numerical linear algebra"], "title": "Bareiss algorithm"}
{"summary": "In mathematics, in particular in computational algebra, the Berlekamp\u2013Zassenhaus algorithm is an algorithm for factoring polynomials over the integers, named after Elwyn Berlekamp and Hans Zassenhaus. As a consequence of Gauss's lemma, this amounts to solving the problem also over the rationals.\nThe algorithm starts by finding factorizations over suitable finite fields using Hensel's lemma to lift the solution from modulo a prime p to a convenient power of p. After this the right factors are found as a subset of these. The worst case of this algorithm is exponential in the number of factors.\nVan Hoeij (2002) improved this algorithm by using the LLL algorithm, substantially reducing the time needed to choose the right subsets of mod p factors.", "links": ["Algebra", "Algorithm", "Bell System Technical Journal", "Berlekamp's algorithm", "Computational algebra", "Data structure", "Digital object identifier", "Elwyn Berlekamp", "Finite field", "Gauss's lemma (number theory)", "Hans Zassenhaus", "Hensel's lemma", "Integer", "International Standard Book Number", "JSTOR", "Journal of Number Theory", "LLL algorithm", "MathWorld", "Mathematical Reviews", "Mathematics", "Mathematics of Computation", "Polynomial"], "categories": ["Algebra stubs", "Algorithms and data structures stubs", "All stub articles", "Computer algebra", "Computer science stubs"], "title": "Berlekamp\u2013Zassenhaus algorithm"}
{"summary": "In computer algebra, the Faug\u00e8re F4 algorithm, by Jean-Charles Faug\u00e8re, computes the Gr\u00f6bner basis of an ideal of a multivariate polynomial ring. The algorithm uses the same mathematical principles as the Buchberger algorithm, but computes many normal forms in one go by forming a generally sparse matrix and using fast linear algebra to do the reductions in parallel.\nThe Faug\u00e8re F5 algorithm first calculates the Gr\u00f6bner basis of a pair of generator polynomials of the ideal. Then it uses this basis to reduce the size of the initial matrices of generators for the next larger basis:\n\nIf Gprev is an already computed Gr\u00f6bner basis (f2, \u2026, fm) and we want to compute a Gr\u00f6bner basis of (f1) + Gprev then we will construct matrices whose rows are m f1 such that m is a monomial not divisible by the leading term of an element of Gprev.\n\nThis strategy allows the algorithm to apply two new criteria based on what Faug\u00e8re calls signatures of polynomials. Thanks to these criteria, the algorithm can compute Gr\u00f6bner bases for a large class of interesting polynomial systems, called regular sequences, without ever simplifying a single polynomial to zero\u2014the most time-consuming operation in algorithms that compute Gr\u00f6bner bases. It is also very effective for a large number of non-regular sequences.", "links": ["Algorithm", "ArXiv", "Buchberger algorithm", "Computer algebra", "Data structure", "Digital object identifier", "Gr\u00f6bner basis", "Hidden Field Equations", "Ideal (ring theory)", "International Standard Book Number", "International Standard Serial Number", "Jean-Charles Faug\u00e8re", "Magma computer algebra system", "Maple (software)", "Maple computer algebra system", "Polynomial ring", "Regular sequence", "SINGULAR", "Sage (mathematics software)", "Sparse matrix"], "categories": ["Algorithms and data structures stubs", "All articles with unsourced statements", "All stub articles", "Articles with unsourced statements from February 2013", "Computer algebra", "Computer science stubs"], "title": "Faug\u00e8re's F4 and F5 algorithms"}
{"summary": "In computational number theory and computational algebra, Pollard's kangaroo algorithm (aka Pollard's lambda algorithm, see Naming below) is an algorithm for solving the discrete logarithm problem. The algorithm was introduced in 1978 by the number theorist J. M. Pollard, in the same paper  as his better-known \u03c1 algorithm for solving the same problem. Although Pollard described the application of his algorithm to the discrete logarithm problem in the multiplicative group of units modulo a prime p, it is in fact a generic discrete logarithm algorithm\u2014it will work in any finite cyclic group.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Binary digit", "Chakravala method", "Cipolla's algorithm", "Computational algebra", "Computational complexity theory", "Computational number theory", "Continued fraction factorization", "Cornacchia's algorithm", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Exponential time", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Greek letter", "Index calculus algorithm", "Integer factorization", "Integer square root", "John Pollard (mathematician)", "Kangaroo", "Karatsuba algorithm", "Lambda", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "Pseudorandom", "Public key cryptosystem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "RSA (algorithm)", "Rainbow table", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Scientific American", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Subexponential time", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Treadmill", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Computer algebra", "Logarithms", "Number theoretic algorithms"], "title": "Pollard's kangaroo algorithm"}
{"summary": "A flowchart is a type of diagram that represents an algorithm, workflow or process, showing the steps as boxes of various kinds, and their order by connecting them with arrows. This diagrammatic representation illustrates a solution model to a given problem. Flowcharts are used in analyzing, designing, documenting or managing a process or program in various fields.", "links": ["ASME", "Abraham Haskel Taub", "Activity diagram", "Algorithm", "Allan H. Mogensen", "American Society of Mechanical Engineers", "Augmented transition network", "Baseball field", "Benjamin S. Graham", "Business process mapping", "Check sheet", "Computer algorithm", "Computer programming", "Computer terminal", "Control chart", "Control flow", "Control flow diagram", "Control flow graph", "Coroutine", "DRAKON", "Data flow diagram", "Deployment flowchart", "Diagram", "Diagramming software", "Douglas Hartree", "Douglas Rayner Hartree", "Flow graph", "Flow map", "Flow process chart", "Flowchart (band)", "Flowgorithm (programming language)", "Frank Bunker Gilbreth, Sr.", "Functional flow block diagram", "Graph (mathematics)", "Herman Goldstine", "Herman H. Goldstine", "Hexagon", "Histogram", "Industrial engineering", "Integrated Authority File", "Interactive EasyFlow", "International Standard Book Number", "Ishikawa diagram", "Iteration", "John von Neumann", "Kaoru Ishikawa", "LARP (programming language)", "Lake Placid, New York", "Logic gate", "Markov chain", "Nassi-Shneiderman diagram", "Nassi\u2013Shneiderman diagram", "New York", "Parallelogram", "Pareto chart", "Pentagon", "Petri nets", "Problem solving", "Process architecture", "Procter and Gamble", "Programming language", "Project management", "Pseudo-code", "Pseudocode", "Quadrilateral", "Random walk", "Raptor (programming language)", "Rectangle", "Rectangles", "Recursive transition network", "Rhombus", "Sankey diagram", "Scatter diagram", "Schultheiss, Louis A.", "Source code", "Spreadsheet", "Stadium (geometry)", "Standard Register Industrial", "State diagram", "Swimlane", "Third-generation programming language", "Trapezoid", "Tree structure", "Unified Modeling Language", "VisiRule", "Visual Logic (programming language)", "Warnier/Orr diagram", "Workflow"], "categories": ["Algorithm description languages", "American inventions", "Articles with example code", "CS1 errors: external links", "Computer programming", "Diagrams", "Quality control tools", "Technical communication", "Wikipedia articles with GND identifiers"], "title": "Flowchart"}
{"summary": "In computer programming, pidgin code is a mixture of several programming languages in the same program, or pseudocode that is a mixture of a programming language with natural language descriptions. Hence the name: the mixture is a programming language analogous to a pidgin in natural languages.\nIn numerical computation, mathematical style pseudocode is sometimes called pidgin code, for example pidgin ALGOL (the origin of the concept), pidgin Fortran, pidgin BASIC, pidgin Pascal, and pidgin C. It is a compact and often informal notation that blends syntax taken from a conventional programming language with mathematical notation, typically using set theory and matrix operations, and perhaps also natural language descriptions.\nIt can be understood by a wide range of mathematically trained people, and is used as a way to describe algorithms where the control structure is made explicit at a rather high level of detail, while some data structures are still left at an abstract level, independent of any specific programming language.\nNormally non-ASCII typesetting is used for the mathematical equations, for example by means of TeX or MathML markup, or proprietary Formula editor formats.\nThese are examples of articles that contain mathematical style pseudo code:", "links": ["ALGOL", "ASCII", "Algorithm", "BASIC", "C (programming language)", "Computer programming", "Conjugate gradient method", "Control structure", "Ford-Fulkerson algorithm", "Formula editor", "Fortran", "Gauss\u2013Seidel method", "Generalized minimal residual method", "Jacobi eigenvalue algorithm", "Jacobi method", "Karmarkar's algorithm", "MathML", "Mathematical notation", "Matrix (mathematics)", "Natural language", "Numerical computation", "Particle swarm optimization", "Pascal (programming language)", "Pidgin", "Programming language", "Pseudocode", "Set theory", "Stone method", "Successive over-relaxation", "Symbolic Cholesky decomposition", "Syntax", "TeX", "Tridiagonal matrix algorithm", "Typesetting"], "categories": ["Algorithm description languages", "All articles lacking sources", "All stub articles", "Articles lacking sources from June 2009", "Programming language topic stubs"], "title": "Pidgin code"}
{"summary": "PlusCal (formerly called +CAL) is a formal specification language created by Leslie Lamport, which transpiles to TLA+. In contrast to TLA+'s action-oriented focus on distributed systems, PlusCal most resembles an imperative programming language and is better-suited to specifying sequential algorithms. PlusCal was designed to replace pseudocode, retaining its simplicity while providing a formally-defined and verifiable language. A one-bit clock is written in PlusCal as follows:\n\nPlusCal tools and documentation are found on the PlusCal Algorithm Language page.", "links": ["Action language", "Computer science", "Digital object identifier", "Distributed computing", "Imperative programming", "Leslie Lamport", "Pseudocode", "Sequential algorithm", "Source-to-source compiler", "TLA+"], "categories": ["Algorithm description languages", "All stub articles", "Computer science stubs", "Formal methods", "Formal specification languages"], "title": "PlusCal"}
{"summary": "Pseudocode is an informal high-level description of the operating principle of a computer program or other algorithm.\nIt uses the structural conventions of a programming language, but is intended for human reading rather than machine reading. Pseudocode typically omits details that are essential for machine understanding of the algorithm, such as variable declarations, system-specific code and some subroutines. The programming language is augmented with natural language description details, where convenient, or with compact mathematical notation. The purpose of using pseudocode is that it is easier for people to understand than conventional programming language code, and that it is an efficient and environment-independent description of the key principles of an algorithm. It is commonly used in textbooks and scientific publications that are documenting various algorithms, and also in planning of computer program development, for sketching out the structure of the program before the actual coding takes place.\nNo standard for pseudocode syntax exists, as a program in pseudocode is not an executable program. Pseudocode resembles, but should not be confused with skeleton programs which can be compiled without errors. Flowcharts, drakon-charts and Unified Modeling Language (UML) charts can be thought of as a graphical alternative to pseudocode, but are more spacious on paper.", "links": ["A+ (programming language)", "ALGOL", "APLX", "APL (programming language)", "ASCII", "Algorithm", "AppleScript", "Array programming", "Association for Computing Machinery", "BASIC", "Boilerplate code", "C++", "C (programming language)", "Capital-pi notation", "Capital-sigma notation", "Compiler", "Computer program", "Computer science", "Concept programming", "DRAKON", "Dynamic typing", "Fizz buzz", "Flowchart", "Ford-Fulkerson", "Formula editor", "Fortran", "High-level programming language", "HyperTalk", "Inform", "Java (programming language)", "Lingo (programming language)", "Lisp (programming language)", "Literate programming", "MathCAD", "MathML", "Mathematical game", "Mathematical notation", "Matrix (mathematics)", "Natural language", "Numerical computation", "Pascal (programming language)", "Pidgin code", "Program Design Language", "Programmer", "Programming language", "Python (programming language)", "SQL", "Scientific publication", "Set theory", "Short Code", "Skeleton (computer programming)", "Specification language", "Structured English", "Subroutines", "Syntax", "TeX", "Top-down and bottom-up design", "Typesetting", "Unified Modeling Language", "Variable declaration", "Vienna Development Method", "Z notation"], "categories": ["Algorithm description languages", "All articles lacking sources", "Articles lacking sources from October 2012", "Articles with example pseudocode", "Source code"], "title": "Pseudocode"}
{"summary": "In computer science, divide and conquer (D&C) is an algorithm design paradigm based on multi-branched recursion. A divide and conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same (or related) type (divide), until these become simple enough to be solved directly (conquer). The solutions to the sub-problems are then combined to give a solution to the original problem.\nThis divide and conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. Karatsuba), syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFTs).\nUnderstanding and designing D&C algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization. These D&C complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion.\nThe correctness of a divide and conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations.", "links": ["Akra\u2013Bazzi method", "Algorithm", "Algorithm design", "Analysis of algorithms", "Anatolii Alexeevitch Karatsuba", "Andrey Kolmogorov", "Arm's-length recursion", "Asymptotic complexity", "Babylonia", "Big O notation", "Binary search", "Bisection algorithm", "Bottom-up design", "Branch and bound", "Breadth first recursion", "Cache-oblivious algorithm", "Call stack", "Carl Friedrich Gauss", "Chart parsing", "Computer science", "Conditional (programming)", "Cooley-Tukey FFT algorithm", "Digital object identifier", "Discrete Fourier transform", "Divide and rule", "Doklady Akademii Nauk SSSR", "Donald Knuth", "Dynamic programming", "Euclidean algorithm", "Fast Fourier transform", "Fibonacci number", "Floating point", "Fork\u2013join model", "Geometric series", "Greatest common divisor", "Heuristic (computer science)", "Hybrid algorithm", "IBM 80 series Card Sorters", "Insertion sort", "International Standard Book Number", "John Mauchly", "John von Neumann", "Karatsuba algorithm", "Loop (computing)", "Loop nest optimization", "Loop unwinding", "MapReduce", "Master theorem", "Mathematical induction", "Memoization", "Memory cache", "Merge sort", "Multiplication algorithm", "Non-Uniform Memory Access", "Numerical algorithm", "Pairwise summation", "Paradigm", "Partial evaluation", "Post office", "Priority queue", "Prune and search", "Queue (data structure)", "Quicksort", "Radix sort", "Recurrence relation", "Recursion", "Recursion (computer science)", "Root-finding algorithm", "Sorting algorithm", "Source code generation", "Stack (data structure)", "Stack overflow", "Strassen algorithm", "Subroutine", "Syntactic analysis", "Tail recursion", "The Art of Computer Programming", "Theorem", "Top-down parser", "Tower of Hanoi", "Virtual memory", "Yuri Petrovich Ofman"], "categories": ["Algorithms", "Operations research", "Optimization algorithms and methods", "Pages with citations lacking titles"], "title": "Divide and conquer algorithms"}
{"summary": "In computer science, a sequential algorithm or serial algorithm is an algorithm that is executed sequentially \u2013 once through, from start to finish, without other processing executing \u2013 as opposed to concurrently or in parallel. The term is primarily used to contrast with concurrent algorithm or parallel algorithm; most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap \u2013 many distributed algorithms are both concurrent and parallel \u2013 and thus \"sequential\" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.\n\"Sequential algorithm\" may also refer specifically to an algorithm for decoding a convolutional code.", "links": ["Algorithm", "Computer science", "Concurrent algorithm", "Concurrent computing", "Convolutional code", "Data structure", "Distributed algorithm", "Online algorithm", "Parallel algorithm", "Parallel computing", "Streaming algorithm"], "categories": ["Algorithms", "Algorithms and data structures stubs", "All stub articles", "Computer science stubs"], "title": "Sequential algorithm"}
{"summary": "In computer science and operations research, approximation algorithms are algorithms used to find approximate solutions to optimization problems. Approximation algorithms are often associated with NP-hard problems; since it is unlikely that there can ever be efficient polynomial-time exact algorithms solving NP-hard problems, one settles for polynomial-time sub-optimal solutions. Unlike heuristics, which usually only find reasonably good solutions reasonably fast, one wants provable solution quality and provable run-time bounds. Ideally, the approximation is optimal up to a small constant factor (for instance within 5% of the optimal solution). Approximation algorithms are increasingly being used for problems where exact polynomial-time algorithms are known but are too expensive due to the input size. A typical example for an approximation algorithm is the one for vertex cover in graphs: find an uncovered edge and add both endpoints to the vertex cover, until none remain. It is clear that the resulting cover is at most twice as large as the optimal one. This is a constant factor approximation algorithm with a factor of 2.\nNP-hard problems vary greatly in their approximability; some, such as the bin packing problem, can be approximated within any factor greater than 1 (such a family of approximation algorithms is often called a polynomial time approximation scheme or PTAS). Others are impossible to approximate within any constant, or even polynomial factor unless P = NP, such as the maximum clique problem.\nNP-hard problems can often be expressed as integer programs (IP) and solved exactly in exponential time. Many approximation algorithms emerge from the linear programming relaxation of the integer program.\nNot all approximation algorithms are suitable for all practical applications. They often use IP/LP/Semidefinite solvers, complex data structures or sophisticated algorithmic techniques which lead to difficult implementation problems. Also, some approximation algorithms have impractical running times even though they are polynomial time, for example O(n2156) . Yet the study of even very expensive algorithms is not a completely theoretical pursuit as they can yield valuable insights. A classic example is the initial PTAS for Euclidean TSP due to Sanjeev Arora which had prohibitive running time, yet within a year, Arora refined the ideas into a linear time algorithm. Such algorithms are also worthwhile in some applications where the running times and cost can be justified e.g. computational biology, financial engineering, transportation planning, and inventory management. In such scenarios, they must compete with the corresponding direct IP formulations.\nAnother limitation of the approach is that it applies only to optimization problems and not to \"pure\" decision problems like satisfiability, although it is often possible to conceive optimization versions of such problems, such as the maximum satisfiability problem (Max SAT).\nInapproximability has been a fruitful area of research in computational complexity theory since the 1990 result of Feige, Goldwasser, Lov\u00e1sz, Safra and Szegedy on the inapproximability of Independent Set. After Arora et al. proved the PCP theorem a year later, it has now been shown that Johnson's 1974 approximation algorithms for Max SAT, Set Cover, Independent Set and Coloring all achieve the optimal approximation ratio, assuming P != NP.", "links": ["APX", "Algorithm", "Approximation-preserving reduction", "Approximation Algorithms for NP-Hard problems", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bin packing problem", "Boolean satisfiability problem", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Cambridge University Press", "Charles E. Leiserson", "Clifford Stein", "Combinatorial optimization", "Comparison of optimization software", "Computational biology", "Computer science", "Constant factor approximation algorithm", "Convex minimization", "Convex optimization", "Convex programming", "Criss-cross algorithm", "Cutting-plane method", "David P. Williamson", "David Shmoys", "Davidon\u2013Fletcher\u2013Powell formula", "Decision problem", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Domination analysis", "Dorit S. Hochbaum", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Euclidean traveling salesman problem", "Evolutionary algorithm", "Exact algorithm", "Exchange algorithm", "Exponential time", "Financial engineering", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Gerhard J. Woeginger", "Golden section search", "Gradient", "Gradient descent", "Graph (mathematics)", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic (computer science)", "Heuristic algorithm", "Hill climbing", "Independent set (graph theory)", "Integer programming", "Integer programs", "Integrated Authority File", "International Standard Book Number", "International Standard Serial Number", "Introduction to Algorithms", "Inventory management", "Iterative method", "Johan H\u00e5stad", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Linear programming relaxation", "Local convergence", "Local search (optimization)", "MAX-3SAT", "Marek Karpinski", "Mathematical optimization", "Matroid", "Maximum clique problem", "Maximum satisfiability problem", "Metaheuristic", "Minimum spanning tree", "NP-hard", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Operations research", "Optimization algorithm", "Optimization problem", "PCP theorem", "P = NP", "Penalty method", "Polynomial-time approximation scheme", "Polynomial time", "Polynomial time approximation scheme", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Ronald L. Rivest", "Sanjeev Arora", "Semidefinite programming", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Thomas H. Cormen", "Transportation planning", "Truncated Newton method", "Trust region", "Vertex cover problem", "Vijay Vazirani", "Wolfe conditions"], "categories": ["All articles lacking in-text citations", "Approximation algorithms", "Articles lacking in-text citations from April 2009", "Computational complexity theory", "Wikipedia articles with GND identifiers"], "title": "Approximation algorithm"}
{"summary": "In mathematics, a submodular set function (also known as a submodular function) is a set function whose value, informally, has the property that the difference in the incremental value of the function, that a single element makes when added to an input set , decreases as the size of the input set increases. Submodular functions have a natural diminishing returns property which makes them suitable for many applications, including approximation algorithms, game theory (as functions modeling user preferences) and electrical networks. Recently, submodular functions have also found immense utility in several real world problems in machine learning and artificial intelligence, including automatic summarization, multi-document summarization, feature selection, active learning, sensor placement, image collection summarization and many other domains.", "links": ["Active learning (machine learning)", "Alexander Schrijver", "Approximation algorithms", "Artificial intelligence", "Automatic summarization", "Cambridge University Press", "Closure (mathematics)", "Computer vision", "Concave function", "Convex function", "Diminishing returns", "Directed graph", "Economics", "Electrical network", "Elsevier", "Entropy (information theory)", "Feature selection", "Game theory", "George Nemhauser", "Graph (mathematics)", "Greedy algorithm", "International Standard Book Number", "Jon Lee (mathematician)", "Linear combination", "Local search (optimization)", "L\u00e1szl\u00f3 Lov\u00e1sz", "Machine learning", "Matroid", "Matroid rank", "Max cut", "Maximum coverage problem", "Maximum cut", "Minimum cut", "Multi-document summarization", "Mutual information", "NP-hard", "NP hard", "Optimization problem", "Oxford University Press", "Polymatroid", "Polynomial time", "Power set", "Random variable", "Random variables", "Set (mathematics)", "Set function", "Springer Publishing", "Subadditive set function", "Supermodular function", "Uniform distribution (continuous)", "Uriel Feige", "Utility functions on indivisible goods", "Zentralblatt MATH"], "categories": ["All articles with unsourced statements", "Approximation algorithms", "Articles with unsourced statements from August 2014", "Articles with unsourced statements from November 2013", "Combinatorial optimization", "Matroid theory"], "title": "Submodular set function"}
{"summary": "The alpha max plus beta min algorithm is a high-speed approximation of the square root of the sum of two squares. The square root of the sum of two squares, also known as Pythagorean addition, is a useful function, because it finds the hypotenuse of a right triangle given the two side lengths, the norm of a 2-D vector, or the magnitude of a complex number z=a+bi given the real and imaginary parts.\n\nThe algorithm avoids performing the square and square-root operations, instead using simple operations such as comparison, multiplication, and addition. Some choices of the \u03b1 and \u03b2 parameters of the algorithm allow the multiplication operation to be reduced to a simple shift of binary digits that is particularly well suited to implementation in high-speed digital circuitry.\nThe approximation is expressed as:\n\nWhere  is the maximum absolute value of a and b and  is the minimum absolute value of a and b.\nFor the closest approximation, the optimum values for  and  are  and , giving a maximum error of 3.96%.", "links": ["Alpha\u2013beta pruning", "Complex number", "Hypot", "Hypotenuse", "Imaginary number", "Magnitude (mathematics)", "Minimax", "Norm (mathematics)", "Pythagorean addition", "Real number", "Richard G. Lyons", "Square root", "Vector (geometric)"], "categories": ["All articles with links needing disambiguation", "Approximation algorithms", "Articles with links needing disambiguation from November 2014", "Root-finding algorithms"], "title": "Alpha max plus beta min algorithm"}
{"summary": "In computability theory and computational complexity theory, especially the study of approximation algorithms, an approximation-preserving reduction is an algorithm for transforming one optimization problem into another problem, such that the distance of solutions from optimal is preserved to some degree. Approximation-preserving reductions are a subset of more general reductions in complexity theory; the difference is that approximation-preserving reductions usually make statements on approximation problems or optimization problems, as opposed to decision problems.\nIntuitively, problem A is reducible to problem B via an approximation-preserving reduction if, given an instance of problem A and a (possibly approximate) solver for problem B, one can convert the instance of problem A into an instance of problem B, apply the solver for problem B, and recover a solution for problem A that also has some guarantee of approximation.", "links": ["APX", "Algorithm", "Approximation algorithm", "Complete (complexity)", "Computability theory", "Computational complexity theory", "Computational problem", "Decision problem", "Gap reduction", "L-reduction", "Log-APX", "Optimization problem", "PTAS", "PTAS reduction", "Poly-APX", "Reduction (complexity)"], "categories": ["Approximation algorithms", "Computational complexity theory", "Structural complexity theory"], "title": "Approximation-preserving reduction"}
{"summary": "In complexity theory the class APX (an abbreviation of \"approximable\") is the set of NP optimization problems that allow polynomial-time approximation algorithms with approximation ratio bounded by a constant (or constant-factor approximation algorithms for short). In simple terms, problems in this class have efficient algorithms that can find an answer within some fixed multiplicative factor of the optimal answer. The class APX is also sometimes known as MaxSNP because the basis of its definition is formed by SNP.\nAn approximation algorithm is called a -approximation algorithm for input size  if it can be proven that the solution that the algorithm finds is at most a multiplicative factor of  times worse than the optimal solution. Here,  is called the approximation ratio. Problems in APX are those with algorithms for which the approximation ratio  is a constant . The approximation ratio is conventionally stated greater than 1. In the case of minimization problems,  is the found solution's score divided by the optimum solution's score, while for maximization problems the reverse is the case. For maximization problems, where an inferior solution has a smaller score,  is sometimes stated as less than 1; in such cases, the reciprocal of  is the ratio of the score of the found solution to the score of the optimum solution.\nIf there is a polynomial-time algorithm to solve a problem to within every multiplicative factor of the optimum other than 1, then the problem is said to have a polynomial-time approximation scheme (PTAS). Unless P=NP there exist problems that are in APX but without a PTAS, so the class of problems with a PTAS is strictly contained in APX. One such problem is the bin packing problem.", "links": ["AC0", "ACC0", "ALL (complexity)", "APX (disambiguation)", "Algorithm", "Approximation-preserving reduction", "Approximation algorithm", "Arithmetical hierarchy", "Arthur\u2013Merlin protocol", "BPP (complexity)", "BQP", "Bin packing problem", "Boolean hierarchy", "Boolean satisfiability problem", "CC (complexity)", "Co-NP", "Co-NP-complete", "Complexity Zoo", "Complexity class", "Computational complexity theory", "Conjunctive normal form", "DLOGTIME", "DSPACE", "DTIME", "Dominating set", "ELEMENTARY", "EXPSPACE", "EXPTIME", "Edge coloring", "Exponential hierarchy", "Gerhard J. Woeginger", "Grzegorczyk hierarchy", "IP (complexity)", "Independent set (graph theory)", "Interactive proof system", "L-reduction", "L (complexity)", "List of complexity classes", "MAX-3SAT", "Marek Karpinski", "Max/min CSP/Ones classification theorems", "Metric (mathematics)", "NC (complexity)", "NEXPTIME", "NL (complexity)", "NP-complete", "NP-hard", "NP (complexity)", "NSPACE", "NTIME", "Optimization problem", "P-complete", "PH (complexity)", "PP (complexity)", "PR (complexity)", "PSPACE", "PSPACE-complete", "PTAS", "PTAS reduction", "P (complexity)", "P = NP problem", "Parity P", "Polynomial-time", "Polynomial-time approximation scheme", "Polynomial hierarchy", "Probabilistically checkable proof", "QMA", "RE (complexity)", "RL (complexity)", "RP (complexity)", "R (complexity)", "SC (complexity)", "SL (complexity)", "SNP (complexity)", "Sharp-P", "Sharp-P-complete", "TC0", "Token reconfiguration", "Travelling salesman problem", "UP (complexity)", "Vertex cover", "ZPP (complexity)"], "categories": ["Approximation algorithms", "Complexity classes"], "title": "APX"}
{"summary": "Bidimensionality theory characterizes a broad range of graph problems (bidimensional) that admit efficient approximate, fixed-parameter or kernel solutions in a broad range of graphs. These graph classes include planar graphs, map graphs, bounded-genus graphs and graphs excluding any fixed minor. In particular, bidimensionality theory builds on the graph minor theory of Robertson and Seymour by extending the mathematical results and building new algorithmic tools. The theory was introduced in the work of Demaine, Fomin, Hajiaghayi, and Thilikos, for which the authors received the Nerode Prize in 2015.", "links": ["Apex graph", "ArXiv", "Combinatorica", "Connected dominating set", "Digital object identifier", "Dominating set", "Edge dominating set", "Erik Demaine", "Feedback vertex set", "Graph embedding", "Graph minor", "Halin's grid theorem", "Independent set (graph theory)", "J. ACM", "Kernelization", "Longest path", "Mathematical Reviews", "Maximum internal spanning tree", "Minor (graph theory)", "Neil Robertson (mathematician)", "Nerode Prize", "Parameterized complexity", "Paul Seymour (mathematician)", "Planar graph", "Polynomial-time approximation scheme", "SIAM Journal on Discrete Mathematics", "Treewidth", "Vertex cover"], "categories": ["Analysis of algorithms", "Approximation algorithms", "Graph minor theory", "Parameterized complexity"], "title": "Bidimensionality"}
{"summary": "The goal of the Christofides approximation algorithm (named after Nicos Christofides) is to find a solution to the instances of the traveling salesman problem where the edge weights satisfy the triangle inequality. Let  be an instance of TSP, i.e.  is a complete graph on the set  of vertices with weight function  assigning a nonnegative real weight to every edge of .", "links": ["Approximation algorithm", "Complete graph", "Degree (graph theory)", "Eulerian circuit", "Eulerian path", "Hamiltonian circuit", "Minimum spanning tree", "Multigraph", "Perfect matching", "Pseudo-code", "Traveling salesman problem", "Triangle inequality"], "categories": ["Approximation algorithms", "Graph algorithms", "Spanning tree", "Travelling salesman problem"], "title": "Christofides algorithm"}
{"summary": "Domination analysis of an approximation algorithm is a way to estimate its performance, introduced by Glover and Punnen in 1997. Unlike the classical approximation ratio analysis, which compares the numerical quality of a calculated solution with that of an optimal solution, domination analysis involves examining the rank of the calculated solution in the sorted order of all possible solutions. In this style of analysis, an algorithm is said to have dominance number or domination number K, if there exists a subset of K different solutions to the problem among which the algorithm's output is the best. Domination analysis can also be expressed using a domination ratio, which is the fraction of the solution space that is no better than the given solution; this number always lies within the interval [0,1], with larger numbers indicating better solutions. Domination analysis is most commonly applied to problems for which the total number of possible solutions is known and for which exact solution is difficult.\nFor instance, in the Traveling salesman problem, there are (n-1)! possible solutions for a problem instance with n cities. If an algorithm can be shown to have dominance number close to (n-1)!, or equivalently to have domination ratio close to 1, then it can be taken as preferable to an algorithm with lower dominance number.\nIf it is possible to efficiently find random samples of a problem's solution space, as it is in the Traveling salesman problem, then it is straightforward for a randomized algorithm to find a solution that with high probability has high domination ratio: simply construct a set of samples and select the best solution from among them. (See, e.g., Orlin and Sharma.)\nThe dominance number described here should not be confused with the domination number of a graph, which refers to the number of vertices in the smallest dominating set of the graph.\nRecently, a growing number of articles in which domination analysis has been applied to assess the performance of heuristics has appeared. This kind of analysis may be seen as competing with the classical approximation ratio analysis tradition. The two measures may also be viewed as complementary.", "links": ["Approximation algorithm", "Approximation ratio", "Digital object identifier", "Dominating set", "James B. Orlin", "P = NP problem", "Random sample", "Randomized algorithm", "Traveling salesman problem"], "categories": ["Approximation algorithms"], "title": "Domination analysis"}
{"summary": "In computational complexity theory, a gap reduction is a reduction to a particular type of decision problem, known as a c-gap problem. Such reductions provide information about the hardness of approximating solutions to optimization problems. In short, a gap problem refers to one wherein the objective is to distinguish between cases where the best solution is above one threshold from cases where the best solution is below another threshold, such that the two thresholds have a gap in between. Gap reductions can be used to demonstrate inapproximability results, as if a problem may be approximated to a better factor than the size of gap, then the approximation algorithm can be used to solve the corresponding gap problem.\n\n", "links": ["Approximation-preserving reduction", "Approximation algorithm", "Boolean satisfiability problem", "Computational complexity theory", "Hamiltonian path", "Metric (mathematics)", "Optimization problem", "PTAS reduction", "Reduction (complexity)", "Traveling Salesman problem"], "categories": ["All articles needing expert attention", "All articles that are too technical", "Approximation algorithms", "Articles needing expert attention from December 2014", "Computational problems", "Pages using web citations with no URL", "Wikipedia articles that are too technical from December 2014"], "title": "Gap reduction"}
{"summary": "In computer science, hardness of approximation is a field that studies the algorithmic complexity of finding near-optimal solutions to optimization problems.", "links": ["Approximation algorithm", "Approximation ratio", "Computational complexity theory", "Computer science", "Journal of the ACM", "Luca Trevisan", "Mathematical Reviews", "NP-hard", "NP=P", "Optimization problem", "PCP (complexity)", "PCP theorem", "P = NP", "Polynomial time", "Sartaj Sahni", "Set cover", "Teofilo F. Gonzalez", "Theoretical computer science", "Unique games conjecture", "University of Washington", "Venkatesan Guruswami"], "categories": ["All stub articles", "Approximation algorithms", "Computational complexity theory", "Mathematical optimization", "Theoretical computer science stubs"], "title": "Hardness of approximation"}
{"summary": "In computer science, k-approximation of k-hitting set is an approximation algorithm for weighted hitting set. The input is a collection S of subsets of some universe T and a mapping W from T to non-negative numbers called the weights of the elements of T. In k-hitting set the size of the sets in S cannot be larger than k. That is, . The problem is now to pick some subset T' of T such that every set in S contains some element of T', and such that the total weight of all elements in T' is as small as possible.", "links": ["Approximation algorithm", "Collection (computing)", "Computer science", "David P. Williamson", "Digital object identifier", "Dorit S. Hochbaum", "Duality (mathematics)", "Hitting set", "International Standard Book Number", "Jon Kleinberg", "Linear programming", "Map (mathematics)", "Michel Goemans", "Primal-dual method", "Shimon Even", "Subset", "\u00c9va Tardos"], "categories": ["Approximation algorithms"], "title": "K-approximation of k-hitting set"}
{"summary": "The Karloff\u2013Zwick algorithm, in computational complexity theory, is a randomised approximation algorithm taking an instance of MAX-3SAT Boolean satisfiability problem as input. If the instance is satisfiable, then the expected weight of the assignment found is at least 7/8 of optimal. It provides strong evidence (but not a mathematical proof) that the algorithm performs equally well on arbitrary MAX-3SAT instances. Howard Karloff and Uri Zwick presented the algorithm in 1997.\nFor the related MAX-E3SAT problem, in which all clauses in the input 3SAT formula are guaranteed to have exactly three literals, the simple randomized approximation algorithm which assigns a truth value to each variable independently and uniformly at random satisfies 7/8 of all clauses in expectation, irrespective of whether the original formula is satisfiable. Further, this simple algorithm can also be easily derandomized using the method of conditional expectations. The Karloff\u2013Zwick algorithm, however, does not require the restriction that the input formula should have three literals in every clause.\nBuilding upon previous work on the PCP theorem, Johan H\u00e5stad showed that, assuming P \u2260 NP, no polynomial-time algorithm for MAX 3SAT can achieve a performance ratio exceeding 7/8, even when restricted to satisfiable instances of the problem in which each clause contains exactly three literals. Both the Karloff\u2013Zwick algorithm and the above simple algorithm are therefore optimal in this sense.", "links": ["Algorithm", "Approximation algorithm", "Boolean satisfiability problem", "Computational complexity theory", "Data structure", "Digital object identifier", "Howard Karloff", "Johan H\u00e5stad", "Journal of the ACM", "MAX-3SAT", "Mathematical proof", "Method of conditional probabilities", "PCP theorem", "Randomized algorithm", "Symposium on Foundations of Computer Science", "Uri Zwick"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Approximation algorithms", "CS1 errors: chapter ignored", "Computer science stubs", "Probabilistic complexity theory"], "title": "Karloff\u2013Zwick algorithm"}
{"summary": "In computer science, particularly the study of approximation algorithms, an L-reduction (\"linear reduction\") is a transformation of optimization problems which linearly preserves approximability features; it is one type of approximation-preserving reduction. L-reductions in studies of approximability of optimization problems play a similar role to that of polynomial reductions in the studies of computational complexity of decision problems.\nThe term L reduction is sometimes used to refer to log-space reductions, by analogy with the complexity class L, but this is a different concept.", "links": ["Approximation-preserving reduction", "Approximation algorithms", "Computational complexity theory", "Computer science", "Decision problem", "Digital object identifier", "Dominating set", "International Standard Book Number", "L (complexity)", "Log-space reduction", "MAXSNP", "Optimization problem", "PTAS reduction", "Polynomial-time reduction", "Polynomial time", "Theoretical computer science", "Token reconfiguration"], "categories": ["All stub articles", "Approximation algorithms", "Computational complexity theory", "Theoretical computer science stubs"], "title": "L-reduction"}
{"summary": "In mathematics and computer science, the probabilistic method is used to prove the existence of mathematical objects with desired combinatorial properties. The proofs are probabilistic \u2014 they work by showing that a random object, chosen from some probability distribution, has the desired properties with positive probability. Consequently, they are nonconstructive \u2014 they don't explicitly describe an efficient method for computing the desired objects.\nThe method of conditional probabilities (Erd\u0151s & Selfridge 1973), (Spencer 1987), (Raghavan 1988) converts such a proof, in a \"very precise sense\", into an efficient deterministic algorithm, one that is guaranteed to compute an object with the desired properties. That is, the method derandomizes the proof. The basic idea is to replace each random choice in a random experiment by a deterministic choice, so as to keep the conditional probability of failure, given the choices so far, below 1.\nThe method is particularly relevant in the context of randomized rounding (which uses the probabilistic method to design approximation algorithms).\nWhen applying the method of conditional probabilities, the technical term pessimistic estimator refers to a quantity used in place of the true conditional probability (or conditional expectation) underlying the proof.", "links": ["Approximation algorithm", "Cambridge University Press", "Computer science", "Convex function", "Derandomization", "Deterministic algorithm", "Digital object identifier", "Expected value", "Graph (mathematics)", "Independent set (graph theory)", "International Standard Book Number", "Joel Spencer", "Journal of Combinatorial Theory, Series A", "Journal of Computer and System Sciences", "Mathematical Reviews", "Mathematics", "Max cut", "Maximum cut", "Noga Alon", "Nonconstructive proof", "Paul Erd\u0151s", "Polynomial time", "Prabhakar Raghavan", "Probabilistic method", "Rajeev Motwani", "Randomized rounding", "Springer Verlag", "Tur\u00e1n's theorem", "Vijay Vazirani"], "categories": ["All articles lacking in-text citations", "Approximation algorithms", "Articles lacking in-text citations from June 2012", "Probabilistic arguments"], "title": "Method of conditional probabilities"}
{"summary": "The nearest neighbour algorithm was one of the first algorithms used to determine a solution to the travelling salesman problem. In it, the salesman starts at a random city and repeatedly visits the nearest city until all have been visited. It quickly yields a short tour, but usually not the optimal one.\nBelow is the application of nearest neighbour algorithm on TSP\nThese are the steps of the algorithm:\nstart on an arbitrary vertex as current vertex.\nfind out the shortest edge connecting current vertex and an unvisited vertex V.\nset current vertex to V.\nmark V as visited.\nif all the vertices in domain are visited, then terminate.\nGo to step 2.\nThe sequence of the visited vertices is the output of the algorithm.\nThe nearest neighbour algorithm is easy to implement and executes quickly, but it can sometimes miss shorter routes which are easily noticed with human insight, due to its \"greedy\" nature. As a general guide, if the last few stages of the tour are comparable in length to the first stages, then the tour is reasonable; if they are much greater, then it is likely that there are much better tours. Another check is to use an algorithm such as the lower bound algorithm to estimate if this tour is good enough.\nIn the worst case, the algorithm results in a tour that is much longer than the optimal tour. To be precise, for every constant r there is an instance of the traveling salesman problem such that the length of the tour computed by the nearest neighbour algorithm is greater than r times the length of the optimal tour. Moreover, for each number of cities there is an assignment of distances between the cities for which the nearest neighbor heuristic produces the unique worst possible tour.\nThe nearest neighbour algorithm may not find a feasible tour at all, even when one exists.", "links": ["Algorithm", "K-nearest neighbor algorithm", "Lower bound algorithm", "Nearest neighbor (disambiguation)", "Travelling salesman problem"], "categories": ["Approximation algorithms", "Graph algorithms", "Heuristic algorithms", "Travelling salesman problem"], "title": "Nearest neighbour algorithm"}
{"summary": "The paper is a joint work by Martin Dyer, Alan M. Frieze and Ravindran Kannan.\nThe main result of the paper is a randomized algorithm for finding an  approximation to the volume of a convex body  in -dimensional Euclidean space by assuming the existence of a membership oracle. The algorithm takes time bounded by a polynomial in , the dimension of  and .\nThe algorithm is a sophisticated usage of the so-called Markov chain Monte Carlo (MCMC) method. The basic scheme of the algorithm is a nearly uniform sampling from within  by placing a grid consisting -dimensional cubes and doing a random walk over these cubes. By using the theory of rapidly mixing Markov chains, they show that it takes a polynomial time for the random walk to settle down to being a nearly uniform distribution.", "links": ["Alan M. Frieze", "Digital object identifier", "Markov chain Monte Carlo", "Markov chain mixing time", "Martin Dyer", "Random walk", "Ravindran Kannan"], "categories": ["Approximation algorithms", "Computational geometry"], "title": "Polynomial-time algorithm for approximating the volume of convex bodies"}
{"summary": "In computer science, a polynomial-time approximation scheme (PTAS) is a type of approximation algorithm for optimization problems (most often, NP-hard optimization problems).\nA PTAS is an algorithm which takes an instance of an optimization problem and a parameter \u03b5 > 0 and, in polynomial time, produces a solution that is within a factor 1 + \u03b5 of being optimal (or 1 - \u03b5 for maximization problems). For example, for the Euclidean traveling salesman problem, a PTAS would produce a tour with length at most (1 + \u03b5)L, with L being the length of the shortest tour.\nThe running time of a PTAS is required to be polynomial in n for every fixed \u03b5 but can be different for different \u03b5. Thus an algorithm running in time O(n1/\u03b5) or even O(nexp(1/\u03b5)) counts as a PTAS.", "links": ["APX", "Approximation-preserving reduction", "Approximation algorithm", "Big O notation", "Computer science", "Digital object identifier", "Fixed-parameter tractable", "Gerhard J. Woeginger", "International Standard Book Number", "Knapsack problem", "L-reduction", "List of knapsack problems", "Marek Karpinski", "NP-hard", "Optimization problem", "PTAS reduction", "P = NP problem", "Randomized algorithm", "Sanjeev Arora", "Strongly NP-complete", "Traveling salesman problem"], "categories": ["Approximation algorithms", "Complexity classes"], "title": "Polynomial-time approximation scheme"}
{"summary": "In computer science, a property testing algorithm for a decision problem is an algorithm whose query complexity to its input is much smaller than the instance size of the problem. Typically property testing algorithms are used to decide if some mathematical object (such as a graph or a boolean function) has a \"global\" property, or is \"far\" from having this property, using only a small number of \"local\" queries to the object.\nFor example, the following promise problem admits an algorithm whose query complexity is independent of the instance size (for an arbitrary constant \u03b5 > 0):\n\"Given a graph G on n vertices, decide if G is bipartite, or G cannot be made bipartite even after removing an arbitrary subset of at most  edges of G.\"\nProperty testing algorithms are important in the theory of probabilistically checkable proofs.", "links": ["Algorithm", "ArXiv", "Bipartite graph", "Boolean function", "Computational complexity theory", "Computer science", "Dana Ron", "Decision problem", "Digital object identifier", "Graph (mathematics)", "International Standard Book Number", "Oded Goldreich", "Probabilistically checkable proofs", "Promise problem", "Query complexity", "Randomized algorithm", "Szemer\u00e9di regularity lemma", "Tetration", "Time complexity", "Triangle-free graph"], "categories": ["Approximation algorithms", "Randomized algorithms", "Theoretical computer science"], "title": "Property testing"}
{"summary": "In computational complexity theory, the Unique Games Conjecture is a conjecture made by Subhash Khot in 2002. The conjecture postulates that the problem of determining the approximate value of a certain type of game, known as a unique game, has NP-hard algorithmic complexity. It has broad applications in the theory of hardness of approximation. If it is true, then for many important problems it is not only impossible to get an exact solution in polynomial time (as postulated by the P versus NP problem), but also impossible to get a good polynomial-time approximation. The problems for which such an inapproximability result would hold include constraint satisfaction problems which crop up in a wide variety of disciplines.\nThe conjecture is unusual in that the academic world seems about evenly divided on whether it is true or not.\n\n\"Some very natural, intrinsically interesting statements about things like voting and foams just popped out of studying the UGC.... Even if the UGC turns out to be false, it has inspired a lot of interesting math research.", "links": ["2-satisfiability", "Annals of Mathematics", "Approximation algorithms", "Betweenness", "Computational complexity theory", "Constraint satisfaction problem", "David P. Williamson", "Digital object identifier", "Graph (mathematics)", "Hardness of approximation", "International Standard Book Number", "Johan H\u00e5stad", "List of unsolved problems in computer science", "Madhu Sudan", "Mathematical Reviews", "Maximum cut", "Michel Goemans", "Moses Charikar", "NP-hard", "NP (complexity)", "Oded Regev", "P versus NP problem", "Parallel repetition theorem", "Permutation", "Polynomial time", "Probabilistically checkable proof", "Probability distribution", "Promise problem", "SIAM Journal on Computing", "SIAM Journal on Discrete Mathematics", "Semidefinite programming", "Shmuel Safra", "Subhash Khot", "Venkatesan Guruswami", "Vertex cover"], "categories": ["Approximation algorithms", "Computational complexity theory", "Computational hardness assumptions", "Conjectures", "Unsolved problems in computer science"], "title": "Unique games conjecture"}
{"summary": "In electrical engineering, computer science, statistical computing and bioinformatics, the Baum\u2013Welch algorithm is used to find the unknown parameters of a hidden Markov model (HMM). It makes use of the forward-backward algorithm and is named for Leonard E. Baum and Lloyd R. Welch.\n\n", "links": ["Base-pairs", "Bioinformatics", "C Sharp (programming language)", "Coding theory", "Computer science", "Copy-number variation", "Cryptanalysis", "DNA microarray", "Digital object identifier", "EM algorithm", "Electrical engineering", "Eukaryotic", "Exon", "Forward-backward algorithm", "GENSCAN", "GLIMMER", "Genomics", "Hidden Markov Model", "Hidden Markov model", "Introns", "Isochore (genetics)", "James K. Baker", "Java (programming language)", "Leonard E. Baum", "Lloyd R. Welch", "Locus (genetics)", "MATLAB", "Maximum Likelihood", "Maximum likelihood", "Mendelian inheritance", "Phonemes", "Prokaryotic", "PubMed Central", "PubMed Identifier", "Python (programming language)", "R (programming language)", "Specificity (statistics)", "Speech Recognition", "Speech processing", "Statistical computing", "Structural variations", "Viterbi algorithm"], "categories": ["Bioinformatics algorithms", "Markov models", "Pages using citations with accessdate and no URL", "Statistical algorithms"], "title": "Baum\u2013Welch algorithm"}
{"summary": "In bioinformatics, BLAST for Basic Local Alignment Search Tool is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. A BLAST search enables a researcher to compare a query sequence with a library or database of sequences, and identify library sequences that resemble the query sequence above a certain threshold.\nDifferent types of BLASTs are available according to the query sequences. For example, following the discovery of a previously unknown gene in the mouse, a scientist will typically perform a BLAST search of the human genome to see if humans carry a similar gene; BLAST will identify sequences in the human genome that resemble the mouse gene based on similarity of sequence. The BLAST algorithm and program were designed by Stephen Altschul, Warren Gish, Webb Miller, Eugene Myers, and David J. Lipman at the National Institutes of Health and was published in the Journal of Molecular Biology in 1990 and cited over 50,000 times.", "links": ["AIX operating system", "Algorithm", "Amino acid", "Apple Macintosh", "BLASTZ", "BLAT (bioinformatics)", "BLOSUM62", "BWT", "Bacteria", "Barcode of Life Data Systems", "Basic Local Alignment Search Tool", "Bioinformatics", "Blast (disambiguation)", "Burrows-Wheeler Aligner", "CLC bio", "CS-BLAST", "CUDA", "Computational phylogenetics", "Computer program", "DNA Data Bank of Japan", "DNA sequence", "Database", "David J. Lipman", "David Lipman", "Digital object identifier", "ETBLAST", "Ensembl", "Eugene Koonin", "Eugene Myers", "European Bioinformatics Institute", "European Nucleotide Archive", "ExPASy", "FASTA", "FASTA format", "FPGA", "Field-programmable gate array", "FlyBase", "Formatdb", "GNU/Linux", "GenBank", "Genbank", "Gene Myers", "Gene Ontology", "HMMER", "HTML", "Heuristic", "Heuristic algorithm", "Hidden Markov Models", "Human genome", "InterPro", "International Standard Book Number", "Journal of Molecular Biology", "K-mer", "Linux", "List of biological databases", "List of software categories", "MS-Windows", "Mac OS X", "Makeblastdb", "Message Passing Interface", "Microsoft Windows", "Mitrionics", "Molecular phylogenetics", "Mus musculus", "National Center for Biotechnology Information", "National Institute of Genetics", "National Institutes of Health", "Needleman-Wunsch algorithm", "Nucleotide", "Operating system", "PHI-base", "PSI Protein Classifier", "PatternHunter", "Pennsylvania State University", "Pfam", "Phylogenetic tree", "Plain text", "Primary structure", "Protein", "Protein Data Bank", "Protein Information Resource", "Pthreads", "PubMed Central", "PubMed Identifier", "Public domain", "Reading frame", "SIMD", "Saccharomyces Genome Database", "SciEngines GmbH", "Sequence alignment", "Sequence alignment software", "Sequence database", "Sequencing", "Sequerome", "Short Oligonucleotide Analysis Package", "Smith-Waterman algorithm", "Smith\u2013Waterman algorithm", "Software developer", "Software license", "Software release life cycle", "Solaris (operating system)", "Species", "Stephen Altschul", "Structural motif", "Swiss-Prot", "Swiss Institute of Bioinformatics", "TBLASTx", "The Arabidopsis Information Resource", "TrEMBL", "UNIX", "UniProt", "University of Arizona", "VectorBase", "Warren Gish", "Webb Miller", "WormBase", "XML", "Zebrafish Information Network"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from August 2012", "Bioinformatics algorithms", "Bioinformatics software", "Computational phylogenetics", "Laboratory software", "Public domain software"], "title": "BLAST"}
{"summary": "Blast2GO, first published in 2005, is a bioinformatics software tool for the automatic, high-throughput functional annotation of novel sequence data (genes proteins). It makes use of the BLAST algorithm to identify similar sequences to then transfers existing functional annotation from yet characterised sequences to the novel one. The functional information is represented via the Gene Ontology (GO), a controlled vocabulary of functional attributes. The Gene Ontology, or GO, is a major bioinformatics initiative to unify the representation of gene and gene product attributes across all species.", "links": ["2-D electrophoresis", "BLAST", "Biochip", "Bioinformatics", "Call-map proteomics", "Cheminformatics", "Chemogenomics", "Cognitive genomics", "Comparative genomics", "Computational genomics", "Connectomics", "DNA Data Bank of Japan", "Digital object identifier", "Electrospray ionization", "European Molecular Biology Laboratory", "Expression proteomics", "Functional genomics", "Gene", "Gene Ontology", "Gene product", "Genome project", "Genomics", "Glycomics", "Human Genome Project", "Human Proteome Project", "Immunomics", "Isotope affinity tags", "Lipidomics", "Mass spectrometer", "Matrix-assisted laser desorption ionization", "Matrix-assisted laser desorption ionization-time of flight mass spectrometer", "Metabolomics", "Metagenomics", "Microbiome", "Microfluidic-based tools", "Molecular scanner", "National Institutes of Health", "Nutrigenomics", "Omics", "Paleopolyploidy", "Personal genomics", "Pharmacogenetics", "Pharmacogenomics", "Protein", "Protein function prediction", "Proteomics", "PubMed Central", "PubMed Identifier", "Sanger Centre", "Social genomics", "Species", "Spotted array-based tools", "Structural biology", "Structural genomics", "Structure-based drug design", "Systems biology", "Toxicogenomics", "Transcriptomics"], "categories": ["All stub articles", "Bioinformatics algorithms", "Bioinformatics software", "Bioinformatics stubs", "Genomics", "Laboratory software", "Official website different in Wikidata and Wikipedia", "Omics", "Public domain software"], "title": "Blast2GO"}
{"summary": "Flower pollination algorithm (FPA) is a metaheuristic algorithm that was developed by Xin-She Yang, based on the pollination process of flowering plants. FPA has been applied to solve practical problems in engineering, solar PV parameter estimation, and fuzzy selection for dynamic economic dispatch.", "links": ["Algorithm", "Heuristic (computer science)", "Plants", "Pollination", "Xin-She Yang"], "categories": ["All articles needing expert attention", "Articles needing expert attention from July 2015", "Bioinformatics algorithms", "Computer science articles needing expert attention", "Pollination"], "title": "Flower pollination algorithm"}
{"summary": "In computer science, Hirschberg's algorithm, named after its inventor, Dan Hirschberg, is a dynamic programming algorithm that finds the optimal sequence alignment between two strings. Optimality is measured with the Levenshtein distance, defined to be the sum of the costs of insertions, replacements, deletions, and null actions needed to change one string into the other. Hirschberg's algorithm is simply described as a divide and conquer version of the Needleman\u2013Wunsch algorithm. Hirschberg's algorithm is commonly used in computational biology to find maximal global alignments of DNA and protein sequences.", "links": ["Aho\u2013Corasick algorithm", "Algorithm", "Apostolico\u2013Giancarlo algorithm", "Approximate string matching", "Arg max", "BLAST", "Big O Notation", "Bitap algorithm", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "Commentz-Walter algorithm", "Communications of the ACM", "Comparison of regular expression engines", "Compressed pattern matching", "Computational biology", "Computer science", "DNA", "Damerau\u2013Levenshtein distance", "Dan Hirschberg", "Deterministic acyclic finite state automaton", "Diff", "Digital object identifier", "Directed acyclic word graph", "Divide and conquer algorithm", "Dynamic programming", "Edit distance", "FASTA", "Generalized suffix tree", "Hamming distance", "Heuristic (computer science)", "Jaro\u2013Winkler distance", "Knuth\u2013Morris\u2013Pratt algorithm", "Lee distance", "Levenshtein automaton", "Levenshtein distance", "List of regular expression software", "Longest common subsequence", "Longest common subsequence problem", "Longest common substring", "Mathematical Reviews", "Needleman-Wunsch algorithm", "Needleman\u2013Wunsch algorithm", "Nondeterministic finite automaton", "Parsing", "Pattern matching", "Protein", "Rabin\u2013Karp algorithm", "Rabin\u2013Karp string search algorithm", "Regular expression", "Regular tree grammar", "Rope (data structure)", "Sequence alignment", "Sequential pattern mining", "Smith\u2013Waterman algorithm", "String (computer science)", "String metric", "String searching algorithm", "Suffix array", "Suffix automaton", "Suffix tree", "Ternary search tree", "Thompson's construction", "Trie", "Wagner\u2013Fischer algorithm"], "categories": ["Articles with example pseudocode", "Bioinformatics algorithms", "Dynamic programming", "Sequence alignment algorithms"], "title": "Hirschberg's algorithm"}
{"summary": "The island algorithm is an algorithm for performing inference on hidden Markov models, or their generalization, dynamic Bayesian networks. It calculates the marginal distribution for each unobserved node, conditional on any observed nodes.\nThe island algorithm is a modification of belief propagation. It trades smaller memory usage for longer running time: while belief propagation takes O(n) time and O(n) memory, the island algorithm takes O(n log n) time and O(log n) memory. On a computer with an unlimited number of processors, this can be reduced to O(n) total time, while still taking only O(log n) memory.\n\n", "links": ["Algorithm", "Belief propagation", "Dynamic Bayesian networks", "Hidden Markov models", "Junction tree", "Marginal distribution"], "categories": ["Bioinformatics algorithms", "Hidden Markov models"], "title": "Island algorithm"}
{"summary": "The Kabsch algorithm, named after Wolfgang Kabsch, is a method for calculating the optimal rotation matrix that minimizes the RMSD (root mean squared deviation) between two paired sets of points. It is useful in graphics, cheminformatics to compare molecular structures, and also bioinformatics for comparing protein structures (in particular, see root-mean-square deviation (bioinformatics)).\nThe algorithm only computes the rotation matrix, but it also requires the computation of a translation vector. When both the translation and rotation are actually performed, the algorithm is sometimes called partial Procrustes superimposition (see also orthogonal Procrustes problem).", "links": ["Bioinformatics", "Centroid", "Cheminformatics", "Coordinate system", "Covariance matrix", "Digital object identifier", "Eigen (C++ library)", "Matlab", "Matrix (mathematics)", "Orthogonal Procrustes problem", "Procrustes superimposition", "Protein", "PubMed Identifier", "PyMol", "Python (programming language)", "Quaternion", "RMSD", "Root-mean-square deviation (bioinformatics)", "Root mean square", "Rotation matrix", "Singular value decomposition", "Visual Molecular Dynamics", "Wahba's problem", "Wolfgang Kabsch"], "categories": ["Bioinformatics algorithms"], "title": "Kabsch algorithm"}
{"summary": "The Needleman\u2013Wunsch algorithm is an algorithm used in bioinformatics to align protein or nucleotide sequences. It was one of the first applications of dynamic programming to compare biological sequences. The algorithm was developed by Saul B. Needleman and Christian D. Wunsch and published in 1970. The algorithm essentially divides a large problem (e.g. the full sequence) into a series of smaller problems and uses the solutions to the smaller problems to reconstruct a solution to the larger problem. It is also sometimes referred to as the optimal matching algorithm and the global alignment technique. The Needleman\u2013Wunsch algorithm is still widely used for optimal global alignment, particularly when the quality of the global alignment is of the utmost importance.", "links": ["Aho\u2013Corasick algorithm", "Algorithm", "Apostolico\u2013Giancarlo algorithm", "Approximate string matching", "Array data structure", "BLOSUM", "Bellman equation", "Bioinformatics", "Bitap algorithm", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "Camera resectioning", "Commentz-Walter algorithm", "Comparison of regular expression engines", "Compressed pattern matching", "Computer stereo vision", "DNA sequences", "Damerau\u2013Levenshtein distance", "Deterministic acyclic finite state automaton", "Digital object identifier", "Directed acyclic word graph", "Distortions", "Dynamic programming", "Dynamic time warping", "Edit distance", "Embedded systems", "Gap penalty", "Generalized suffix tree", "Hamming distance", "Hirschberg's algorithm", "Indel", "Jaro\u2013Winkler distance", "Journal of the ACM", "Knuth\u2013Morris\u2013Pratt algorithm", "Lee distance", "Levenshtein automaton", "Levenshtein distance", "List of regular expression software", "Longest common subsequence", "Longest common substring", "Matrix (mathematics)", "Michael J. Fischer", "Nondeterministic finite automaton", "Nucleotide", "Optimal matching", "Parsing", "Pattern matching", "Pixels", "Point accepted mutation", "Protein", "PubMed Central", "PubMed Identifier", "Rabin\u2013Karp algorithm", "Rabin\u2013Karp string search algorithm", "Real-time computing", "Recursion", "Regular expression", "Regular tree grammar", "Reverse Engineering", "Rope (data structure)", "Scan lines", "Sequence alignment", "Sequence mining", "Sequential pattern mining", "Similarity matrix", "Smith-Waterman algorithm", "Smith\u2013Waterman algorithm", "String (computer science)", "String metric", "String searching algorithm", "Suffix array", "Suffix automaton", "Suffix tree", "Ternary search tree", "Thompson's construction", "Trie", "Vladimir Levenshtein", "Wagner\u2013Fischer algorithm"], "categories": ["All articles needing expert attention", "All articles that are too technical", "Articles needing expert attention from September 2013", "Articles with example pseudocode", "Bioinformatics algorithms", "Computational phylogenetics", "Dynamic programming", "Pages using citations with accessdate and no URL", "Pages using web citations with no URL", "Sequence alignment algorithms", "Wikipedia articles that are too technical from September 2013"], "title": "Needleman\u2013Wunsch algorithm"}
{"summary": "In bioinformatics, neighbor joining is a bottom-up (agglomerative) clustering method for the creation of phylogenetic trees, created by Naruya Saitou and Masatoshi Nei in 1987. Usually used for trees based on DNA or protein sequence data, the algorithm requires knowledge of the distance between each pair of taxa (e.g., species or sequences) to form the tree.", "links": ["Autapomorphy", "Bayesian inference in phylogeny", "Bioinformatics", "Bootstrapping (statistics)", "Clade", "Cladistics", "Cluster analysis", "Computation", "Computational phylogenetics", "DNA", "DNA barcoding", "Digital object identifier", "Distance matrices in phylogeny", "Distance matrix", "Evolutionary biology", "Evolutionary grade", "Evolutionary taxonomy", "Ghost lineage", "Greedy algorithm", "Human genetic clustering", "Index of evolutionary biology articles", "International Standard Book Number", "Least squares inference in phylogeny", "Lior Pachter", "List of phylogenetics software", "Long branch attraction", "Masatoshi Nei", "Maximum likelihood", "Maximum parsimony", "Maximum parsimony (phylogenetics)", "Mike Steel (mathematician)", "Molecular clock hypothesis", "Molecular phylogenetics", "Monophyly", "Naruya Saitou", "Nearest neighbor search", "Neighbor-joining", "Neighbour-joining", "Paraphyly", "PhyloCode", "Phylogenetic comparative methods", "Phylogenetic network", "Phylogenetic niche conservatism", "Phylogenetic tree", "Phylogenetic trees", "Phylogenetics", "Phylogenomics", "Phylogeography", "Polyphyly", "Primary structure", "Protein", "PubMed Identifier", "Star network", "Statistical consistency", "Symplesiomorphy", "Synapomorphy", "Taxa", "Taxon", "Three-taxon analysis", "UPGMA"], "categories": ["All articles with unsourced statements", "Articles containing Japanese-language text", "Articles with unsourced statements from November 2012", "Bioinformatics algorithms", "Computational phylogenetics", "Data clustering algorithms", "Phylogenetics"], "title": "Neighbor joining"}
{"summary": "A Pairwise Algorithm  is an algorithmic technique with its origins in Dynamic programming. Pairwise algorithms have several uses including comparing a protein profile (a residue scoring matrix for one or more aligned sequences) against the three translation frames of a DNA strand, allowing frameshifting. The most remarkable feature of PairWise as compared to other Protein-DNA alignment tools is that PairWise allows frameshifting during alignment.", "links": ["Bioinformatics", "Digital object identifier", "Dynamic programming", "Ewan Birney", "Frameshifting", "Open reading frame", "Protein sequence", "PubMed Central", "PubMed Identifier", "Smith\u2013Waterman algorithm", "Translate"], "categories": ["Bioinformatics algorithms"], "title": "Pairwise Algorithm"}
{"summary": "Sequential Pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence. It is usually presumed that the values are discrete, and thus time series mining is closely related, but usually considered a different activity. Sequential pattern mining is a special case of structured data mining.\nThere are several key traditional computational problems addressed within this field. These include building efficient databases and indexes for sequence information, extracting the frequently occurring patterns, comparing sequences for similarity, and recovering missing sequence members. In general, sequence mining problems can be classified as string mining which is typically based on string processing algorithms and itemset mining which is typically based on association rule learning.", "links": ["ASCII", "Alphabet", "Amino acids", "Approximate string matching", "Apriori algorithm", "Association rule learning", "BLAST", "Biology", "ClustalW", "DNA", "DNA sequences", "Data Mining", "Data mining", "Deletion (genetics)", "Digital object identifier", "FreeSpan", "Function (biology)", "GSP Algorithm", "Gene", "Insertion (genetics)", "International Standard Book Number", "MAPres", "Mutations", "Nucleotide", "PrefixSpan", "Process mining", "Protein", "Protein sequences", "PubMed Identifier", "Sequence", "Sequence alignment", "Sequence analysis", "Sequence clustering", "Sequence labeling", "Sequential P\u0410ttern Discovery using Equivalence classes", "String (computer science)", "String searching algorithm", "Structured data mining", "Time series"], "categories": ["Bioinformatics", "Bioinformatics algorithms", "Data mining"], "title": "Sequential pattern mining"}
{"summary": "The Smith\u2013Waterman algorithm performs local sequence alignment; that is, for determining similar regions between two strings or nucleotide or protein sequences. Instead of looking at the total sequence, the Smith\u2013Waterman algorithm compares segments of all possible lengths and optimizes the similarity measure.\nThe algorithm was first proposed by Temple F. Smith and Michael S. Waterman in 1981. Like the Needleman\u2013Wunsch algorithm, of which it is a variation, Smith\u2013Waterman is a dynamic programming algorithm. As such, it has the desirable property that it is guaranteed to find the optimal local alignment with respect to the scoring system being used (which includes the substitution matrix and the gap-scoring scheme). The main difference to the Needleman\u2013Wunsch algorithm is that negative scoring matrix cells are set to zero, which renders the (thus positively scoring) local alignments visible. Backtracking starts at the highest scoring matrix cell and proceeds until a cell with score zero is encountered, yielding the highest scoring local alignment. One does not actually implement the algorithm as described because improved alternatives are now available that have better scaling (Gotoh, 1982)  and are more accurate (Altschul and Erickson, 1986).", "links": ["AMD", "Aho\u2013Corasick algorithm", "Alphabet", "Altivec", "Apostolico\u2013Giancarlo algorithm", "Approximate string matching", "BLAST", "BMC Bioinformatics", "Backtracking", "Big O notation", "Bitap algorithm", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "Bulletin of Mathematical Biology", "CLC bio", "CUDA", "Cell Broadband Engine", "Commentz-Walter algorithm", "Comparison of regular expression engines", "Compressed pattern matching", "Core (microarchitecture)", "Cray", "Damerau\u2013Levenshtein distance", "Deterministic acyclic finite state automaton", "Digital object identifier", "Directed acyclic word graph", "Dynamic programming", "Edit distance", "European Bioinformatics Institute", "Expectation value", "FASTA", "Field-programmable gate array", "GNU Affero General Public License", "GPU", "Gap penalty", "GeForce 8 Series", "Generalized suffix tree", "Graphics processing units", "Hamming distance", "Hirschberg's algorithm", "Homology (biology)", "IBM BladeCenter", "Intel", "Jaro\u2013Winkler distance", "Joint Genome Institute", "Journal of Molecular Biology", "Journal of molecular biology", "Knuth\u2013Morris\u2013Pratt algorithm", "Lawrence Livermore National Laboratory", "Lee distance", "Levenshtein automaton", "Levenshtein distance", "List of regular expression software", "Longest common subsequence", "Longest common substring", "MMX (instruction set)", "Mathematical optimization", "Matrix (mathematics)", "Michael S. Waterman", "NVIDIA", "Needleman\u2013Wunsch algorithm", "Nondeterministic finite automaton", "Nucleotide sequences", "Parsing", "Pattern matching", "Pentium (brand)", "PlayStation 3", "PowerPC", "Protein sequence", "PubMed Central", "PubMed Identifier", "Rabin\u2013Karp algorithm", "Rabin\u2013Karp string search algorithm", "Reconfigurable computing", "Regular expression", "Regular tree grammar", "Rope (data structure)", "SSE2", "SSE4", "SSSE3", "Scalability", "Sequence alignment", "Sequence database", "Sequence mining", "Sequential pattern mining", "String (computer science)", "String metric", "String searching algorithm", "Substitution matrix", "Suffix array", "Suffix automaton", "Suffix tree", "Temple F. Smith", "Ternary search tree", "Thompson's construction", "Trie", "Wagner\u2013Fischer algorithm", "Xeon"], "categories": ["All articles with dead external links", "All articles with unsourced statements", "Articles with dead external links from October 2010", "Articles with unsourced statements from November 2011", "Bioinformatics algorithms", "CS1 errors: external links", "Computational phylogenetics", "Dynamic programming", "Sequence alignment algorithms"], "title": "Smith\u2013Waterman algorithm"}
{"summary": "SPAdes (St. Petersburg genome assembler) is a genome assembly algorithm which was designed for single cell and multi-cells bacterial data sets. However, it might not be suitable for large genomes projects.\nSPAdes works with Ion Torrent, PacBio and Illumina paired-end, mate-pairs and single reads. Recently, SPAdes has been integrated into Galaxy pipelines by Guy Lionel and Philip Mabon.", "links": ["Algorithm", "Antibiotics", "Assembler (bioinformatics)", "Bacterial", "Bioinformatics", "Chimera (genetics)", "DNA", "De Bruijn graph", "Digital object identifier", "E. coli", "Escherichia coli", "Galaxy (computational biology)", "Genetic diversity", "Genome", "Human Microbiome Project", "Illumina (company)", "Ion Torrent", "Iterative", "K-mer", "List of sequence alignment software", "List of software categories", "MALBAC", "Multiple displacement amplification", "Operating system", "PCR", "PubMed Central", "PubMed Identifier", "SMRT sequencing", "Saint Petersburg Academic University", "Sequencing", "Software developer", "Software license", "Software release life cycle", "University of California, San Diego", "Velvet assembler"], "categories": ["Bioinformatics algorithms", "Bioinformatics software", "DNA sequencing", "Metagenomics software", "Pages with duplicate reference names", "Pages with reference errors", "Use mdy dates from October 2013"], "title": "SPAdes (software)"}
{"summary": "UCLUST is an algorithm designed to cluster nucleotide or amino-acid sequences into clusters based on sequence similarity. The algorithm was published in 2010 and implemented in a program also named UCLUST. The algorithm is described by the author as following two simple clustering criteria, in regard to the requested similarity threshold T. The first criterion states that any given cluster's centroid sequence will have a similarity smaller than T to any other clusters' centroid sequence. The second criterion states that each member sequence in a given cluster will have similarity to the cluster's centroid sequence that is equal or greater than T.\nUCLUST algorithm is a greedy one. As a result, the order of the sequences in the input file will have an impact on the resulting clusters and their quality. For this reason, it's advised that the sequences will be sorted before entering clustering stage. The program UCLUST is equipped with some options to sort the input sequences prior to clustering them.\nUCLUST program is widely utilized among the bioinformatic research community, where it used for multiple applications including OTU assignment (e.g. 16s), creating non-redundant gene catalogs, taxonomic assignment and phylogenetic analysis.", "links": ["Algorithm", "Digital object identifier", "International Standard Serial Number", "Sequence clustering"], "categories": ["2010 software", "All articles covered by WikiProject Wikify", "All articles with too few wikilinks", "Articles covered by WikiProject Wikify from March 2015", "Articles with too few wikilinks from March 2015", "Bioinformatics algorithms", "Metagenomics"], "title": "UCLUST"}
{"summary": "In computer science, Ukkonen's algorithm is a linear-time, online algorithm for constructing suffix trees, proposed by Esko Ukkonen in 1995.\nThe algorithm begins with an implicit suffix tree containing the first character of the string. Then it steps through the string adding successive characters until the tree is complete. This order addition of characters gives Ukkonen's algorithm its \"on-line\" property. The original algorithm presented by Peter Weiner proceeded backward from the last character to the first one from the shortest to the longest suffix. A simpler algorithm was found by Edward M. McCreight, going from the longest to the shortest suffix.\nThe naive implementation for generating a suffix tree going forward requires O(n2) or even O(n3) time complexity in big O notation, where n is the length of the string. By exploiting a number of algorithmic techniques, Ukkonen reduced this to O(n) (linear) time, for constant-size alphabets, and O(n log n) in general, matching the runtime performance of the earlier two algorithms.", "links": ["Big O notation", "Computer science", "Digital object identifier", "Edward M. McCreight", "Esko Ukkonen", "Journal of the ACM", "Online algorithm", "Peter Weiner", "Suffix tree", "Symposium on Switching and Automata Theory"], "categories": ["Algorithms on strings", "All stub articles", "Bioinformatics algorithms", "Computer science stubs", "Substring indices"], "title": "Ukkonen's algorithm"}
{"summary": "UPGMA (Unweighted Pair Group Method with Arithmetic Mean) is a simple agglomerative (bottom-up) hierarchical clustering method. It is one of the most popular methods in ecology for the classification of sampling units (such as vegetation plots) on the basis of their pairwise similarities in relevant descriptor variables (such as species composition). In bioinformatics, UPGMA is used for the creation of phenetic trees (phenograms). In a phylogenetic context, UPGMA assumes a constant rate of evolution (molecular clock hypothesis), and is not a well-regarded method for inferring relationships unless this assumption has been tested and justified for the data set being used. UPGMA was initially designed for use in protein electrophoresis studies, but is currently most often used to produce guide trees for more sophisticated phylogenetic reconstruction algorithms.\nThe UPGMA algorithm constructs a rooted tree (dendrogram) that reflects the structure present in a pairwise similarity matrix (or a dissimilarity matrix).\nAt each step, the nearest two clusters are combined into a higher-level cluster. The distance between any two clusters A and B is taken to be the average of all distances between pairs of objects \"x\" in A and \"y\" in B, that is, the mean distance between elements of each cluster:\n\nThe method is generally attributed to Sokal and Michener. Fionn Murtagh found a time optimal  time algorithm to construct the UPGMA tree.", "links": ["Autapomorphy", "Bayesian inference in phylogeny", "Bioinformatics", "Clade", "Cladistics", "Cluster analysis", "Complete-linkage clustering", "Computational phylogenetics", "DNA barcoding", "Dendrogram", "Distance matrices in phylogeny", "Distance matrix", "Ecology", "Evolutionary biology", "Evolutionary grade", "Evolutionary taxonomy", "Ghost lineage", "Hierarchical clustering", "Index of evolutionary biology articles", "Least squares inference in phylogeny", "List of phylogenetics software", "Long branch attraction", "Maximum likelihood", "Maximum parsimony (phylogenetics)", "Models of DNA evolution", "Molecular clock", "Molecular clock hypothesis", "Molecular phylogenetics", "Monophyly", "Neighbor-joining", "Paraphyly", "Phenetic", "PhyloCode", "Phylogenetic comparative methods", "Phylogenetic network", "Phylogenetic niche conservatism", "Phylogenetic tree", "Phylogenetic trees", "Phylogenetics", "Phylogenomics", "Phylogeography", "Polyphyly", "Protein electrophoresis", "Similarity matrix", "Single-linkage clustering", "Symplesiomorphy", "Synapomorphy", "Three-taxon analysis"], "categories": ["All stub articles", "Bioinformatics", "Bioinformatics algorithms", "Bioinformatics stubs", "Computational phylogenetics", "Data clustering algorithms", "Phylogenetics"], "title": "UPGMA"}
{"summary": "Velvet is an algorithm package that has been designed to deal with de novo genome assembly and short read sequencing alignments. This is achieved through the manipulation of de Bruijn graphs for genomic sequence assembly via the removal of errors and the simplification of repeated regions. Velvet has also been implemented inside of commercial packages, such as Geneious, MacVector and BioNumerics.", "links": ["BioNumerics", "Breadth-first search", "C language", "Complementarity (molecular biology)", "Contig", "DNA sequencing", "De Bruijn graph", "Digital object identifier", "Dijkstra's algorithm", "European Bioinformatics Institute", "Ewan Birney", "GNU General Public License", "Genome", "Genome assembly", "International Standard Book Number", "K-mer", "List of software categories", "MacVector", "Mammal", "N50 statistic", "Next-generation sequencing", "Operating system", "Polymorphism (biology)", "Prokaryotes", "PubMed Central", "PubMed Identifier", "SPAdes (software)", "Sequence alignment", "Software developer", "Software license", "Software release life cycle"], "categories": ["Bioinformatics algorithms", "Bioinformatics software", "DNA sequencing", "Metagenomics software"], "title": "Velvet assembler"}
{"summary": "In the field of artificial intelligence, a genetic algorithm (GA) is a search heuristic that mimics the process of natural selection. This heuristic (also sometimes called a metaheuristic) is routinely used to generate useful solutions to optimization and search problems. Genetic algorithms belong to the larger class of evolutionary algorithms (EA), which generate solutions to optimization problems using techniques inspired by natural evolution, such as inheritance, mutation, selection, and crossover.", "links": ["Alex Fraser (scientist)", "Algorithm", "Ant colony optimization", "Artificial evolution", "Artificial intelligence", "Artificial selection", "Associative array", "Average information", "Bacteriologic algorithm", "Bin packing problem", "Bit array", "CMA-ES", "Candidate solution", "Cellular automata", "Chromosomal inversion", "Chromosome", "Cluster analysis", "Computational fluid dynamics", "Computer simulation", "Cross-entropy method", "Crossover (genetic algorithm)", "Cultural algorithm", "Data structure", "David B. Fogel", "Decision problem", "Differential Search Algorithm", "Digital object identifier", "Edge recombination operator", "Emanuel Falkenauer", "Engineering", "Ergodicity", "Evolution strategy", "Evolutionary Computation", "Evolutionary algorithm", "Evolutionary algorithms", "Evolutionary computing", "Evolutionary ecology", "Evolutionary programming", "Evolved antenna", "Evolver (software)", "Extremal optimization", "Fitness (biology)", "Fitness approximation", "Fitness function", "Fitness landscape", "Floating point", "Gaussian adaptation", "Gene expression programming", "Genetic drift", "Genetic operator", "Genetic programming", "Genetic representation", "Genotype", "Global optimization", "Global optimum", "Gray coding", "Grouping genetic algorithm", "Hans-Joachim Bremermann", "Hans-Paul Schwefel", "Harmony search", "Heredity", "Heuristic (computer science)", "Hill climbing", "Holland's Schema Theorem", "Ingo Rechenberg", "Institute for Advanced Study", "Integer", "Integer linear programming", "Intelligent Water Drops", "Interactive evolutionary algorithm", "Interactive evolutionary computation", "International Conference on Machine Learning", "International Standard Book Number", "Iteration", "Jean-Marc J\u00e9z\u00e9quel", "John Henry Holland", "John Koza", "John Markoff", "Knapsack problem", "Lawrence J. Fogel", "Linked list", "List (computing)", "List of genetic algorithm applications", "Local optima", "Local optimum", "Local search (optimization)", "Markov chain", "Mean fitness", "Meme", "Memetic algorithm", "Metaheuristic", "Metaheuristics", "Mutation (genetic algorithm)", "National Diet Library", "Natural selection", "Nils Aall Barricelli", "No free lunch in search and optimization", "Object (computer science)", "Objective function", "Optimization (mathematics)", "Particle filter", "Particle swarm optimization", "Phenotype", "Pittsburgh, Pennsylvania", "Population", "Princeton, New Jersey", "Propagation of schema", "Reactive search optimization", "Schema (genetic algorithms)", "Search algorithm", "Selection (genetic algorithm)", "Simulated annealing", "Space Technology 5", "Springer Science+Business Media", "Steven Skiena", "Stochastic optimization", "Stochastics", "Swarm intelligence", "Tabu search", "The New York Times", "Timeline", "Travelling salesman problem", "Tree (data structure)", "Universal Darwinism", "University of Michigan"], "categories": ["All articles needing additional references", "All articles with dead external links", "All articles with unsourced statements", "Articles needing additional references from May 2011", "Articles with dead external links from September 2011", "Articles with unsourced statements from August 2007", "Articles with unsourced statements from December 2011", "CS1 errors: dates", "CS1 errors: external links", "Cybernetics", "Digital organisms", "Genetic algorithms", "Mathematical optimization", "Optimization algorithms and methods", "Pages using citations with accessdate and no URL", "Search algorithms", "Use dmy dates from July 2013"], "title": "Genetic algorithm"}
{"summary": "In genetic algorithms, a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution to the problem that the genetic algorithm is trying to solve. The set of all solutions is known as the population. The chromosome is often represented as a binary string, although a wide variety of other data structures are also used.\n\n", "links": ["Chromosome", "Crossover (genetic algorithm)", "Data structure", "Digital object identifier", "Genetic algorithm", "Genetic operator", "Mutation (genetic algorithm)", "Selection (genetic algorithm)", "String (computer science)", "Travelling salesman problem"], "categories": ["Genetic algorithms", "Pages using citations with accessdate and no URL"], "title": "Chromosome (genetic algorithm)"}
{"summary": "In artificial immune systems, Clonal selection algorithms are a class of algorithms inspired by the clonal selection theory of acquired immunity that explains how B and T lymphocytes improve their response to antigens over time called affinity maturation. These algorithms focus on the Darwinian attributes of the theory where selection is inspired by the affinity of antigen-antibody interactions, reproduction is inspired by cell division, and variation is inspired by somatic hypermutation. Clonal selection algorithms are most commonly applied to optimization and pattern recognition domains, some of which resemble parallel hill climbing and the genetic algorithm without the recombination operator.", "links": ["Acquired immunity", "Affinity maturation", "Antigen-antibody interaction", "Antigens", "Artificial immune system", "Artificial immune systems", "Biologically inspired computing", "Cell division", "Clonal selection", "Computational immunology", "Computational intelligence", "Digital object identifier", "Evolutionary computation", "Genetic algorithm", "Hill climbing", "Immunocomputing", "Lymphocyte", "Natural computation", "Optimization (mathematics)", "PDF", "Pattern recognition", "Somatic hypermutation", "Swarm intelligence", "Universal Darwinism"], "categories": ["Genetic algorithms"], "title": "Clonal Selection Algorithm"}
{"summary": "In genetic algorithms, crossover is a genetic operator used to vary the programming of a chromosome or chromosomes from one generation to the next. It is analogous to reproduction and biological crossover, upon which genetic algorithms are based. Cross over is a process of taking more than one parent solutions and producing a child solution from them. There are methods for selection of the chromosomes. Those are also given below.", "links": ["Chromosomal crossover", "Chromosome (genetic algorithm)", "Edge recombination operator", "Fitness approximation", "Fitness function", "Fitness proportionate selection", "Genetic algorithm", "Genetic operator", "Hamming distance", "International Standard Book Number", "Mutation (genetic algorithm)", "Reproduction", "Tournament selection", "Traveling salesman problem", "Truncation selection"], "categories": ["All accuracy disputes", "All articles to be expanded", "Articles to be expanded from June 2013", "Articles with disputed statements from June 2014", "Genetic algorithms"], "title": "Crossover (genetic algorithm)"}
{"summary": "Cultural algorithms (CA) are a branch of evolutionary computation where there is a knowledge component that is called the belief space in addition to the population component. In this sense, cultural algorithms can be seen as an extension to a conventional genetic algorithm. Cultural algorithms were introduced by Reynolds (see references).", "links": ["Artificial intelligence", "Artificial life", "Evolutionary computation", "Fitness function", "Genetic algorithm", "Harmony search", "Iteration", "Machine learning", "Memetic algorithm", "Memetics", "Metaheuristic", "Nomr (sociology)", "Population", "Process optimization", "Search algorithm", "Social simulation", "Sociocultural evolution", "Stochastic optimization", "Swarm intelligence"], "categories": ["Evolutionary algorithms", "Genetic algorithms"], "title": "Cultural algorithm"}
{"summary": "In genetic algorithms and genetic programming defining length L(H) is the maximum distance between two defining symbols (that is symbols that have a fixed value as opposed to symbols that can take any value, commonly denoted as # or *) in schema H. In tree GP schemata, L(H) is the number of links in the minimum tree fragment including all the non-= symbols within a schema H.", "links": ["Computer science", "Crossover (genetic algorithm)", "Genetic algorithms", "Genetic programming", "Mutation", "Schema (genetic algorithms)"], "categories": ["All articles needing expert attention", "All articles that are too technical", "All stub articles", "Articles needing expert attention from August 2011", "Computer science stubs", "Genetic algorithms", "Use dmy dates from August 2011", "Wikipedia articles that are too technical from August 2011"], "title": "Defining length"}
{"summary": "The edge recombination operator (ERO) is an operator that creates a path that is similar to a set of existing paths (parents) by looking at the edges rather than the vertices. The main application of this is for crossover in genetic algorithms when a genotype with non-repeating gene sequences is needed such as for the travelling salesman problem. It was described by Darrell Whitley and others in 1989.", "links": ["Adjacency matrix", "Crossover (genetic algorithm)", "Cycle crossover", "Darrell Whitley", "Genetic algorithms", "International Standard Book Number", "Partially mapped crossover", "Path (graph theory)", "Travelling salesman problem", "Union (set theory)", "University of the Basque Country"], "categories": ["All articles needing additional references", "All articles needing expert attention", "Articles needing additional references from June 2011", "Articles needing expert attention from June 2011", "Articles needing expert attention with no reason or talk parameter", "Articles needing unspecified expert attention", "Genetic algorithms"], "title": "Edge recombination operator"}
{"summary": "Evolver is a software package that allows users to solve a wide variety of optimization problems using a genetic algorithm. Launched in 1990, it was the first commercially available genetic algorithm package for personal computers. The program was originally developed by Axcelis, Inc. and is now owned by Palisade Corporation.", "links": ["Axcelis, Inc.", "Genetic algorithm", "Optimization problem", "Palisade Corporation", "Software engineering", "Software package (installation)"], "categories": ["All stub articles", "Genetic algorithms", "Software engineering stubs"], "title": "Evolver (software)"}
{"summary": "A fitness function is a particular type of objective function that is used to summarise, as a single figure of merit, how close a given design solution is to achieving the set aims.\nIn particular, in the fields of genetic programming and genetic algorithms, each design solution is commonly represented as a string of numbers (referred to as a chromosome). After each round of testing, or simulation, the idea is to delete the 'n' worst design solutions, and to breed 'n' new ones from the best design solutions. Each design solution, therefore, needs to be awarded a figure of merit, to indicate how close it came to meeting the overall specification, and this is generated by applying the fitness function to the test, or simulation, results obtained from that solution.\nThe reason that genetic algorithms cannot be considered to be a lazy way of performing design work is precisely because of the effort involved in designing a workable fitness function. Even though it is no longer the human designer, but the computer, that comes up with the final design, it is the human designer who has to design the fitness function. If this is designed badly, the algorithm will either converge on an inappropriate solution, or will have difficulty converging at all.\nMoreover, the fitness function must not only correlate closely with the designer's goal, it must also be computed quickly. Speed of execution is very important, as a typical genetic algorithm must be iterated many times in order to produce a usable result for a non-trivial problem.\nFitness approximation may be appropriate, especially in the following cases:\nFitness computation time of a single solution is extremely high\nPrecise model for fitness computation is missing\nThe fitness function is uncertain or noisy.\nTwo main classes of fitness functions exist: one where the fitness function does not change, as in optimizing a fixed function or testing with a fixed set of test cases; and one where the fitness function is mutable, as in niche differentiation or co-evolving the set of test cases.\nAnother way of looking at fitness functions is in terms of a fitness landscape, which shows the fitness for each possible chromosome.\nDefinition of the fitness function is not straightforward in many cases and often is performed iteratively if the fittest solutions produced by GA are not what is desired. In some cases, it is very hard or impossible to come up even with a guess of what fitness function definition might be. Interactive genetic algorithms address this difficulty by outsourcing evaluation to external agents (normally humans).", "links": ["Chromosome (genetic algorithm)", "Co-evolution", "Crossover (genetic algorithm)", "Evolutionary computation", "Evolutionary robotics", "Figure of merit", "Fitness approximation", "Fitness landscape", "Genetic algorithm", "Genetic programming", "Inferential programming", "Interactive genetic algorithms", "Niche differentiation", "Objective function", "PDF", "Test functions for optimization"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from May 2015", "Genetic algorithms"], "title": "Fitness function"}
{"summary": "Fitness proportionate selection, also known as roulette wheel selection, is a genetic operator used in genetic algorithms for selecting potentially useful solutions for recombination.\nIn fitness proportionate selection, as in all selection methods, the fitness function assigns a fitness to possible solutions or chromosomes. This fitness level is used to associate a probability of selection with each individual chromosome. If  is the fitness of individual  in the population, its probability of being selected is , where  is the number of individuals in the population.\nThis could be imagined similar to a Roulette wheel in a casino. Usually a proportion of the wheel is assigned to each of the possible selections based on their fitness value. This could be achieved by dividing the fitness of a selection by the total fitness of all the selections, thereby normalizing them to 1. Then a random selection is made similar to how the roulette wheel is rotated.\nWhile candidate solutions with a higher fitness will be less likely to be eliminated, there is still a chance that they may be. Contrast this with a less sophisticated selection algorithm, such as truncation selection, which will eliminate a fixed percentage of the weakest candidates. With fitness proportionate selection there is a chance some weaker solutions may survive the selection process; this is an advantage, as though a solution may be weak, it may include some component which could prove useful following the recombination process.\nThe analogy to a roulette wheel can be envisaged by imagining a roulette wheel in which each candidate solution represents a pocket on the wheel; the size of the pockets are proportionate to the probability of selection of the solution. Selecting N chromosomes from the population is equivalent to playing N games on the roulette wheel, as each candidate is drawn independently.\nOther selection techniques, such as stochastic universal sampling or tournament selection, are often used in practice. This is because they have less stochastic noise, or are fast, easy to implement and have a constant selection pressure [Blickle, 1996].\nThe naive implementation is carried out by first generating the cumulative probability distribution (CDF) over the list of individuals using a probability proportional to the fitness of the individual. A uniform random number from the range [0,1) is chosen and the inverse of the CDF for that number gives an individual. This corresponds to the roulette ball falling in the bin of an individual with a probability proportional to its width. The \"bin\" corresponding to the inverse of the uniform random number can be found most quickly by using a binary search over the elements of the CDF. It takes in the O(log n) time to choose an individual. A faster alternative that generates individuals in O(1) time will be to use the alias method.\nRecently, a very simple O(1) algorithm was introduced that is based on \"stochastic acceptance\". The algorithm randomly selects an individual (say ) and accepts the selection with probability , where  is the maximum fitness in the population. Certain analysis indicates that the stochastic acceptance version has a considerably better performance than versions based on linear or binary search, especially in applications where fitness values might change during the run.", "links": ["Alias method", "Big O notation", "Binary search algorithm", "Chromosome", "Cumulative probability distribution function", "Fitness function", "Genetic algorithm", "Genetic operator", "Probability", "Reward-based selection", "Stochastic universal sampling", "Tournament selection", "Truncation selection", "Uniform distribution (continuous)"], "categories": ["Genetic algorithms"], "title": "Fitness proportionate selection"}
{"summary": "In the field of artificial intelligence, a genetic algorithm (GA) is a search heuristic that mimics the process of natural selection. This heuristic (also sometimes called a metaheuristic) is routinely used to generate useful solutions to optimization and search problems. Genetic algorithms belong to the larger class of evolutionary algorithms (EA), which generate solutions to optimization problems using techniques inspired by natural evolution, such as inheritance, mutation, selection, and crossover.", "links": ["Alex Fraser (scientist)", "Algorithm", "Ant colony optimization", "Artificial evolution", "Artificial intelligence", "Artificial selection", "Associative array", "Average information", "Bacteriologic algorithm", "Bin packing problem", "Bit array", "CMA-ES", "Candidate solution", "Cellular automata", "Chromosomal inversion", "Chromosome", "Cluster analysis", "Computational fluid dynamics", "Computer simulation", "Cross-entropy method", "Crossover (genetic algorithm)", "Cultural algorithm", "Data structure", "David B. Fogel", "Decision problem", "Differential Search Algorithm", "Digital object identifier", "Edge recombination operator", "Emanuel Falkenauer", "Engineering", "Ergodicity", "Evolution strategy", "Evolutionary Computation", "Evolutionary algorithm", "Evolutionary algorithms", "Evolutionary computing", "Evolutionary ecology", "Evolutionary programming", "Evolved antenna", "Evolver (software)", "Extremal optimization", "Fitness (biology)", "Fitness approximation", "Fitness function", "Fitness landscape", "Floating point", "Gaussian adaptation", "Gene expression programming", "Genetic drift", "Genetic operator", "Genetic programming", "Genetic representation", "Genotype", "Global optimization", "Global optimum", "Gray coding", "Grouping genetic algorithm", "Hans-Joachim Bremermann", "Hans-Paul Schwefel", "Harmony search", "Heredity", "Heuristic (computer science)", "Hill climbing", "Holland's Schema Theorem", "Ingo Rechenberg", "Institute for Advanced Study", "Integer", "Integer linear programming", "Intelligent Water Drops", "Interactive evolutionary algorithm", "Interactive evolutionary computation", "International Conference on Machine Learning", "International Standard Book Number", "Iteration", "Jean-Marc J\u00e9z\u00e9quel", "John Henry Holland", "John Koza", "John Markoff", "Knapsack problem", "Lawrence J. Fogel", "Linked list", "List (computing)", "List of genetic algorithm applications", "Local optima", "Local optimum", "Local search (optimization)", "Markov chain", "Mean fitness", "Meme", "Memetic algorithm", "Metaheuristic", "Metaheuristics", "Mutation (genetic algorithm)", "National Diet Library", "Natural selection", "Nils Aall Barricelli", "No free lunch in search and optimization", "Object (computer science)", "Objective function", "Optimization (mathematics)", "Particle filter", "Particle swarm optimization", "Phenotype", "Pittsburgh, Pennsylvania", "Population", "Princeton, New Jersey", "Propagation of schema", "Reactive search optimization", "Schema (genetic algorithms)", "Search algorithm", "Selection (genetic algorithm)", "Simulated annealing", "Space Technology 5", "Springer Science+Business Media", "Steven Skiena", "Stochastic optimization", "Stochastics", "Swarm intelligence", "Tabu search", "The New York Times", "Timeline", "Travelling salesman problem", "Tree (data structure)", "Universal Darwinism", "University of Michigan"], "categories": ["All articles needing additional references", "All articles with dead external links", "All articles with unsourced statements", "Articles needing additional references from May 2011", "Articles with dead external links from September 2011", "Articles with unsourced statements from August 2007", "Articles with unsourced statements from December 2011", "CS1 errors: dates", "CS1 errors: external links", "Cybernetics", "Digital organisms", "Genetic algorithms", "Mathematical optimization", "Optimization algorithms and methods", "Pages using citations with accessdate and no URL", "Search algorithms", "Use dmy dates from July 2013"], "title": "Genetic algorithm"}
{"summary": "In computer programming, gene expression programming (GEP) is an evolutionary algorithm that creates computer programs or models. These computer programs are complex tree structures that learn and adapt by changing their sizes, shapes, and composition, much like a living organism. And like living organisms, the computer programs of GEP are also encoded in simple linear chromosomes of fixed length. Thus, GEP is a genotype-phenotype system, benefiting from a simple genome to keep and transmit the genetic information and a complex phenotype to explore the environment and adapt to it.", "links": ["Artificial intelligence", "Artificial neural network", "Boolean algebra (logic)", "C (programming language)", "Chromosome", "Computer program", "Computer programming", "Confusion matrix", "Correlation and dependence", "Crossover (genetic algorithm)", "DNA double helix", "Decision trees", "Evolution strategies", "Evolutionary algorithm", "Evolutionary algorithms", "Exclusive or", "Expression tree", "F-measure", "Fitness function", "Fitness landscape", "Fortran", "Gene", "GeneXproTools", "Gene expression programming", "Genetic algorithms", "Genetic operator", "Genetic operators", "Genetic programming", "Genotype", "Genotype-phenotype distinction", "Gepsoft", "Google Code", "Hinge loss", "International Standard Book Number", "Jaccard similarity", "Java (programming language)", "Karva notation", "Logic synthesis", "Logistic regression", "Loss function", "Machine learning", "Matthews correlation coefficient", "Maximum likelihood estimation", "Mean absolute error", "Mean squared error", "Mutation (genetic algorithm)", "Parse tree", "Pearson product-moment correlation coefficient", "Phenotype", "Precision and recall", "Predictive analytics", "Python (programming language)", "R-square", "Receiver operating characteristic", "Recursion (computer science)", "Regression analysis", "Root mean squared error", "Roulette-wheel selection", "Sensitivity and specificity", "SourceForge", "Statistical classification", "Time series prediction", "Transposition (genetics)", "Tree structure"], "categories": ["Evolutionary algorithms", "Evolutionary computation", "Gene expression programming", "Genetic algorithms", "Genetic programming", "Wikipedia articles with possible conflicts of interest from November 2012"], "title": "Gene expression programming"}
{"summary": "To be competitive, corporations must minimize inefficiencies and maximize productivity. In manufacturing, productivity is inherently linked to how well you can optimize the resources you have, reduce waste and increase efficiency. Finding the best way to maximize efficiency in a manufacturing process can be extremely complex. Even on simple projects, there are multiple inputs, multiple steps, many constraints and limited resources. In general a resource constrained scheduling problem consists of:\nA set of jobs that must be executed\nA finite set of resources that can be used to complete each job\nA set of constraints that must be satisfied\nTemporal Constraints\u2013the time window to complete the task\nProcedural Constraints\u2013the order each task must be completed\nResource Constraints - is the resource available\n\nA set of objectives to evaluate the scheduling performance\nA typical factory floor setting is a good example of this where scheduling which jobs need to be completed on which machines, by which employees in what order and at what time. In very complex problems such as scheduling there is no known way to get to a final answer, so we resort to searching for it trying to find a \u201cgood\u201d answer. Scheduling problems most often use heuristic algorithms to search for the optimal solution. Heuristic search methods suffer as the inputs become more complex and varied. This type of problem is known in computer science as an NP-Hard problem. This means that there are no known algorithms for finding an optimal solution in polynomial time.\n\nGenetic algorithms are well suited to solving production scheduling problems, because unlike heuristic methods genetic algorithms operate on a population of solutions rather than a single solution. In production scheduling this population of solutions consists of many answers that may have different sometimes conflicting objectives. For example, in one solution we may be optimizing a production process to be completed in a minimal amount of time. In another solution we may be optimizing for a minimal amount of defects. By cranking up the speed at which we produce we may run into an increase in defects in our final product.\nAs we increase the number of objectives we are trying to achieve we also increase the number of constraints on the problem and similarly increase the complexity. Genetic algorithms are ideal for these types of problems where the search space is large and the number of feasible solutions is small.\n\nTo apply a genetic algorithm to a scheduling problem we must first represent it as a genome. One way to represent a scheduling genome is to define a sequence of tasks and the start times of those tasks relative to one another. Each task and its corresponding start time represents a gene.\nA specific sequence of tasks and start times (genes) represents one genome in our population. To make sure that our genome is a feasible solution we must take care that it obeys our precedence constraints. We generate an initial population using random start times within the precedence constraints. With genetic algorithms we then take this initial population and cross it, combining genomes along with a small amount of randomness (mutation). The offspring of this combination is selected based on a fitness function that includes one or many of our constraints, such as minimizing time and minimizing defects. We let this process continue either for a pre-allotted time or until we find a solution that fits our minimum criteria. Overall each successive generation will have a greater average fitness i.e. taking less time with higher quality than the preceding generations. In scheduling problems, as with other genetic algorithm solutions, we must make sure that we do not select offspring that are infeasible, such as offspring that violate our precedence constraint. We of course may have to add further fitness values such as minimizing costs however each constraint that we add greatly increases the search space and lowers the number of solutions that are good matches.", "links": ["Candidate solution", "Computer science", "Finite set", "Fitness function", "Genetic algorithm", "Genetic algorithm in economics", "Job Shop Scheduling", "NP-hard", "Quality control and genetic algorithms", "Scheduling (production processes)"], "categories": ["All articles covered by WikiProject Wikify", "All pages needing cleanup", "Articles covered by WikiProject Wikify from September 2009", "Genetic algorithms", "Operations research", "Pages missing lead section", "Production and manufacturing", "Wikipedia introduction cleanup from September 2009"], "title": "Genetic algorithm scheduling"}
{"summary": "Genetic algorithms have increasingly been applied to economics since the pioneering work by John H. Miller in 1986. It has been used to characterize a variety of models including the cobweb model, the overlapping generations model, game theory, schedule optimization and asset pricing. Specifically, it has been used as a model to represent learning, rather than as a means for fitting a model.", "links": ["Asset pricing", "Cobweb model", "Game theory", "Genetic algorithm", "Genetic algorithm scheduling", "Overlapping generations model", "Rational expectations"], "categories": ["Computational economics", "Econometrics", "Genetic algorithms", "Optimization algorithms and methods", "Production economics"], "title": "Genetic algorithms in economics"}
{"summary": "Genetic fuzzy systems are fuzzy systems constructed by using genetic algorithms or genetic programming, which mimic the process of natural evolution, to identify its structure and parameter.\nWhen it comes to automatically identifying and building a fuzzy system, given the high degree of nonlinearity of the output, traditional linear optimization tools have several limitations. Therefore, in the framework of soft computing, genetic algorithms (GAs) and genetic programming (GP) methods have been used successfully to identify structure and parameters of fuzzy systems.", "links": ["Fuzzy system", "Genetic algorithms", "George Klir", "Linguistics", "Multi-objective optimization", "Pareto efficiency"], "categories": ["All Wikipedia articles needing context", "All pages needing cleanup", "Computational linguistics", "Genetic algorithms", "Wikipedia articles needing context from October 2009", "Wikipedia introduction cleanup from October 2009"], "title": "Genetic fuzzy systems"}
{"summary": "In computer science, genetic memory refers to an artificial neural network combination of genetic algorithm and the mathematical model of sparse distributed memory. It can be used to predict weather patterns. Genetic memory and genetic algorithms have also gained an interest in the creation of artificial life.", "links": ["Artificial neural network", "Computer science", "Digital object identifier", "Genetic algorithm", "International Standard Book Number", "PubMed Identifier", "Sparse distributed memory"], "categories": ["All stub articles", "Computer science stubs", "Genetic algorithms"], "title": "Genetic memory (computer science)"}
{"summary": "A genetic operator is an operator used in genetic algorithms to guide the algorithm towards a solution to a given problem. There are three main types of operators (mutation, crossover and selection), which must work in conjunction with one another in order for the algorithm to be successful. Genetic operators are used to create and maintain genetic diversity (mutation operator), combine existing solutions (also known as chromosomes) into new solutions (crossover) and select between solutions (selection). In his book discussing the use of genetic programming for the optimization of complex problems, computer scientist John Koza has also identified an 'inversion' or 'permutation' operator; however, the effectiveness of this operator has never been conclusively demonstrated and this operator is rarely discussed.\nMutation (or mutation-like) operators are said to be unary operators, as they only operate on one chromosome at a time. In contrast, crossover operators are said to be binary operators, as they operate on two chromosomes at a time, combining two existing chromosomes into one new chromosome.", "links": ["Binary operation", "Chromosome (genetic algorithm)", "Crossover (genetic algorithm)", "Evolution", "Fitness function", "Fitness proportionate selection", "Gaussian distribution", "Genetic algorithm", "Genetic algorithms", "Genetic diversity", "Genetic programming", "International Standard Book Number", "John Koza", "Local minimum", "Mutation (genetic algorithm)", "Objective function", "Operator (programming)", "Random walk", "Selection (genetic algorithm)", "Survival of the fittest", "Tournament selection", "Travelling salesman problem", "Unary operation", "Uniform distribution (continuous)"], "categories": ["Genetic algorithms"], "title": "Genetic operator"}
{"summary": "In artificial intelligence, genetic programming (GP) is an evolutionary algorithm-based methodology inspired by biological evolution to find computer programs that perform a user-defined task. Essentially GP is a set of instructions and a fitness function to measure how well a computer has performed a task. It is a specialization of genetic algorithms (GA) where each individual is a computer program. It is a machine learning technique used to optimize a population of computer programs according to a fitness landscape determined by a program's ability to perform a given computational task.\n\n", "links": ["Artificial intelligence", "Assembly language", "Bio-inspired computing", "Biological evolution", "Cartesian genetic programming", "Computer program", "Douglas Lenat", "Eurisko", "Evolution strategies", "Evolutionary algorithm", "Evolvable hardware", "Fail-safe", "Finite-state machine", "Fitness approximation", "Fitness landscape", "Functional programming", "Gene expression programming", "Generic programming", "Genetic algorithms", "Genetic engineering", "Genetic representation", "Gianna Giavelli", "Grammatical evolution", "Imperative languages", "Inductive programming", "Ingo Rechenberg", "Intron", "John Henry Holland", "John Koza", "J\u00fcrgen Schmidhuber", "Lawrence J. Fogel", "Learning classifier system", "Linear genetic programming", "Lisp (programming language)", "Machine learning", "Markov decision process", "Meta learning (computer science)", "Moore's law", "Multi expression programming", "Nichael L. Cramer", "Nils Aall Barricelli", "Peter Nordin", "PhD", "Programming language", "Propagation of schema", "Quantum computer", "Search algorithm", "Sorting", "Tree structure"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from December 2014", "CS1 Italian-language sources (it)", "Evolutionary algorithms", "Genetic algorithms", "Genetic programming", "Mathematical optimization", "Operations research"], "title": "Genetic programming"}
{"summary": "Holland's schema theorem, also called the fundamental theorem of genetic algorithms, is widely taken to be the foundation for explanations of the power of genetic algorithms. It says that short, low-order schemata with above-average fitness increase exponentially in successive generations. The theorem was proposed by John Holland in the 1970s.\nA schema is a template that identifies a subset of strings with similarities at certain string positions. Schemata are a special case of cylinder sets, and hence form a topological space.", "links": ["Cylinder set", "Defining length", "Genetic algorithm", "Genetic algorithms", "Genetic drift", "Genetic operator", "John Henry Holland", "Multimodal distribution", "Sampling error", "Schema (genetic algorithms)", "Subset", "Topological space", "Wildcard character"], "categories": ["Genetic algorithms", "Theorems in discrete mathematics"], "title": "Holland's schema theorem"}
{"summary": "Hypercube-based NEAT, or HyperNEAT, is a generative encoding that evolves artificial neural networks (ANNs) with the principles of the widely used NeuroEvolution of Augmented Topologies (NEAT) algorithm. It is a novel technique for evolving large-scale neural networks utilizing the geometric regularities of the task domain. It uses Compositional Pattern Producing Networks  (CPPNs), which are used to generate the images for Picbreeder.org and shapes for EndlessForms.com. HyperNEAT has recently been extended to also evolve plastic ANNs  and to evolve the location of every neuron in the network.", "links": ["Artificial neural networks", "CPPN", "NeuroEvolution of Augmented Topologies"], "categories": ["Artificial neural networks", "Evolutionary algorithms", "Evolutionary computation", "Genetic algorithms"], "title": "HyperNEAT"}
{"summary": "In genetic algorithms, inheritance is the ability of modeled objects to mate, mutate (similar to biological mutation), and propagate their problem solving genes to the next generation, in order to produce an evolved solution to a particular problem. The selection of objects that will be inherited from in each successive generation is determined by a fitness function, which varies depending upon the problem being addressed.\nThe traits of these objects are passed on through chromosomes by a means similar to biological reproduction. These chromosomes are generally represented by a series of genes, which in turn are usually represented using binary numbers. This propagation of traits between generations is similar to the inheritance of traits between generations of biological organisms. This process can also be viewed as a form of reinforcement learning, because the evolution of the objects is driven by the passing of traits from successful objects which can be viewed as a reward for their success, thereby promoting beneficial traits.\n^ a b Russell, Stuart J.; Norvig, Peter (1995). Artificial Intelligence: A Modern Approach. Englewood Heights, NJ: Prentice-Hall.", "links": ["Artificial Intelligence: A Modern Approach", "Artificial intelligence", "Binary number", "Bioinformatics", "Biology", "Chromosome", "Crossover (genetic algorithm)", "Evolution", "Fitness function", "Gene", "Generation", "Genetic algorithm", "Genetic operator", "Heredity", "Mating", "Mutation", "Mutation (genetic algorithm)", "Organism", "Phenotypic trait", "Reinforcement", "Reinforcement learning", "Reproduction", "Selection (genetic algorithm)", "Speciation (genetic algorithm)"], "categories": ["All orphaned articles", "Genetic algorithms", "Orphaned articles from April 2013"], "title": "Inheritance (genetic algorithm)"}
{"summary": "This is a list of Genetic Algorithm (GA) applications", "links": ["Anti-terrorism", "Artificial neural networks", "Audio watermark insertion/detection", "Automated", "Bioinformatics", "Bond graphs", "Bound state", "Cellular manufacturing", "Cipher", "CiteSeer", "Cobweb model", "Code-breaking", "Composite material", "Computational creativity", "Computer-automated design", "Control engineering", "Crashworthiness", "Digital object identifier", "Distributed computer network", "Distributed system", "Evolvable hardware", "Expression profiling", "Facial composite", "Feature matching", "Fullerene", "Game theory", "Genetic Algorithm", "Genetic Algorithm for Rule Set Production", "Genetic algorithm in economics", "Genetic algorithm scheduling", "Genetic programming", "Grammar induction", "Hierarchical decomposition", "International Standard Book Number", "International equity", "Job Shop Scheduling", "Kluwer Academic", "Ligand docking", "Local-density approximation", "Marketing mix analysis", "Mechanical engineering", "Mechatronics", "Methodologies", "Motif Discovery", "Multi-objective", "Multidimensional systems", "Multiple Sequence Alignment", "Multiple sequence alignment", "Mutation testing", "Natural language processing", "Nesting problem (geometric optimization)", "Neural network", "Neuroevolution", "Operon", "Optimization (mathematics)", "Parallelization", "Phylogenetic tree", "Plant floor layout", "Pop music", "Power electronics", "Printed circuit board", "Problem domains", "Protein folding", "PubMed Central", "PubMed Identifier", "Quality control and genetic algorithms", "RNA", "Real options valuation", "Recurrent neural networks", "Robot", "Sequence-dependent setup", "Software engineering", "Tactical asset", "Topologies", "Traveling salesman problem", "United States Geological Survey", "Water resource", "Wavelet", "Witness"], "categories": ["All articles with dead external links", "All articles with unsourced statements", "Articles with dead external links from December 2014", "Articles with dead external links from October 2015", "Articles with unsourced statements from November 2008", "Genetic algorithms", "Mathematics-related lists"], "title": "List of genetic algorithm applications"}
{"summary": "Mutation is a genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next. It is analogous to biological mutation. Mutation alters one or more gene values in a chromosome from its initial state. In mutation, the solution may change entirely from the previous solution. Hence GA can come to better solution by using mutation. Mutation occurs during evolution according to a user-definable mutation probability. This probability should be set low. If it is set too high, the search will turn into a primitive random search.\nThe classic example of a mutation operator involves a probability that an arbitrary bit in a genetic sequence will be changed from its original state. A common method of implementing the mutation operator involves generating a random variable for each bit in a sequence. This random variable tells whether or not a particular bit will be modified. This mutation procedure, based on the biological point mutation, is called single point mutation. Other types are inversion and floating point mutation. When the gene encoding is restrictive as in permutation problems, mutations are swaps, inversions, and scrambles.\nThe purpose of mutation in GAs is preserving and introducing diversity. Mutation should allow the algorithm to avoid local minima by preventing the population of chromosomes from becoming too similar to each other, thus slowing or even stopping evolution. This reasoning also explains the fact that most GA systems avoid only taking the fittest of the population in generating the next but rather a random (or semi-random) selection with a weighting toward those that are fitter.\nFor different genome types, different mutation types are suitable:\nBit string mutation\n\nThe mutation of bit strings ensue through bit flips at random positions.\n\nExample:\n\nThe probability of a mutation of a bit is , where  is the length of the binary vector. Thus, a mutation rate of  per mutation and individual selected for mutation is reached.\n\nFlip Bit\nThis mutation operator takes the chosen genome and inverts the bits (i.e. if the genome bit is 1, it is changed to 0 and vice versa).\nBoundary\nThis mutation operator replaces the genome with either lower or upper bound randomly. This can be used for integer and float genes.\nNon-Uniform\nThe probability that amount of mutation will go to 0 with the next generation is increased by using non-uniform mutation operator. It keeps the population from stagnating in the early stages of the evolution. It tunes solution in later stages of evolution. This mutation operator can only be used for integer and float genes.\nUniform\nThis operator replaces the value of the chosen gene with a uniform random value selected between the user-specified upper and lower bounds for that gene. This mutation operator can only be used for integer and float genes.\nGaussian\nThis operator adds a unit Gaussian distributed random value to the chosen gene. If it falls outside of the user-specified lower or upper bounds for that gene, the new gene value is clipped. This mutation operator can only be used for integer and float genes.", "links": ["Bit", "Chromosome (genetic algorithm)", "Fitness function", "Genetic algorithm", "Genetic diversity", "Genetic operator", "Genome (genetic algorithm)", "Local minimum", "Mutation", "Point mutation", "Random variable"], "categories": ["Genetic algorithms"], "title": "Mutation (genetic algorithm)"}
{"summary": "NeuroEvolution of Augmenting Topologies (NEAT) is a genetic algorithm for the generation of evolving artificial neural networks (a neuroevolution technique) developed by Ken Stanley in 2002 while at The University of Texas at Austin. It alters both the weighting parameters and structures of networks, attempting to find a balance between the fitness of evolved solutions and their diversity. It is based on applying three key techniques: tracking genes with history markers to allow crossover among topologies, applying speciation (the evolution of species) to preserve innovations, and developing topologies incrementally from simple initial structures (\"complexifying\").\n\n", "links": ["Artificial neural network", "C++", "CPPN", "C Sharp (programming language)", "Digital object identifier", "Encog", "Evolutionary Acquisition of Neural Topologies", "GNU General Public License", "GNU Guile", "Galactic Arms Race", "Genetic algorithm", "HyperNEAT", "Java (programming language)", "Javascript (programming language)", "NEAT Particles", "Network topology", "Neuroevolution", "Perceptron", "PubMed Identifier", "Python (programming language)", "Reinforcement learning", "Ruby (programming language)", "Scheme (programming language)", "The University of Texas at Austin"], "categories": ["Artificial neural networks", "Evolutionary algorithms", "Evolutionary computation", "Genetic algorithms"], "title": "Neuroevolution of augmenting topologies"}
{"summary": "Parallel metaheuristic is a class of techniques that are capable of reducing both the numerical effort and the run time of a metaheuristic. To this end, concepts and technologies from the field of parallelism in computer science are used to enhance and even completely modify the behavior of existing metaheuristics. Just as it exists a long list of metaheuristics like evolutionary algorithms, particle swarm, ant colony optimization, simulated annealing, etc. it also exists a large set of different techniques strongly or loosely based in these ones, whose behavior encompasses the multiple parallel execution of algorithm components that cooperate in some way to solve a problem on a given parallel hardware platform.", "links": ["Ant colony optimization", "Cellular Evolutionary Algorithms", "Computer science", "Differential evolution", "Enrique Alba", "Estimation distribution algorithms", "Evolutionary algorithm", "Greedy algorithm", "Metaheuristic", "Metaheuristics", "NP-hard", "Paradiseo", "Particle swarm", "Particle swarm optimization", "Scatter search", "Simulated annealing"], "categories": ["All accuracy disputes", "All articles lacking in-text citations", "All articles needing additional references", "All articles with specifically marked weasel-worded phrases", "Articles lacking in-text citations from June 2015", "Articles needing additional references from June 2015", "Articles with disputed statements from June 2015", "Articles with specifically marked weasel-worded phrases from April 2012", "Genetic algorithms", "Mathematical optimization", "Optimization algorithms and methods", "Search algorithms", "Wikipedia articles needing clarification from June 2015", "Wikipedia articles with possible conflicts of interest from June 2015"], "title": "Parallel metaheuristic"}
{"summary": "In computer science and machine learning, population-based incremental learning (PBIL) is an optimization algorithm, and an estimation of distribution algorithm. This is a type of genetic algorithm where the genotype of an entire population (probability vector) is evolved rather than individual members. The algorithm is proposed by Shumeet Baluja in 1994. The algorithm is simpler than a standard genetic algorithm, and in many cases leads to better results than a standard genetic algorithm.", "links": ["Algorithm", "Allele", "Computer science", "Estimation of Distribution Algorithm", "Estimation of distribution algorithm", "Euclidean vector", "Gene", "Genetic algorithm", "Genotype", "International Standard Book Number", "Java (programming language)", "Machine learning", "Optimization (mathematics)", "Probability"], "categories": ["Articles with example Java code", "Genetic algorithms"], "title": "Population-based incremental learning"}
{"summary": "In genetic algorithms, the term of premature convergence means that a population for an optimization problem converged too early, resulting in being suboptimal. In this context, the parental solutions, through the aid of genetic operators, are not able to generate offsprings that are superior to their parents. Premature convergence can happen in case of loss of genetic variation (every individual in the population is identical, see convergence).", "links": ["Convergence (evolutionary computing)", "Crossover (genetic algorithm)", "Evolution", "Evolutionary computation", "Genetic algorithm", "Genetic operator", "International Standard Book Number", "Mutation", "Optimization problem", "Suboptimal"], "categories": ["All stub articles", "Evolution stubs", "Evolutionary biology", "Genetic algorithms"], "title": "Premature convergence"}
{"summary": "The promoter based genetic algorithm (PBGA) is a genetic algorithm for neuroevolution developed by F. Bellas and R.J. Duro in the Integrated Group for Engineering Research (GII) at the University of Coru\u00f1a, in Spain. It evolves variable size feedforward artificial neural networks (ANN) that are encoded into sequences of genes for constructing a basic ANN unit. Each of these blocks is preceded by a gene promoter acting as an on/off switch that determines if that particular unit will be expressed or not.", "links": ["Artificial neural network", "Genetic algorithm", "Genotype", "Integrated Group for Engineering Research", "Multilevel Darwinist Brain", "Neuron"], "categories": ["Artificial neural networks", "Evolutionary algorithms", "Evolutionary computation", "Genetic algorithms"], "title": "Promoter based genetic algorithm"}
{"summary": "The combination of quality control and genetic algorithms led to novel solutions of complex quality control design and optimization problems. Quality control is a process by which entities review the quality of all factors involved in production. Quality is the degree to which a set of inherent characteristics fulfils a need or expectation that is stated, general implied or obligatory. Genetic algorithms are search algorithms, based on the mechanics of natural selection and natural genetics.", "links": ["Algorithms", "Boolean logic", "Clin Chem", "Enumerative", "Evolution", "Gene", "Genetic algorithm", "Genetic algorithms", "Genetic programming", "Isomorphic", "Knowledge", "Mean", "Molecular biology", "Mutation", "Null hypothesis", "Optimization (mathematics)", "Paradigm", "Probability density function", "Quality control", "Range (statistics)", "Reproduction", "San Francisco", "Standard deviation", "Statistical hypothesis testing"], "categories": ["Genetic algorithms", "Quality control"], "title": "Quality control and genetic algorithms"}
{"summary": "Reward-based selection is a technique used in evolutionary algorithms for selecting potentially useful solutions for recombination. The probability of being selected for an individual is proportional to the cumulative reward, obtained by the individual. The cumulative reward can be computed as a sum of the individual reward and the reward, inherited from parents.", "links": ["Digital object identifier", "Evolutionary algorithm", "Fitness proportionate selection", "Hypervolume indicator", "Multi-armed bandit", "Multi-objective optimization", "Non-dominated sorting", "Pareto efficiency", "Selection (genetic algorithm)", "Stochastic universal sampling", "Tournament selection"], "categories": ["Articles created via the Article Wizard", "Evolutionary algorithms", "Genetic algorithms"], "title": "Reward-based selection"}
{"summary": "The Santa Fe Trail problem is a Genetic programming exercise in which Artificial ants search for food pellets according to a programmed set of instructions. The layout of food pellets in the Santa Fe Trail problem has become a standard for comparing different genetic programming algorithms and solutions.\nOne method for programming and testing algorithms on the Santa Fe Trail problem is by using the NetLogo application. There is at least one case of a student creating a Lego robotic ant to solve the problem.", "links": ["Agent-based model", "Artificial ants", "Genetic programming", "Java Grammatical Evolution"], "categories": ["Genetic algorithms", "Genetic programming"], "title": "Santa Fe Trail problem"}
{"summary": "A schema is a template in computer science used in the field of genetic algorithms that identifies a subset of strings with similarities at certain string positions. Schemata are a special case of cylinder sets; and so form a topological space.", "links": ["Artificial intelligence", "Computer science", "Cylinder set", "Defining length", "Genetic algorithm", "Holland's schema theorem", "International Standard Book Number", "Propagation of schema", "Subset", "Topological space", "Wildcard character"], "categories": ["All articles to be merged", "All stub articles", "Articles to be merged from December 2011", "Artificial intelligence stubs", "Bioinformatics stubs", "Computer science stubs", "Genetic algorithms", "Genetic programming"], "title": "Schema (genetic algorithms)"}
{"summary": "Search-based software engineering (SBSE) applies metaheuristic search techniques such as genetic algorithms, simulated annealing and tabu search to software engineering problems. Many activities in software engineering can be stated as optimization problems. Optimization techniques of operations research such as linear programming or dynamic programming are mostly impractical for large scale software engineering problems because of their computational complexity. Researchers and practitioners use metaheuristic search techniques to find near-optimal or \"good-enough\" solutions.\nSBSE problems can be divided into two types:\nblack-box optimization problems, for example, assigning people to tasks (a typical combinatorial optimization problem).\nwhite-box problems where operations on source code need to be considered.", "links": ["Abstract syntax tree", "Code coverage", "Code smell", "Coevolution", "Combinatorial optimization", "Computational complexity theory", "Debugging", "Digital object identifier", "Dynamic program analysis", "Dynamic programming", "EvoSuite", "Genetic algorithms", "Genetic programming", "Instrumentation", "International Standard Book Number", "International Standard Serial Number", "Linear programming", "MCDM", "Mark Harman (computer scientist)", "Metaheuristic", "Metaphor", "Mutation testing", "OpenPAT", "Operations research", "Optimization (computer science)", "Optimization (mathematics)", "Profiling (computer programming)", "Program analysis (computer science)", "Program optimization", "Program slicing", "Refactoring", "Regression testing", "Requirements analysis", "Requirements engineering", "Search-based application", "Search engine technology", "Simulated annealing", "Software bug", "Software design", "Software development", "Software engineering", "Software life cycle", "Software maintenance", "Software testing", "Static program analysis", "Tabu search", "Unit Testing", "Webb Miller"], "categories": ["2001 introductions", "All articles with unsourced statements", "Articles with unsourced statements from October 2013", "Genetic algorithms", "Optimization algorithms and methods", "Pages containing cite templates with deprecated parameters", "Program analysis", "Search algorithms", "Software articles needing expert attention", "Software engineering", "Software quality", "Software testing", "Use dmy dates from November 2011"], "title": "Search-based software engineering"}
{"summary": "Selection is the stage of a genetic algorithm in which individual genomes are chosen from a population for later breeding (using the crossover operator).\nA generic selection procedure may be implemented as follows:\nThe fitness function is evaluated for each individual, providing fitness values, which are then normalized. Normalization means dividing the fitness value of each individual by the sum of all fitness values, so that the sum of all resulting fitness values equals 1.\nThe population is sorted by descending fitness values.\nAccumulated normalized fitness values are computed (the accumulated fitness value of an individual is the sum of its own fitness value plus the fitness values of all the previous individuals). The accumulated fitness of the last individual should be 1 (otherwise something went wrong in the normalization step).\nA random number R between 0 and 1 is chosen.\nThe selected individual is the first one whose accumulated normalized value is greater than R.\nIf this procedure is repeated until there are enough selected individuals, this selection method is called fitness proportionate selection or roulette-wheel selection. If instead of a single pointer spun multiple times, there are multiple, equally spaced pointers on a wheel that is spun once, it is called stochastic universal sampling. Repeatedly selecting the best individual of a randomly chosen subset is tournament selection. Taking the best half, third or another proportion of the individuals is truncation selection.\nThere are other selection algorithms that do not consider all individuals for selection, but only those with a fitness value that is higher than a given (arbitrary) constant. Other algorithms select from a restricted pool where only a certain percentage of the individuals are allowed, based on fitness value.\nRetaining the best individuals in a generation unchanged in the next generation, is called elitism or elitist selection. It is a successful (slight) variant of the general process of constructing a new population.", "links": ["Crossover (genetic algorithm)", "Fitness function", "Fitness proportionate selection", "Genetic algorithm", "Reward-based selection", "Stochastic universal sampling", "Tournament selection", "Truncation selection"], "categories": ["Genetic algorithms"], "title": "Selection (genetic algorithm)"}
{"summary": "Speciation is a process that occurs naturally in evolution and is modeled explicitly in some genetic algorithms.\nSpeciation in nature occurs when two similar reproducing beings evolve to become too dissimilar to share genetic information effectively or correctly. In the case of living organisms, they are incapable of mating to produce offspring. Interesting special cases of different species being able to breed exist, such as a horse and a donkey mating to produce a mule. However in this case the Mule is usually infertile, and so the genetic isolation of the two parent species is maintained.\nIn implementations of genetic search algorithms, the event of speciation is defined by some mathematical function that describes the similarity between two candidate solutions (usually described as individuals) in the population. If the result of the similarity is too low, the crossover operator is disallowed between those individuals.", "links": ["Candidate solutions", "Chromosomal crossover", "Donkey", "Evolution", "Genetic algorithms", "Horse", "Mule", "Offspring", "Operation (mathematics)", "Organisms", "Speciation"], "categories": ["All articles lacking sources", "Articles lacking sources from December 2007", "Evolutionary algorithms", "Genetic algorithms"], "title": "Speciation (genetic algorithm)"}
{"summary": "Stochastic universal sampling (SUS) is a technique used in genetic algorithms for selecting potentially useful solutions for recombination. It was introduced by James Baker.\nSUS is a development of fitness proportionate selection (FPS) which exhibits no bias and minimal spread. Where FPS chooses several solutions from the population by repeated random sampling, SUS uses a single random value to sample all of the solutions by choosing them at evenly spaced intervals. This gives weaker members of the population (according to their fitness) a chance to be chosen and thus reduces the unfair nature of fitness-proportional selection methods.\nOther methods like roulette wheel can have bad performance when a member of the population has a really large fitness in comparison with other members. Using a comb-like ruler, SUS starts from a small random number, and chooses the next candidates from the rest of population remaining, not allowing the fittest members to saturate the candidate space.\nDescribed as an algorithm, pseudocode for SUS looks like:\n\nSUS(Population, N)\n    F := total fitness of Population\n    N := number of offspring to keep\n    P := distance between the pointers (F/N)\n    Start := random number between 0 and P\n    Pointers := [Start + i*P | i in [0..(N-1)]]\n    return RWS(Population,Pointers)\n\nRWS(Population, Points)\n    Keep = []\n    i := 0\n    for P in Points\n        while fitness sum of Population[0..i] < P\n            i++\n        add Population[i] to Keep\n    return Keep\n\nWhere Population[0..i] is the set of individuals with array-index 0 to (and including) i.\nHere RWS() describes the bulk of fitness proportionate selection (also known as \"roulette wheel selection\") - in true fitness proportional selection the parameter Points is always a (sorted) list of random numbers from 0 to F. The algorithm above is intended to be illustrative rather than canonical.", "links": ["Fitness proportionate selection", "Genetic algorithm", "Reward-based selection", "Roulette wheel"], "categories": ["Genetic algorithms", "Pages using citations with accessdate and no URL", "Stochastic algorithms"], "title": "Stochastic universal sampling"}
{"summary": "Tournament selection is a method of selecting an individual from a population of individuals in a genetic algorithm. Tournament selection involves running several \"tournaments\" among a few individuals (or 'chromosomes') chosen at random from the population. The winner of each tournament (the one with the best fitness) is selected for crossover. Selection pressure is easily adjusted by changing the tournament size. If the tournament size is larger, weak individuals have a smaller chance to be selected.\nThe tournament selection method may be described in pseudo code:\n\nchoose k (the tournament size) individuals from the population at random\nchoose the best individual from pool/tournament with probability p\nchoose the second best individual with probability p*(1-p)\nchoose the third best individual with probability p*((1-p)^2)\nand so on...\n\nDeterministic tournament selection selects the best individual (when p = 1) in any tournament. A 1-way tournament (k = 1) selection is equivalent to random selection. The chosen individual can be removed from the population that the selection is made from if desired, otherwise individuals can be selected more than once for the next generation. In comparison with the (stochastic) fitness proportionate selection method, tournament selection is often implemented in practice due to its lack of stochastic noise.\nTournament selection has several benefits over alternative selection methods for genetic algorithms (for example, fitness proportionate selection and reward-based selection): it is efficient to code, works on parallel architectures and allows the selection pressure to be easily adjusted. Tournament selection has also been shown to be independent of the scaling of the genetic algorithm fitness function (or 'objective function') in some classifier systems.", "links": ["Chromosome (genetic algorithm)", "Crossover (genetic algorithm)", "Digital object identifier", "Fitness function", "Fitness proportionate selection", "Genetic algorithm", "International Standard Book Number", "Loss function", "Reward-based selection"], "categories": ["Genetic algorithms", "Pages using citations with accessdate and no URL"], "title": "Tournament selection"}
{"summary": "Truncation selection is a selection method used in genetic algorithms to select potential candidate solutions for recombination.\nIn truncation selection the candidate solutions are ordered by fitness, and some proportion, p, (e.g. p = 1/2, 1/3, etc.), of the fittest individuals are selected and reproduced 1/p times. Truncation selection is less sophisticated than many other selection methods, and is not often used in practice. It is used in Muhlenbein's Breeder Genetic Algorithm.", "links": ["Artificial intelligence", "Breeder Genetic Algorithm", "Computer science", "Genetic algorithm", "Selection (genetic algorithm)"], "categories": ["All stub articles", "Artificial intelligence stubs", "Bioinformatics stubs", "Computer science stubs", "Genetic algorithms"], "title": "Truncation selection"}
{"summary": "Astronomical algorithms are the algorithms used to calculate ephemerides, calendars, and positions (as in celestial navigation or satellite navigation). Examples of large and complex astronomical algorithms are those used to calculate the position of the Moon. A simple example is the calculation of the Julian day.\nNumerical model of solar system discusses a generalized approach to local astronomical modeling. The variations s\u00e9culaires des orbites plan\u00e9taires describes an often used model.", "links": ["Algorithm", "Astrodynamics", "Astronomy", "Calendar", "Celestial mechanics", "Celestial navigation", "Charge-coupled device", "Data structure", "Doomsday rule", "Ephemeris", "Jean Meeus", "Julian day", "List of algorithms", "List of astronomical objects", "Moon", "Numerical model of solar system", "Satellite navigation", "Transformation from spherical coordinates to rectangular coordinates", "Variations s\u00e9culaires des orbites plan\u00e9taires"], "categories": ["Algorithms and data structures stubs", "All articles lacking sources", "All stub articles", "Articles lacking sources from April 2010", "Astrodynamics", "Astronomy stubs", "Calendar algorithms", "Computational physics", "Computer science stubs"], "title": "Astronomical algorithm"}
{"summary": "A calendrical calculation is a calculation concerning calendar dates. Calendrical calculations can be considered an area of applied mathematics. Some examples of calendrical calculations:\nConverting a Julian or Gregorian calendar date to its Julian day number and vice versa (see the section on calculation in that article for details).\nThe number of days between two dates, which is simply the difference in their Julian day numbers.\nThe date of a religious holiday, like Easter (the calculation is known as Computus) or Passover, for a given year.\nConverting a date between different calendars. For instance, dates in the Gregorian calendar can be converted to dates in the Islamic calendar with the Kuwaiti algorithm.\nCalculating the day of the week.\n\n", "links": ["Applied mathematics", "Calculating the day of the week", "Calculation", "Calendar date", "Cambridge University Press", "Computus", "Easter", "Edward M. Reingold", "Gregorian calendar", "Islamic calendar", "Julian calendar", "Julian day", "Kuwaiti algorithm", "Nachum Dershowitz", "Passover"], "categories": ["All stub articles", "Applied mathematics stubs", "Calendar algorithms"], "title": "Calendrical calculation"}
{"summary": "There are various methods to calculate the day of the week for any particular date in the past or future. These methods ultimately rely on algorithms to determine the day of the week for any given date, including those based solely on tables as found in perpetual calendars that require no calculations to be performed by the user. A typical application is to calculate the day of the week on which someone was born or any other specific event occurred.", "links": ["%NDAY OF WEEK%", "12-hour clock", "24-hour clock", "4DOS", "6502", "ANSI C", "APL2", "ASCII", "Abacus", "Absolute time and space", "Algorithms", "Assembly language", "Astrarium", "Astronomical chronology", "Astronomical year numbering", "Atomic clock", "Barycentric Coordinate Time", "Barycentric Dynamical Time", "Buddhist calendar", "C language", "Calendar", "Calendar year", "Carl Friedrich Gauss", "Century", "Chronology", "Chronometry", "Chronon", "Civil time", "Clock", "Common year", "Common year starting on Thursday", "Complication (horology)", "Continuous signal", "Coordinate time", "Coordinated Universal Time", "Cosmological decade", "DR-DOS", "DUT1", "Data type", "Dating methodologies in archaeology", "Day", "Day of the week", "Daylight saving time", "Decade", "Determination of the day of the week", "Dialing scales", "Digital object identifier", "Discrete time", "Dominical letter", "Doomsday rule", "Duration (philosophy)", "EBCDIC", "Epact", "Ephemeris time", "Equation of time", "Equinox", "Fortnight", "Galactic year", "Gauss' Method", "Geocentric Coordinate Time", "Geologic time scale", "George Washington", "Gravitational time dilation", "Greenwich Mean Time", "Gregorian calendar", "HP Prime", "Hebrew calendar", "High level programming language", "Hindu calendar", "History of sundials", "History of timekeeping devices", "Horology", "Hour", "Hourglass", "IBM", "ISO 31-1", "ISO 8601", "Intercalation (timekeeping)", "International Atomic Time", "International Commission on Stratigraphy", "International Date Line", "International Earth Rotation and Reference Systems Service", "International Standard Book Number", "Islamic calendar", "Jiffy (time)", "Julian calendar", "Julian day", "K&R C", "Leap day", "Leap second", "Leap year", "Leap year starting on Friday", "Lewis Carroll", "Lunar calendar", "Lunisolar calendar", "Lustrum", "Marine chronometer", "Marine sandglass", "Mental Calculation World Cup", "Mental chronometry", "Metric time", "Metrology", "Millennium", "Minute", "Modular arithmetic", "Moment (time)", "Month", "Motorola 68000", "NetWare", "New Earth Time", "Nuclear timescale", "Old Style", "Old Style and New Style dates", "Opcodes", "Paksha", "Perpetual calendar", "Perpetual calendars", "Planck epoch", "Planck time", "Precession (astronomy)", "Prime meridian", "Processor registers", "Proleptic Gregorian calendar", "Proper time", "REXX", "Radio clock", "Rata Die", "Remainder", "Saeculum", "Schema for horizontal dials", "Second", "Shake (unit)", "Sidereal time", "Software portability", "Solar calendar", "Solar time", "Solstice", "Spacetime", "Sundial", "System time", "T-symmetry", "Terrestrial Time", "Theory of relativity", "Tide (time)", "Time", "Time (Orders of magnitude)", "Time dilation", "Time domain", "Time in physics", "Time standard", "Time value of money", "Time zone", "Timekeeper", "Tropical year", "UTC offset", "Unit of time", "Universal Time", "Usenet newsgroup", "Watch", "Water clock", "Week", "Weekday determination", "Weekday names", "Year", "Zeller's congruence", "\u0394T"], "categories": ["CS1 German-language sources (de)", "Calendar algorithms", "Days of the week", "Gregorian calendar", "Julian calendar"], "title": "Determination of the day of the week"}
{"summary": "The Doomsday rule or Doomsday algorithm is a way of calculating the day of the week of a given date. It provides a perpetual calendar because the Gregorian calendar moves in cycles of 400 years.\nThis algorithm for mental calculation was devised by John Conway after drawing inspiration from Lewis Carroll's work on a perpetual calendar algorithm. It takes advantage of each year having a certain day of the week (the doomsday) upon which certain easy-to-remember dates fall; for example, 4/4, 6/6, 8/8, 10/10, 12/12, and the last day of February all occur on the same day of the week in any given year. Applying the Doomsday algorithm involves three steps:\nDetermine the \"anchor day\" for the century.\nUse the anchor day for the century to calculate the doomsday for the year.\nChoose the closest date out of the ones that always fall on the doomsday (e.g. 4/4, 6/6, 8/8), and count the number of days (modulo 7) between that date and the date in question to arrive at the day of the week.\nThis technique applies to both the Gregorian calendar A.D. and the Julian calendar, although their doomsdays will usually be different days of the week.\nSince this algorithm involves treating days of the week like numbers modulo 7, John Conway suggests thinking of the days of the week as \"Noneday\" or \"Sansday\" (for Sunday), \"Oneday\", \"Twosday\", \"Treblesday\", \"Foursday\", \"Fiveday\", and \"Six-a-day\".\nThe algorithm is simple enough for anyone with basic arithmetic ability to do the calculations mentally. Conway can usually give the correct answer in under two seconds. To improve his speed, he practices his calendrical calculations on his computer, which is programmed to quiz him with random dates every time he logs on.\n\n", "links": ["7-Eleven", "Algorithm", "American Civil War", "Anno Domini", "Astronomical year numbering", "Calculating the day of the week", "Christmas Day", "Common subexpression", "Common year starting on Friday", "Common year starting on Monday", "Common year starting on Saturday", "Common year starting on Sunday", "Common year starting on Thursday", "Common year starting on Tuesday", "Common year starting on Wednesday", "Computus", "Dominical letter", "Floor and ceiling functions", "Floor function", "Fort Sumter", "Gregorian Calendar", "Gregorian calendar", "Halloween", "ISO week date", "John Horton Conway", "Julian calendar", "Julian day", "Leap year starting on Friday", "Leap year starting on Monday", "Leap year starting on Saturday", "Leap year starting on Sunday", "Leap year starting on Thursday", "Leap year starting on Tuesday", "Leap year starting on Wednesday", "Lewis Carroll", "March 0", "Mental calculation", "Modular arithmetic", "Ordinal date", "Perpetual calendar", "Proleptic Gregorian calendar", "Proleptic Julian calendar", "Quotient", "Scientific American", "September 11, 2001 attacks", "Solstices", "Wayback Machine", "Working time", "World Trade Center (1973-2001)", "Y2K", "Zeller's congruence"], "categories": ["1973 introductions", "All articles with unsourced statements", "Articles with unsourced statements from January 2008", "Calendar algorithms", "Gregorian calendar", "Julian calendar"], "title": "Doomsday rule"}
{"summary": "The top-nodes algorithm is an algorithm for managing a resource reservation calendar.\nIt is used when a resource is shared among lots of users (for example bandwidth in a telecommunication link, or disk capacity in a large data center).\nThe algorithm allows\nto check if an amount of resource is available during a specific period of time,\nto reserve an amount of resource for a specific period of time,\nto delete a previous reservation,\nto move the calendar forward (the calendar covers a defined duration, and it must be moved forward as time goes by).", "links": ["Algorithm", "Bandwidth (signal processing)", "Binary tree", "Data center", "Disk capacity", "Telecommunication"], "categories": ["All articles covered by WikiProject Wikify", "All articles with too few wikilinks", "Articles covered by WikiProject Wikify from December 2012", "Articles with French-language external links", "Articles with too few wikilinks from December 2012", "Calendar algorithms", "Scheduling algorithms"], "title": "Top-nodes algorithm"}
{"summary": "Zeller's congruence is an algorithm devised by Christian Zeller to calculate the day of the week for any Julian or Gregorian calendar date. It can be considered to be based on the conversion between Julian day and the calendar date.", "links": ["Algorithm", "Calculate the day of the week", "Christian Zeller", "Determination of the day of the week", "Dictionary of Algorithms and Data Structures", "Digital object identifier", "Doomsday rule", "Gregorian calendar", "ISO week date", "Julian calendar", "Julian day", "Modulo operation", "National Institute of Standards and Technology", "Proleptic calendar", "Zero-based"], "categories": ["CS1 German-language sources (de)", "CS1 Latin-language sources (la)", "Calendar algorithms", "Gregorian calendar", "Julian calendar", "Modular arithmetic"], "title": "Zeller's congruence"}
{"summary": "A checksum or hash sum is a small-size datum from a block of digital data for the purpose of detecting errors which may have been introduced during its transmission or storage. It is usually applied to an installation file after it is received from the download server. By themselves checksums are often used to verify data integrity, but should not be relied upon to also verify data authenticity.\nThe actual procedure which yields the checksum, given a data input is called a checksum function or checksum algorithm. Depending on its design goals, a good checksum algorithm will usually output a significantly different value, even for small changes made to the input. This is especially true of cryptographic hash functions, which may be used to detect many data corruption errors and verify overall data integrity; if the computed checksum for the current data input matches the stored value of a previously computed checksum, there is a very high probability the data has not been accidentally altered or corrupted.\nChecksum functions are related to hash functions, fingerprints, randomization functions, and cryptographic hash functions. However, each of those concepts has different applications and therefore different design goals. Checksums are used as cryptographic primitives in larger authentication algorithms. For cryptographic systems with these two specific design goals, see HMAC.\nCheck digits and parity bits are special cases of checksums, appropriate for small blocks of data (such as Social Security numbers, bank account numbers, computer words, single bytes, etc.). Some error-correcting codes are based on special checksums which not only detect common errors but also allow the original data to be recovered in certain cases.", "links": ["Adler-32", "Algorithm", "Analysis of algorithms", "Authentication", "BSD checksum", "Bank account", "Byte", "C (programming language)", "Check digit", "Checksum algorithm", "Cksum", "Computer storage", "Cryptographic hash function", "Cyclic redundancy check", "Damm algorithm", "Data degradation", "Data integrity", "Digital data", "Error-correcting code", "Error detection", "Exclusive or", "File verification", "Fingerprint (computing)", "Fletcher's checksum", "Frame check sequence", "HMAC", "Hamming code", "Hash function", "IPv4 header checksum", "J1708", "Java (software platform)", "List of hash functions", "Longitudinal redundancy check", "Luhn algorithm", "MD5", "Md5sum", "Microsoft Windows", "Parchive", "Parity bit", "Randomization function", "Rolling checksum", "SYSV checksum", "Secure Hash Algorithm", "Sha1sum", "Social Security number", "Sum (Unix)", "Telecommunication", "Two's complement", "Verhoeff algorithm", "Word (data type)", "ZFS"], "categories": ["All articles needing additional references", "Articles needing additional references from August 2012", "Checksum algorithms"], "title": "Checksum"}
{"summary": "Adler-32 is a checksum algorithm which was invented by Mark Adler in 1995, and is a modification of the Fletcher checksum. Compared to a cyclic redundancy check of the same length, it trades reliability for speed (preferring the latter). Adler-32 is more reliable than Fletcher-16, and slightly less reliable than Fletcher-32.", "links": ["16-bit", "ASCII", "Algorithm", "Byte", "CRC-32", "CRC32", "C (programming language)", "Checksum", "Composite number", "Cyclic redundancy check", "Endianness", "Fletcher's checksum", "Fletcher-16", "Fletcher-32", "Hexadecimal", "Hierarchical Data Format", "List of hash functions", "Mark Adler", "Modular arithmetic", "Prime number", "Rolling checksum", "Rsync", "Stream Control Transmission Protocol", "Word (data type)", "Zlib"], "categories": ["Checksum algorithms"], "title": "Adler-32"}
{"summary": "The BSD checksum algorithm is a commonly used, legacy checksum algorithm. It has been implemented in BSD and is also available through the GNU sum command line utility.", "links": ["Berkeley Software Distribution", "Checksum", "Endianness", "GNU", "GPL", "Sum (Unix)"], "categories": ["Checksum algorithms"], "title": "BSD checksum"}
{"summary": "In error detection, the Damm algorithm is a check digit algorithm that detects all single-digit errors and all adjacent transposition errors. It was presented by H. Michael Damm in 2004.", "links": ["Algorithm", "Cayley table", "Check digit", "Digital object identifier", "Error detection", "Exponentiation", "ISBN", "International Standard Book Number", "International Standard Serial Number", "Inverse element", "Latin square", "Order (group theory)", "Permutation", "Quasigroup", "Transcription error", "Verhoeff algorithm", "X"], "categories": ["Algebraic structures", "CS1 German-language sources (de)", "CS1 errors: external links", "Checksum algorithms", "Group theory", "Latin squares"], "title": "Damm algorithm"}
{"summary": "The Fletcher checksum is an algorithm for computing a position-dependent checksum devised by John G. Fletcher (1934-2012) at Lawrence Livermore Labs in the late 1970s. The objective of the Fletcher checksum was to provide error-detection properties approaching those of a cyclic redundancy check but with the lower computational effort associated with summation techniques.", "links": ["Adler-32", "Algorithm", "Arithmetic overflow", "Array data structure", "Binary data", "Byte", "C (programming language)", "Checksum", "Cyclic redundancy check", "Digital object identifier", "End-around carry", "For loop", "IPv4 header checksum", "Lawrence Livermore National Laboratory", "Mark Adler", "Modular Arithmetic", "Modulo operation", "Signed number representations", "Subroutine", "Triangular number", "Variable (programming)"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from May 2012", "Checksum algorithms"], "title": "Fletcher's checksum"}
{"summary": "ISO 7064 define algorithms for calculating check digit characters.", "links": ["Check digit", "Global Release Identifier", "International Bank Account Number", "International Standard Name Identifier", "International Standard Text Code", "ORCID", "Personal identification number (Croatia)", "Resident Identity Card"], "categories": ["All stub articles", "Checksum algorithms", "Computing stubs", "Error detection and correction", "ISO standards"], "title": "ISO 7064"}
{"summary": "The Luhn algorithm or Luhn formula, also known as the \"modulus 10\" or \"mod 10\" algorithm, is a simple checksum formula used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the US, and Canadian Social Insurance Numbers. It was created by IBM scientist Hans Peter Luhn and described in U.S. Patent No. 2,950,048, filed on January 6, 1954, and granted on August 23, 1960.\nThe algorithm is in the public domain and is in wide use today. It is specified in ISO/IEC 7812-1. It is not intended to be a cryptographically secure hash function; it was designed to protect against accidental errors, not malicious attacks. Most credit cards and many government identification numbers use the algorithm as a simple method of distinguishing valid numbers from mistyped or otherwise incorrect numbers.", "links": ["Algorithm", "Bank card number", "Canada", "Check digit", "Checksum", "Credit card number", "Cryptographic hash function", "Damm algorithm", "Hans Peter Luhn", "IBM", "IMEI", "ISO/IEC 7812", "Luhn mod N algorithm", "Modular arithmetic", "National Provider Identifier", "Public domain", "Python (programming language)", "Social Insurance Number", "Verhoeff algorithm"], "categories": ["Articles with example Python code", "Checksum algorithms", "Error detection and correction", "Modular arithmetic"], "title": "Luhn algorithm"}
{"summary": "The Luhn mod N algorithm is an extension to the Luhn algorithm (also known as mod 10 algorithm) that allows it to work with sequences of non-numeric characters. This can be useful when a check digit is required to validate an identification string composed of letters, a combination of letters and digits or even any arbitrary set of characters.", "links": ["Associative array", "International Securities Identification Number", "Luhn algorithm"], "categories": ["All articles lacking sources", "Articles lacking sources from May 2010", "Articles with example code", "Checksum algorithms", "Modular arithmetic"], "title": "Luhn mod N algorithm"}
{"summary": "The MD5 message-digest algorithm is a widely used cryptographic hash function producing a 128-bit (16-byte) hash value, typically expressed in text format as a 32 digit hexadecimal number. MD5 has been utilized in a wide variety of cryptographic applications, and is also commonly used to verify data integrity.\nMD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function, MD4. The source code in RFC 1321 contains a \"by attribution\" RSA license.\nIn 1996 a flaw was found in the design of MD5. While it was not deemed a fatal weakness at the time, cryptographers began recommending the use of other algorithms, such as SHA-1\u2014which has since been found to be vulnerable as well. In 2004 it was shown that MD5 is not collision resistant. As such, MD5 is not suitable for applications like SSL certificates or digital signatures that rely on this property for digital security. Also in 2004 more serious flaws were discovered in MD5, making further use of the algorithm for security purposes questionable; specifically, a group of researchers described how to create a pair of files that share the same MD5 checksum. Further advances were made in breaking MD5 in 2005, 2006, and 2007. In December 2008, a group of researchers used this technique to fake SSL certificate validity. As of 2010, the CMU Software Engineering Institute considers MD5 \"cryptographically broken and unsuitable for further use\", and most U.S. government applications now require the SHA-2 family of hash functions. In 2012, the Flame malware exploited the weaknesses in MD5 to fake a Microsoft digital signature.", "links": ["ASCII", "Alexander Sotirov", "Arjen Lenstra", "Authenticated encryption", "Avalanche effect", "BLAKE (hash function)", "Bates numbering", "Bcrypt", "Benne de Weger", "Birthday attack", "Bit", "Block cipher", "Brute force attack", "Byte", "CA certificate", "CBC-MAC", "CCM mode", "CMAC", "CMU Software Engineering Institute", "CNET.com", "CRYPTREC", "CWC mode", "Chaos Communication Congress", "Checksum", "Chosen-prefix collision attack", "Collision (computer science)", "Collision attack", "Collision resistance", "Collision resistant", "Comparison of cryptographic hash functions", "Crypt (C)", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Data Authentication Algorithm", "Data integrity", "Digital certificate", "Digital object identifier", "Digital signature", "Distributed computing", "EAX mode", "EPFL", "Electronic discovery", "Elliptic curve only hash", "Endianness", "Eurocrypt", "Fast Syndrome Based Hash", "Flame (malware)", "GOST (hash function)", "Galois/Counter Mode", "Google", "Graphics processing unit", "Gr\u00f8stl", "HAS-160", "HAVAL", "HMAC", "Hans Dobbertin", "HashClash", "Hash collision", "Hash function security summary", "Hash value", "Hexadecimal", "History of cryptography", "IAPM (mode)", "IBM p690", "Initialization vector", "Institute of Electrical and Electronics Engineers", "International Standard Book Number", "JH (hash function)", "Jacob Appelbaum", "Key derivation function", "Key stretching", "Kupyna", "LM hash", "Length extension attack", "Logical conjunction", "Logical disjunction", "MD2 (cryptography)", "MD4", "MD5CRK", "MD6", "MDC-2", "Massachusetts Institute of Technology", "Md5deep", "Md5sum", "Merkle\u2013Damg\u00e5rd construction", "Message Passing Interface", "Message authentication", "Message authentication code", "Message digest", "Microsoft", "Modular addition", "N-Hash", "NESSIE", "NIST hash function competition", "Negation", "Nibble", "OCB mode", "Octet (computing)", "One-key MAC", "One-way compression function", "Outline of cryptography", "PBKDF2", "PMAC (cryptography)", "Padding (cryptography)", "Password", "PlayStation 3", "Poly1305", "PostScript", "Preimage attack", "Public-key cryptography", "Public key certificate", "RIPEMD", "RIPEMD-160", "RSA Laboratories", "RadioGat\u00fan", "Rainbow table", "RapidSSL", "Request for Comments", "Ronald Rivest", "SHA-1", "SHA-2", "SHA-3", "SWIFFT", "Salt (cryptography)", "Scrypt", "Search engine", "Sic", "Side-channel attack", "SipHash", "Skein (hash function)", "Snefru", "Software", "Sony", "Springer Berlin Heidelberg", "Steganography", "Stream cipher", "Streebog", "Symmetric-key algorithm", "The quick brown fox jumps over the lazy dog", "Tiger (cryptography)", "Transport Layer Security", "UMAC", "Uniform Resource Locator", "United States Cyber Command", "VMAC", "VeriSign", "Very smooth hash", "Vlastimil Klima", "Whirlpool (cryptography)", "Wired News", "X.509", "XOR", "Xiaoyun Wang", "Xuejia Lai"], "categories": ["All articles with unsourced statements", "Articles with example pseudocode", "Articles with unsourced statements from August 2014", "Broken hash functions", "Checksum algorithms", "Cryptographic hash functions", "Use dmy dates from April 2014"], "title": "MD5"}
{"summary": "In cryptography, SHA-1 (Secure Hash Algorithm 1) is a cryptographic hash function designed by the United States National Security Agency and is a U.S. Federal Information Processing Standard published by the United States NIST. SHA-1 is considered insecure against well-funded opponents, and it is recommended to use SHA-2 or SHA-3 instead.\nSHA-1 produces a 160-bit (20-byte) hash value known as a message digest. A SHA-1 hash value is typically rendered as a hexadecimal number, 40 digits long.\nSHA-1 is a member of the Secure Hash Algorithm family. The four SHA algorithms are structured differently and are named SHA-0, SHA-1, SHA-2, and SHA-3. SHA-0 is the original version of the 160-bit hash function published in 1993 under the name SHA: it was not adopted by many applications. Published in 1995, SHA-1 is very similar to SHA-0, but alters the original SHA hash specification to correct weaknesses that were unknown to the public at that time. SHA-2, published in 2001, is significantly different from the SHA-1 hash function.\nIn 2005, cryptanalysts found attacks on SHA-1 suggesting that the algorithm might not be secure enough for ongoing use. NIST required many applications in federal agencies to move to SHA-2 after 2010 because of the weakness. Although no successful attacks have yet been reported on SHA-2, it is algorithmically similar to SHA-1. In 2012, following a long-running competition, NIST selected an additional algorithm, Keccak, for standardization under SHA-3.\nMicrosoft, Google and Mozilla have all announced that their respective browsers will stop accepting SHA-1 SSL certificates by 2017. Windows XP SP2 and earlier, and Android 2.2 and earlier, do not support SHA2 certificates.", "links": ["AMD Opteron", "ASCII", "ASIACRYPT", "Algorithm", "Amazon Elastic Compute Cloud", "Andrew Yao", "Antoine Joux", "Authenticated encryption", "Avalanche effect", "BLAKE (hash function)", "BOINC", "Base64", "Bcrypt", "Big O notation", "Birthday attack", "Bit", "Block cipher", "Boomerang attack", "Booting", "Brute-force search", "Brute force attack", "Byte", "CBC-MAC", "CCM mode", "CMAC", "CRYPTO", "CRYPTO (conference)", "CRYPTREC", "CWC mode", "Circular shift", "Collision (computer science)", "Collision attack", "Communications Security Establishment", "Comparison of cryptographic hash functions", "Crypt (C)", "Cryptanalysis", "Cryptlib", "Crypto++", "Cryptographic Module Validation Program", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Cryptosystem", "DMOZ", "Data Authentication Algorithm", "Data corruption", "Digital Signature Algorithm", "Digital timestamping", "Distributed revision control", "EAX mode", "Eli Biham", "Elisabeth Oswald", "Elliptic curve only hash", "Endianness", "Fast Syndrome Based Hash", "Federal Information Processing Standard", "Florent Chabaud", "Frances Yao", "GOST (hash function)", "Galois/Counter Mode", "Git (software)", "Google", "Graz University of Technology", "Gr\u00f8stl", "HAS-160", "HAVAL", "HMAC", "Hash collision", "Hash function security summary", "Hashcash", "Helena Handschuh", "Henri Gilbert", "Hexadecimal", "History of cryptography", "Hongbo Yu", "IAPM (mode)", "IPsec", "Intelligence agency", "International Association for Cryptologic Research", "Itanium 2", "JH (hash function)", "Key derivation function", "Key stretching", "Kupyna", "LM hash", "Length extension attack", "Libgcrypt", "Linus Torvalds", "MD2 (cryptography)", "MD4", "MD5", "MD6", "MDC-2", "Massachusetts Institute of Technology", "Md5deep", "Mebibyte", "Mercurial", "Mercurial (software)", "Merkle tree", "Merkle\u2013Damg\u00e5rd construction", "Message authentication", "Message authentication code", "Message digest", "Microsoft", "Modular arithmetic", "Monotone (software)", "Mozilla", "N-Hash", "NESSIE", "NIST", "NIST hash function competition", "NSA", "NVIDIA", "National Institute of Standards and Technology", "National Security Agency", "Nintendo", "Nothing up my sleeve number", "OCB mode", "One-key MAC", "One-way compression function", "OpenSSL", "Outline of cryptography", "PBKDF2", "PMAC (cryptography)", "Password strength", "PolarSSL", "Poly1305", "Portable Document Format", "Preimage attack", "Pretty Good Privacy", "Pseudocode", "Public-key cryptography", "RIPEMD", "RIPEMD-160", "RadioGat\u00fan", "Rainbow table", "Revision control", "Ron Rivest", "S/MIME", "SHA-0", "SHA-2", "SHA-3", "SHACAL", "SSL certificate", "SWIFFT", "Salt (cryptography)", "Scrypt", "Second preimage resistance", "Secure Hash Algorithm", "Secure Hash Standard", "Secure Shell", "Secure Sockets Layer", "Selected Areas in Cryptography", "Sha1sum", "Shandong University", "Sic", "Side-channel attack", "SipHash", "Skein (hash function)", "Snefru", "Steganography", "Stream cipher", "Streaming SIMD Extensions", "Streebog", "Supercomputer", "Symmetric-key algorithm", "Tiger (cryptography)", "Transport Layer Security", "U.S. Government", "UMAC", "Usenet", "Usenet newsgroup", "VMAC", "Very smooth hash", "Vincent Rijmen", "Whirlpool (cryptography)", "Wii", "Word (data type)", "X86", "Xiaoyun Wang", "Yiqun Lisa Yin", "YouTube"], "categories": ["All articles containing potentially dated statements", "All articles needing additional references", "All articles with specifically marked weasel-worded phrases", "All articles with unsourced statements", "Articles containing potentially dated statements from 2013", "Articles containing potentially dated statements from October 2015", "Articles needing additional references from May 2013", "Articles with Chinese-language external links", "Articles with DMOZ links", "Articles with example pseudocode", "Articles with specifically marked weasel-worded phrases from September 2015", "Articles with unsourced statements from August 2012", "Articles with unsourced statements from June 2015", "Broken hash functions", "CS1 errors: dates", "Checksum algorithms", "Cryptographic hash functions", "National Security Agency cryptography"], "title": "SHA-1"}
{"summary": "SHA-2 (Secure Hash Algorithm 2) is a set of cryptographic hash functions designed by the NSA. SHA stands for Secure Hash Algorithm. Cryptographic hash functions are mathematical operations run on digital data; by comparing the computed \"hash\" (the output from execution of the algorithm) to a known and expected hash value, a person can determine the data's integrity. For example, computing the hash of a downloaded file and comparing the result to a previously published hash result can show whether the download has been modified or tampered with. A key aspect of cryptographic hash functions is their collision resistance: nobody should be able to find two different input values that result in the same hash output.\nSHA-2 includes significant changes from its predecessor, SHA-1. The SHA-2 family consists of six hash functions with digests (hash values) that are 224, 256, 384 or 512 bits: SHA-224, SHA-256, SHA-384, SHA-512, SHA-512/224, SHA-512/256.\nSHA-256 and SHA-512 are novel hash functions computed with 32-bit and 64-bit words, respectively. They use different shift amounts and additive constants, but their structures are otherwise virtually identical, differing only in the number of rounds. SHA-224 and SHA-384 are simply truncated versions of the first two, computed with different initial values. SHA-512/224 and SHA-512/256 are also truncated versions of SHA-512, but the initial values are generated using the method described in FIPS PUB 180-4. SHA-2 was published in 2001 by the NIST as a U.S. federal standard (FIPS). The SHA-2 family of algorithms are patented in US 6829355 . The United States has released the patent under a royalty-free license.\nIn 2005, an algorithm emerged for finding SHA-1 collisions in about 2000-times fewer steps than was previously thought possible. Although (as of 2015) no example of a SHA-1 collision has been published yet, the security margin left by SHA-1 is weaker than intended, and its use is therefore no longer recommended for applications that depend on collision resistance, such as digital signatures. Although SHA-2 bears some similarity to the SHA-1 algorithm, these attacks have not been successfully extended to SHA-2.\nCurrently, the best public attacks break preimage resistance 52 rounds of SHA-256 or 57 rounds of SHA-512, and collision resistance for 46 rounds of SHA-256, as shown in the Cryptanalysis and validation section below.", "links": ["AMD Opteron", "ASIACRYPT", "ASIC", "Authenticated encryption", "Avalanche effect", "BLAKE (hash function)", "Bcrypt", "Biclique attack", "Birthday attack", "Bit", "Bitcoin", "Bitwise operation", "Block cipher", "Brute-force attack", "Brute force attack", "CBC-MAC", "CCM mode", "CMAC", "CRYPTREC", "CWC mode", "Collision (computer science)", "Collision attack", "Collision resistance", "Communications Security Establishment", "Comparison of cryptographic hash functions", "Crypt (C)", "Cryptanalysis", "Cryptocurrency", "Cryptographic Module Validation Program", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "DKIM", "DNSSEC", "Data Authentication Algorithm", "Debian GNU/Linux", "Differential cryptanalysis", "Digital object identifier", "Digital signature", "Digital timestamping", "EAX mode", "EUROCRYPT", "Elliptic curve only hash", "Fast Software Encryption", "Fast Syndrome Based Hash", "Federal Information Processing Standard", "GOST (hash function)", "Galois/Counter Mode", "Google Chrome", "Gr\u00f8stl", "HAS-160", "HAVAL", "HMAC", "Hash function security summary", "Hashcash", "History of cryptography", "IAPM (mode)", "IPsec", "International Association for Cryptologic Research", "International Criminal Tribunal for Rwanda", "International Standard Book Number", "International Standard Serial Number", "Ivy Bridge (microarchitecture)", "JH (hash function)", "Key derivation function", "Key stretching", "Kupyna", "LM hash", "Length extension attack", "MD2 (cryptography)", "MD4", "MD5", "MD6", "MDC-2", "Mebibyte", "Meet-in-the-middle attack", "Merkle\u2013Damg\u00e5rd construction", "Message authentication", "Message authentication code", "Message digest", "Modular arithmetic", "N-Hash", "NESSIE", "NIST", "NIST hash function competition", "National Institute of Standards and Technology", "National Security Agency", "OCB mode", "One-key MAC", "Outline of cryptography", "PBKDF2", "PMAC (cryptography)", "Password strength", "Piledriver (microarchitecture)", "Poly1305", "Portable Document Format", "Preimage attack", "Preimage resistance", "Pretty Good Privacy", "Proof-of-stake", "Proof-of-work", "Pseudocode", "Public-key cryptography", "RIPEMD", "RadioGat\u00fan", "Rainbow table", "S/MIME", "SHA-0", "SHA-1", "SHA-3", "SWIFFT", "Salt (cryptography)", "Scrypt", "Secure Hash Algorithm", "Secure Hash Standard", "Secure Shell", "Secure Sockets Layer", "Selected Areas in Cryptography", "Sha1sum", "Shadow password", "Side-channel attack", "SipHash", "Skein (hash function)", "Snefru", "Sony", "Steganography", "Stream cipher", "Streebog", "Symmetric-key algorithm", "The quick brown fox jumps over the lazy dog", "Tiger (cryptography)", "Transport Layer Security", "Triple DES", "U.S. Government", "UMAC", "United States", "University of Illinois at Chicago", "VMAC", "Very smooth hash", "Whirlpool (cryptography)", "X86", "X86-64", "XORed"], "categories": ["All articles containing potentially dated statements", "All articles with dead external links", "Articles containing potentially dated statements from 2013", "Articles with dead external links from November 2012", "Articles with example pseudocode", "Checksum algorithms", "Cryptographic hash functions", "National Security Agency cryptography"], "title": "SHA-2"}
{"summary": "The SYSV checksum algorithm is commonly used, legacy checksum algorithms. It has been implemented in UNIX System V and is also available through the GNU sum command line utility.", "links": ["BSD checksum", "Checksum", "Sum (Unix)", "UNIX System V"], "categories": ["Checksum algorithms"], "title": "SYSV checksum"}
{"summary": "The Verhoeff algorithm is a checksum formula for error detection developed by the Dutch mathematician Jacobus Verhoeff and was first published in 1969. It was the first decimal check digit algorithm which detects all single-digit errors, and all transposition errors involving two adjacent digits, which was at the time thought impossible with such a code.", "links": ["CPAN", "Cayley table", "Checksum", "Commutative", "Damm algorithm", "Digital object identifier", "Dihedral group", "Error detection", "International Standard Book Number", "Jacobus Verhoeff", "Library of Congress Control Number", "Lookup table", "Permutation"], "categories": ["Checksum algorithms", "Error detection and correction", "Modular arithmetic", "Wikipedia articles needing clarification from April 2014"], "title": "Verhoeff algorithm"}
{"summary": "In computer science and operations research, the Bees Algorithm is a population-based search algorithm which was developed in 2005. It mimics the food foraging behaviour of honey bee colonies. In its basic version the algorithm performs a kind of neighbourhood search combined with global search, and can be used for both combinatorial optimization and continuous optimization. The only condition for the application of the Bees Algorithm is that some measure of topological distance between the solutions is defined. The effectiveness and specific abilities of the Bees Algorithm have been proven in a number of studies.", "links": ["Active matter", "Agent-based model", "Agent-based model in biology", "Allee effect", "Altitudinal migration", "Animal migration", "Animal migration tracking", "Animal navigation", "Ant colony optimization algorithms", "Ant robotics", "Approximation algorithm", "Artificial Ants", "Artificial bee colony algorithm", "Augmented Lagrangian method", "Bait ball", "Barrier function", "Bat algorithm", "Bellman\u2013Ford algorithm", "Bird migration", "Boids", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Cell migration", "Clustering of self-propelled particles", "Coded wire tag", "Collective animal behavior", "Collective intelligence", "Collective motion", "Combinatorial optimization", "Comparison of optimization software", "Computer science", "Continuous optimization", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Crowd simulation", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Decentralised system", "Diel vertical migration", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Eusociality", "Evolutionary algorithm", "Evolutionary computation", "Exchange algorithm", "Feeding frenzy", "Firefly algorithm", "Fish migration", "Flock (birds)", "Flocking (behavior)", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Glowworm swarm optimization", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Group size measures", "Herd", "Herd behavior", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Homing (biology)", "Honey bees", "Insect migration", "Integer programming", "Intelligent Small World Autonomous Robots for Micro-manipulation", "Intelligent Water Drops", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Lepidoptera migration", "Lessepsian migration", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "L\u00e9vy flight foraging hypothesis", "Manufacturing Engineering Centre", "Mathematical optimization", "Matroid", "Metaheuristic", "Microbial intelligence", "Microbotics", "Minimum spanning tree", "Mixed-species foraging flock", "Mobbing (animal behavior)", "Monarch butterfly migration", "Mutualism (biology)", "Natal homing", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Operations research", "Optimization algorithm", "Pack (canine)", "Pack hunter", "Particle swarm optimization", "Patterns of self-organization in ants", "Penalty method", "Philopatry", "Powell's method", "Predator satiation", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Quorum sensing", "Reverse migration (birds)", "Revised simplex algorithm", "Salmon run", "Sardine run", "Sea turtle migration", "Search algorithm", "Self-propelled particles", "Sequential quadratic programming", "Shoaling and schooling", "Simplex algorithm", "Simulated annealing", "Sort sol (bird flock)", "Spatial organization", "Stigmergy", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Swarm (simulation)", "Swarm behaviour", "Swarm intelligence", "Swarm robotics", "Swarming (honey bee)", "Swarming (military)", "Swarming motility", "Symbrion", "Symmetric rank-one", "Symmetry breaking of escaping ants", "Tabu search", "Task allocation and partitioning of social insects", "Truncated Newton method", "Trust region", "Vicsek model", "Waggle dance", "Wolfe conditions"], "categories": ["Artificial intelligence", "Bees", "Collective intelligence", "Combinatorial algorithms", "Optimization algorithms and methods"], "title": "Bees algorithm"}
{"summary": "In mathematical optimization, the criss-cross algorithm denotes a family of algorithms for linear programming. Variants of the criss-cross algorithm also solve more general problems with linear inequality constraints and nonlinear objective functions; there are criss-cross algorithms for linear-fractional programming problems, quadratic-programming problems, and linear complementarity problems.\nLike the simplex algorithm of George B. Dantzig, the criss-cross algorithm is not a polynomial-time algorithm for linear programming. Both algorithms visit all 2D corners of a (perturbed) cube in dimension D, the Klee\u2013Minty cube (after Victor Klee and George J. Minty), in the worst case. However, when it is started at a random corner, the criss-cross algorithm on average visits only D additional corners. Thus, for the three-dimensional cube, the algorithm visits all 8 corners in the worst case and exactly 3 additional corners on average.", "links": ["Albert W. Tucker", "Algorithm", "Approximation algorithm", "Arithmetic operation", "Augmented Lagrangian method", "Average-case complexity", "Average case complexity", "Barrier function", "Bellman\u2013Ford algorithm", "Bernd Sturmfels", "Big Oh notation", "Big oh", "Bland's rule", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Buchberger's algorithm", "CiteSeer", "Combinatorial optimization", "Combinatorics", "Comparison of optimization software", "Convex hull", "Convex minimization", "Convex optimization", "Crisscross method", "Cubic polynomial", "Cutting-plane method", "David Avis", "Davidon\u2013Fletcher\u2013Powell formula", "Degree of a polynomial", "Digital object identifier", "Dijkstra's algorithm", "Dimension (vector space)", "Dinic's algorithm", "Discrete and Computational Geometry", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Ellipsoidal algorithm", "Euclidean metric", "Evolutionary algorithm", "Exchange algorithm", "Facet", "Farkas lemma", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gaussian elimination", "Gauss\u2013Newton algorithm", "George Dantzig", "George J. Minty", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "G\u00fcnter M. Ziegler", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Hyperplane", "Integer programming", "Interior-point method", "International Standard Book Number", "International Standard Serial Number", "Iterative method", "JSTOR", "Jack Edmonds", "Johnson's algorithm", "Karmarkar", "Karmarkar's algorithm", "Khachiyan", "Klee\u2013Minty cube", "Komei Fukuda", "Kris Kross", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear-fractional programming", "Linear algebra", "Linear complementarity problem", "Linear inequality", "Linear programming", "Linear system", "Local convergence", "Local search (optimization)", "Mathematical Reviews", "Mathematical optimization", "Matroid", "Max-flow min-cut theorem", "Metaheuristic", "Michael J. Todd (mathematician)", "Michel Las Vergnas", "Minimum spanning tree", "Mixed complementarity problem", "Mixed linear complementarity problem", "Multivariate polynomial", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear complementarity problem", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization (mathematics)", "Optimization algorithm", "Oriented matroid", "P-matrix", "Penalty method", "Polyhedron", "Positive-definite matrix", "Powell's method", "Principal minor", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "R. C. Bose", "R. Tyrrell Rockafellar", "Real number", "Revised simplex algorithm", "Richard W. Cottle", "Sequential quadratic programming", "Sign function", "Simplex algorithm", "Simulated annealing", "Space complexity", "Stephen Smale", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Sufficient matrix", "Symmetric rank-one", "Tabu search", "Tam\u00e1s Terlaky", "Time complexity", "Truncated Newton method", "Trust region", "Unit cube", "Unit sphere", "Vertex (geometry)", "Vertex enumeration problem", "Victor Klee", "Wolfe conditions", "Worst-case complexity", "Worst case complexity", "Zentralblatt MATH"], "categories": ["All articles to be expanded", "Articles to be expanded from April 2011", "Combinatorial algorithms", "Combinatorial optimization", "Exchange algorithms", "Geometric algorithms", "Linear programming", "Optimization algorithms and methods", "Oriented matroids", "Use dmy dates from December 2013"], "title": "Criss-cross algorithm"}
{"summary": "In computer science, cycle detection is the algorithmic problem of finding a cycle in a sequence of iterated function values.\nFor any function \u0192 that maps a finite set S to itself, and any initial value x0 in S, the sequence of iterated function values\n\nmust eventually use the same value twice: there must be some i \u2260 j such that xi = xj. Once this happens, the sequence must continue periodically, by repeating the same sequence of values from xi to xj\u22121. Cycle detection is the problem of finding i and j, given \u0192 and x0.", "links": ["Abelian group", "Alan Sherman", "Algorithm", "Andrew Yao", "Average case analysis", "Burt Kaliski", "Celestial mechanics", "Cellular automaton", "Claus P. Schnorr", "Common Lisp", "Computational group theory", "Computer program", "Computer science", "Computer simulation", "Cryptographic hash function", "Cryptography", "Cycle detection (graph theory)", "Cycle graph", "Data Encryption Standard", "Data structure", "Digital object identifier", "Directed graph", "Discrete logarithm", "Donald Knuth", "Eric Allender", "Faith Ellen", "Finite set", "Function (mathematics)", "Functional graph", "Glossary of graph theory", "Graph theory", "Greatest common divisor", "Hash collision", "Hash table", "Hendrik Lenstra", "Infinite loop", "Integer factorization", "Iterated function", "JSTOR", "Linear congruential generator", "Linked list", "Link\u00f6ping University", "Maria Klawe", "Mathematical folklore", "Number theory", "Oscillator (cellular automaton)", "Periodic sequence", "Phase space", "Pointer algorithm", "Pollard's kangaroo algorithm", "Pollard's rho algorithm", "Power of two", "Pseudorandom number generator", "Python (programming language)", "Reachability", "Rho (letter)", "Richard Brent (scientist)", "Robert Sedgewick (computer scientist)", "Robert W. Floyd", "Ron Rivest", "S-expression", "Sequence", "Shape analysis (software)", "Space complexity", "Stack (data structure)", "The Tortoise and the Hare", "William Kahan"], "categories": ["Articles with example Python code", "Combinatorial algorithms", "Fixed points (mathematics)"], "title": "Cycle detection"}
{"summary": "The Fisher\u2013Yates shuffle (named after Ronald Fisher and Frank Yates), also known as the Knuth shuffle (after Donald Knuth), is an algorithm for generating a random permutation of a finite set\u2014in plain terms, for randomly shuffling the set. A variant of the Fisher\u2013Yates shuffle, known as Sattolo's algorithm, may be used to generate random cyclic permutations of length n instead. The Fisher\u2013Yates shuffle is unbiased, so that every permutation is equally likely. The modern version of the algorithm is also rather efficient, requiring only time proportional to the number of items being shuffled and no additional storage space.\nFisher\u2013Yates shuffling is similar to randomly picking numbered tickets (combinatorics: distinguishable objects) out of a hat without replacement until there are none left.\n\n", "links": ["-yllion", "AMS Euler", "Algorithm", "Array data structure", "Bernoulli process", "Biased sample", "Binomial distribution", "C. R. Rao", "CWEB", "Combinatorics", "Computer Modern", "Computers and Typesetting", "Concrete Mathematics", "Concrete Roman", "Cycle notation", "Cyclic permutation", "Dancing Links", "Digital object identifier", "Dijkstra's algorithm", "Donald E. Knuth", "Donald Knuth", "Expected value", "Factorial", "Finite set", "Floating-point", "Font", "Frank Yates", "GNU MIX Development Kit", "International Standard Book Number", "International Standard Serial Number", "Knuth's Algorithm X", "Knuth's Simpath algorithm", "Knuth Prize", "Knuth reward check", "Knuth\u2013Bendix completion algorithm", "Knuth\u2013Morris\u2013Pratt algorithm", "Linear congruential generator", "Literate programming", "METAFONT", "MIX", "MMIX", "Man or boy test", "Merge sort", "Modulo operator", "National Institute of Standards and Technology", "OCLC", "Off-by-one error", "Pivot element", "Playing card", "Potrzebie", "Power of 2", "Prime factor", "Pseudorandom", "Pseudorandom number generator", "Python (programming language)", "Quater-imaginary base", "Quicksort", "RC4", "Radix sort", "Random number generator", "Random permutation", "Reservoir sampling", "Richard Durstenfeld", "Robinson\u2013Schensted\u2013Knuth correspondence", "Ronald A. Fisher", "Ronald Fisher", "Sandra Sattolo", "Selected papers series of Knuth", "Sentinel value", "Shuffling", "Software", "Surreal Numbers (book)", "TeX", "The Art of Computer Programming", "The Complexity of Songs", "Things a Computer Scientist Rarely Talks About", "Time complexity", "Trabb Pardo\u2013Knuth algorithm", "Transitive relation", "Uniform distribution (discrete)", "WEB"], "categories": ["CS1 errors: external links", "Combinatorial algorithms", "Monte Carlo methods", "Permutations", "Randomness"], "title": "Fisher\u2013Yates shuffle"}
{"summary": "A greedy algorithm is an algorithm that follows the problem solving heuristic of making the locally optimal choice at each stage with the hope of finding a global optimum. In many problems, a greedy strategy does not in general produce an optimal solution, but nonetheless a greedy heuristic may yield locally optimal solutions that approximate a global optimal solution in a reasonable time.\nFor example, a greedy strategy for the traveling salesman problem (which is of a high computational complexity) is the following heuristic: \"At each stage visit an unvisited city nearest to the current city\". This heuristic need not find a best solution, but terminates in a reasonable number of steps; finding an optimal solution typically requires unreasonably many steps. In mathematical optimization, greedy algorithms solve combinatorial problems having the properties of matroids.", "links": ["Activity selection problem", "Ad hoc network", "Algorithm", "Approximation algorithm", "Artificial intelligence", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Change-making problem", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Crystal Quest", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Decision tree learning", "Dijkstra's algorithm", "Dinic's algorithm", "Distributed hash table", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Encyclopedia of Mathematics", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Geographic routing", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Graph coloring problem", "Greedoid", "Greedy algorithm for Egyptian fractions", "Greedy coloring", "Greedy source", "Hessian matrix", "Heuristic (computer science)", "Heuristic algorithm", "Hill climbing", "Huffman coding", "Huffman tree", "Integer programming", "International Standard Book Number", "Introduction to Algorithms", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Macintosh computer", "Malfatti circles", "Matching pursuit", "Mathematical optimization", "Mathematical problem", "Matroid", "Metaheuristic", "Minimum spanning tree", "Multi-armed bandit", "NP-complete", "National Institute of Standards and Technology", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimal substructure", "Optimization algorithm", "Penalty method", "Powell's method", "Prim's algorithm", "Problem solving", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Routing", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Small world routing", "Springer Science+Business Media", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Traveling salesman problem", "Travelling salesman problem", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["Combinatorial algorithms", "Commons category without a link on Wikidata", "Exchange algorithms", "Matroid theory", "Optimization algorithms and methods"], "title": "Greedy algorithm"}
{"summary": "Heap's algorithm generates all possible permutations of N objects. It was first proposed by B. R. Heap in 1963. The algorithm minimizes movement: it generates each permutation from the previous one by interchanging a single pair of elements; the other N\u22122 elements are not disturbed. In a 1977 review of permutation-generating algorithms, Robert Sedgewick concluded that it was at that time the most effective algorithm for generating permutations by computer.", "links": ["Algorithm", "Digital object identifier", "Permutation", "Robert Sedgewick (computer scientist)", "Steinhaus\u2013Johnson\u2013Trotter algorithm"], "categories": ["Combinatorial algorithms", "Permutations"], "title": "Heap's algorithm"}
{"summary": "This article is about the heuristic algorithm for the graph partitioning problem. For a heuristic for the traveling salesperson problem, see Lin\u2013Kernighan heuristic.\nKernighan\u2013Lin is a O(n2 log(n)) heuristic algorithm for solving the graph partitioning problem. The algorithm has important applications in the layout of digital circuits and components in VLSI.", "links": ["Brian Kernighan", "Digital object identifier", "Graph partitioning problem", "Heuristic algorithm", "International Standard Book Number", "Lin\u2013Kernighan heuristic", "OCLC", "Shen Lin", "VLSI"], "categories": ["Combinatorial algorithms", "Combinatorial optimization", "Heuristic algorithms"], "title": "Kernighan\u2013Lin algorithm"}
{"summary": "The Lemke\u2013Howson algorithm  is an algorithm that computes a Nash equilibrium of a bimatrix game. It is said to be \u201cthe best known among the combinatorial algorithms for finding a Nash equilibrium\u201d.", "links": ["Algorithm", "Christos Papadimitriou", "Digital object identifier", "Game theory", "Homotopy", "International Standard Book Number", "Nash equilibrium", "Noam Nisan", "PSPACE-complete", "Polytope", "Vijay Vazirani", "\u00c9va Tardos"], "categories": ["Combinatorial algorithms", "Game theory", "Non-cooperative games"], "title": "Lemke\u2013Howson algorithm"}
{"summary": "This article is about the heuristic for the travelling salesman problem. For a heuristic algorithm for the graph partitioning problem, see Kernighan\u2013Lin algorithm.\nIn combinatorial optimization, Lin\u2013Kernighan is one of the best heuristics for solving the travelling salesman problem. Briefly, it involves swapping pairs of sub-tours to make a new tour. It is a generalization of 2-opt and 3-opt. 2-opt and 3-opt work by switching two or three paths to make the tour shorter. Lin\u2013Kernighan is adaptive and at each step decides how many paths between cities need to be switched to find a shorter tour.", "links": ["2-opt", "3-opt", "Algorithm", "Applied mathematics", "Brian Kernighan", "Combinatorial optimization", "Data structure", "Digital object identifier", "Heuristic algorithm", "Jan Karel Lenstra", "Kernighan\u2013Lin algorithm", "Lin\u2013Kernighan\u2013Johnson", "Local search (optimization)", "Shen Lin", "Travelling salesman problem"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Applied mathematics stubs", "Combinatorial algorithms", "Combinatorial optimization", "Computer science stubs", "Heuristic algorithms", "Travelling salesman problem"], "title": "Lin\u2013Kernighan heuristic"}
{"summary": "In computational combinatorics, a loopless algorithm or loopless imperative algorithm is an imperative algorithm that generates successive combinatorial objects, such as partitions, permutations, and combinations, in constant time and the first object in linear time. The objects must be immediately available in simple form without requiring any additional steps.\nA loopless functional algorithm is a functional algorithm takes the form unfoldr step \u2022 prolog where step takes constant time and prolog takes linear time in the size of the input. The standard function unfoldr is a right-associative Bird unfold.", "links": ["Addison\u2013Wesley", "Algorithm", "Anamorphism", "Association for Computing Machinery", "Combination", "Combinatorics", "Constant time", "Digital object identifier", "Donald Knuth", "Functional programming", "Germany", "Heidelberg", "Imperative programming", "International Standard Book Number", "International Standard Serial Number", "Journal of the ACM", "Linear time", "New York City", "OCLC", "Oxford", "Partition of a set", "Permutation", "Richard Bird (computer scientist)", "Springer Science+Business Media", "Subroutine", "The Art of Computer Programming", "United Kingdom", "University of Oxford", "Upper Saddle River, New Jersey"], "categories": ["All stub articles", "Combinatorial algorithms", "Combinatorics stubs"], "title": "Loopless algorithm"}
{"summary": "In mathematics, the Robinson\u2013Schensted correspondence is a bijective correspondence between permutations and pairs of standard Young tableaux of the same shape. It has various descriptions, all of which are of algorithmic nature, it has many remarkable properties, and it has applications in combinatorics and other areas such as representation theory. The correspondence has been generalized in numerous ways, notably by Knuth to what is known as the Robinson\u2013Schensted\u2013Knuth correspondence, and a further generalization to pictures by Zelevinsky.\nThe simplest description of the correspondence is using the Schensted algorithm (Schensted 1961), a procedure that constructs one tableau by successively inserting the values of the permutation according to a specific rule, while the other tableau records the evolution of the shape during construction. The correspondence had been described, in a rather different form, much earlier by Robinson (Robinson 1938), in an attempt to prove the Littlewood\u2013Richardson rule. The correspondence is often referred to as the Robinson\u2013Schensted algorithm, although the procedure used by Robinson is radically different from the Schensted\u2013algorithm, and almost entirely forgotten. Other methods of defining the correspondence include a nondeterministic algorithm in terms of jeu de taquin.\nThe bijective nature of the correspondence relates it to the enumerative identity:\n\nwhere  denotes the set of partitions of n (or of Young diagrams with n squares), and t\u03bb denotes the number of standard Young tableaux of shape \u03bb.", "links": ["American Journal of Mathematics", "Bijection", "Cambridge University Press", "Canadian Journal of Mathematics", "Combinatorics", "Craige Schensted", "Digital object identifier", "Donald Ervin Knuth", "Donald Knuth", "Encyclopedia of Mathematics", "Enumerative combinatorics", "Gilbert de Beauregard Robinson", "International Standard Book Number", "Involution (mathematics)", "JSTOR", "James Alexander Green", "Jeu de taquin", "Littlewood\u2013Richardson rule", "Longest increasing subsequence", "Mathematical Reviews", "Mathematics", "Nondeterministic algorithm", "Pacific Journal of Mathematics", "Partition (number theory)", "Permutation", "Picture (mathematics)", "Pseudocode", "Representation theory", "Richard P. Stanley", "Robinson\u2013Schensted\u2013Knuth correspondence", "Sch\u00fctzenberger involution", "Springer-Verlag", "Springer Science+Business Media", "The Art of Computer Programming", "Viennot's geometric construction", "William Fulton (mathematician)", "Young diagram", "Young tableaux", "Zentralblatt MATH"], "categories": ["Algebraic combinatorics", "Combinatorial algorithms", "Permutations", "Representation theory of finite groups"], "title": "Robinson\u2013Schensted correspondence"}
{"summary": "In mathematics, the Robinson\u2013Schensted\u2013Knuth correspondence, also referred to as the RSK correspondence or RSK algorithm, is a combinatorial bijection between matrices A with non-negative integer entries and pairs (P,Q) of semistandard Young tableaux of equal shape, whose size equals the sum of the entries of A. More precisely the weight of P is given by the column sums of A, and the weight of Q by its row sums. It is a generalization of the Robinson\u2013Schensted correspondence, in the sense that taking A to be a permutation matrix, the pair (P,Q) will be the pair of standard tableaux associated to the permutation under the Robinson\u2013Schensted correspondence.\nThe Robinson\u2013Schensted\u2013Knuth correspondence extends many of the remarkable properties of the Robinson\u2013Schensted correspondence, notably its symmetry: transposition of the matrix A results in interchange of the tableaux P,Q.", "links": ["-yllion", "AMS Euler", "Algorithm", "Bijection", "Bijective", "CWEB", "Cambridge University Press", "Computer Modern", "Computers and Typesetting", "Concrete Mathematics", "Concrete Roman", "Dancing Links", "Digital object identifier", "Dijkstra's algorithm", "Donald Knuth", "Fisher\u2013Yates shuffle", "Font", "GNU MIX Development Kit", "International Standard Book Number", "Knuth's Algorithm X", "Knuth's Simpath algorithm", "Knuth Prize", "Knuth reward check", "Knuth\u2013Bendix completion algorithm", "Knuth\u2013Morris\u2013Pratt algorithm", "Kostka number", "Literate programming", "METAFONT", "MIX", "MMIX", "Man or boy test", "Mathematical Reviews", "Mathematics", "Non-negative integer", "Partition (number theory)", "Permutation", "Permutation matrix", "Potrzebie", "Quater-imaginary base", "Robinson\u2013Schensted correspondence", "Schur polynomial", "Selected papers series of Knuth", "Software", "Surreal Numbers (book)", "TeX", "The Art of Computer Programming", "The Complexity of Songs", "Things a Computer Scientist Rarely Talks About", "Trabb Pardo\u2013Knuth algorithm", "WEB", "Young tableau", "Zentralblatt MATH"], "categories": ["Algebraic combinatorics", "Combinatorial algorithms", "Permutations", "Symmetric functions"], "title": "Robinson\u2013Schensted\u2013Knuth correspondence"}
{"summary": "The SMAWK algorithm is an algorithm for finding the minimum value in each row of an implicitly-defined totally monotone matrix. It is named after the initials of its five inventors, Peter Shor, Shlomo Moran, Alok Aggarwal, Robert Wilber, and Maria Klawe.\nFor the purposes of this algorithm, a matrix is defined to be monotone if each row's minimum value occurs in a column which is equal to or greater than the column of the previous row's minimum. It is totally monotone if the same property is true for every submatrix (defined by an arbitrary subset of the rows and columns of the given matrix). Equivalently, a matrix is totally monotone if there does not exist a 2\u00d72 submatrix whose row minima are in the top right and bottom left corners. Every Monge array is totally monotone, but not necessarily vice versa.\nFor the SMAWK algorithm, the matrix to be searched should be defined as a function, and this function is given as input to the algorithm (together with the dimensions of the matrix). The algorithm then evaluates the function whenever it needs to know the value of a particular matrix cell. If this evaluation takes O(1), then, for a matrix with r rows and c columns, the running time and number of function evaluations are both O(c(1 + log(r/c))). This is much faster than the O(rc) time of a naive algorithm that evaluates all matrix cells.\nThe main applications of this method presented in the original paper by Aggarwal et al. were in computational geometry, in finding the farthest point from each point of a convex polygon, and in finding optimal enclosing polygons. Subsequent research found applications of the same algorithm in breaking paragraphs into lines, RNA secondary structure prediction, DNA and protein sequence alignment, the construction of prefix codes, and image thresholding, among others.", "links": ["Algorithm", "Computational geometry", "DNA", "Data structure", "Digital object identifier", "Lawrence L. Larmore", "Maria Klawe", "Mathematical Reviews", "Matrix (mathematics)", "Monge array", "Nucleic acid secondary structure", "Peter Shor", "Prefix code", "Protein", "RNA", "Sequence alignment", "Shlomo Moran", "Thresholding (image processing)", "Word wrap"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Combinatorial algorithms", "Computer science stubs", "Matrix theory"], "title": "SMAWK algorithm"}
{"summary": "The Steinhaus\u2013Johnson\u2013Trotter algorithm or Johnson\u2013Trotter algorithm, also called plain changes, is an algorithm named after Hugo Steinhaus, Selmer M. Johnson and Hale F. Trotter that generates all of the permutations of n elements. Each permutation in the sequence that it generates differs from the previous permutation by swapping two adjacent elements of the sequence. Equivalently, this algorithm finds a Hamiltonian path in the permutohedron.\nThis method was known already to 17th-century English change ringers, and Sedgewick (1977) calls it \"perhaps the most prominent permutation enumeration algorithm\". As well as being simple and computationally efficient, it has the advantage that subsequent computations on the permutations that it generates may be sped up because these permutations are so similar to each other.", "links": ["Algorithm", "Carla Savage", "Cayley graph", "Change ringing", "CiteSeer", "Convex hull", "Cut-the-knot", "Digital object identifier", "Donald Knuth", "Edsger W. Dijkstra", "Fabian Stedman", "Factorial number system", "Gray code", "Hale F. Trotter", "Hamiltonian path", "Heap's algorithm", "Hugo Steinhaus", "Inverse element", "Inverse function", "Inversion (discrete mathematics)", "JSTOR", "Loopless algorithm", "Mathematical Reviews", "Mixed radix", "Nachum Dershowitz", "Parity of a permutation", "Permutation", "Permutohedron", "Polytope", "Pseudocode", "Radix", "Recursive algorithm", "Robert Sedgewick (computer scientist)", "SIAM Review", "Selmer M. Johnson", "Shimon Even", "Sourceforge", "Symmetric group", "The Art of Computer Programming", "Truncated octahedron"], "categories": ["Combinatorial algorithms", "Permutations"], "title": "Steinhaus\u2013Johnson\u2013Trotter algorithm"}
{"summary": "The Tompkins\u2013Paige algorithm is a computer algorithm for generating all permutations of a finite set of objects.", "links": ["Algorithm", "Digital object identifier", "Permutation", "Pseudocode", "Robert Sedgewick"], "categories": ["All articles covered by WikiProject Wikify", "All articles with too few wikilinks", "Articles covered by WikiProject Wikify from March 2013", "Articles with too few wikilinks from March 2013", "Combinatorial algorithms", "Permutations"], "title": "Tompkins\u2013Paige algorithm"}
{"summary": "In computer science, A* (pronounced as \"A star\" ( listen)) is a computer algorithm that is widely used in pathfinding and graph traversal, the process of plotting an efficiently traversable path between multiple points, called nodes. Noted for its performance and accuracy, it enjoys widespread use. However, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, although other work has found A* to be superior to other approaches.\nPeter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first described the algorithm in 1968. It is an extension of Edsger Dijkstra's 1959 algorithm. A* achieves better performance by using heuristics to guide its search.", "links": ["A*", "A* (disambiguation)", "AI Magazine", "A star", "Admissible heuristic", "Amortized time", "Any-angle path planning", "Artificial Intelligence: A Modern Approach", "Association for Computing Machinery", "Bertram Raphael", "Best, worst and average case", "Best-first search", "Bidirectional search", "Binary heap", "Branch and bound", "Branching factor", "Breadth-first search", "Computational complexity theory", "Computer algorithm", "Computer performance", "Computer science", "Consistent heuristic", "D*", "Depth-first search", "Digital object identifier", "Dijkstra's algorithm", "Dorothea Wagner", "Edsger Dijkstra", "Exponential time", "Fibonacci heap", "Fringe search", "Goal node", "Graph (data structure)", "Graph traversal", "Greedy algorithm", "Hash table", "Heuristic", "Heuristic (computer science)", "IDA*", "Incremental heuristic search", "Informed search algorithm", "Institute of Electrical and Electronics Engineers", "International Standard Book Number", "Journal of the ACM", "Jump point search", "LIFO (computing)", "Logarithm", "Motion planning", "Natural language processing", "Nils Nilsson (researcher)", "Node (graph theory)", "Parsing", "Pathfinding", "Peter E. Hart", "Peter Norvig", "Peter Sanders (computer scientist)", "Polynomial time", "Princeton University", "Priority queue", "Pseudocode", "Reduced cost", "Robotics", "Routing", "SMA*", "SRI International", "Search algorithm", "Shakey the Robot", "Stochastic context-free grammar", "Stuart J. Russell", "Tree (data structure)"], "categories": ["All accuracy disputes", "Articles with disputed statements from February 2014", "Articles with example pseudocode", "Combinatorial optimization", "Game artificial intelligence", "Graph algorithms", "Routing algorithms", "Search algorithms"], "title": "A* search algorithm"}
{"summary": "In computer science, B* (pronounced \"B star\") is a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals). First published by Hans Berliner in 1979, it is related to the A* search algorithm.", "links": ["A* search algorithm", "Alpha-beta pruning", "Alpha\u2013beta pruning", "Artificial Intelligence: A Modern Approach", "Artificial Intelligence (journal)", "B*-tree", "B-Tree", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Computer science", "D*", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Goal node", "Graph search algorithm", "Graph traversal", "Hans Berliner", "Hill climbing", "International Standard Book Number", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Maven (Scrabble)", "Node (graph theory)", "Prim's algorithm", "SMA*", "Search game", "Tree (graph theory)", "Tree traversal"], "categories": ["Combinatorial optimization", "Game artificial intelligence", "Graph algorithms", "Routing algorithms", "Search algorithms"], "title": "B*"}
{"summary": "The Bottleneck traveling salesman problem (bottleneck TSP) is a problem in discrete or combinatorial optimization. It is stated as follows: Find the Hamiltonian cycle in a weighted graph which minimizes the weight of the most weighty edge of the cycle.\nThe problem is known to be NP-hard. The decision problem version of this, \"for a given length x, is there a Hamiltonian cycle in a graph g with no edge longer than x?\", is NP-complete.\nIn an asymmetric bottleneck TSP, there are cases where the weight from node A to B is different from the weight from B to A (e. g. travel time between two cities with a traffic jam in one direction).\nEuclidean bottleneck TSP, or planar bottleneck TSP, is the bottleneck TSP with the distance being the ordinary Euclidean distance. The problem still remains NP-hard, however many heuristics work better.\nIf the graph is a metric space then there is an efficient approximation algorithm that finds a Hamiltonian cycle with maximum edge weight being no more than twice the optimum.", "links": ["Approximation algorithm", "Combinatorial optimization", "Computers and Intractability: A Guide to the Theory of NP-Completeness", "David S. Johnson", "Decision problem", "Discrete optimization", "Euclidean distance", "Glossary of graph theory", "Hamiltonian path", "International Standard Book Number", "Metric space", "Michael R. Garey", "NP-complete", "NP-hard", "Travelling salesman problem"], "categories": ["Combinatorial optimization", "Graph algorithms", "Hamiltonian paths and cycles"], "title": "Bottleneck traveling salesman problem"}
{"summary": "Branch and bound (BB or B&B) is an algorithm design paradigm for discrete and combinatorial optimization problems, as well as general real valued problems. A branch-and-bound algorithm consists of a systematic enumeration of candidate solutions by means of state space search: the set of candidate solutions is thought of as forming a rooted tree with the full set at the root. The algorithm explores branches of this tree, which represent subsets of the solution set. Before enumerating the candidate solutions of a branch, the branch is checked against upper and lower estimated bounds on the optimal solution, and is discarded if it cannot produce a better solution than the best one found so far by the algorithm.\nThe algorithm depends on the efficient estimation of the lower and upper bounds of a region/branch of the search space and approaches exhaustive enumeration as the size (n-dimensional volume) of the region tends to zero.\nThe method was first proposed by A. H. Land and A. G. Doig in 1960 for discrete programming, and has become the most commonly used tool for solving NP-hard optimization problems. The name \"branch and bound\" first occurred in the work of Little et al. on the traveling salesman problem.", "links": ["0/1 knapsack problem", "A* search algorithm", "Algorithm", "Alpha-beta pruning", "Alpha\u2013beta pruning", "Approximation algorithm", "Artificial intelligence", "Augmented Lagrangian method", "B*", "Backtracking", "Barrier function", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and cut", "Breadth-first search", "British Museum algorithm", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Brute-force search", "Candidate solution", "Carnegie Mellon University", "Combinatorial optimization", "Comparison of optimization software", "Computational phylogenetics", "Computer vision", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Cutting plane", "Cutting stock problem", "D*", "Data structure", "Davidon\u2013Fletcher\u2013Powell formula", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Discrete optimization", "Disjoint sets", "Dynamic programming", "Edmonds' algorithm", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "False noise analysis", "Feasible region", "Feature selection", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Fringe search", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Graph traversal", "Greedy algorithm", "Hessian matrix", "Heuristic", "Heuristic algorithm", "Hill climbing", "Integer linear programs", "Integer programming", "International Standard Book Number", "Interval arithmetic", "Interval contractor", "Iterative deepening A*", "Iterative deepening depth-first search", "Iterative method", "Johnson's algorithm", "Jump point search", "Karmarkar's algorithm", "Kruskal's algorithm", "Kurt Mehlhorn", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Lexicographic breadth-first search", "Limited-memory BFGS", "Line search", "Linear programming", "List of algorithms", "Local convergence", "Local search (optimization)", "Machine learning", "Mathematical optimization", "Matroid", "Maximum satisfiability problem", "Metaheuristic", "Minimum spanning tree", "NP-hard", "Nearest neighbor search", "Nelder\u2013Mead method", "Newton's method in optimization", "Noise", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization algorithm", "Penalty method", "Peter Sanders (computer scientist)", "Powell's method", "Prim's algorithm", "Priority queue", "Probability", "Push\u2013relabel maximum flow algorithm", "Quadratic assignment problem", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "SMA*", "Search game", "Search tree", "Sequential quadratic programming", "Set estimation", "Set inversion", "Simplex algorithm", "Simulated annealing", "Stack (data structure)", "State space search", "Statistics", "Structured prediction", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Traveling salesman problem", "Travelling salesman problem", "Tree (graph theory)", "Tree traversal", "Truncated Newton method", "Trust region", "University of Copenhagen", "Without loss of generality", "Wolfe conditions"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from July 2015", "Articles with unsourced statements from September 2015", "Combinatorial optimization", "Optimization algorithms and methods", "Wikipedia articles needing clarification from July 2015"], "title": "Branch and bound"}
{"summary": "Branch and cut is a method of combinatorial optimization for solving integer linear programs (ILPs), that is, linear programming (LP) problems where some or all the unknowns are restricted to integer values. Branch and cut involves running a branch and bound algorithm and using cutting planes to tighten the linear programming relaxations. Note that if cuts are only used to tighten the initial LP relaxation, the algorithm is called cut and branch.", "links": ["Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Cutting plane", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer", "Integer linear program", "Integer programming", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Linear programming relaxation", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["Combinatorial optimization", "Optimization algorithms and methods"], "title": "Branch and cut"}
{"summary": "In computer science and artificial intelligence, combinatorial search studies search algorithms for solving instances of problems that are believed to be hard in general, by efficiently exploring the usually large solution space of these instances. Combinatorial search algorithms achieve this efficiency by reducing the effective size of the search space or employing heuristics. Some algorithms are guaranteed to find the optimal solution, while others may only return the best solution found in the part of the state space that was explored.\nClassic combinatorial search problems include solving the eight queens puzzle or evaluating moves in games with a large game tree, such as reversi or chess.\nA study of computational complexity theory helps to motivate combinatorial search. Combinatorial search algorithms are typically concerned with problems that are NP-hard. Such problems are not believed to be efficiently solvable in general. However, the various approximations of complexity theory suggest that some instances (e.g. \"small\" instances) of these problems could be efficiently solved. This is indeed the case, and such instances often have important practical ramifications.\n\n", "links": ["A* search algorithm", "Alpha-beta pruning", "Artificial intelligence", "Branch-and-bound", "Breadth-first search", "Brute-force search", "Chess", "Combinatorial explosion", "Combinatorial optimization", "Computational complexity theory", "Computer Go", "Computer chess", "Computer science", "Eight queens puzzle", "Exponential growth", "Game tree", "Graph (data structure)", "Minimax", "NP-hard", "Reversi", "Search algorithm", "Search algorithms", "State space search"], "categories": ["All articles lacking in-text citations", "Analysis of algorithms", "Articles lacking in-text citations from January 2013", "Combinatorial optimization", "Computational complexity theory", "Game artificial intelligence", "Search algorithms"], "title": "Combinatorial search"}
{"summary": "Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.\nThe algorithm exists in many variants; Dijkstra's original variant found the shortest path between two nodes, but a more common variant fixes a single node as the \"source\" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest path tree.\nFor a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road, Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. As a result, the shortest path algorithm is widely used in network routing protocols, most notably IS-IS and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson's.\nDijkstra's original algorithm does not use a min-priority queue and runs in time  (where  is the number of nodes). The idea of this algorithm is also given in (Leyzorek et al. 1957). The implementation based on a min-priority queue implemented by a Fibonacci heap and running in  (where  is the number of edges) is due to (Fredman & Tarjan 1984). This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights.\nIn some fields, artificial intelligence in particular, Dijkstra's algorithm or a variant of it is known as uniform-cost search and formulated as an instance of the more general idea of best-first search.", "links": ["A* algorithm", "A* search algorithm", "A-star algorithm", "Adjacency list", "Admissible heuristic", "Algorithm", "Alpha\u2013beta pruning", "Artificial Intelligence: A Modern Approach", "Artificial intelligence", "Asymptotic computational complexity", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best, worst and average case", "Best-first search", "Bidirectional search", "Big O notation", "Binary heap", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Brodal queue", "Charles Babbage Institute", "Charles E. Leiserson", "Clifford Stein", "Computer scientist", "Consistent heuristic", "D*", "Depth-first search", "Depth-limited search", "Digital object identifier", "Directed acyclic graph", "Directed graph", "Donald Knuth", "Dover Publications", "Dual linear program", "Dykstra's projection algorithm", "Dynamic programming", "Edmonds' algorithm", "Edsger W. Dijkstra", "Euclidean shortest path", "Fast marching method", "Fibonacci heap", "Flood fill", "Floyd\u2013Warshall algorithm", "Francis & Taylor", "Fringe search", "Graph (abstract data type)", "Graph (data structure)", "Graph labeling", "Graph traversal", "Greedy algorithm", "Hill climbing", "IEEE", "IS-IS", "Information Processing Letters", "International Standard Book Number", "Intersection (road)", "Introduction to Algorithms", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Kurt Mehlhorn", "Lexicographic breadth-first search", "Linear programming", "Link-state routing protocol", "List of algorithms", "Longest path problem", "MIT Press", "McGraw\u2013Hill", "Michael Fredman", "Min-priority queue", "Minimum spanning tree", "Motion planning", "Negative cycle", "Neighbourhood (graph theory)", "OSPF", "PDF", "Pairing heap", "Peter Norvig", "Peter Sanders (computer scientist)", "Prim's algorithm", "Principle of Optimality", "Priority queue", "Probability distribution", "Radix heap", "Reduced cost", "Richard Bellman", "Robert Endre Tarjan", "Robert Tarjan", "Robotics", "Ronald L. Rivest", "Routing protocol", "SMA*", "Search algorithm", "Search game", "Self-balancing binary search tree", "Shortest path problem", "Shortest path tree", "Sparse graph", "Stuart J. Russell", "Subroutine", "Thomas H. Cormen", "Time complexity", "Transportation Science", "Tree traversal", "Vertex (graph theory)", "Wavefront"], "categories": ["1959 in computer science", "Articles with example pseudocode", "Combinatorial optimization", "Commons category with local link same as on Wikidata", "Dutch inventions", "Graph algorithms", "Routing algorithms", "Search algorithms", "Use dmy dates from February 2011"], "title": "Dijkstra's algorithm"}
{"summary": "In computer science and operations research, harmony search (HS) is a phenomenon-mimicking algorithm (also known as metaheuristic algorithm, soft computing algorithm or evolutionary algorithm) inspired by the improvisation process of musicians proposed by Zong Woo Geem in 2001. In the HS algorithm, each musician (= decision variable) plays (= generates) a note (= a value) for finding a best harmony (= global optimum) all together. Proponents claim the following merits:\nHS does not require differential gradients, thus it can consider discontinuous functions as well as continuous functions.\nHS can handle discrete variables as well as continuous variables.\nHS does not require initial value setting for the variables.\nHS is free from divergence.\nHS may escape local optima.\nHS may overcome the drawback of GA's building block theory which works well only if the relationship among variables in a chromosome is carefully considered. If neighbor variables in a chromosome have weaker relationship than remote variables, building block theory may not work well because of crossover operation. However, HS explicitly considers the relationship using ensemble operation.\nHS has a novel stochastic derivative applied to discrete variables, which uses musician's experiences as a searching direction.\nCertain HS variants do not require algorithm parameters such as HMCR and PAR, thus novice users can easily use the algorithm.", "links": ["Algorithm", "Ant colony optimization", "Antwerp University", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Computer science", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cross-entropy method", "Cutting-plane method", "Dalle Molle Institute for Artificial Intelligence Research", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolution Strategies", "Evolutionary algorithm", "Evolutionary algorithms", "Evolutionary computation", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Genetic algorithm", "Genetic algorithms", "Genetic programming", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Intelligent Water Drops", "International Journal of Applied Metaheuristic Computing", "International Transactions in Operational Research", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Operations research", "Optimization (mathematics)", "Optimization algorithm", "Particle swarm optimization", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Soft computing", "Stochastic optimization", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Swarm Intelligence", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions", "Zong Woo Geem"], "categories": ["All NPOV disputes", "All articles lacking in-text citations", "All articles with unsourced statements", "Articles lacking in-text citations from April 2013", "Articles with unsourced statements from April 2013", "Combinatorial optimization", "Evolutionary algorithms", "NPOV disputes from April 2013", "Optimization algorithms and methods", "Wikipedia articles with possible conflicts of interest from April 2013"], "title": "Harmony search"}
{"summary": "Job shop scheduling (or job-shop problem) is an optimization problem in computer science and operations research in which ideal jobs are assigned to resources at particular times. The most basic version is as follows:\nWe are given n jobs J1, J2, ..., Jn of varying sizes, which need to be scheduled on m identical machines, while trying to minimize the makespan. The makespan is the total length of the schedule (that is, when all the jobs have finished processing). Nowadays, the problem is presented as an online problem (dynamic scheduling), that is, each job is presented, and the online algorithm needs to make a decision about that job before the next job is presented.\nThis problem is one of the best known online problems, and was the first problem for which competitive analysis was presented, by Graham in 1966. Best problem instances for basic model with makespan objective are due to Taillard.", "links": ["AMPL", "Bin packing problem", "CiteSeer", "Coffman\u2013Graham algorithm", "Competitive analysis (online algorithm)", "Computer science", "David Karger", "David Shmoys", "Deadlock", "Digital object identifier", "Disjunctive graph", "Dorit S. Hochbaum", "Dynamic programming", "Edward G. Coffman, Jr.", "Finite set", "Flow shop scheduling", "Genetic algorithm", "Genetic algorithm scheduling", "International Standard Book Number", "JSTOR", "Johnson's rule", "Journal of the ACM", "Linear programming", "List of NP-complete problems", "Lp space", "Mathematical Reviews", "Multiprocessor scheduling", "NP-complete", "NP-hard", "Online algorithm", "Online problem", "Open shop scheduling", "Operations Research", "Optimal control", "P=NP", "Polynomial-time approximation scheme", "Ravi Sethi", "Ronald Graham", "SIAM Journal on Computing", "Scheduling (disambiguation)", "Scheduling (production processes)", "Sequence-dependent setup", "Set (mathematics)", "Springer Verlag", "Susanne Albers", "Theory of Scheduling", "Travelling salesman problem", "Workflow"], "categories": ["Combinatorial optimization", "Mathematical optimization", "Operations research", "Optimization algorithms and methods", "Pages using citations with accessdate and no URL", "Wikipedia articles needing context from October 2009"], "title": "Job shop scheduling"}
{"summary": "In mathematical optimization, Bland's rule (also known as Bland's algorithm or Bland's anti-cycling rule) is an algorithmic refinement of the simplex method for linear optimization.\nWith Bland's rule, the simplex algorithm solves feasible linear optimization problems without cycling. There are examples of degenerate linear optimization problems on which the original simplex algorithm would cycle forever. Such cycles are avoided by Bland's rule for choosing a column to enter the basis.\nBland's rule was developed by Robert G. Bland, now a professor of operations research at Cornell University.", "links": ["Albert W. Tucker", "Alexander Schrijver", "Brown University", "Christos H. Papadimitriou", "Cornell University", "Criss-cross algorithm", "Digital object identifier", "George B. Dantzig", "JSTOR", "Jack Edmonds", "Linear optimization", "Linear programming", "Mathematical Reviews", "Mathematical optimization", "Oriented matroid", "Robert G. Bland", "Simplex method"], "categories": ["Exchange algorithms", "Optimization algorithms and methods", "Oriented matroids"], "title": "Bland's rule"}
{"summary": "In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the associated matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777\u20131855), although it was known to Chinese mathematicians as early as 179 CE (see History section).\nTo perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and in every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.\n\nUsing row operations to convert a matrix into reduced row echelon form is sometimes called Gauss\u2013Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced.", "links": ["Absolute value", "Addison-Wesley", "Algorithm", "American Mathematical Monthly", "ArXiv", "Argmax", "Arithmetic", "Array data structure", "Augmented matrix", "BLAS", "Bareiss algorithm", "Basic Linear Algebra Subprograms", "Basis (linear algebra)", "Big O notation", "Bit complexity", "Block matrix", "Carl Friedrich Gauss", "Charles F. Van Loan", "Column space", "Comparison of linear algebra libraries", "Comparison of numerical analysis software", "Computer hardware", "Cramer's rule", "Determinant", "Diagonally dominant", "Digital object identifier", "Dot product", "Dual space", "Eigenvalues and eigenvectors", "Elementary matrix", "Elementary row operations", "Euclidean vector", "Field (mathematics)", "Finite field", "Floating point", "Floating point number", "Frobenius matrix", "Gene H. Golub", "Gram\u2013Schmidt process", "Human computer", "Identity matrix", "Inner product space", "Integer", "International Standard Book Number", "International Standard Serial Number", "Invertible matrix", "Isaac Newton", "Iterative method", "JSTOR", "John Wiley & Sons", "Kernel (linear algebra)", "LU decomposition", "Leading coefficient", "Library (computing)", "Linear algebra", "Linear combination", "Linear independence", "Linear map", "Linear span", "Liu Hui", "MATLAB", "Matrix (mathematics)", "Matrix decomposition", "Matrix multiplication", "McGraw-Hill", "Minor (linear algebra)", "Nicholas Higham", "Numerical linear algebra", "Numerical stability", "O notation", "Ordinary least squares", "Orthogonality", "Outer product", "Pivot element", "Positive-definite matrix", "Prentice Hall", "Projection (linear algebra)", "Rank (linear algebra)", "Rank of a matrix", "Rational number", "Reduced row echelon form", "Rod calculus", "Row-echelon form", "Row echelon form", "Row space", "Scalar (mathematics)", "Society for Industrial and Applied Mathematics", "Sparse matrix", "System of linear equations", "Systems of linear equations", "Tensors", "The Nine Chapters on the Mathematical Art", "Transformation matrix", "Transpose", "Triangular form", "Triangular matrix", "Vector projection", "Vector space", "Wiley-Interscience", "Wilhelm Jordan (geodesist)", "YouTube"], "categories": ["All articles to be merged", "All articles with specifically marked weasel-worded phrases", "Articles to be merged from March 2013", "Articles with example pseudocode", "Articles with specifically marked weasel-worded phrases from January 2014", "Exchange algorithms", "Numerical linear algebra", "Pages using citations with accessdate and no URL", "Pages using web citations with no URL"], "title": "Gaussian elimination"}
{"summary": "The pivot or pivot element is the element of a matrix, or an array, which is selected first by an algorithm (e.g. Gaussian elimination, simplex algorithm, etc.), to do certain calculations. In the case of matrix algorithms, a pivot entry is usually required to be at least distinct from zero, and often distant from it; in this case finding this element is called pivoting. Pivoting may be followed by an interchange of rows or columns to bring the pivot to a fixed position and allow the algorithm to proceed successfully, and possibly to reduce round-off error. It is often used for verifying row echelon form\nPivoting might be thought of as swapping or sorting rows or columns in a matrix, and thus it can be represented as multiplication by permutation matrices. However, algorithms rarely move the matrix elements because this would cost too much time; instead, they just keep track of the permutations.\nOverall, pivoting adds more operations to the computational cost of an algorithm. These additional operations are sometimes necessary for the algorithm to work at all. Other times these additional operations are worthwhile because they add numerical stability to the final result.", "links": ["Absolute value", "Algorithm", "Array data structure", "Basic Linear Algebra Subprograms", "CPU cache", "Cache-oblivious algorithm", "CiteSeer", "Comparison of linear algebra libraries", "Comparison of numerical analysis software", "Digital object identifier", "Floating point", "Gaussian elimination", "International Standard Serial Number", "Mathematical Reviews", "Matrix (mathematics)", "Matrix decomposition", "Matrix multiplication", "Matrix multiplication algorithm", "Multiprocessing", "Numerical linear algebra", "Numerical stability", "Permutation matrix", "PlanetMath", "Quicksort", "Reduced row echelon form", "Row echelon form", "SIMD", "Simplex algorithm", "Sparse matrix", "System of linear equations", "Translation lookaside buffer"], "categories": ["Exchange algorithms", "Numerical linear algebra", "Pages using duplicate arguments in template calls", "Wikipedia articles incorporating text from PlanetMath"], "title": "Pivot element"}
{"summary": "In mathematical optimization, Dantzig's simplex algorithm (or simplex method) is a popular algorithm for linear programming. The journal Computing in Science and Engineering listed it as one of the top 10 algorithms of the twentieth century.\nThe name of the algorithm is derived from the concept of a simplex and was suggested by T. S. Motzkin. Simplices are not actually used in the method, but one interpretation of it is that it operates on simplicial cones, and these become proper simplices with an additional constraint. The simplicial cones in question are the corners (i.e., the neighborhoods of the vertices) of a geometric object called a polytope. The shape of this polytope is defined by the constraints applied to the objective function.", "links": ["Albert W. Tucker", "Alexander Schrijver", "Algorithm", "Approximation algorithm", "ArXiv", "Augmented Lagrangian method", "Baire category theory", "Barrier function", "Bellman\u2013Ford algorithm", "Best, worst and average case", "Bland's rule", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Canonical form", "Charles E. Leiserson", "Christos H. Papadimitriou", "CiteSeer", "Clifford Stein", "Combinatorial optimization", "Comparison of optimization software", "Computer science", "Computing in Science and Engineering", "Cone (geometry)", "Convex minimization", "Convex optimization", "Convex polytope", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Elementary matrix", "Ellipsoid method", "Ellipsoidal algorithm", "Evolutionary algorithm", "Exchange algorithm", "Exponential time", "Feasible region", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Fourier\u2013Motzkin elimination", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "General topology", "George B. Dantzig", "George Dantzig", "George J. Minty", "Gilbert Strang", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Identity matrix", "Integer programming", "Interior point method", "International Standard Book Number", "International Standard Serial Number", "Iterative method", "JSTOR", "Jerzy Neyman", "Johnson's algorithm", "Karmarkar", "Karmarkar's algorithm", "Katta G. Murty", "Khachiyan", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear-fractional programming", "Linear complementarity problem", "Linear functional", "Linear independence", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical Reviews", "Mathematical optimization", "Matroid", "Metaheuristic", "Michael J. Todd (mathematician)", "Minimum spanning tree", "Mixed complementarity problem", "Mixed linear complementarity problem", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear complementarity problem", "Nonlinear conjugate gradient method", "Nonlinear programming", "Normal distribution", "Operations research", "Optimization (mathematics)", "Optimization algorithm", "Oriented matroid", "Penalty method", "Polynomial time", "Polytope", "Porous set", "Powell's method", "Probability distribution", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Random matrix", "Random vector", "Revised simplex algorithm", "Ronald L. Rivest", "SIAM Review", "Sequential quadratic programming", "Shanghua Teng", "Simplex", "Simulated annealing", "Smoothed complexity", "Sparse matrix", "Structural stability", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "System of linear inequalities", "Tabu search", "The Mathematical Intelligencer", "Theodore Motzkin", "Thomas H. Cormen", "Truncated Newton method", "Trust region", "Vertex (geometry)", "Victor Klee", "Wassily Leontief", "Wolfe conditions"], "categories": ["1947 in computer science", "Exchange algorithms", "Linear programming", "Operations research", "Optimization algorithms and methods"], "title": "Simplex algorithm"}
{"summary": "Linear programming (LP; also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (mathematical optimization).\nMore formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists.\nLinear programs are problems that can be expressed in canonical form as\n\nwhere x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and  is the matrix transpose. The expression to be maximized or minimized is called the objective function (cTx in this case). The inequalities Ax \u2264 b and x \u2265 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second then we can say the first vector is less-than or equal-to the second vector.\nLinear programming can be applied to various fields of study. It is widely used in business and economics, and is also utilized for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proved useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.\n\n", "links": [".NET Framework", "AIMMS", "AMPL", "API", "APMonitor", "Affine function", "Albert W. Tucker", "Algebraic modeling language", "Algorithm", "Apache License", "Approximation algorithm", "Approximation algorithms", "ArXiv", "Arrangement polytope", "Assignment problem", "Augmented Lagrangian method", "BSD licenses", "Barrier function", "Bellman\u2013Ford algorithm", "Benders' decomposition", "Block diagram", "Block matrix", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Branch and price", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "COIN-OR CLP", "CPLEX", "Canonical form", "Cassowary constraint solver", "Christos H. Papadimitriou", "Coefficient", "Combinatorial optimization", "Comparability", "Comparison of optimization software", "Computers and Intractability: A Guide to the Theory of NP-Completeness", "Concave function", "Constraint (mathematics)", "Convex function", "Convex lattice polytope", "Convex minimization", "Convex optimization", "Convex polytope", "Convex programming", "Convex set", "Coopr", "Copyleft", "Covering problem", "Covering problems", "Criss-cross algorithm", "Cutting-plane method", "Dantzig-Wolfe decomposition", "David S. Johnson", "Davidon\u2013Fletcher\u2013Powell formula", "Delayed column generation", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dominating set problem", "Dual problem", "Duality (optimization)", "Dynamic programming", "Dynamical system", "Economics", "Edge cover", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Euler", "Evolutionary algorithm", "Exchange algorithm", "FICO Xpress", "Feasible region", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "FortMP", "Fourier\u2013Motzkin elimination", "Fractional coloring", "Frank Lauren Hitchcock", "Frank\u2013Wolfe algorithm", "FreeMat", "Function (mathematics)", "GNU Linear Programming Kit", "GNU MathProg", "Game theory", "Gauss\u2013Newton algorithm", "General Algebraic Modeling System", "George B. Dantzig", "George Dantzig", "Gilbert Strang", "Global maximum", "Global minimum", "Golden section search", "Gradient", "Gradient descent", "Graph (mathematics)", "Graph algorithm", "Graph diameter", "Greedy algorithm", "Gurobi", "G\u00fcnter M. Ziegler", "Half-space (geometry)", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Hirsch conjecture", "IMSL Numerical Libraries", "Independent set (graph theory)", "Independent set problem", "Integer decomposition property", "Integer programming", "Interior-point method", "Interior point method", "International Standard Book Number", "International Standard Serial Number", "Intersection (mathematics)", "Iterative method", "JSTOR", "Java (programming language)", "Ji\u0159\u00ed Matou\u0161ek (mathematician)", "Job shop scheduling", "John von Neumann", "Johnson's algorithm", "Joseph Fourier", "Karmarkar's algorithm", "Karp's 21 NP-complete problems", "Katta G. Murty", "Klee\u2013Minty cube", "Kruskal's algorithm", "LINDO", "LP-type problem", "Lemke's algorithm", "Leonid Kantorovich", "Leonid Khachiyan", "Level set", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear", "Linear-fractional programming (LFP)", "Linear complementarity problem", "Linear equality", "Linear function", "Linear functional", "Linear inequality", "Linear programming", "Linear programming relaxation", "Lingo (programming language)", "List of numerical analysis topics", "List of unsolved problems in computer science", "Local convergence", "Local maximum", "Local minimum", "Local search (optimization)", "LpSolve", "MATLAB", "MINTO", "MOSEK", "Maple (software)", "Mark Overmars", "Matching (graph theory)", "Mathcad", "Mathematica", "Mathematical Reviews", "Mathematical model", "Mathematical optimization", "Mathematical programming", "Matrix (mathematics)", "Matrix transpose", "Matroid", "Maximum principle", "Metaheuristic", "Michael R. Garey", "Microeconomics", "Microsoft Excel", "Microsoft Solver Foundation", "Minimum spanning tree", "Mixed complementarity problem", "Mixed linear complementarity problem", "NAG Numerical Library", "NMath Stats", "NP-hard", "Narendra Karmarkar", "National Diet Library", "Naum Z. Shor", "Nelder\u2013Mead method", "Newton's method in optimization", "Nobel prize in economics", "Nonlinear complementarity problem", "Nonlinear conjugate gradient method", "Nonlinear programming", "Numerical Algorithms Group", "O-Matrix", "OR/MS Today", "Objective function", "Octave", "Odysseus", "OpenOpt", "Operations research", "OptimJ", "Optimization (mathematics)", "Optimization Toolbox", "Optimization algorithm", "Oriented matroid", "PHP", "P (complexity)", "Packing problem", "Packing problems", "Path-following", "Penalty method", "Permissive free software licence", "Plane (geometry)", "Polygon", "Polyhedron", "Polynomial-time", "Polynomial time", "Polytope", "Powell's method", "Proceedings of the USSR Academy of Sciences", "Profit maximization", "Projective method", "Proprietary software", "Push\u2013relabel maximum flow algorithm", "Python (programming language)", "Qoca", "Quadratic programming", "Quasi-Newton method", "R", "R-Project", "Real number", "Revised simplex algorithm", "Robert J. Vanderbei", "Routing", "SAS System", "SCIP (optimization software)", "Sage (mathematics software)", "Scheduling (production processes)", "Scilab", "Semidefinite programming", "Sequential quadratic programming", "Set cover problem", "Set packing", "Shadow price", "Simple polygon", "Simplex algorithm", "Simulated annealing", "Slack variable", "Smale's problems", "Springer-Verlag", "Stephen Smale", "Stochastic programming", "Strong duality", "Subgradient method", "Submodular", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Sysquake", "TOMLAB", "Tabu search", "The Mathematical Intelligencer", "Time complexity", "Tjalling Koopmans", "Total dual integrality", "Totally unimodular", "Totally unimodular matrix", "Traveling salesman problem", "Truncated Newton method", "Trust region", "Unit cube", "Variable (programming)", "Vector space", "Vertex cover", "Vertex cover problem", "Vijay Vazirani", "VisSim", "Weak duality", "What'sBest", "Wolfe conditions", "World War II", "Worst-case complexity", "Yinyu Ye"], "categories": ["All articles needing additional references", "Articles containing Russian-language text", "Articles needing additional references from October 2015", "Convex optimization", "Geometric algorithms", "Linear programming", "Mathematical and quantitative methods (economics)", "Operations research", "P-complete problems", "Unsolved problems in computer science", "Wikipedia external links cleanup from August 2010", "Wikipedia spam cleanup from August 2010"], "title": "Linear programming"}
{"summary": "Benson's algorithm, named after Harold Benson, is a method for solving linear multi-objective optimization problems. This works by finding the \"efficient extreme points in the outcome set\". The primary concept in Benson's algorithm is to evaluate the upper image of the vector optimization problem by cutting planes.", "links": ["Applied mathematics", "Benson's algorithm (Go)", "Cutting-plane method", "Digital object identifier", "Extreme point", "Go (game)", "Harold Benson", "International Standard Book Number", "Linear programming", "Multi-objective optimization", "Multiobjective optimization", "Vector optimization"], "categories": ["All stub articles", "Applied mathematics stubs", "Linear programming", "Optimization algorithms and methods"], "title": "Benson's algorithm"}
{"summary": "The Klee\u2013Minty cube (named after Victor Klee and George J. Minty) is a unit cube whose corners have been slightly perturbed. Klee and Minty demonstrated that Dantzig's simplex algorithm has poor worst-case performance when initialized at one corner of their \"squashed cube\".\nIn particular, many optimization algorithms for linear optimization exhibit poor performance when applied to the Klee\u2013Minty cube. In 1973 Klee and Minty showed that Dantzig's simplex algorithm was not a polynomial-time algorithm when applied to their cube. Later, modifications of the Klee\u2013Minty cube have shown poor behavior both for other basis-exchange pivoting algorithms and also for interior-point algorithms.", "links": ["Algorithm", "Analysis of algorithms", "Approximation algorithm", "Arithmetic operation", "Augmented Lagrangian method", "Average-case complexity", "Barrier function", "Bellman\u2013Ford algorithm", "Big Oh", "Big oh", "Bland's rule", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Buchberger's algorithm", "Central path", "CiteSeer", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cubic polynomial", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Degree of a polynomial", "Digital object identifier", "Dijkstra's algorithm", "Dimension (vector space)", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Ellipsoidal method", "Euclidean metric", "Evolutionary algorithm", "Exchange algorithm", "Expected value", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gaussian elimination", "Gauss\u2013Newton algorithm", "George Dantzig", "George J. Minty", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "G\u00fcnter M. Ziegler", "G\u00fcnter Ziegler", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Interior-point method", "International Standard Book Number", "International Standard Serial Number", "Iterative method", "JSTOR", "Johnson's algorithm", "Karmarkar's algorithm", "Katta G. Murty", "Komei Fukuda", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear complementarity problem", "Linear optimization", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical Reviews", "Mathematical optimization", "Matroid", "Metaheuristic", "Michael Shub", "Minimum spanning tree", "Mixed complementarity problem", "Mixed linear complementarity problem", "Multivariate polynomial", "Nelder\u2013Mead method", "Newton's method in optimization", "Nimrod Megiddo", "Nonlinear complementarity problem", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization (mathematics)", "Optimization algorithm", "Penalty method", "Polynomial-time algorithm", "Polytope", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic polynomial", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simplex method", "Simulated annealing", "Stephen Smale", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Tam\u00e1s Terlaky", "Time complexity", "Truncated Newton method", "Trust region", "Unit cube", "Unit sphere", "Vertex (geometry)", "Victor Klee", "Wolfe conditions", "Worst-case complexity"], "categories": ["All articles to be expanded", "All articles with unsourced statements", "Analysis of algorithms", "Articles to be expanded from April 2011", "Articles with unsourced statements from October 2015", "Computational complexity theory", "Convex geometry", "Cubes", "Linear programming", "Mathematical optimization", "Pages containing links to subscription-only content", "Pages using duplicate arguments in template calls"], "title": "Klee\u2013Minty cube"}
{"summary": "In mathematical optimization, the network simplex algorithm is a graph theoretic specialization of the simplex algorithm. The algorithm is usually formulated in terms of a standard problem, minimum-cost flow problem and can be efficiently solved in polynomial time. The network simplex method works very well in practice, typically 200 to 300 times faster than the simplex method applied to general linear program of same dimensions.", "links": ["Assignment problem", "Bipartite graph", "Digital object identifier", "Dynamic trees", "Graph theory", "International Standard Book Number", "International Standard Serial Number", "James B. Orlin", "Mathematical Programming", "Mathematical optimization", "Minimum-cost flow problem", "Partially ordered set", "Robert Tarjan", "Simplex algorithm", "System of distinct representatives", "Transportation theory (mathematics)", "Transshipment problem", "\u00c9va Tardos"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from May 2015", "Computational problems in graph theory", "Graph algorithms", "Linear programming", "Mathematical problems", "Network flow", "Network theory", "Operations research", "Optimization algorithms and methods", "Polynomial-time problems"], "title": "Network simplex algorithm"}
{"summary": "Prune and search is a method of solving optimization problems suggested by Nimrod Megiddo in 1983. \nThe basic idea of the method is a recursive procedure in which at each step the input size is reduced (\"pruned\") by a constant factor 0 < p < 1. As such, it is a form of decrease and conquer algorithm, where at each step the decrease is by a constant factor. Let n be the input size, T(n) be the time complexity of the whole prune-and-search algorithm, S(n) is the time complexity of the pruning step, then T(n) obeys the following recurrence relation:\n\nwhich has the solution T(n) = O(S(n)), since summing a geometric series only multiplies by a constant factor, namely \nIn particular, Megiddo himself used this approach in his linear time algorithm for the linear programming problem when the dimension is fixed and for the minimal enclosing sphere problem for a set of points in space.", "links": ["Big Oh notation", "Decrease and conquer algorithm", "Geometric series", "Linear programming", "Linear time", "Minimal enclosing sphere", "Nimrod Megiddo", "Optimization (mathematics)", "Recurrence relation", "Time complexity"], "categories": ["Geometric algorithms", "Linear programming"], "title": "Prune and search"}
{"summary": "Dinic's algorithm or Dinitz's algorithm is a strongly polynomial algorithm for computing the maximum flow in a flow network, conceived in 1970 by Israeli (formerly Soviet) computer scientist Yefim (Chaim) A. Dinitz. The algorithm runs in  time and is similar to the Edmonds\u2013Karp algorithm, which runs in  time, in that it uses shortest augmenting paths. The introduction of the concepts of the level graph and blocking flow enable Dinic's algorithm to achieve its performance.\n^ Yefim Dinitz (1970). \"Algorithm for solution of a problem of maximum flow in a network with power estimation\" (PDF). Doklady Akademii nauk SSSR 11: 1277\u20131280.", "links": ["Bipartite matching", "Breadth-first search", "Dynamic trees", "Edmonds\u2013Karp algorithm", "Flow network", "Ford\u2013Fulkerson algorithm", "Hopcroft\u2013Karp algorithm", "International Standard Book Number", "Maximum flow", "Maximum flow problem", "Oded Goldreich", "Shimon Even", "Strongly polynomial", "Yefim Dinitz"], "categories": ["Graph algorithms", "Network flow"], "title": "Dinic's algorithm"}
{"summary": "In computer science, the Edmonds\u2013Karp algorithm is an implementation of the Ford\u2013Fulkerson method for computing the maximum flow in a flow network in O(V E2) time. The algorithm was first published by Yefim (Chaim) Dinic in 1970 and independently published by Jack Edmonds and Richard Karp in 1972. Dinic's algorithm includes additional techniques that reduce the running time to O(V2E).", "links": ["Association for Computing Machinery", "Augmenting path", "Big O notation", "Breadth-first search", "Charles E. Leiserson", "Clifford Stein", "Computer science", "Digital object identifier", "Dinic's algorithm", "Flow network", "Ford\u2013Fulkerson algorithm", "International Standard Book Number", "Introduction to Algorithms", "Jack Edmonds", "Max flow min cut theorem", "Maximum flow problem", "Richard Karp", "Ronald L. Rivest", "Thomas H. Cormen"], "categories": ["Graph algorithms", "Network flow"], "title": "Edmonds\u2013Karp algorithm"}
{"summary": "The Ford\u2013Fulkerson method or Ford\u2013Fulkerson algorithm (FFA) is an algorithm that computes the maximum flow in a flow network. It is called a \"method\" instead of an \"algorithm\" as the approach to finding augmenting paths in a residual graph is not fully specified or it is specified in several implementations with different running times. It was published in 1956 by L. R. Ford, Jr. and D. R. Fulkerson. The name \"Ford\u2013Fulkerson\" is often also used for the Edmonds\u2013Karp algorithm, which is a specialization of Ford\u2013Fulkerson.\nThe idea behind the algorithm is as follows: as long as there is a path from the source (start node) to the sink (end node), with available capacity on all edges in the path, we send flow along one of the paths. Then we find another path, and so on. A path with available capacity is called an augmenting path.", "links": ["Algorithm", "Approximate max-flow min-cut theorem", "Augmenting path", "Big O notation", "Breadth-first search", "Canadian Journal of Mathematics", "Charles E. Leiserson", "Clifford Stein", "D. R. Fulkerson", "Depth-first search", "Digital object identifier", "Edmonds\u2013Karp algorithm", "Flow network", "International Standard Book Number", "Introduction to Algorithms", "L. R. Ford, Jr.", "Max-flow min-cut theorem", "Maximum flow problem", "Oreilly Media", "Ronald L. Rivest", "Theoretical Computer Science (journal)", "Thomas H. Cormen"], "categories": ["Articles with example pseudocode", "Graph algorithms", "Network flow"], "title": "Ford\u2013Fulkerson algorithm"}
{"summary": "Iterative compression is an algorithmic technique invented by Reed, Smith and Vetta to show that the problem Odd Cycle Transversal was solvable in time O(3k kmn). Odd Cycle Transversal was a longstanding central open question in parameterized complexity. This technique later proved very useful in showing fixed-parameter tractability results. It is now considered to be one of the fundamental techniques in the area of parameterized algorithmics.\nIterative compression has been used successfully in many problems, for instance odd cycle transversal (see below) and edge bipartization, feedback vertex set, cluster vertex deletion and directed feedback vertex set. It has also been used successfully for exact exponential time algorithms for independent set.", "links": ["Algorithm", "Bipartite graph", "Bruce Reed (mathematician)", "Brute-force search", "Decision problem", "Digital object identifier", "Feedback vertex set", "Fixed-parameter tractable", "Graph theory", "Independent set (graph theory)", "Induced subgraph", "International Standard Book Number", "Kernelization", "Mathematical Reviews", "Max-flow min-cut theorem", "NP-hard", "Natural number", "Parameterized complexity", "Time complexity", "Vertex cover"], "categories": ["Analysis of algorithms", "Computational complexity theory", "Graph algorithms", "Network flow", "Pages using citations with accessdate and no URL", "Parameterized complexity"], "title": "Iterative compression"}
{"summary": "The out-of-kilter algorithm is an algorithm that computes the solution to the minimum-cost flow problem in a flow network. It was published in 1961 by D. R. Fulkerson.", "links": ["Algorithm", "D. R. Fulkerson", "Data structure", "Flow network", "JSTOR", "Journal of the Society for Industrial and Applied Mathematics", "Minimum-cost flow problem", "YouTube"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Network flow"], "title": "Out-of-kilter algorithm"}
{"summary": "In mathematical optimization, the push\u2013relabel algorithm (alternatively, preflow\u2013push algorithm) is an algorithm for computing maximum flows. The name \"push\u2013relabel\" comes from the two basic operations used in the algorithm. Throughout its execution, the algorithm maintains a \"preflow\" and gradually converts it into a maximum flow by moving flow locally between neighboring vertices using push operations under the guidance of an admissible network maintained by relabel operations. In comparison, the Ford\u2013Fulkerson algorithm performs global augmentations that send flow following paths from the source all the way to the sink.\nThe push\u2013relabel algorithm is considered one of the most efficient maximum flow algorithms. The generic algorithm has a strongly polynomial O(V2E) time complexity, which is asymptotically more efficient than the O(VE2) Edmonds\u2013Karp algorithm. Specific variants of the algorithms achieve even lower time complexities. The variant based on the highest label vertex selection rule has O(V2\u221aE) time complexity and is generally regarded as the benchmark for maximum flow algorithms. Subcubic O(VE\u200alog\u200a(V2/E)) time complexity can be achieved using dynamic trees, although in practice it is less efficient.\nThe push\u2013relabel algorithm has been extended to compute minimum cost flows. The idea of distance labels has led to a more efficient augmenting path algorithm, which in turn can be incorporated back into the push\u2013relabel algorithm to create a variant with even higher empirical performance.", "links": ["Andrew V. Goldberg", "Breadth-first search", "C (programming language)", "Charles E. Leiserson", "Clifford Stein", "Digital object identifier", "Edmonds\u2013Karp algorithm", "FIFO (computing and electronics)", "Flow network", "Ford\u2013Fulkerson algorithm", "Function (mathematics)", "International Standard Book Number", "Introduction to Algorithms", "Link-cut tree", "Mathematical optimization", "Max-flow min-cut theorem", "Maximum flow", "Minimum cost flow", "Potential method", "Python (programming language)", "Real number", "Robert Tarjan", "Ron Rivest", "Strongly polynomial", "Thomas H. Cormen", "Topological sorting"], "categories": ["Graph algorithms", "Network flow"], "title": "Push\u2013relabel maximum flow algorithm"}
{"summary": "The travelling salesman problem (TSP) asks the following question: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city? It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science.\n\nTSP is a special case of the travelling purchaser problem and the Vehicle routing problem.\nIn the theory of computational complexity, the decision version of the TSP (where, given a length L, the task is to decide whether the graph has any tour shorter than L) belongs to the class of NP-complete problems. Thus, it is possible that the worst-case running time for any algorithm for the TSP increases superpolynomially (perhaps, specifically, exponentially) with the number of cities.\nThe problem was first formulated in 1930 and is one of the most intensively studied problems in optimization. It is used as a benchmark for many optimization methods. Even though the problem is computationally difficult, a large number of heuristics and exact methods are known, so that some instances with tens of thousands of cities can be solved completely and even problems with millions of cities can be approximated within a small fraction of 1%.\nThe TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing. In these applications, the concept city represents, for example, customers, soldering points, or DNA fragments, and the concept distance represents travelling times or cost, or a similarity measure between DNA fragments. The TSP also appears in astronomy, as astronomers observing many sources will want to minimise the time spent slewing the telescope between the sources. In many applications, additional constraints such as limited resources or time windows may be imposed.", "links": ["2-opt", "APX", "Alexander Schrijver", "Alpha processor", "Analyst's traveling salesman theorem", "Ant colony optimization", "Ant colony optimization algorithms", "Approximation algorithm", "Artificial intelligence", "Best, worst and average case", "Bibcode", "Bitonic tour", "Bottleneck traveling salesman problem", "Bounded convergence theorem", "Branch-and-bound", "Branch and bound", "Branch and cut", "Brian Kernighan", "Brute force search", "Canadian traveller problem", "Charles E. Leiserson", "Chemistry", "Christofides algorithm", "Christos Papadimitriou", "Clifford Stein", "Cognitive psychology", "Combinatorial optimization", "Complete graph", "Complexity class", "Computational complexity theory", "Computer science", "Concorde TSP Solver", "Constructive heuristic", "Cross entropy method", "Cutting-plane method", "Cutting plane", "Cutting stock problem", "D. R. Fulkerson", "DNA sequencing", "David S. Johnson", "Decision problem", "Delbert Ray Fulkerson", "Digital object identifier", "Directed graph", "Distance function", "Distance matrix", "Drill", "Dynamic programming", "Edge (graph theory)", "Electronic Colloquium on Computational Complexity", "Emergence", "Euclidean distance", "Euclidean minimum spanning tree", "Euclidean space", "Eugene Lawler", "Eulerian graph", "Eulerian tour", "Evolutionary computing", "Exponential time hypothesis", "Factorial", "Function problem", "Genetic algorithm", "Geometric measure theory", "George Dantzig", "George Nemhauser", "Gerhard J. Woeginger", "Glossary of graph theory", "Graph (mathematics)", "Graph theory", "Graph traversal", "Greedy algorithm", "G\u00f6del Prize", "Hamiltonian cycle", "Hamiltonian path problem", "Hassler Whitney", "Heikki Mannila", "Held\u2013Karp algorithm", "Heuristic", "Heuristic (computer science)", "Heuristic algorithm", "ILOG", "Icosian Game", "Integer linear program", "Integer programming", "Integrated circuit", "International Standard Book Number", "Introduction to Algorithms", "JSTOR", "Jan Karel Lenstra", "Joseph S. B. Mitchell", "Journal of the ACM", "Karl Menger", "Leonard Adleman", "Linear programming", "Lin\u2013Kernighan", "Lin\u2013Kernighan\u2013Johnson", "Local minimum", "Logistics", "Manhattan distance", "Marco Dorigo", "Marek Karpinski", "Markov chain", "Matching (graph theory)", "Mathematical Reviews", "Mathematics", "Maximum metric", "Metric (mathematics)", "Metric space", "Michael Held", "Michael R. Garey", "Minimum spanning tree", "Monotone polygon", "NP-complete", "NP-hard", "Nearest neighbour algorithm", "One-way street", "Operations Research Letters", "Operations research", "Optimization problem", "P vs. NP", "Perfect matching", "Permutation", "Pheromone", "Physics", "Planning", "Polynomial-time approximation scheme", "Princeton University", "Printed circuit board", "PubMed Identifier", "RAND Corporation", "Rectifiable curve", "Rice University", "Richard Karp", "Richard M. Karp", "River formation dynamics", "Ronald L. Rivest", "Route inspection problem", "Running time", "Sanjeev Arora", "Santa Monica", "Selmer M. Johnson", "Semiconductor", "Set TSP problem", "Seven Bridges of K\u00f6nigsberg", "Shen Lin", "Shortest path", "Simulated annealing", "Swarm intelligence", "Symposium on Theory of Computing", "Tabu search", "Theoretical computer science", "Thomas H. Cormen", "Thomas Kirkman", "Time complexity", "Traffic collision", "Traveling purchaser problem", "Travelling Salesman (2012 film)", "Triangle inequality", "Triangular inequality", "Tube Challenge", "Undirected graph", "University of Heidelberg", "University of Waterloo", "Va\u0161ek Chv\u00e1tal", "Vehicle routing problem", "Vertex (graph theory)", "William J. Cook", "William Rowan Hamilton"], "categories": ["Commons category with local link same as on Wikidata", "Computational problems in graph theory", "Graph algorithms", "Hamiltonian paths and cycles", "NP-complete problems", "NP-hard problems", "Operations research", "Pages containing cite templates with deprecated parameters", "Travelling salesman problem", "Use dmy dates from July 2012"], "title": "Travelling salesman problem"}
{"summary": "In optimization, 2-opt is a simple local search algorithm first proposed by Croes in 1958 for solving the traveling salesman problem. The main idea behind it is to take a route that crosses over itself and reorder it so that it does not.\n\n - A   B -             - A - B -\n     X         ==>     \n - C   D -             - C - D -\n\nA complete 2-opt local search will compare every possible valid combination of the swapping mechanism. This technique can be applied to the travelling salesman problem as well as many related problems. These include the vehicle routing problem (VRP) as well as the capacitated VRP, which require minor modification of the algorithm.\nThis is the mechanism by which the 2-opt swap manipulates a given route:\n\n   2optSwap(route, i, k) {\n       1. take route[1] to route[i-1] and add them in order to new_route\n       2. take route[i] to route[k] and add them in reverse order to new_route\n       3. take route[k+1] to end and add them in order to new_route\n       return new_route;\n   }\n\nHere is an example of the above with arbitrary input:\n\n   example route: A ==> B ==> C ==> D ==> E ==> F ==> G ==> H ==> A\n   example i = 4, example k = 7\n   new_route:\n       1. (A ==> B ==> C)\n       2. A ==> B ==> C ==> (G ==> F ==> E ==> D)\n       3. A ==> B ==> C ==> G ==> F ==> E ==> D (==> H ==> A)\n\nThis is the complete 2-opt swap making use of the above mechanism:\n\n   repeat until no improvement is made {\n       start_again:\n       best_distance = calculateTotalDistance(existing_route)\n       for (i = 0; i < number of nodes eligible to be swapped - 1; i++) {\n           for (k = i + 1; k < number of nodes eligible to be swapped; k++) {\n               new_route = 2optSwap(existing_route, i, k)\n               new_distance = calculateTotalDistance(new_route)\n               if (new_distance < best_distance) {\n                   existing_route = new_route\n                   goto start_again\n               }\n           }\n       }\n   }\n\nNote: If you start/end at a particular node or depot, then you must remove this from the search as an eligible candidate for swapping, as reversing the order will cause an invalid path.\nFor example, with depot at A:\n\n   A ==> B ==> C ==> D ==> A\n\nSwapping using node[0] and node[2] would yield\n\n   C ==> B ==> A ==> D ==> A \n\nwhich is not valid (does not leave from A, the depot).", "links": ["3-opt", "Lin\u2013Kernighan heuristic", "Local search (optimization)", "Mathematical analysis", "Optimization (mathematics)", "Traveling salesman problem", "Vehicle routing problem"], "categories": ["All stub articles", "Heuristic algorithms", "Mathematical analysis stubs", "Mathematical optimization", "Travelling salesman problem"], "title": "2-opt"}
{"summary": "In optimization, 3-opt is a simple local search algorithm for solving the travelling salesman problem and related network optimization problems.\n3-opt analysis involves deleting 3 connections (or edges) in a network (or tour), reconnecting the network in all other possible ways, and then evaluating each reconnection method to find the optimum one. This process is then repeated for a different set of 3 connections.", "links": ["2-opt", "Combinatorics", "Lin\u2013Kernighan heuristic", "Local search (optimization)", "Network optimization", "ORSA (OR)", "Travelling salesman problem"], "categories": ["All stub articles", "Combinatorics stubs", "Heuristic algorithms", "Mathematical optimization", "Travelling salesman problem"], "title": "3-opt"}
{"summary": "In computational geometry, a coreset is a small set of points that approximates the shape of a larger point set, in the sense that applying some geometric measure to the two sets (such as their minimum bounding box volume) results in approximately equal numbers. Many natural geometric optimization problems have coresets that approximate an optimal solution to within a factor of 1 + \u03b5, that can be found quickly (in linear time or near-linear time), and that have size bounded by a function of 1/\u03b5 independent of the input size, where \u03b5 is an arbitrary positive number. When this is the case, one obtains a linear-time or near-linear time approximation scheme, based on the idea of finding a coreset and then applying an exact optimization algorithm to the coreset. Regardless of how slow the exact optimization algorithm is, for any fixed choice of \u03b5, the running time of this approximation scheme will be O(1) plus the time to find the coreset.", "links": ["Algorithm", "Computational geometry", "Data structure", "Emo Welzl", "Jacob E. Goodman", "J\u00e1nos Pach", "Linear time", "Mathematical Reviews", "Minimum bounding box", "Pankaj K. Agarwal", "Volume"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computational geometry", "Computer science stubs"], "title": "Coreset"}
{"summary": "The Euclidean shortest path problem is a problem in computational geometry: given a set of polyhedral obstacles in a Euclidean space, and two points, find the shortest path between the points that does not intersect any of the obstacles.\nIn two dimensions, the problem can be solved in polynomial time in a model of computation allowing addition and comparisons of real numbers, despite theoretical difficulties involving the numerical precision needed to perform such calculations. These algorithms are based on two different principles, either performing a shortest path algorithm such as Dijkstra's algorithm on a visibility graph derived from the obstacles or (in an approach called the continuous Dijkstra method) propagating a wavefront from one of the points until it meets the other.\nIn three (and higher) dimensions the problem is NP-hard in the general case , but there exist efficient approximation algorithms that run in polynomial time based on the idea of finding a suitable sample of points on the obstacle edges and performing a visibility graph calculation using these sample points.\nThere are many results on computing shortest paths which stays on a polyhedral surface. Given two points s and t, say on the surface of a convex polyhedron, the problem is to compute a shortest path that never leaves the surface and connects s with t. This is a generalization of the problem from 2-dimension but it is much easier than the 3-dimensional problem.\nAlso, there are variations of this problem, where the obstacles are weighted, i.e., one can go through an obstacle, but it incurs an extra cost to go through an obstacle. The standard problem is the special case where the obstacles have infinite weight. This is termed as the weighted region problem in the literature.", "links": ["Algorithmica", "Combinatorics", "Computational geometry", "Convex polyhedron", "Der-Tsai Lee", "Digital object identifier", "Dijkstra's algorithm", "Discrete and Computational Geometry", "Euclidean space", "Franco P. Preparata", "Geometry", "Godfried Toussaint", "International Standard Book Number", "Journal of the ACM", "KernelCAD", "NP-hard", "Numerical precision", "Polyhedron", "Polynomial time", "SIAM Journal on Computing", "Shortest path problem", "Springer-Verlag", "Subhash Suri", "Symposium on Computational Geometry", "Visibility graph", "Weighted region problem"], "categories": ["All stub articles", "Combinatorics stubs", "Computational geometry", "Geometric algorithms", "Geometry stubs"], "title": "Euclidean shortest path"}
{"summary": "In computational geometry, Chan's algorithm, named after Timothy M. Chan, is an optimal output-sensitive algorithm to compute the convex hull of a set P of n points, in 2- or 3-dimensional space. The algorithm takes O(n log h) time, where h is the number of vertices of the output (the convex hull). In the planar case, the algorithm combines an O(n log n) algorithm (Graham scan, for example) with Jarvis march, in order to obtain an optimal O(n log h) time. Chan's algorithm is notable because it is much simpler than the Kirkpatrick\u2013Seidel algorithm, and it naturally extends to 3-dimensional space. This paradigm has been independently developed by Frank Nielsen in his Ph. D. thesis.", "links": ["Binary search", "Computational Geometry", "Computational geometry", "Convex hull", "Discrete and Computational Geometry", "Gift wrapping algorithm", "Graham scan", "Jarvis march", "Kirkpatrick\u2013Seidel algorithm", "Output-sensitive algorithm", "Timothy M. Chan", "Trapezoid"], "categories": ["Convex hull algorithms"], "title": "Chan's algorithm"}
{"summary": "Algorithms that construct convex hulls of various objects have a broad range of applications in mathematics and computer science.\nIn computational geometry, numerous algorithms are proposed for computing the convex hull of a finite set of points, with various computational complexities.\nComputing the convex hull means that a non-ambiguous and efficient representation of the required convex shape is constructed. The complexity of the corresponding algorithms is usually estimated in terms of n, the number of input points, and h, the number of points on the convex hull.", "links": ["Algebraic decision tree", "Analysis of algorithms", "Big O notation", "CGAL", "Chan's algorithm", "Charles E. Leiserson", "Clifford Stein", "Computational geometry", "Computer science", "Convex function", "Convex hull", "Convex polygon", "Convex polyhedron", "Convex polytope", "Data structure", "David Avis", "David G. Kirkpatrick", "David Mount", "Decision tree model", "Digital object identifier", "Dynamic convex hull", "Eric W. Weisstein", "Face (geometry)", "Franco P. Preparata", "G. T. Toussaint", "Gift wrapping algorithm", "Godfried Toussaint", "Graham scan", "Half-space (geometry)", "Integer sorting", "International Standard Book Number", "Introduction to Algorithms", "Kirkpatrick\u2013Seidel algorithm", "Linear time", "Luc Devroye", "Marc van Kreveld", "Mark Overmars", "Mark de Berg", "MathWorld", "Mathematics", "Orthogonal convex hull", "Otfried Schwarzkopf", "Output-sensitive algorithm", "Parabola", "Quadrilateral", "Quickhull", "Quicksort", "Raimund Seidel", "Reduction (complexity)", "Ronald Graham", "Ronald L. Rivest", "S.J. Hong", "Selim Akl", "Simple polygon", "Sorting", "Springer-Verlag", "Thomas H. Cormen", "Timothy M. Chan", "Ultimate convex hull algorithm"], "categories": ["Convex hull algorithms"], "title": "Convex hull algorithms"}
{"summary": "In computational geometry, the gift wrapping algorithm is an algorithm for computing the convex hull of a given set of points.", "links": ["Algorithm", "Arithmetic precision", "Big O notation", "Chan's algorithm", "Charles E. Leiserson", "Clifford Stein", "Collinear", "Computational geometry", "Convex hull", "Coordinates (elementary mathematics)", "Degenerate case", "Digital object identifier", "Extreme point", "General position", "Graham scan", "Information Processing Letters", "International Standard Book Number", "Introduction to Algorithms", "Linear time", "Output-sensitive algorithm", "Polar coordinates", "Ron Rivest", "Thomas H. Cormen", "Time complexity"], "categories": ["Convex hull algorithms", "Polytopes"], "title": "Gift wrapping algorithm"}
{"summary": "Graham's scan is a method of finding the convex hull of a finite set of points in the plane with time complexity O(n log n). It is named after Ronald Graham, who published the original algorithm in 1972. The algorithm finds all vertices of the convex hull ordered along its boundary.", "links": ["All nearest smaller values", "Big O notation", "Charles E. Leiserson", "Clifford Stein", "Convex hull", "Cross product", "Digital object identifier", "Dot product", "Heapsort", "International Standard Book Number", "Interval (mathematics)", "Introduction to Algorithms", "Robert Sedgewick (computer scientist)", "Ron Rivest", "Ronald Graham", "Sorting algorithm", "Springer Science+Business Media", "Thomas H. Cormen", "Time complexity", "Uzi Vishkin", "Vector (geometric)"], "categories": ["Articles with example pseudocode", "Convex hull algorithms"], "title": "Graham scan"}
{"summary": "In computational geometry, the gift wrapping algorithm is an algorithm for computing the convex hull of a given set of points.", "links": ["Algorithm", "Arithmetic precision", "Big O notation", "Chan's algorithm", "Charles E. Leiserson", "Clifford Stein", "Collinear", "Computational geometry", "Convex hull", "Coordinates (elementary mathematics)", "Degenerate case", "Digital object identifier", "Extreme point", "General position", "Graham scan", "Information Processing Letters", "International Standard Book Number", "Introduction to Algorithms", "Linear time", "Output-sensitive algorithm", "Polar coordinates", "Ron Rivest", "Thomas H. Cormen", "Time complexity"], "categories": ["Convex hull algorithms", "Polytopes"], "title": "Gift wrapping algorithm"}
{"summary": "The Kirkpatrick\u2013Seidel algorithm, called by its authors \"the ultimate planar convex hull algorithm\" is an algorithm for computing the convex hull of a set of points in the plane, with O(n log h) time complexity, where n is the number of input points and h is the number of points in the hull. Thus, the algorithm is output-sensitive: its running time depends on both the input size and the output size. Another output-sensitive algorithm, the gift wrapping algorithm, was known much earlier, but the Kirkpatrick\u2013Seidel algorithm has an asymptotic running time that is significantly smaller and that always improves on the O(n log n) bounds of non-output-sensitive algorithms. The Kirkpatrick\u2013Seidel algorithm is named after its inventors, David G. Kirkpatrick and Raimund Seidel.\nAlthough the algorithm is asymptotically very efficient, it is not very practical for moderate-sized problems.", "links": ["Algorithm", "Analysis of algorithms", "Bitangent", "Chan's algorithm", "Convex hull", "David G. Kirkpatrick", "Digital object identifier", "Divide-and-conquer algorithm", "Franco P. Preparata", "Gift wrapping algorithm", "Median", "Output-sensitive algorithm", "Raimund Seidel", "Recursively", "SIAM Journal on Computing"], "categories": ["Convex hull algorithms"], "title": "Kirkpatrick\u2013Seidel algorithm"}
{"summary": "Simpath is an algorithm introduced by Donald Knuth that constructs a zero-suppressed decision diagram (ZDD) representing all simple paths between two vertices in a given graph.", "links": ["Algorithm", "Data structure", "Digital object identifier", "Donald Knuth", "Zero-suppressed decision diagram"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Commons category without a link on Wikidata", "Computer science stubs", "Graph algorithms", "Mathematical logic", "Theoretical computer science"], "title": "Knuth's Simpath algorithm"}
{"summary": "A Simple Algorithm for Constructing Szemer\u00e9di's Regularity Partition is a paper by Alan M. Frieze and Ravi Kannan giving an algorithmic version of the Szemer\u00e9di regularity lemma to find an \u03b5-regular partition of a given graph.", "links": ["Alan M. Frieze", "Algorithm", "CiteSeer", "DIMACS", "J\u00e1nos Koml\u00f3s (mathematician)", "Mikl\u00f3s Simonovits", "Ravi Kannan", "Szemer\u00e9di regularity lemma", "Vojt\u011bch R\u00f6dl"], "categories": ["Graph algorithms", "Mathematics papers"], "title": "Algorithmic version for Szemer\u00e9di regularity partition"}
{"summary": "Alpha\u2013beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Go, etc.). It stops completely evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision.\n\n", "links": ["A* search algorithm", "Alan Kotok", "Albert W. Tucker", "Alexander Brudno", "All-pay auction", "Allen Newell", "Alphabeta (disambiguation)", "Amos Tversky", "Ariel Rubinstein", "Arrow's impossibility theorem", "Arthur Samuel", "B*", "Backtracking", "Backward induction", "Bargaining problem", "Battle of the sexes (game theory)", "Bayesian game", "Beam search", "Bellman\u2013Ford algorithm", "Bertrand paradox (economics)", "Best-first search", "Best first search", "Bidirectional search", "Big O notation", "Blotto games", "Bor\u016fvka's algorithm", "Bounded rationality", "Branch and bound", "Branching factor", "Breadth-first search", "British Museum algorithm", "Centipede game", "Cheap talk", "Chess", "Chicken (game)", "Collusion", "Combinatorial game theory", "Combinatorial optimization", "Confrontation analysis", "Cooperative game", "Coopetition", "Coordination game", "Core (game theory)", "Correlated equilibrium", "Cournot competition", "D*", "Daniel Kahneman", "Dartmouth Conference", "David K. Levine", "David M. Kreps", "Deadlock (game theory)", "Depth-first search", "Depth-limited search", "Dictator game", "Digital object identifier", "Dijkstra's algorithm", "Dollar auction", "Dominance (game theory)", "Donald B. Gillies", "Donald Knuth", "Drew Fudenberg", "Dynamic programming", "Economic equilibrium", "Edmonds' algorithm", "El Farol Bar problem", "Epsilon-equilibrium", "Eric Maskin", "Escalation of commitment", "Evolutionarily stable strategy", "Extensive-form game", "Fair cake-cutting", "Fair division", "Floyd\u2013Warshall algorithm", "Folk theorem (game theory)", "Forward induction", "Fringe search", "Game theory", "Game tree", "Global games", "Go (board game)", "Graph traversal", "Graphical game theory", "Grim trigger", "Guess 2/3 of the average", "Harold W. Kuhn", "Herbert A. Simon", "Herv\u00e9 Moulin", "Heuristic", "Hierarchy of beliefs", "Hill climbing", "Infinity", "Information set (game theory)", "International Standard Book Number", "Iterative deepening A*", "Iterative deepening depth-first search", "Jean-Fran\u00e7ois Mertens", "Jean Tirole", "John Forbes Nash, Jr.", "John Harsanyi", "John Maynard Smith", "John McCarthy (computer scientist)", "John von Neumann", "Johnson's algorithm", "Judea Pearl", "Jump point search", "Kenneth Arrow", "Kenneth Binmore", "Killer heuristic", "Kruskal's algorithm", "Kuhn poker", "Large Poisson game", "Leonid Hurwicz", "Lexicographic breadth-first search", "List of algorithms", "List of game theorists", "List of games in game theory", "Lloyd Shapley", "MTD(f)", "MTD-f", "Markov perfect equilibrium", "Markov strategy", "Matching pennies", "Mechanism design", "Melvin Dresher", "Merrill M. Flood", "Mertens-stable equilibrium", "Minimax", "Monty Hall problem", "N-player game", "Nash bargaining game", "Nash equilibrium", "Negamax", "Negascout", "No-win situation", "Nontransitive game", "Normal-form game", "OCLC", "Oreilly Media", "Oskar Morgenstern", "Pareto efficiency", "Paul Milgrom", "Perfect information", "Peter Norvig", "Peyton Young", "Pirate game", "Ply (game theory)", "Preference (economics)", "Prim's algorithm", "Princess and monster game", "Principal variation search", "Prisoner's dilemma", "Prisoners and hats puzzle", "Proper equilibrium", "Pruning (algorithm)", "Public goods game", "Purification theorem", "Quantal response equilibrium", "Quasi-perfect equilibrium", "Refutation table", "Reinhard Selten", "Rendezvous problem", "Repeated game", "Revelation principle", "Risk dominance", "Robert Aumann", "Robert B. Wilson", "Rock-paper-scissors", "Roger Myerson", "SMA*", "SSS*", "Samuel Bowles (economist)", "Screening game", "Search algorithm", "Search game", "Self-confirming equilibrium", "Sequential equilibrium", "Sequential game", "Shapley value", "Signaling game", "Simultaneous game", "Solution concept", "Square root", "Stag hunt", "Stochastic game", "Strategy (game theory)", "Strictly determined game", "Strong Nash equilibrium", "Stuart J. Russell", "Subgame perfect equilibrium", "Succinct game", "Symmetric game", "Thomas Schelling", "Tic-tac-toe", "Tit for tat", "Tragedy of the commons", "Transposition table", "Traveler's dilemma", "Tree traversal", "Trembling hand perfect equilibrium", "Tyranny of small decisions", "UMI Research Press", "Ultimatum game", "United States", "Unscrupulous diner's dilemma", "Variation (game tree)", "Volunteer's dilemma", "War of attrition (game)", "William Vickrey", "Zero-sum game"], "categories": ["All articles with dead external links", "Articles with dead external links from September 2010", "Articles with example pseudocode", "Articles with inconsistent citation formats", "CS1 errors: dates", "Game artificial intelligence", "Graph algorithms", "Optimization algorithms and methods", "Search algorithms"], "title": "Alpha\u2013beta pruning"}
{"summary": "The Barab\u00e1si\u2013Albert (BA) model is an algorithm for generating random scale-free networks using a preferential attachment mechanism. Scale-free networks are widely observed in natural and human-made systems, including the Internet, the world wide web, citation networks, and some social networks. The algorithm is named for its inventors Albert-L\u00e1szl\u00f3 Barab\u00e1si and R\u00e9ka Albert.", "links": ["A. Korn", "A. Schubert", "A. Telcs", "Adjacency list", "Adjacency matrix", "Agent-based model", "Albert-L\u00e1szl\u00f3 Barab\u00e1si", "ArXiv", "Artificial neural network", "Assortativity", "Autocatalysis", "Average path length", "Balance theory", "Betweenness centrality", "Bianconi\u2013Barab\u00e1si model", "Bibcode", "Biological network", "Biometrika", "Bipartite graph", "Boolean network", "Centrality", "Citation analysis", "Clique (graph theory)", "Closeness (graph theory)", "Clustering coefficient", "Combinatorial optimization", "Community structure", "Complete graph", "Complex contagion", "Complex network", "Computer network", "Connected component (graph theory)", "Cut (graph theory)", "Cycle (graph theory)", "Degree (graph theory)", "Degree distribution", "Dependency network", "Derek J. de Solla Price", "Digital object identifier", "Directed graph", "Distance (graph theory)", "Edge (graph theory)", "Efficiency (Network Science)", "Epidemic model", "Erd\u0151s\u2013R\u00e9nyi model", "Evolving networks", "Exponential random graph models", "Flow network", "Google", "Graph (abstract data type)", "Graph (mathematics)", "Graph drawing", "H-index", "Herbert A. Simon", "Hierarchical network model", "Homophily", "Hyperbolic geometric graph", "Hypergraph", "Incidence list", "Incidence matrix", "Interdependent networks", "Internet", "JSTOR", "Journal of the American Society for Information Science", "Link analysis", "List of algorithms", "List of network scientists", "List of network theory topics", "Loop (graph theory)", "Matthew effect (sociology)", "Metrics (networking)", "Modularity (networks)", "Multigraph", "Neighbourhood (graph theory)", "Network controllability", "Network effect", "Network motif", "Network science", "Network theory", "PageRank", "Path (graph theory)", "Percolation theory", "Physical Review E", "Positive feedback", "Power law", "Preferential attachment", "Price's model", "PubMed Identifier", "Random graph", "Reciprocity (network science)", "Reviews of Modern Physics", "Rich get richer", "R\u00e9ka Albert", "SIR model", "Scale-free network", "Scale-free networks", "Science (journal)", "Semantic network", "Small-world network", "Social capital", "Social influence", "Social network", "Social network analysis software", "Social networks", "Spatial network", "Spectral properties", "Stochastic block model", "Telecommunications network", "Transitive relation", "Transport network", "Triadic closure", "Udny Yule", "Vertex (graph theory)", "Watts and Strogatz model", "Weighted network", "Wikipedia", "World wide web"], "categories": ["Graph algorithms", "Pages containing links to subscription-only content", "Random graphs", "Social networks"], "title": "Barab\u00e1si\u2013Albert model"}
{"summary": "Belief propagation, also known as sum-product message passing, is a message passing algorithm for performing inference on graphical models, such as Bayesian networks and Markov random fields. It calculates the marginal distribution for each unobserved node, conditional on any observed nodes. Belief propagation is commonly used in artificial intelligence and information theory and has demonstrated empirical success in numerous applications including low-density parity-check codes, turbo codes, free energy approximation, and satisfiability.\nThe algorithm was first proposed by Judea Pearl in 1982, who formulated this algorithm on trees, and was later extended to polytrees. It has since been shown to be a useful approximate algorithm on general graphs.\nIf X={Xi} is a set of discrete random variables with a joint mass function p, the marginal distribution of a single Xi is simply the summation of p over all other variables:\n\nHowever, this quickly becomes computationally prohibitive: if there are 100 binary variables, then one needs to sum over 299 \u2248 6.338 \u00d7 1029 possible values. By exploiting the polytree structure, belief propagation allows the marginals to be computed much more efficiently.", "links": ["Algorithm", "Approximation error", "Arg max", "Artificial intelligence", "Bayesian network", "Bayesian networks", "Bipartite graph", "Cluster variation method", "Conjugate gradient method", "Cycle (graph theory)", "Diagonally dominant", "Diameter", "Digital object identifier", "Discrete probability distribution", "EXIT chart", "Factor graph", "Gauss\u2013Seidel method", "Generalized survey propagation", "Graph (mathematics)", "Graph coloring", "Graphical model", "IEEE Signal Processing Magazine", "IEEE Trans. Signal Process.", "IEEE Transactions on Information Theory", "Inference", "Information theory", "Internal energy", "International Standard Book Number", "Island algorithm", "Joint distribution", "Journal of Machine Learning Research", "Judea Pearl", "Junction tree", "Low-density parity-check codes", "Marginal distribution", "Markov random field", "Markov random fields", "Mathematical induction", "Maximum A Posteriori", "Monte Carlo method", "NP-complete", "NP-hard", "Neural Computation", "New Scientist", "Normal distribution", "Partition function (mathematics)", "Polytree", "Probability mass function", "PubMed Identifier", "Random variable", "Ryoichi Kikuchi", "Satisfiability", "Sharp-P-complete", "Spectral radius", "Successive over-relaxation", "Survey propagation", "Thermodynamic free energy", "Thermodynamics", "Tree (graph theory)", "Turbo codes", "Variational Bayesian methods", "Viterbi algorithm"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from April 2009", "Coding theory", "Graph algorithms", "Graphical models", "Probability theory", "Use dmy dates from June 2013"], "title": "Belief propagation"}
{"summary": "The Bellman\u2013Ford algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph. It is slower than Dijkstra's algorithm for the same problem, but more versatile, as it is capable of handling graphs in which some of the edge weights are negative numbers. The algorithm is named after two of its developers, Richard Bellman and Lester Ford, Jr., who published it in 1958 and 1956, respectively; however, Edward F. Moore also published the same algorithm in 1957, and for this reason it is also sometimes called the Bellman\u2013Ford\u2013Moore algorithm.\nNegative edge weights are found in various applications of graphs, hence the usefulness of this algorithm. If a graph contains a \"negative cycle\" (i.e. a cycle whose edges sum to a negative value) that is reachable from the source, then there is no cheapest path: any path can be made cheaper by one more walk around the negative cycle. In such a case, the Bellman\u2013Ford algorithm can detect negative cycles and report their existence.", "links": ["A* search algorithm", "Algorithm", "Alpha\u2013beta pruning", "ArXiv", "Autonomous system (Internet)", "B*", "Backtracking", "Beam search", "Best, worst and average case", "Best-first search", "Bidirectional search", "Big O notation", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Charles E. Leiserson", "Count to infinity", "Cycle-cancelling", "Cycle (graph theory)", "D*", "David Eppstein", "Dense graph", "Depth-first search", "Depth-limited search", "Dijkstra's Algorithm", "Dijkstra's algorithm", "Distance-vector routing protocol", "Dynamic programming", "Edmonds' algorithm", "Edward F. Moore", "Expected value", "Flow network", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph (data structure)", "Graph traversal", "Greedy algorithm", "Hill climbing", "International Standard Book Number", "Introduction to Algorithms", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jon Kleinberg", "Jump point search", "Kruskal's algorithm", "L. R. Ford, Jr.", "Lexicographic breadth-first search", "List of algorithms", "Mathematical Reviews", "Mathematical induction", "Network topology", "O'Reilly Media", "Prim's algorithm", "Random permutation", "Relaxation (iterative method)", "Richard Bellman", "Robert Sedgewick (computer scientist)", "Ron Rivest", "Routing Information Protocol", "SMA*", "Search game", "Shortest path", "Single-source shortest path problem", "Thomas H. Cormen", "Tree traversal", "Vertex (graph theory)", "Walk (graph theory)", "Weighted digraph", "\u00c9va Tardos"], "categories": ["Articles with example C code", "Articles with example pseudocode", "Dynamic programming", "Graph algorithms", "Polynomial-time problems"], "title": "Bellman\u2013Ford algorithm"}
{"summary": "Bidirectional search is a graph search algorithm that finds a shortest path from an initial vertex to a goal vertex in a directed graph. It runs two simultaneous searches: one forward from the initial state, and one backward from the goal, stopping when the two meet in the middle. The reason for this approach is that in many cases it is faster: for instance, in a simplified model of search problem complexity in which both searches expand a tree with branching factor b, and the distance from start to goal is d, each of the two searches has complexity O(bd/2) (in Big O notation), and the sum of these two search times is much less than the O(bd) complexity that would result from a single search from the beginning to the goal.\nAs in A* search, bi-directional search can be guided by a heuristic estimate of the remaining distance to the goal (in the forward tree) or from the start (in the backward tree).\nIra Pohl (1971) was the first one to design and implement a bi-directional heuristic search algorithm. Andrew Goldberg and others explained the correct termination conditions for the bidirectional version of Dijkstra\u2019s Algorithm.", "links": ["A*", "A* search algorithm", "Alpha\u2013beta pruning", "Artificial Intelligence: A Modern Approach", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Big O notation", "Bor\u016fvka's algorithm", "Branch and bound", "Branching factor", "Breadth-first search", "British Museum algorithm", "D*", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Dijkstra\u2019s Algorithm", "Directed graph", "Donald Michie", "Dynamic programming", "Edmonds' algorithm", "Fifteen puzzle", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph search algorithm", "Graph traversal", "Heuristic (computer science)", "Hill climbing", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Journal of the ACM", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Peter Norvig", "Prim's algorithm", "SMA*", "Search game", "Shortest path", "State space search", "Stuart J. Russell", "Tree (graph theory)", "Tree traversal", "Vertex (graph theory)"], "categories": ["Graph algorithms", "Search algorithms"], "title": "Bidirectional search"}
{"summary": "Bor\u016fvka's algorithm is an algorithm for finding a minimum spanning tree in a graph for which all edge weights are distinct.\nIt was first published in 1926 by Otakar Bor\u016fvka as a method of constructing an efficient electricity network for Moravia. The algorithm was rediscovered by Choquet in 1938; again by Florek, \u0141ukasiewicz, Perkal, Steinhaus, and Zubrzycki in 1951; and again by Sollin  in 1965. Because Sollin was the only computer scientist in this list living in an English speaking country, this algorithm is frequently called Sollin's algorithm, especially in the parallel computing literature.\nThe algorithm begins by first examining each vertex and adding the cheapest edge from that vertex to another in the graph, without regard to already added edges, and continues joining these groupings in a like manner until a tree spanning all vertices is completed.", "links": ["A* search algorithm", "Ackermann function", "Algorithm", "Alpha\u2013beta pruning", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Bernard Chazelle", "Best-first search", "Bidirectional search", "Big O notation", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Connected component (graph theory)", "D*", "David Eppstein", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Discrete Mathematics (journal)", "Disjoint-set data structure", "Dynamic programming", "Edmonds' algorithm", "Electricity network", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph minor", "Graph traversal", "Gustave Choquet", "Hill climbing", "Hugo Steinhaus", "Iterative deepening A*", "Iterative deepening depth-first search", "Jan \u0141ukasiewicz", "Jaroslav Ne\u0161et\u0159il", "Johnson's algorithm", "Jorge Urrutia Galicia", "Julian Perkal", "Jump point search", "J\u00f6rg-R\u00fcdiger Sack", "Kazimierz Florek", "Kruskal's algorithm", "Lexicographic breadth-first search", "Lexicographic order", "List of algorithms", "M. Sollin", "Mathematical Reviews", "Minimum spanning tree", "Moravia", "Otakar Bor\u016fvka", "Parallel computing", "Planar graph", "Prim's algorithm", "SMA*", "Search game", "Sollin", "Stefan Zubrzycki", "Tree traversal"], "categories": ["CS1 Czech-language sources (cs)", "CS1 French-language sources (fr)", "CS1 maint: Unrecognized language", "Graph algorithms", "Spanning tree"], "title": "Bor\u016fvka's algorithm"}
{"summary": "Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key') and explores the neighbor nodes first, before moving to the next level neighbors.\nBFS was invented in the late 1950s by E. F. Moore, who used it to find the shortest path out of a maze, and discovered independently by C. Y. Lee as a wire routing algorithm (published 1961).", "links": ["A* search algorithm", "Adjacency list", "Adjacency matrix", "Aho-Corasick", "Algorithm", "Alpha\u2013beta pruning", "Artificial Intelligence: A Modern Approach", "Artificial intelligence", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best, worst and average case", "Best-first search", "Bidirectional search", "Bipartite graph", "Bor\u016fvka's algorithm", "Branch and bound", "British Museum algorithm", "Cardinality", "Charles E. Leiserson", "Cheney's algorithm", "Cuthill\u2013McKee algorithm", "D*", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Edward F. Moore", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frankfurt", "Fringe search", "Garbage collection", "Germany", "Graph (data structure)", "Graph traversal", "Hill climbing", "Implicit graph", "International Standard Book Number", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Level structure", "Lexicographic breadth-first search", "List of algorithms", "Maximum flow problem", "Multiple discovery", "Peter Norvig", "Prim's algorithm", "Queue (abstract data type)", "Routing (electronic design automation)", "SMA*", "Search algorithm", "Search game", "Shortest path", "Stack (abstract data type)", "Steven Skiena", "Stuart J. Russell", "Tree (data structure)", "Tree data structure", "Tree traversal"], "categories": ["All articles needing additional references", "Articles needing additional references from April 2012", "Commons category with local link same as on Wikidata", "Graph algorithms", "Search algorithms"], "title": "Breadth-first search"}
{"summary": "In computer science, the Bron\u2013Kerbosch algorithm is an algorithm for finding maximal cliques in an undirected graph. That is, it lists all subsets of vertices with the two properties that each pair of vertices in one of the listed subsets is connected by an edge, and no listed subset can have any additional vertices added to it while preserving its complete connectivity. The Bron\u2013Kerbosch algorithm was designed by Dutch scientists Joep Kerbosch and Coenraad Bron, who published its description in 1973. Although other algorithms for solving the clique problem have running times that are, in theory, better on inputs that have few maximal independent sets, the Bron\u2013Kerbosch algorithm and subsequent improvements to it are frequently reported as being more efficient in practice than the alternatives. It is well-known and widely used in application areas of graph algorithms such as computational chemistry.\nA contemporaneous algorithm of Akkoyunlu (1973), although presented in different terms, can be viewed as being the same as the Bron\u2013Kerbosch algorithm, as it generates the same recursive search tree.", "links": ["Algorithm", "ArXiv", "Backtracking", "Clique (graph theory)", "Clique problem", "Coenraad Bron", "Complete graph", "Computational chemistry", "Computer science", "David Eppstein", "Degeneracy (graph theory)", "Degree (graph theory)", "Digital object identifier", "Empty set", "Glossary of graph theory", "Graph (mathematics)", "International Standard Book Number", "Joep Kerbosch", "Leo Moser", "Linear time", "Mathematical Reviews", "Neighborhood (graph theory)", "Netherlands", "Output-sensitive algorithm", "Polynomial time", "Recursion", "Social network", "Sparse graph"], "categories": ["Articles with example pseudocode", "Graph algorithms"], "title": "Bron\u2013Kerbosch algorithm"}
{"summary": "Chaitin's algorithm is a bottom-up, graph coloring register allocation algorithm that uses cost/degree as its spill metric. It is named after its designer, Gregory Chaitin. Chaitin's algorithm was the first register allocation algorithm that made use of coloring of the interference graph for both register allocations and spilling.\nChaitin's algorithm was presented on the 1982 SIGPLAN Symposium on Compiler Construction, and published in the symposium proceedings. It was extension of an earlier 1981 paper on the use of graph coloring for register allocation. Chaitin's algorithm formed the basis of a large section of research into register allocators.", "links": ["Algorithm", "Graph coloring", "Gregory Chaitin", "Interference graph", "Register allocation", "SIGPLAN", "Spill metric"], "categories": ["Graph algorithms"], "title": "Chaitin's algorithm"}
{"summary": "The clique percolation method is a popular approach for analyzing the overlapping community structure of networks. The term network community (also called a module, cluster or cohesive group) has no widely accepted unique definition and it is usually defined as a group of nodes that are more densely connected to each other than to other nodes in the network. There are numerous alternative methods for detecting communities in networks, for example, the Girvan\u2013Newman algorithm, hierarchical clustering and modularity maximization.", "links": ["Clique (graph theory)", "Community structure", "Digital object identifier", "Erd\u0151s\u2013R\u00e9nyi model", "Geometric mean", "Girvan\u2013Newman algorithm", "Hierarchical clustering", "Hypergraph", "Line graph", "Maximal clique", "Metastasis", "Modularity (networks)", "NP-complete", "Network motif", "Percolation", "Social network", "Statistical physics"], "categories": ["Clustering algorithms", "Graph algorithms", "Network analysis", "Networks"], "title": "Clique percolation method"}
{"summary": "In computer science and graph theory, the method of color-coding efficiently finds k-vertex simple paths, k-vertex cycles, and other small subgraphs within a given graph using probabilistic algorithms, which can then be derandomized and turned into deterministic algorithms. This method shows that many subcases of the subgraph isomorphism problem (an NP-complete problem) can in fact be solved in polynomial time.\nThe theory and analysis of the color-coding method was proposed in 1994 by Noga Alon, Raphael Yuster, and Uri Zwick.", "links": ["Aravind Srinivasan", "Color code", "Color code (disambiguation)", "Computer science", "Coppersmith\u2013Winograd algorithm", "Cycle (graph theory)", "Derandomization", "Deterministic algorithm", "Glossary of graph theory", "Graph theory", "Jeanette P. Schmidt", "Leonard J. Schulman", "Matrix multiplication", "Minor (graph theory)", "Moni Naor", "NC (complexity)", "NP-complete", "Noga Alon", "Path (graph theory)", "Perfect hash", "Planar graph", "Planar graphs", "Polynomial time", "Probabilistic algorithms", "Protein-protein interaction", "Raphael Yuster", "Structural motif", "Subgraph isomorphism", "Treewidth", "Uri Zwick", "Wnt signaling pathway"], "categories": ["Graph algorithms"], "title": "Color-coding"}
{"summary": "In applied mathematics, the method of contraction hierarchies is a technique to speed up shortest-path routing by first creating precomputed \"contracted\" versions of the connection graph. It can be regarded as a special case of \"highway-node routing\".\nContraction hierarchies can be used to generate shortest-path routes much more efficiently than Dijkstra's algorithm or previous highway-node routing approaches, and is used in many advanced routing techniques. It is publicly available in open source software to calculate routes from one place to another.", "links": ["Digital object identifier", "Dijkstra's algorithm", "Dorothea Wagner", "Graph (mathematics)", "Peter Sanders (computer scientist)", "Precomputed", "Routing", "Shortest-path routing"], "categories": ["Graph algorithms", "Routing algorithms"], "title": "Contraction hierarchies"}
{"summary": "In the study of graph algorithms, Courcelle's theorem is the statement that every graph property definable in the monadic second-order logic of graphs can be decided in linear time on graphs of bounded treewidth. The result was first proved by Bruno Courcelle in 1990 and independently rediscovered by Borie, Parker & Tovey (1992). It is considered the archetype of algorithmic meta-theorems.", "links": ["Algorithm", "ArXiv", "Automata theory", "Binary tree", "Bruce Reed (mathematician)", "Bruno Courcelle", "Cambridge University Press", "Cardinality", "Clique-width", "Crossing number (graph theory)", "Cut (graph theory)", "Database theory", "Deterministic Turing machine", "Digital object identifier", "Discrete Morse theory", "Dual graph", "Equivalence class", "Equivalence relation", "Graph (mathematics)", "Graph coloring", "Graph minor", "Graph property", "Graph theory", "Grid graph", "Halin graph", "Hamiltonian cycle", "Hans L. Bodlaender", "Independent set (graph theory)", "Information Processing Letters", "International Congress of Mathematicians", "International Standard Book Number", "Journal of Combinatorial Theory", "Ken-ichi Kawarabayashi", "Knowledge representation and reasoning", "L (complexity)", "Linear time", "Logic of graphs", "Manifold", "Mathematical Reviews", "Meta-theorem", "Michael Fellows", "Model checking", "Modular arithmetic", "Monadic second-order logic", "Parameterized complexity", "Quantum invariants", "Robertson\u2013Seymour theorem", "Rod Downey", "Satisfiability problem", "Simplicial complex", "Space complexity", "Symposium on Foundations of Computer Science", "Symposium on Theory of Computing", "Till Tantau", "Time complexity", "Tree automaton", "Tree decomposition", "Treewidth", "Undecidable problem", "Zentralblatt MATH"], "categories": ["Graph algorithms", "Graph minor theory", "Metatheorems"], "title": "Courcelle's theorem"}
{"summary": "In the mathematical subfield of matrix theory, the Cuthill\u2013McKee algorithm (CM), named for Elizabeth Cuthill and J. McKee , is an algorithm to permute a sparse matrix that has a symmetric sparsity pattern into a band matrix form with a small bandwidth. The reverse Cuthill\u2013McKee algorithm (RCM) due to Alan George is the same algorithm but with the resulting index numbers reversed. In practice this generally results in less fill-in than the CM ordering when Gaussian elimination is applied.\nThe Cuthill McKee algorithm is a variant of the standard breadth-first search algorithm used in graph algorithms. It starts with a peripheral node and then generates levels  for  until all nodes are exhausted. The set  is created from set  by listing all vertices adjacent to all nodes in . These nodes are listed in increasing degree. This last detail is the only difference with the breadth-first search algorithm.", "links": ["Adjacency matrix", "Algorithm", "Association for Computing Machinery", "Band matrix", "Bandwidth (matrix theory)", "Boost C++ Libraries", "Breadth-first search", "Degree (graph theory)", "Graph (mathematics)", "Graph bandwidth", "Level structure", "Mathematics", "Matrix (mathematics)", "N-tuple", "Peripheral vertex", "Sparse matrix", "Symmetric matrix", "Vertex (graph theory)"], "categories": ["Graph algorithms", "Matrix theory", "Sparse matrices"], "title": "Cuthill\u2013McKee algorithm"}
{"summary": "D* (pronounced \"D star\") is any one of the following three related incremental search algorithms:\nThe original D*, by Anthony Stentz, is an informed incremental search algorithm.\nFocused D* is an informed incremental heuristic search algorithm by Anthony Stentz that combines ideas of A* and the original D*. Focused D* resulted from a further development of the original D*.\nD* Lite is an incremental heuristic search algorithm by Sven Koenig and Maxim Likhachev that builds on LPA*, an incremental heuristic search algorithm that combines ideas of A* and Dynamic SWSF-FP.\nAll three search algorithms solve the same assumption-based path planning problems, including planning with the freespace assumption, where a robot has to navigate to given goal coordinates in unknown terrain. It makes assumptions about the unknown part of the terrain (for example: that it contains no obstacles) and finds a shortest path from its current coordinates to the goal coordinates under these assumptions. The robot then follows the path. When it observes new map information (such as previously unknown obstacles), it adds the information to its map and, if necessary, replans a new shortest path from its current coordinates to the given goal coordinates. It repeats the process until it reaches the goal coordinates or determines that the goal coordinates cannot be reached. When traversing unknown terrain, new obstacles may be discovered frequently, so this replanning needs to be fast. Incremental (heuristic) search algorithms speed up searches for sequences of similar search problems by using experience with the previous problems to speed up the search for the current one. Assuming the goal coordinates do not change, all three search algorithms are more efficient than repeated A* searches.\nD* and its variants have been widely used for mobile robot and autonomous vehicle navigation. Current systems are typically based on D* Lite rather than the original D* or Focused D*. In fact, even Stentz's lab uses D* Lite rather than D* in some implementations. Such navigation systems include a prototype system tested on the Mars rovers Opportunity and Spirit and the navigation system of the winning entry in the DARPA Urban Challenge, both developed at Carnegie Mellon University.\nThe original D* was introduced by Anthony Stentz in 1994. The name D* comes from the term \"Dynamic A*\", because the algorithm behaves like A* except that the arc costs can change as the algorithm runs.", "links": ["A*", "A* search algorithm", "Alpha\u2013beta pruning", "Autonomous vehicle", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Carnegie Mellon University", "CiteSeer", "D-STAR", "DARPA Urban Challenge", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph traversal", "Hill climbing", "Incremental heuristic search", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Mobile robot", "Navigation research", "Opportunity rover", "Prim's algorithm", "SMA*", "Search game", "Specific detectivity", "Spirit rover", "Sven Koenig (computer scientist)", "Tree traversal"], "categories": ["Graph algorithms", "Robot control", "Search algorithms"], "title": "D*"}
{"summary": "Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. One starts at the root (selecting some arbitrary node as the root in the case of a graph) and explores as far as possible along each branch before backtracking.\nA version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Tr\u00e9maux as a strategy for solving mazes.", "links": ["A* search algorithm", "Algorithm", "Alpha\u2013beta pruning", "Analysis of algorithms", "Artificial intelligence", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best, worst and average case", "Best-first search", "Bias", "Biconnected graph", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Branching factor", "Breadth-first search", "Bridge (graph theory)", "British Museum algorithm", "Charles E. Leiserson", "Charles Pierre Tr\u00e9maux", "Clifford Stein", "Connected component (graph theory)", "Control flow graph", "D*", "Decision problem", "Degree (graph theory)", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Directed acyclic graph", "Donald Knuth", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph (data structure)", "Graph theory", "Graph traversal", "Group (mathematics)", "Halting problem", "Heuristics", "Hill climbing", "International Standard Book Number", "Introduction to Algorithms", "Iterative deepening A*", "Iterative deepening depth-first search", "Jean Pelletier-Thibert", "John Hopcroft", "Johnson's algorithm", "Jon Kleinberg", "Journal of the ACM", "Jump point search", "Kruskal's algorithm", "Kurt Mehlhorn", "Lexicographic breadth-first search", "List of algorithms", "Maze", "Maze generation", "Maze solving algorithm", "Memory management", "Michael T. Goodrich", "OCLC", "P-complete", "Parallel algorithm", "Parse tree", "Patrice Ossona de Mendez", "Peter Sanders (computer scientist)", "Pierre Rosenstiehl", "Planarity testing", "Polish notation", "Prim's algorithm", "Reverse Polish notation", "Robert Tarjan", "Roberto Tamassia", "Ronald L. Rivest", "SMA*", "Sample (statistics)", "Search algorithm", "Search game", "Search games", "Shimon Even", "Spanning tree (mathematics)", "Strongly connected components", "Thomas H. Cormen", "Time complexity", "Topological sorting", "Tree (data structure)", "Tree data structure", "Tree traversal", "Tr\u00e9maux tree", "\u00c9va Tardos"], "categories": ["All articles needing additional references", "Articles containing video clips", "Articles needing additional references from July 2010", "Articles with example pseudocode", "Commons category with local link same as on Wikidata", "Graph algorithms", "Search algorithms"], "title": "Depth-first search"}
{"summary": "In computer science depth-limited search is an algorithm to explore the vertices of a graph. It is a modification of depth-first search and is used for example in the iterative deepening depth-first search algorithm.", "links": ["A* search algorithm", "Algorithm", "Alpha\u2013beta pruning", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best, worst and average case", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Computational complexity theory", "Computer science", "D*", "Depth-first search", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph (data structure)", "Graph (mathematics)", "Graph traversal", "Hill climbing", "International Standard Book Number", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Path (graph theory)", "Peter Norvig", "Prim's algorithm", "SMA*", "Search Algorithm", "Search game", "Stack (data structure)", "Stuart J. Russell", "Tree traversal", "Uninformed search", "Vertex (graph theory)"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from June 2013", "Graph algorithms", "Search algorithms"], "title": "Depth-limited search"}
{"summary": "The Dijkstra\u2013Scholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Dijkstra and Scholten in 1980.\nFirst, let us consider the case of a simple process graph which is a tree. A distributed computation which is tree-structured is not uncommon. Such a process graph may arise when the computation is strictly a divide-and-conquer type. A node starts the computation and divides the problem in two (or more, usually a multiple of 2) roughly equal parts and distribute those parts to other processors. This process continues recursively until the problems are of sufficiently small size to solve in a single processor.", "links": ["Algorithm", "Carel S. Scholten", "Digital object identifier", "Distributed system", "Divide and conquer algorithm", "Edsger W. Dijkstra", "Huang's algorithm", "International Standard Book Number", "Mathematical Reviews", "Node (networking)", "Process graph", "Spanning tree (mathematics)", "Termination analysis", "Tree (data structure)"], "categories": ["Graph algorithms", "Termination algorithms"], "title": "Dijkstra\u2013Scholten algorithm"}
{"summary": "Disparity filter is a network reduction algorithm to extract the backbone structure of undirected weighted network. Many real world networks such as citation networks, food web, airport networks display heavy tailed statistical distribution of nodes' weight and strength. Disparity filter can sufficiently reduce the network without destroying the multi-scale nature of the network. The algorithm is developed by M. Angeles Serrano, Marian Boguna and Alessandro Vespignani.", "links": ["Adjacency list", "Adjacency matrix", "Agent-based model", "Alessandro Vespignani", "Annals of Internal Medicine", "ArXiv", "Artificial neural network", "Assortativity", "Balance theory", "Barab\u00e1si\u2013Albert model", "Betweenness centrality", "Bibcode", "Biological network", "Bipartite graph", "Boolean network", "Centrality", "Citation analysis", "Clique (graph theory)", "Closeness (graph theory)", "Clustering coefficient", "Combinatorial optimization", "Community structure", "Complete graph", "Complex contagion", "Complex network", "Computer network", "Connected component (graph theory)", "Cut (graph theory)", "Cycle (graph theory)", "Degree (graph theory)", "Degree distribution", "Dependency network", "Digital object identifier", "Directed graph", "Distance (graph theory)", "Ecological Modelling", "Edge (graph theory)", "Efficiency (Network Science)", "Epidemic model", "Erd\u0151s\u2013R\u00e9nyi model", "Evolving networks", "Exponential random graph models", "Flow network", "Food web", "Graph (abstract data type)", "Graph (mathematics)", "Graph drawing", "Graph theory", "Hierarchical network model", "Homophily", "Hyperbolic geometric graph", "Hypergraph", "Incidence list", "Incidence matrix", "Interdependent networks", "K-core", "Link analysis", "List of algorithms", "List of network scientists", "List of network theory topics", "Loop (graph theory)", "Maximal element", "Metrics (networking)", "Minimum spanning tree", "Modularity (networks)", "Multigraph", "Multiscale modeling", "Neighbourhood (graph theory)", "Network controllability", "Network effect", "Network motif", "Network science", "Network theory", "Null model", "P-value", "PageRank", "Path (graph theory)", "Percolation theory", "Physical Review Letters", "Preferential attachment", "Proceedings of the National Academy of Sciences", "PubMed Identifier", "Random graph", "Reciprocity (network science)", "SIR model", "Scale-free network", "Semantic network", "Small-world network", "Social capital", "Social influence", "Social network", "Social network analysis software", "Spatial network", "Statistical significance test", "Stochastic block model", "Strength of a graph", "Telecommunications network", "Transitive relation", "Transport network", "Tree (graph theory)", "Triadic closure", "Vertex (graph theory)", "Watts and Strogatz model", "Weight", "Weighted graph", "Weighted network"], "categories": ["Graph algorithms"], "title": "Disparity filter algorithm of weighted network"}
{"summary": "In graph theory, a branch of mathematics, Edmonds' algorithm or Chu\u2013Liu/Edmonds' algorithm is an algorithm for finding a spanning arborescence of minimum weight (sometimes called an optimum branching). It is the directed analog of the minimum spanning tree problem. The algorithm was proposed independently first by Yoeng-jin Chu and Tseng-hong Liu (1965) and then by Jack Edmonds (1967).", "links": ["A* search algorithm", "Algorithm", "Alpha\u2013beta pruning", "Arborescence (graph theory)", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Blossom algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "C++", "D*", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Directed graph", "Dynamic programming", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph theory", "Graph traversal", "Hill climbing", "International Standard Book Number", "Iterative deepening A*", "Iterative deepening depth-first search", "Jack Edmonds", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "MIT License", "Minimum spanning tree", "Open source", "Prim's algorithm", "Robert Tarjan", "SMA*", "Search game", "Spanning subgraph", "Sparse graph", "Tree traversal"], "categories": ["Graph algorithms"], "title": "Edmonds' algorithm"}
{"summary": "The blossom algorithm is an algorithm in graph theory for constructing maximum matchings on graphs. The algorithm was developed by Jack Edmonds in 1961, and published in 1965. Given a general graph G = (V, E), the algorithm finds a matching M such that each vertex in V is incident with at most one edge in M and |M| is maximized. The matching is constructed by iteratively improving an initial empty matching along augmenting paths in the graph. Unlike bipartite matching, the key new idea is that an odd-length cycle in the graph (blossom) is contracted to a single vertex, with the search continuing iteratively in the contracted graph.\nA major reason that the blossom algorithm is important is that it gave the first proof that a maximum-size matching could be found using a polynomial amount of computation time. Another reason is that it led to a linear programming polyhedral description of the matching polytope, yielding an algorithm for min-weight matching. As elaborated by Alexander Schrijver, further significance of the result comes from the fact that this was the first polytope whose proof of integrality \"does not simply follow just from total unimodularity, and its description was a breakthrough in polyhedral combinatorics.\"", "links": ["Alexander Schrijver", "Algorithm", "Bipartite graph", "Blossom (graph theory)", "Digital object identifier", "Edge contraction", "Forest (graph theory)", "Graph (mathematics)", "Graph theory", "If and only if", "International Standard Book Number", "Jack Edmonds", "Linear programming", "L\u00e1szl\u00f3 Lov\u00e1sz", "Maximum matching", "Michael D. Plummer", "Polyhedral combinatorics", "Polytope", "Total unimodularity"], "categories": ["Graph algorithms", "Matching"], "title": "Blossom algorithm"}
{"summary": "The Euler tour technique (ETT), named after Leonhard Euler, is a method in graph theory for representing trees. The tree is viewed as a directed graph that contains two directed edges for each edge in the tree. The tree can then be represented as a Eulerian circuit of the directed graph, known as the Euler tour representation (ETR) of the tree. The ETT allows for efficient, parallel computation of solutions to common problems in algorithmic graph theory. It was introduced by Tarjan and Vishkin in 1984.", "links": ["Algorithmic graph theory", "Balanced binary search tree", "Depth-first search", "Digital object identifier", "Directed graph", "Eulerian circuit", "Graph theory", "International Standard Book Number", "Leonhard Euler", "Lexicographical order", "Link-cut tree", "Parallel computation", "Prefix sum", "Tree (graph theory)"], "categories": ["Graph algorithms", "Parallel computing"], "title": "Euler tour technique"}
{"summary": "The FKT algorithm, named after Fisher, Kasteleyn, and Temperley, counts the number of perfect matchings in a planar graph in polynomial time. This same task is #P-complete for general graphs. Counting the number of matchings, even for planar graphs, is also #P-complete. The key idea is to convert the problem into a Pfaffian computation of a skew-symmetric matrix derived from a planar embedding of the graph. The Pfaffian of this matrix is then computed efficiently using standard determinant algorithms.", "links": ["Adjacency matrix", "American Scientist", "ArXiv", "Arthur Cayley", "Boolean satisfiability problem", "Brian Hayes (scientist)", "Chemistry", "Complete bipartite graph", "Complete graph", "Determinant", "Diatomic molecule", "Digital object identifier", "Dimer (chemistry)", "Domino tiling", "Dual graph", "FP (complexity)", "Finite graph", "Frank Harary", "Glossary of graph theory", "Graph embedding", "H2O", "Harold Neville Vazeille Temperley", "Holographic algorithm", "Homeomorphism (graph theory)", "Hosoya index", "If and only if", "International Standard Book Number", "International Standard Serial Number", "Kuratowski's theorem", "Lattice graph", "Leslie G. Valiant", "Mark Jerrum", "Matchgates", "Matching (graph theory)", "Michael Fisher", "P (complexity)", "Parity of a permutation", "Partition function (statistical mechanics)", "Perfect matching", "Pfaffian", "Pieter Kasteleyn", "Planar graph", "Regular graph", "Robin Thomas (mathematician)", "Rodney J. Baxter", "Sharp-P", "Sharp-P-complete", "Skew-symmetric matrix", "Spanning tree", "Statistical mechanics", "Tutte matrix", "Vijay Vazirani"], "categories": ["Graph algorithms", "Planar graphs"], "title": "FKT algorithm"}
{"summary": "A flooding algorithm is an algorithm for distributing material to every part of a graph. The name derives from the concept of inundation by a flood.\nFlooding algorithms are used in computer networking and graphics. Flooding algorithms are also useful for solving many mathematical problems, including maze problems and many problems in graph theory.", "links": ["Algorithm", "Computer science", "Flood", "Flood fill", "Flooding (computer networking)", "Graph (mathematics)", "Graph theory", "Maze", "Water retention on mathematical surfaces"], "categories": ["All stub articles", "Computer science stubs", "Graph algorithms", "Routing algorithms"], "title": "Flooding algorithm"}
{"summary": "In computer science, the Floyd\u2013Warshall algorithm is an algorithm for finding shortest paths in a weighted graph with positive or negative edge weights (but with no negative cycles). A single execution of the algorithm will find the lengths (summed weights) of the shortest paths between all pairs of vertices, though it does not return details of the paths themselves. Versions of the algorithm can also be used for finding the transitive closure of a relation , or (in connection with the Schulze voting system) widest paths between all pairs of vertices in a weighted graph.", "links": ["A* search algorithm", "Algorithm", "All-pairs shortest path problem", "Alpha\u2013beta pruning", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Bernard Roy", "Best, worst and average case", "Best-first search", "Bidirectional search", "Big theta", "Binary heap", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "C++", "C. R. Acad. Sci. Paris", "C Sharp (programming language)", "Charles E. Leiserson", "Claude Elwood Shannon", "Communications of the ACM", "Computational complexity theory", "Computer science", "Cycle (graph theory)", "D*", "Dense graph", "Depth-first search", "Depth-limited search", "Deterministic finite automaton", "Digital object identifier", "Dijkstra's algorithm", "Dorit S. Hochbaum", "Dynamic programming", "Edmonds' algorithm", "Eric W. Weisstein", "Fast matrix multiplication", "Finite automaton", "Floyd's algorithm", "Floyd's cycle-finding algorithm", "Floyd\u2013Steinberg dithering", "Fringe search", "Gauss\u2013Jordan elimination", "Graph (data structure)", "Graph traversal", "Hill climbing", "International Standard Book Number", "Introduction to Algorithms", "Invertible matrix", "Iterative deepening A*", "Iterative deepening depth-first search", "Java (programming language)", "John McCarthy (computer scientist)", "Johnson's algorithm", "Journal of the ACM", "Jump point search", "Kleene's algorithm", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Logical conjunction", "Logical disjunction", "MATLAB", "MathWorld", "Matrix (mathematics)", "NetworkX", "PDF", "Pathfinder network", "Perl", "Prim's algorithm", "Programming language", "Python (programming language)", "R programming language", "Real number", "Recursion", "Regular expression", "Regular language", "Robert Floyd", "Robert W. Floyd", "Ron Rivest", "SIAM Journal on Computing", "SMA*", "Schulze method", "SciPy", "Search game", "Shortest-path tree", "Shortest path problem", "Sparse graph", "Stephen Cole Kleene", "Stephen Warshall", "Thomas H. Cormen", "Timothy M. Chan", "Transitive closure", "Tree traversal", "University of California, Berkeley", "Uri Zwick", "Weighted graph", "Widest path problem"], "categories": ["Articles with example pseudocode", "Commons category without a link on Wikidata", "Dynamic programming", "Graph algorithms", "Polynomial-time problems", "Routing algorithms"], "title": "Floyd\u2013Warshall algorithm"}
{"summary": "Force-directed graph drawing algorithms are a class of algorithms for drawing graphs in an aesthetically pleasing way. Their purpose is to position the nodes of a graph in two-dimensional or three-dimensional space so that all the edges are of more or less equal length and there are as few crossing edges as possible, by assigning forces among the set of edges and the set of nodes, based on their relative positions, and then using these forces either to simulate the motion of the edges and nodes or to minimize their energy.\nWhile graph drawing can be a difficult problem, force-directed algorithms, being physical simulations, usually require no special knowledge about graph theory such as planarity.", "links": ["Algorithm", "Angular resolution (graph drawing)", "ArXiv", "Barnes\u2013Hut simulation", "Centrality", "Circular arc", "Connected component (graph theory)", "Convex position", "Coulomb's law", "Cytoscape", "David Eppstein", "David Harel", "Digital object identifier", "Dorothea Wagner", "Edward Reingold", "Electric charge", "Euclidean distance", "Genetic algorithm", "Gephi", "Global optimization", "Graph (mathematics)", "Graph drawing", "Graphviz", "Hooke's law", "International Standard Book Number", "LearnDiscovery \u2013 mindmap of Wikipedia (software)", "Limit of a sequence", "Local minimum", "Mechanical equilibrium", "Michael T. Goodrich", "Multidimensional scaling", "N-body", "N-body problem", "N-body simulation", "Online algorithm", "Optimization (mathematics)", "Peter Eades", "Planar graph", "Polyhedral graph", "Prefuse", "Roberto Tamassia", "Running time", "Simulated annealing", "Spline curve", "Spring (device)", "Stress majorization", "Tulip (software)", "Tutte embedding", "W. T. Tutte"], "categories": ["Articles with example pseudocode", "Graph algorithms", "Graph drawing", "Pages containing cite templates with deprecated parameters"], "title": "Force-directed graph drawing"}
{"summary": "In computer science, fringe search is a recent graph search algorithm that finds the least-cost path from a given initial node to one goal node.\nIn essence, fringe search is a middle ground between A* and the iterative deepening A* variant (IDA*).\nIf g(x) is the cost of the search path from the first node to the current, and h(x) is the heuristic estimate of the cost from the current node to the goal, then \u0192(x) = g(x) + h(x), and h* is the actual path cost to the goal. Consider IDA*, which does a recursive left-to-right depth-first search from the root node, stopping the recursion once the goal has been found or the nodes have reached a maximum value \u0192. If no goal is found in the first threshold \u0192, the threshold is then increased and the algorithm searches again. I.E. It iterates on the threshold.\nThere are three major inefficiencies with IDA*. First, IDA* will repeat states when there are multiple (sometimes non-optimal) paths to a goal node - this is often solved by keeping a cache of visited states. IDA* thus altered is denoted as memory-enhanced IDA* (ME-IDA*), since it uses some storage. Furthermore, IDA* repeats all previous operations in a search when it iterates in a new threshold, which is necessary to operate with no storage. By storing the leaf nodes of a previous iteration and using them as the starting position of the next, IDA*'s efficiency is significantly improved (otherwise, in the last iteration it would always have to visit every node in the tree).\nFringe search implements these improvements on IDA* by making use of a data structure that is more or less two lists to iterate over the frontier or fringe of the search tree. One list now, stores the current iteration, and the other list later stores the immediate next iteration. So from the root node of the search tree, now will be the root and later will be empty. Then the algorithm takes one of two actions: If \u0192(head) is greater than the current threshold, remove head from now and append it to the end of later; i.e. save head for the next iteration. Otherwise, if \u0192(head) is less than or equal to the threshold, expand head and discard head, consider its children, adding them to the beginning of now. At the end of an iteration, the threshold is increased, the later list becomes the now list, and later is emptied.\nAn important difference here between fringe and A* is that the contents of the lists in fringe do not necessarily have to be sorted - a significant gain over A*, which requires the often expensive maintenance of order in its open list. Unlike A*, however, fringe will have to visit the same nodes repeatedly, but the cost for each such visit is constant compared to the worst-case logarithmic time of sorting the list in A*.", "links": ["A*", "A* search algorithm", "Alpha\u2013beta pruning", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Computer science", "D*", "Depth-first search", "Depth-limited search", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Goal node", "Graph search algorithm", "Graph traversal", "Heuristic algorithm", "Hill climbing", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List (computing)", "List of algorithms", "Node (graph theory)", "Prim's algorithm", "Recursion (computer science)", "SMA*", "Search game", "Tree traversal"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from June 2013", "Graph algorithms"], "title": "Fringe search"}
{"summary": "The Girvan\u2013Newman algorithm (named after Michelle Girvan and Mark Newman) is a hierarchical method used to detect communities in complex systems.", "links": ["Betweenness centrality", "Centrality", "Closeness (mathematics)", "Community structure", "Complex system", "Dendrogram", "Hierarchical clustering", "Mark Newman", "Modularity (networks)"], "categories": ["Graph algorithms", "Network analysis", "Networks"], "title": "Girvan\u2013Newman algorithm"}
{"summary": "In computer science, a goal node is a node in a graph that meets defined criteria for success or termination.\nHeuristical artificial intelligence algorithms, like A* and B*, attempt to reach such nodes in optimal time by defining the distance to the goal node. When the goal node is reached, A* defines the distance to the goal node as 0 and all other nodes' distances as positive values.", "links": ["A*", "Algorithm", "Artificial intelligence", "B*", "Computer science", "Data structure", "Graph (mathematics)", "Heuristic algorithm", "Node (computer science)", "Tree traversal"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Graph algorithms"], "title": "Goal node (computer science)"}
{"summary": "In structure mining, a domain of learning on structured data objects in machine learning, a graph kernel is a kernel function that computes an inner product on graphs. Graph kernels can be intuitively understood as functions measuring the similarity of pairs of graphs. They allow kernelized learning algorithms such as support vector machines to work directly on graphs, without having to do feature extraction to transform them to fixed-length, real-valued feature vectors. They find applications in bioinformatics, in chemoinformatics (as a type of molecule kernels), and in social network analysis.\nGraph kernels were first described in 2002 by R. I. Kondor and John Lafferty as kernels on graphs, i.e. similarity functions between the nodes of a single graph, with the World Wide Web hyperlink graph as a suggested application. Vishwanathan et al. instead defined kernels between graphs.\nAn example of a kernel between graphs is the random walk kernel, which conceptually performs random walks on two graphs simultaneously, then counts the number of paths that were produced by both walks. This is equivalent to doing random walks on the direct product of the pair of graphs, and from this, a kernel can be derived that can be efficiently computed.", "links": ["Bioinformatics", "Chemoinformatics", "Computer science", "Digital object identifier", "Feature extraction", "Feature vector", "Glossary of graph theory", "Graph (abstract data type)", "Hyperlink", "Inner product space", "Journal of Machine Learning Research", "Kernel trick", "Machine learning", "Molecule kernel", "Molecule mining", "Path (graph theory)", "Positive-definite kernel", "Random walk", "Social network analysis", "Structure mining", "Support vector machine", "Tensor product of graphs", "Tree kernel", "World Wide Web"], "categories": ["All stub articles", "Computer science stubs", "Graph algorithms", "Kernel methods for machine learning"], "title": "Graph kernel"}
{"summary": "Hierarchical clustering is one method for finding community structures in a network. The technique arranges the network into a hierarchy of groups according to a specified weight function. The data can then be represented in a tree structure known as a dendrogram. Hierarchical clustering can either be agglomerative or divisive depending on whether one proceeds through the algorithm by adding links to or removing links from the network, respectively. One divisive technique is the Girvan\u2013Newman algorithm.", "links": ["Betweenness centrality", "Community structure", "Dendrogram", "Girvan\u2013Newman algorithm", "Hierarchical clustering", "Modularity (networks)", "Network topology", "Numerical taxonomy", "Tree structure"], "categories": ["Graph algorithms", "Network analysis"], "title": "Hierarchical clustering of networks"}
{"summary": "In computer science, the Hopcroft\u2013Karp algorithm is an algorithm that takes as input a bipartite graph and produces as output a maximum cardinality matching \u2013 a set of as many edges as possible with the property that no two edges share an endpoint. It runs in  time in the worst case, where  is set of edges in the graph, and  is set of vertices of the graph. In the case of dense graphs the time bound becomes , and for random graphs it runs in near-linear time.\nThe algorithm was found by John Hopcroft and Richard Karp (1973). As in previous methods for matching such as the Hungarian algorithm and the work of Edmonds (1965), the Hopcroft\u2013Karp algorithm repeatedly increases the size of a partial matching by finding augmenting paths. However, instead of finding just a single augmenting path per iteration, the algorithm finds a maximal set of shortest augmenting paths. As a result, only  iterations are needed. The same principle has also been used to develop more complicated algorithms for non-bipartite matching with the same asymptotic running time as the Hopcroft\u2013Karp algorithm.", "links": ["Algorithm", "Algorithmica", "Assignment problem", "Augmenting path", "Average case analysis", "Best, worst and average case", "Bipartite graph", "Bipartite matching", "Breadth-first search", "Cardinality", "Computer science", "Dense graph", "Depth first search", "Digital object identifier", "Dinic's algorithm", "Graph (data structure)", "Hungarian algorithm", "Jack Edmonds", "James B. Orlin", "John Hopcroft", "Kurt Mehlhorn", "Logarithm", "Matching (graph theory)", "Mathematical Reviews", "Maximum flow problem", "Push-relabel maximum flow algorithm", "Rajeev Motwani", "Random graph", "Ravindra K. Ahuja", "Richard Karp", "Robert Tarjan", "Silvio Micali", "Sparse graph", "Symmetric difference", "Symposium on Foundations of Computer Science", "Thomas L. Magnanti", "Vijay Vazirani", "Worst case analysis"], "categories": ["Graph algorithms", "Matching"], "title": "Hopcroft\u2013Karp algorithm"}
{"summary": "Iterative deepening A* (IDA*) is a graph traversal and path search algorithm that can find the shortest path between a designated start node and any member of a set of goal nodes in a weighted graph. It is a variant of iterative deepening depth-first search that borrows the idea to use a heuristic function to evaluate the remaining cost to get to the goal from the A* search algorithm. Since it is a depth-first search algorithm, its memory usage is lower than in A*, but unlike ordinary iterative deepening search, it concentrates on exploring the most promising nodes and thus doesn't go to the same depth everywhere in the search tree. Unlike A*, IDA* doesn't utilize dynamic programming and therefore often ends up exploring the same nodes many times.\nWhile the standard iterative deepening depth-first search uses search depth as the cutoff for each iteration, the IDA* uses the more informative  where  is the cost to travel from the root to node  and  is a problem-specific heuristic estimate of the cost to travel from  to the solution. As in A*, the heuristic has to have particular properties to guarantee optimality (shortest paths); see Properties, below.\nApplications of IDA* are found in such problems as planning. The algorithm was first described by Richard Korf in 1985.", "links": ["A* search algorithm", "Alpha\u2013beta pruning", "Artificial Intelligence (journal)", "Automated planning and scheduling", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "D*", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph traversal", "Hill climbing", "Iterative deepening depth-first search", "Ivan Bratko (computer scientist)", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Path (graph theory)", "Prim's algorithm", "Recursive best-first search", "SMA*", "Search game", "Shortest path problem", "Space complexity", "Tree traversal"], "categories": ["All articles needing expert attention", "All articles that are too technical", "Articles needing expert attention from November 2009", "Articles with example pseudocode", "Game artificial intelligence", "Graph algorithms", "Routing algorithms", "Search algorithms", "Wikipedia articles that are too technical from November 2009"], "title": "Iterative deepening A*"}
{"summary": "Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in a sparse, edge weighted, directed graph. It allows some of the edge weights to be negative numbers, but no negative-weight cycles may exist. It works by using the Bellman\u2013Ford algorithm to compute a transformation of the input graph that removes all negative weights, allowing Dijkstra's algorithm to be used on the transformed graph. It is named after Donald B. Johnson, who first published the technique in 1977.\nA similar reweighting technique is also used in Suurballe's algorithm for finding two disjoint paths of minimum total length between the same two vertices in a graph with non-negative edge weights.", "links": ["A* search algorithm", "All-pairs shortest path problem", "Alpha\u2013beta pruning", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best, worst and average case", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Charles E. Leiserson", "Clifford Stein", "Cycle (graph theory)", "D*", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Directed graph", "Donald B. Johnson", "Dynamic programming", "Edge (graph theory)", "Edmonds' algorithm", "Fibonacci heap", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph (data structure)", "Graph traversal", "Hill climbing", "International Standard Book Number", "Introduction to Algorithms", "Iterative deepening A*", "Iterative deepening depth-first search", "Job Shop Scheduling", "Journal of the ACM", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "National Institute of Standards and Technology", "Negative number", "Prim's algorithm", "Ronald L. Rivest", "SMA*", "Search game", "Shortest path", "Sparse graph", "Suurballe's algorithm", "Thomas H. Cormen", "Time complexity", "Tree traversal", "Vertex (graph theory)", "Weighted graph"], "categories": ["Graph algorithms"], "title": "Johnson's algorithm"}
{"summary": "The Journal of Graph Algorithms and Applications is an open access peer-reviewed scientific journal covering the subject of graph algorithms and graph drawing. The journal was established in 1997 and the editor-in-chief is Giuseppe Liotta (University of Perugia). It is abstracted and indexed by Scopus and MathSciNet.", "links": ["Digital object identifier", "Editor-in-chief", "Graph algorithm", "Graph drawing", "IEEE Symposium on Visual Analytics Science and Technology", "IEEE Transactions in Information Visualization and Computer Graphics", "ISO 4", "International Standard Serial Number", "Library of Congress Control Number", "MathSciNet", "OCLC", "Open access", "Open access journal", "Outline of academic disciplines", "Peer-reviewed", "Scientific journal", "Scopus", "University of Perugia"], "categories": ["Computer science journals", "English-language journals", "Graph algorithms", "Graph drawing", "Mathematics journals", "Publications established in 1997"], "title": "Journal of Graph Algorithms and Applications"}
{"summary": "In computer science, jump point search is an optimization to the A* search algorithm pathfinding algorithm for uniform-cost grids. It reduces symmetries in the search procedure by means of graph pruning, eliminating certain nodes in the grid based on assumptions that can be made about the current node's neighbors, as long as certain conditions relating to the grid are satisfied. As a result, the algorithm can consider long \"jumps\" along straight (horizontal, vertical and diagonal) lines in the grid, rather than the small steps from one grid position to the next that ordinary A* considers.\nJump point search preserves A*'s optimality, while potentially reducing its running time by an order of magnitude.", "links": ["A* search algorithm", "Algorithm", "Alpha\u2013beta pruning", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Computer science", "D*", "Data structure", "Depth-first search", "Depth-limited search", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph traversal", "Hill climbing", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Pathfinding", "Prim's algorithm", "SMA*", "Search game", "Tree traversal"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Game artificial intelligence", "Graph algorithms", "Search algorithms"], "title": "Jump point search"}
{"summary": "The junction tree algorithm (also known as 'Clique Tree') is a method used in machine learning to extract marginalization in general graphs. In essence, it entails performing belief propagation on a modified graph called a junction tree. The basic premise is to eliminate cycles by clustering them into single nodes.\n\n", "links": ["Artificial intelligence", "Belief propagation", "Chordal graph", "Conditional probability", "Cycle (graph theory)", "David Spiegelhalter", "Digital object identifier", "Graph (mathematics)", "JSTOR", "Journal of the Royal Statistical Society", "Junction tree", "Machine learning", "Mathematical Reviews", "Moral graph", "Philip Dawid", "Treewidth"], "categories": ["All articles to be expanded", "All articles with empty sections", "All stub articles", "Articles to be expanded from November 2010", "Articles with empty sections from November 2010", "Artificial intelligence stubs", "Bayesian networks", "Graph algorithms"], "title": "Junction tree algorithm"}
{"summary": "The K shortest path routing algorithm is an extension algorithm of the shortest path routing algorithm in a given network.\nIt is sometimes crucial to have more than one path between two nodes in a given network. In the event there are additional constraints, other paths different from the shortest path can be computed. To find the shortest path one can use shortest path algorithms such as Dijkstra\u2019s algorithm or Bellman Ford algorithm and extend them to find more than one path. The K Shortest path routing algorithm is a generalization of the shortest path problem. The algorithm not only finds the shortest path, but also K-1 other paths in order of increasing cost. K is the number of shortest paths to find. The problem can be restricted to have the K shortest path without loops (loopless K shortest path) or with loop.", "links": ["Andrew V. Goldberg", "Bellman\u2013Ford algorithm", "Breadth-first search", "Computational linguistics", "Constrained Shortest Path First", "David Eppstein", "Digital object identifier", "Dijkstra's algorithm", "Dijkstra algorithm", "Directed graph", "Edge (geometry)", "Floyd\u2013Warshall algorithm", "Johnson's algorithm", "Loop (graph theory)", "Loopless algorithm", "Michael M. Gunter", "Network theory", "Optical mesh network", "Perturbation theory", "Ravindra K. Ahuja", "Shortest-path routing", "Shortest path algorithms", "Shortest path problem", "Sparse graph", "Vertex (graph theory)", "Yen's algorithm"], "categories": ["Computational problems in graph theory", "Graph algorithms", "Network theory", "Polynomial-time problems"], "title": "K shortest path routing"}
{"summary": "In computer science and graph theory, Karger's algorithm is a randomized algorithm to compute a minimum cut of a connected graph. It was invented by David Karger and first published in 1993.\nThe idea of the algorithm is based on the concept of contraction of an edge  in an undirected graph . Informally speaking, the contraction of an edge merges the nodes  and  into one, reducing the total number of nodes of the graph by one. All other edges connecting either  or  are \"reattached\" to the merged node, effectively producing a multigraph. Karger's basic algorithm iteratively contracts randomly chosen edges until only two nodes remain; those nodes represent a cut in the original graph. By iterating this basic algorithm a sufficient number of times, a minimum cut can be found with high probability.", "links": ["Adjacency list", "Adjacency matrix", "Clifford Stein", "Computer science", "Cut (graph theory)", "Cycle graph", "David Karger", "Dense graph", "Digital object identifier", "Edge contraction", "Graph (mathematics)", "Graph theory", "Kruskal\u2019s algorithm", "Mathematical Reviews", "Max-flow min-cut theorem", "Maximum flow", "Minimum cut", "Minimum spanning tree", "Multigraph", "Polynomial time", "Push\u2013relabel maximum flow algorithm", "Randomized algorithm"], "categories": ["Graph algorithms", "Graph connectivity"], "title": "Karger's algorithm"}
{"summary": "In computer science, Kosaraju's algorithm (also known as the Kosaraju\u2013Sharir algorithm) is a linear time algorithm to find the strongly connected components of a directed graph. Aho, Hopcroft and Ullman credit it to an unpublished paper from 1978 by S. Rao Kosaraju. The same algorithm was independently discovered by Micha Sharir and published by him in 1981. It makes use of the fact that the transpose graph (the same graph with the direction of every edge reversed) has exactly the same strongly connected components as the original graph.", "links": ["Adjacency list", "Adjacency matrix", "Alfred V. Aho", "Algorithm", "Asymptotically optimal", "Breadth-first search", "Charles E. Leiserson", "Clifford Stein", "Computer science", "Depth-first search", "Directed graph", "Introduction to Algorithms", "Jeffrey D. Ullman", "John E. Hopcroft", "Linear time", "Micha Sharir", "Path-based strong component algorithm", "Ronald L. Rivest", "S. Rao Kosaraju", "Stack (data structure)", "Strongly connected component", "Tarjan's strongly connected components algorithm", "Thomas H. Cormen", "Transpose graph"], "categories": ["Graph algorithms", "Graph connectivity"], "title": "Kosaraju's algorithm"}
{"summary": "Kruskal's algorithm is a minimum-spanning-tree algorithm which finds an edge of the least possible weight that connects any two trees in the forest. It is a greedy algorithm in graph theory as it finds a minimum spanning tree for a connected weighted graph adding increasing cost arcs at each step. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. If the graph is not connected, then it finds a minimum spanning forest (a minimum spanning tree for each connected component).\nThis algorithm first appeared in Proceedings of the American Mathematical Society, pp. 48\u201350 in 1956, and was written by Joseph Kruskal.\nOther algorithms for this problem include Prim's algorithm, Reverse-delete algorithm, and Bor\u016fvka's algorithm.", "links": ["A* search algorithm", "Ackermann function", "Alpha\u2013beta pruning", "Arbitrary", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Big-O notation", "Binary logarithm", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Charles E. Leiserson", "Clifford Stein", "Comparison sort", "Connected component (graph theory)", "Connectivity (graph theory)", "Counting sort", "D*", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Disjoint-set data structure", "Dynamic programming", "Edge (graph theory)", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Glossary of graph theory", "Graph theory", "Graph traversal", "Greedy algorithm", "Hill climbing", "International Standard Book Number", "Introduction to Algorithms", "Iterative deepening A*", "Iterative deepening depth-first search", "JSTOR", "Johnson's algorithm", "Joseph Kruskal", "Jump point search", "Lexicographic breadth-first search", "List of algorithms", "Mathematical induction", "Michael T. Goodrich", "Minimum spanning tree", "Nonempty", "Prim's algorithm", "Proceedings of the American Mathematical Society", "Radix sort", "Reverse-delete algorithm", "Roberto Tamassia", "Ronald L. Rivest", "SMA*", "Search game", "Single-linkage clustering", "Spanning tree", "Thomas H. Cormen", "Tree (graph theory)", "Tree traversal", "Vertex (graph theory)"], "categories": ["All articles lacking in-text citations", "Articles containing proofs", "Articles lacking in-text citations from June 2013", "Articles with example pseudocode", "Graph algorithms", "Spanning tree"], "title": "Kruskal's algorithm"}
{"summary": "In computer science, lexicographic breadth-first search or Lex-BFS is a linear time algorithm for ordering the vertices of a graph. The algorithm is different from breadth first search, but it produces an ordering that is consistent with breadth-first search.\nThe lexicographic breadth-first search algorithm is based on the idea of partition refinement and was first developed by Donald J. Rose, Robert E. Tarjan, and George S. Lueker (1976). A more detailed survey of the topic is presented by Corneil (2004). It has been used as a subroutine in other graph algorithms including the recognition of chordal graphs, and optimal coloring of distance-hereditary graphs.", "links": ["A* search algorithm", "Adjacency matrix", "Alpha\u2013beta pruning", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "Breadth first search", "British Museum algorithm", "Chordal graph", "Cograph", "Comparability graph", "Complement graph", "Computer science", "D*", "Depth-first search", "Depth-limited search", "Depth first search", "Derek Corneil", "Digital object identifier", "Dijkstra's algorithm", "Distance-hereditary graph", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph coloring", "Graph traversal", "Greedy coloring", "Hill climbing", "Induced subgraph", "International Standard Book Number", "Interval graph", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographical order", "Linear time", "List of algorithms", "Partition (set theory)", "Partition refinement", "Prim's algorithm", "Pseudocode", "Queue (data structure)", "Robert Tarjan", "SIAM Journal on Computing", "SIAM Journal on Discrete Mathematics", "SMA*", "Search game", "Sorting algorithm", "Tree traversal"], "categories": ["Graph algorithms", "Search algorithms"], "title": "Lexicographic breadth-first search"}
{"summary": "The Misra & Gries edge coloring algorithm is a polynomial time algorithm in graph theory that finds an edge coloring of any graph. The coloring produces uses at most  colors, where  is the maximum degree of the graph. This is optimal for some graphs, and by Vizing's theorem it uses at most one color more than the optimal for all others.\nIt was first published by Jayadev Misra and David Gries in 1992. It is a simplification of a prior algorithm by B\u00e9la Bollob\u00e1s.\nThis algorithm is the fastest known almost-optimal algorithm for edge coloring, executing in  time. A faster time bound of  was claimed in a 1985 technical report by Gabow et al., but this has never been published.\nIn general, optimal edge coloring is NP-complete, so it is very unlikely that a polynomial time algorithm exists. There are however exponential time exact edge coloring algorithms that give an optimal solution.", "links": ["B\u00e9la Bollob\u00e1s", "David Gries", "Degree (graph theory)", "Digital object identifier", "Edge coloring", "Graph theory", "Information Processing Letters", "Mathematical induction", "Polynomial time", "Takao Nishizeki", "Vizing's theorem"], "categories": ["Graph algorithms", "Graph coloring"], "title": "Misra & Gries edge coloring algorithm"}
{"summary": "In graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, one to keep track of the vertices in the current component and the second to keep track of the current search path. Versions of this algorithm have been proposed by Purdom (1970), Munro (1971), Dijkstra (1976), Cheriyan & Mehlhorn (1996), and Gabow (2000); of these, Dijkstra's version was the first to achieve linear time.", "links": ["Algorithmica", "Array (data type)", "Depth-first search", "Digital object identifier", "Directed graph", "Edsger Dijkstra", "Graph theory", "Kurt Mehlhorn", "Linear time", "Mathematical Reviews", "Stack (data structure)", "Strongly connected component", "Tarjan's strongly connected components algorithm"], "categories": ["Graph algorithms", "Graph connectivity"], "title": "Path-based strong component algorithm"}
{"summary": "In computer science, Prim's algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. The algorithm operates by building this tree one vertex at a time, from an arbitrary starting vertex, at each step adding the cheapest possible connection from the tree to another vertex.\nThe algorithm was developed in 1930 by Czech mathematician Vojt\u011bch Jarn\u00edk and later rediscovered and republished by computer scientists Robert C. Prim in 1957 and Edsger W. Dijkstra in 1959. Therefore, it is also sometimes called the DJP algorithm, Jarn\u00edk's algorithm, the Prim\u2013Jarn\u00edk algorithm, or the Prim\u2013Dijkstra algorithm.\nOther well-known algorithms for this problem include Kruskal's algorithm and Bor\u016fvka's algorithm. These algorithms find the minimum spanning forest in a possibly disconnected graph; in contrast, the most basic form of Prim's algorithm only finds minimum spanning trees in connected graphs. However, running Prim's algorithm separately for each connected component of the graph, it can also be used to find the minimum spanning forest. In terms of their asymptotic time complexity, these three algorithms are equally fast for sparse graphs, but slower than other more sophisticated algorithms. However, for graphs that are sufficiently dense, Prim's algorithm can be made to run in linear time, meeting or improving the time bounds for other algorithms.", "links": ["A* search algorithm", "Adjacency list", "Adjacency matrix", "Alpha\u2013beta pruning", "Array data structure", "Asymptotic computational complexity", "B*", "Backtracking", "Beam search", "Bell System Technical Journal", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Big-O notation", "Binary heap", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Computer science", "Computer scientist", "Connected component (graph theory)", "Czech people", "D*", "D-ary heap", "David Cheriton", "Dense graph", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Donald B. Johnson", "Dynamic programming", "Edge (graph theory)", "Edmonds' algorithm", "Edsger W. Dijkstra", "Fibonacci heap", "Flag value", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph theory", "Graph traversal", "Greedy algorithm", "Grid graph", "Heap (data structure)", "Hill climbing", "International Standard Book Number", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "Linear time", "Linked list", "List of algorithms", "Mathematical Reviews", "Maze generation", "Minimum spanning tree", "Priority queue", "Pseudocode", "Robert C. Prim", "Robert Sedgewick (computer scientist)", "Robert Tarjan", "SIAM Journal on Computing", "SMA*", "Search game", "Shortest path problem", "Society for Industrial and Applied Mathematics", "Sparse graph", "Time complexity", "Tree (graph theory)", "Tree traversal", "Undirected graph", "Vertex (graph theory)", "Vojt\u011bch Jarn\u00edk", "Weighted graph"], "categories": ["Articles containing proofs", "Articles containing video clips", "CS1 Czech-language sources (cs)", "Graph algorithms", "Spanning tree"], "title": "Prim's algorithm"}
{"summary": "Proof-number search (short: PN search) is a game tree search algorithm invented by Victor Allis, with applications mostly in endgame solvers, but also for sub-goals during games.\nUsing a binary goal (e.g. first player wins the game), game trees of two-person perfect-information games can be mapped to an and\u2013or tree. Maximizing nodes become OR-nodes, minimizing nodes are mapped to AND-nodes. For all nodes proof and disproof numbers are stored, and updated during the search.\nTo each node of the partially expanded game tree the proof number and disproof number are associated. A proof number represents the minimum number of leaf nodes which have to be proved in order to prove the node. Analogously, a disproof number represents the minimum number of leaves which have to be disproved in order to disprove the node. Because the goal of the tree is to prove a forced win, winning nodes are regarded as proved. Therefore, they have proof number 0 and disproof number \u221e. Lost or drawn nodes are regarded as disproved. They have proof number \u221e and disproof number 0. Unknown leaf nodes have a proof and disproof number of unity. The proof number of an internal AND node is equal to the sum of its childrens\u2019 proof numbers, since to prove an AND node all the children have to be proved. The disproof number of an AND node is equal to the minimum of its childrens\u2019 disproof numbers. The disproof number of an internal OR node is equal to the sum of its childrens\u2019 disproof numbers, since to disprove an OR node all the children have to be disproved. Its proof number is equal to the minimum of its childrens\u2019 proof numbers.\nThe procedure of selecting the most-proving node to expand is the following. We start at the root. Then, at each OR node the child with the lowest proof number is selected as successor, and at each AND node the child with the lowest disproof number is selected as successor. Finally, when a leaf node is reached, it is expanded and its children are evaluated.\nThe proof and disproof numbers represent lower bounds on the number of nodes to be evaluated to prove (or disprove) certain nodes. By always selecting the most proving (disproving) node to expand, an efficient search is generated.\nSome variants of proof number search like dfPN, PN2, PDS-PN have been developed to address the quite big memory requirements of the algorithm.", "links": ["And\u2013or tree", "Endgame solver", "Game tree", "International Standard Book Number", "Perfect-information game", "Search algorithm", "Victor Allis"], "categories": ["Game artificial intelligence", "Graph algorithms", "Search algorithms"], "title": "Proof-number search"}
{"summary": "The reverse-delete algorithm is an algorithm in graph theory used to obtain a minimum spanning tree from a given connected, edge-weighted graph. It first appeared in Kruskal (1956), but it should not be confused with Kruskal's algorithm which appears in the same paper. If the graph is disconnected, this algorithm will find a minimum spanning tree for each disconnected part of the graph. The set of these minimum spanning trees is called a minimum spanning forest, which contains every vertex in the graph.\nThis algorithm is a greedy algorithm, choosing the best choice given any situation. It is the reverse of Kruskal's algorithm, which is another greedy algorithm to find a minimum spanning tree. Kruskal\u2019s algorithm starts with an empty graph and adds edges while the Reverse-Delete algorithm starts with the original graph and deletes edges from it. The algorithm works as follows:\nStart with graph G, which contains a list of edges E.\nGo through E in decreasing order of edge weights.\nFor each edge, check if deleting the edge will further disconnect the graph.\nPerform any deletion that does not lead to additional disconnection.", "links": ["Algorithm", "Big-O notation", "Bor\u016fvka's algorithm", "Digital object identifier", "Dijkstra's algorithm", "Graph theory", "Greedy algorithm", "JSTOR", "Joseph Kruskal", "Kruskal's algorithm", "Mikkel Thorup", "Minimum spanning tree", "Prim's algorithm", "Proceedings of the American Mathematical Society", "Symposium on Theory of Computing", "Weighted graph", "\u00c9va Tardos"], "categories": ["Graph algorithms", "Spanning tree"], "title": "Reverse-delete algorithm"}
{"summary": "Rocha\u2013Thatte algorithm is a distributed algorithm in graph theory for detecting cycles on large-scale directed graphs based on the bulk synchronous message passing abstraction. This algorithm for detecting cycles by message passing is suitable to be implemented in distributed graph processing systems, and it is also suitable for implementations in systems for disk-based computations, such as the GraphChi, where the computation is mainly based on secondary memory. Disk-based computations are necessary when we have a single computer for processing large-scale graphs, and the computation exceeds the primary memory capacity.", "links": ["Bulk synchronous parallel", "Cycle (graph theory)", "Directed graph", "Distributed algorithm", "Graph theory"], "categories": ["Graph algorithms"], "title": "Rocha\u2013Thatte cycle detection algorithm"}
{"summary": "In computer science, the Sethi\u2013Ullman algorithm is an algorithm named after Ravi Sethi and Jeffrey D. Ullman, its inventors, for translating abstract syntax trees into machine code that uses as few registers as possible.", "links": ["Abstract syntax tree", "Algorithm", "Associativity", "Code generation (compiler)", "Commutativity", "Compiler", "Computer science", "Digital object identifier", "Jeffrey D. Ullman", "Journal of the Association for Computing Machinery", "Machine code", "Order of evaluation", "Processor register", "RISC", "Ravi Sethi", "Register allocation", "Register spilling", "Strahler number"], "categories": ["Compiler construction", "Graph algorithms"], "title": "Sethi\u2013Ullman algorithm"}
{"summary": "The Shortest Path Faster Algorithm (SPFA) is an improvement of the Bellman\u2013Ford algorithm which computes single-source shortest paths in a weighted directed graph. The algorithm is believed to work well on random sparse graphs and is particularly suitable for graphs that contain negative-weight edges. However, the worst-case complexity of SPFA is the same as that of Bellman\u2013Ford, so for graphs with nonnegative edge weights Dijkstra's algorithm is preferred. The SPFA algorithm was published in 1994 by Fanding Duan.", "links": ["Bellman\u2013Ford algorithm", "Dijkstra's algorithm"], "categories": ["Graph algorithms"], "title": "Shortest Path Faster Algorithm"}
{"summary": "SMA* or Simplified Memory Bounded A* is a shortest path algorithm based on the A* algorithm. The main advantage of SMA* is that it uses a bounded memory, while the A* algorithm might need exponential memory. All other characteristics of SMA* are inherited from A*.", "links": ["A*", "A* search algorithm", "Alpha\u2013beta pruning", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "CiteSeer", "D*", "Depth-first search", "Depth-limited search", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph traversal", "Heuristic", "Hill climbing", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Prim's algorithm", "Search game", "Shortest path algorithm", "Tree traversal"], "categories": ["All articles needing additional references", "All articles needing expert attention", "All articles that are too technical", "Articles needing additional references from March 2015", "Articles needing expert attention from November 2009", "Articles with example pseudocode", "Game artificial intelligence", "Graph algorithms", "Routing algorithms", "Search algorithms", "Wikipedia articles that are too technical from November 2009"], "title": "SMA*"}
{"summary": "Spectral layout is a class of algorithm for drawing graphs. The layout uses the eigenvectors of a matrix, such as the Laplace matrix of the graph, as Cartesian coordinates of the graph's vertices.", "links": ["Algorithm", "Applied mathematics", "Cartesian coordinate", "Digital object identifier", "Eigenvectors", "Graph drawing", "Laplace matrix", "Mathematical Reviews"], "categories": ["All stub articles", "Applied mathematics stubs", "Graph algorithms", "Graph drawing"], "title": "Spectral layout"}
{"summary": "In theoretical computer science and network routing, Suurballe's algorithm is an algorithm for finding two disjoint paths in a nonnegatively-weighted directed graph, so that both paths connect the same pair of vertices and have minimum total length. The algorithm was conceived by John W. Suurballe and published in 1974. The main idea of Suurballe's algorithm is to use Dijkstra's algorithm to find one path, to modify the weights of the graph edges, and then to run Dijkstra's algorithm a second time. The modification to the weights is similar to the weight modification in Johnson's algorithm, and preserves the non-negativity of the weights while allowing the second instance of Dijkstra's algorithm to find the correct second path.\nThe objective is strongly related to that of minimum cost flow algorithms, where in this case there are two units of \"flow\" and nodes have unit \"capacity\".", "links": ["Digital object identifier", "Dijkstra's algorithm", "Directed graph", "Edge disjoint shortest pair algorithm", "Fibonacci heap", "International Standard Book Number", "Johnson's algorithm", "Minimum cost flow", "Network routing", "Robert Tarjan", "Shortest path", "Theoretical computer science", "Vertex (graph theory)"], "categories": ["Graph algorithms", "Routing algorithms"], "title": "Suurballe's algorithm"}
{"summary": "In computer science, Tarjan's off-line lowest common ancestors algorithm is an algorithm for computing lowest common ancestors for pairs of nodes in a tree, based on the union-find data structure. The lowest common ancestor of two nodes d and e in a rooted tree T is the node g that is an ancestor of both d and e and that has the greatest depth in T. It is named after Robert Tarjan, who discovered the technique in 1979. Tarjan's algorithm is an offline algorithm; that is, unlike other lowest common ancestor algorithms, it requires that all pairs of nodes for which the lowest common ancestor is desired must be specified in advance. The simplest version of the algorithm uses the union-find data structure, which unlike other lowest common ancestor data structures can take more than constant time per operation when the number of pairs of nodes is similar in magnitude to the number of nodes. A later refinement by Gabow & Tarjan (1983) speeds the algorithm up to linear time.", "links": ["Algorithm", "Computer science", "Digital object identifier", "Disjoint-set data structure", "Journal of the ACM", "Linear time", "Lowest common ancestor", "Robert Tarjan", "Rooted tree", "Tarjan's strongly connected components algorithm", "Union-find"], "categories": ["Graph algorithms"], "title": "Tarjan's off-line lowest common ancestors algorithm"}
{"summary": "Tarjan's Algorithm is an algorithm in graph theory for finding the strongly connected components of a graph. Although proposed earlier, it can be seen as an improved version of Kosaraju's algorithm, and is comparable in efficiency to the path-based strong component algorithm. Tarjan's Algorithm is named for its discoverer, Robert Tarjan.", "links": ["Algorithm", "Best, worst and average case", "Digital object identifier", "Directed acyclic graph", "Directed graph", "Graph (data structure)", "Graph theory", "Invariant (computer science)", "Kosaraju's algorithm", "Partition of a set", "Path-based strong component algorithm", "Robert Tarjan", "SIAM Journal on Computing", "Spanning forest", "Stack (data structure)", "Strongly connected component", "Topological sorting", "Vertex (graph theory)"], "categories": ["Articles with example pseudocode", "Graph algorithms", "Graph connectivity"], "title": "Tarjan's strongly connected components algorithm"}
{"summary": "In the field of computer science, a topological sort (sometimes abbreviated toposort) or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks. A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time.", "links": ["Adaptive sort", "Adjacency matrix", "Algorithm", "American flag sort", "Analysis of algorithms", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "Clifford Stein", "Cocktail sort", "Coffman\u2013Graham algorithm", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer science", "Counting sort", "Cycle sort", "D. E. Knuth", "Dependency (disambiguation)", "Dependency graph", "Dependency resolution", "Depth-first search", "Digital object identifier", "Directed acyclic graph", "Directed cycle", "Directed graph", "Directed path", "Eric W. Weisstein", "Feedback arc set", "Flashsort", "Gnome sort", "Hamiltonian path", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Instruction scheduling", "Integer sorting", "International Standard Book Number", "Introduction to Algorithms", "Introsort", "JSort", "Job shop scheduling", "Layered graph drawing", "Lexicographic order", "Library sort", "Linear extension", "Linear time", "Linker (computing)", "List (computing)", "Logic synthesis", "Longest path problem", "Makefile", "MathWorld", "Mathematical Reviews", "Merge sort", "Min-plus matrix multiplication", "NC (complexity)", "NP-hard", "Odd\u2013even sort", "Online algorithm", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Parallel computer", "Partial order", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Program Evaluation and Review Technique", "Project management", "Proxmap sort", "Quicksort", "Radix sort", "Reachability", "Robert Tarjan", "Ron Rivest", "Ronald L. Rivest", "Selection algorithm", "Selection sort", "Serialization", "Shellsort", "Shortest path problem", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsheet", "Spreadsort", "Stephen Cook", "Stooge sort", "Strand sort", "The Art of Computer Programming", "Thomas H. Cormen", "Timsort", "Total order", "Tournament sort", "Transitive reduction", "Transitive relation", "Tree sort", "Tsort (Unix)", "Vertex (graph theory)", "Weighted graph"], "categories": ["Articles with example pseudocode", "Directed graphs", "Graph algorithms", "Sorting algorithms"], "title": "Topological sorting"}
{"summary": "In computer science, tree traversal (also known as tree search) is a form of graph traversal and refers to the process of visiting (examining and/or updating) each node in a tree data structure, exactly once, in a depth-first manner. Such traversals are classified by the order in which the nodes are visited. The following algorithms are described for a binary tree, but they may be generalized to other trees as well.", "links": ["A* search algorithm", "Alpha\u2013beta pruning", "Array data structure", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bijection", "Binary search tree", "Binary tree", "Bor\u016fvka's algorithm", "Branch and bound", "Branching factor", "Breadth-first search", "British Museum algorithm", "Call stack", "Chess", "Composition (number theory)", "Computer science", "Corecursion", "D*", "Depth-first search", "Depth-limited search", "Diagonal argument (disambiguation)", "Digital object identifier", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Functional programming", "Game tree", "Go (game)", "Graph traversal", "Hill climbing", "Information Processing Letters", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lazy evaluation", "Lexicographic breadth-first search", "Lexicographic order", "Linked list", "List of algorithms", "List of data structures", "Monte Carlo method", "Monte Carlo tree search", "Ordinal number", "Parse tree", "Polish notation", "Prim's algorithm", "Queue (abstract data type)", "Queue (data structure)", "Recursion", "Reverse Polish notation", "Rosetta Code", "SMA*", "Search game", "Search tree", "Stack (abstract data type)", "Threaded binary tree", "Tree (data structure)"], "categories": ["All articles needing additional references", "Articles needing additional references from June 2013", "Articles needing additional references from May 2009", "Articles with example Haskell code", "Articles with example Java code", "Articles with example pseudocode", "Graph algorithms", "Iteration in programming", "Recursion", "Trees (data structures)"], "title": "Tree traversal"}
{"summary": "In graph algorithms, the widest path problem, also known as the bottleneck shortest path problem or the maximum capacity path problem, is the problem of finding a path between two designated vertices in a weighted graph, maximizing the weight of the minimum-weight edge in the path.\nFor instance, if the graph represents connections between routers in the Internet, and the weight of an edge represents the bandwidth of a connection between two routers, the widest path problem is the problem of finding an end-to-end path between two Internet nodes that has the maximum possible bandwidth. The weight of the minimum-weight edge is known as the capacity or bandwidth of the path. As well as its applications in network routing, the widest path problem is also an important component of the Schulze method for deciding the winner of a multiway election, and has been applied to digital compositing, metabolic analysis, and the computation of maximum flows. It is possible to adapt most shortest path algorithms to compute widest paths, by modifying them to use the bottleneck distance instead of path length. However, in many cases even faster algorithms are possible.\nA closely related problem, the minimax path problem, asks for the path that minimizes the maximum weight of any of its edges. It has applications that include transportation planning. Any algorithm for the widest path problem can be transformed into an algorithm for the minimax path problem, or vice versa, by reversing the sense of all the weight comparisons performed by the algorithm, or equivalently by replacing every edge weight by its negation.", "links": ["Aerial photography", "Algorithm", "American Mathematical Monthly", "Asymptotic computational complexity", "Bandwidth (computing)", "Breadth first search", "Cartesian tree", "Condorcet method", "Data structure", "Dense graph", "Depth first search", "Digital compositing", "Digital object identifier", "Dijkstra's algorithm", "Directed graph", "Edmonds\u2013Karp algorithm", "Election", "Erik Demaine", "Euclidean minimum spanning tree", "Euclidean plane", "Fast matrix multiplication", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Gaussian integer", "Gaussian moat", "Geometric spanner", "Graph algorithm", "Grid graph", "Image registration", "Integer sorting", "International Standard Book Number", "Internet", "Iterated logarithm", "JSTOR", "James B. Orlin", "Jit Bose", "Linear time", "Lowest common ancestor", "Mathematical Reviews", "Maximum flow", "Maximum flow problem", "Median", "Metabolic network", "Mikkel Thorup", "Minimum spanning tree", "Monotonic", "Number theory", "Path (graph theory)", "Positive number", "Priority queue", "Ranked voting systems", "Ravindra K. Ahuja", "Recursion", "Robert Tarjan", "Router (computing)", "Ryan Williams (computer scientist)", "Schulze method", "Shortest path", "Sorting algorithm", "Sparse graph", "Stan Wagon", "Symposium on Foundations of Computer Science", "Symposium on Theory of Computing", "Thomas L. Magnanti", "Tournament (graph theory)", "Ultrametric", "Undirected graph", "Vertex (graph theory)", "Wikimedia Foundation"], "categories": ["CS1 French-language sources (fr)", "Computational problems in graph theory", "Graph algorithms", "Network theory", "Polynomial-time problems"], "title": "Widest path problem"}
{"summary": "Yen's algorithm computes single-source K-shortest loopless paths for a graph with non-negative edge cost. The algorithm was published by Jin Y. Yen in 1971 and employs any shortest path algorithm to find the best path, then proceeds to find K \u2212 1 deviations of the best path.", "links": ["A* search algorithm", "Alpha\u2013beta pruning", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "D*", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Eugene Lawler", "Floyd\u2013Warshall algorithm", "Fringe search", "Glossary of graph theory", "Graph (mathematics)", "Graph traversal", "Hill climbing", "IEEE", "International Standard Book Number", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "K shortest path routing", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Mathematical Reviews", "Michael Fredman", "Prim's algorithm", "Robert Tarjan", "SMA*", "Search game", "Shortest path", "Shortest path algorithm", "Tree traversal"], "categories": ["Articles with example pseudocode", "Graph algorithms", "Polynomial-time problems"], "title": "Yen's algorithm"}
{"summary": "In job shop scheduling and graph drawing, the Coffman\u2013Graham algorithm is an algorithm, named after Edward G. Coffman, Jr. and Ronald Graham, for arranging the elements of a partially ordered set into a sequence of levels. The algorithm chooses an arrangement such that an element that comes after another in the order is assigned to a lower level, and such that each level has a number of elements that does not exceed a fixed width bound W. When W = 2, it uses the minimum possible number of distinct levels, and in general it uses at most 2 \u2212 2/W times as many levels as necessary.", "links": ["Acta Informatica", "Alexander Rinnooy Kan", "Algorithm", "Covering relation", "Crossing number (graph theory)", "Digital object identifier", "Directed acyclic graph", "Directed graph", "Disjoint-set data structure", "Dorothea Wagner", "Edward G. Coffman, Jr.", "Feedback arc set", "Graph drawing", "IEEE Systems, Man & Cybernetics Society", "IEEE Transactions on Computers", "Integer programming", "International Standard Book Number", "International Symposium on Graph Drawing", "Interval order", "JSTOR", "Jan Karel Lenstra", "Job shop scheduling", "Layered graph drawing", "Lexicographic order", "Linear time", "Makespan", "Mathematical Reviews", "Microsoft Automatic Graph Layout", "Partially ordered set", "Partition refinement", "Permutation", "Peter Eades", "Ravi Sethi", "Reachability", "Reverse graph", "Robert Tarjan", "Roberto Tamassia", "Ronald Graham", "SIAM Journal on Computing", "SIAM Journal on Discrete Mathematics", "Topological sorting", "Total flow time", "Transitive reduction"], "categories": ["Graph drawing", "Optimization algorithms and methods", "Scheduling algorithms"], "title": "Coffman\u2013Graham algorithm"}
{"summary": "A sorting algorithm is an algorithm that puts elements of a list in a certain order. The most-used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the use of other algorithms (such as search and merge algorithms) which require input data to be in sorted lists; it is also often useful for canonicalizing data and for producing human-readable output. More formally, the output must satisfy two conditions:\nThe output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);\nThe output is a permutation (reordering) of the input.\nFurther, the data is often taken to be in an array, which allows random access, rather than a list, which only allows sequential access, though often algorithms can be applied with suitable modification to either type of data.\nSince the dawn of computing, the sorting problem has attracted a great deal of research, perhaps due to the complexity of solving it efficiently despite its simple, familiar statement. For example, bubble sort was analyzed as early as 1956. A fundamental limit of comparison sorting algorithms is that they require linearithmic time \u2013 O(n log n) \u2013 in the worst case, though better performance is possible on real-world data (such as almost-sorted data), and algorithms not based on comparison, such as counting sort, can have better performance. Although many consider sorting a solved problem \u2013 asymptotically optimal algorithms have been known since the mid-20th century \u2013 useful new algorithms are still being invented, with the now widely used Timsort dating to 2002, and the library sort being first published in 2006.\nSorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, time-space tradeoffs, and upper and lower bounds.", "links": ["Adaptive sort", "Algorithm", "American flag sort", "Array data type", "Average performance", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Best-case performance", "Big O notation", "Binary tree", "Binary tree sort", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Byte Magazine", "Canonicalization", "Cartesian tree", "Cascade merge sort", "Central Processing Unit", "Charles E. Leiserson", "CiteSeer", "Clifford Stein", "Cocktail sort", "Collation", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer bus", "Counting sort", "Cubesort", "Cycle sort", "Data structure", "Decrease and conquer", "Digital object identifier", "Distributed algorithm", "Divide and conquer algorithm", "Domain of a function", "Donald Knuth", "Donald Shell", "Endre Szemer\u00e9di", "External sorting", "Fisher\u2013Yates shuffle", "Flashsort", "Ford-Johnson algorithm", "Gnome sort", "Heap (data structure)", "Heapsort", "Hybrid algorithm", "In-place algorithm", "In-place merge sort", "Information Processing Letters", "Insertion sort", "Integer sorting", "International Conference on Theory and Applications of Models of Computation", "International Standard Book Number", "Introduction to Algorithms", "Introsort", "Inversion (discrete mathematics)", "JDK7", "JSort", "Java (programming language)", "Java version history", "J\u00e1nos Koml\u00f3s (mathematician)", "Least significant digit", "Lecture Notes in Computer Science", "Lexicographical order", "Library sort", "Linearithmic", "List (computing)", "Longest increasing subsequence", "Median", "Median of medians", "Merge algorithm", "Merge sort", "Mergesort", "Michael T. Goodrich", "Mikkel Thorup", "Mikl\u00f3s Ajtai", "Most significant digit", "Na\u00efve algorithm", "Niklaus Wirth", "Odd-even sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Partial sorting", "Patience sorting", "Perl", "Permutation", "Pigeonhole sort", "Polyphase merge sort", "Postman sort", "Proxmap sort", "Python (programming language)", "Quantum sort", "Quickselect", "Quicksort", "Radix sort", "Random access", "Randomized algorithm", "Relational database", "Roberto Tamassia", "Ron Rivest", "Samplesort", "Schwartzian transform", "Search algorithm", "Selection algorithm", "Selection sort", "Self-balancing binary search tree", "Shell sort", "Shellsort", "Shuffling algorithm", "Smoothsort", "Sort (C++)", "Sorting", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Standard Template Library", "Stooge sort", "Strand sort", "Swap (computer science)", "Symposium on Foundations of Computer Science", "Symposium on Theory of Computing", "The Computer Journal", "Thomas H. Cormen", "Time-space tradeoff", "Time complexity", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "Unstable sort", "Upper and lower bounds", "Virtual memory", "Worst-case performance"], "categories": ["All articles lacking in-text citations", "All articles with specifically marked weasel-worded phrases", "All articles with unsourced statements", "Articles lacking in-text citations from September 2009", "Articles with specifically marked weasel-worded phrases from September 2015", "Articles with unsourced statements from December 2010", "Commons category with local link same as on Wikidata", "Data processing", "Sorting algorithms"], "title": "Sorting algorithm"}
{"summary": "The adaptive heap sort is a sorting algorithm that is similar to heap sort, but uses a randomized binary search tree to structure the input according to any preexisting order. The randomized binary search tree is used to select candidates that are put into the heap, so the heap doesn't need to keep track of all elements. Adaptive heap sort is a part of the adaptive sorting family.\nThe first adaptive heapsort was Dijkstra's Smoothsort.", "links": ["Adaptive sort", "Algorithm", "Data structure", "Dictionary of Algorithms and Data Structures", "Heap sort", "National Institute of Standards and Technology", "Randomized binary search tree", "Smoothsort", "Sorting algorithm"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Comparison sorts", "Computer science stubs", "Heaps (data structures)", "Sorting algorithms"], "title": "Adaptive heap sort"}
{"summary": "A sorting algorithm falls into the adaptive sort family if it takes advantage of existing order in its input. It benefits from the presortedness in the input sequence \u2013 or a limited amount of disorder for various definitions of measures of disorder \u2013 and sorts faster. Adaptive sorting is usually performed by modifying existing sorting algorithms.", "links": ["Adaptive heap sort", "American flag sort", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "CiteSeer", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Digital object identifier", "Flashsort", "Gnome sort", "Heap sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "International Standard Book Number", "International Standard Serial Number", "Introsort", "Inversion (discrete mathematics)", "JSort", "Lecture Notes in Computer Science", "Library sort", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Pseudo-code", "Quicksort", "Radix sort", "Randomness", "Sartaj Sahni", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting algorithms", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Tim Peters (programmer)", "Time complexity", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Sorting algorithms"], "title": "Adaptive sort"}
{"summary": "An American flag sort is an efficient, in-place variant of radix sort that distributes items into hundreds of buckets. Non-comparative sorting algorithms such as radix sort and American flag sort are typically used to sort large objects such as strings, for which comparison is not a unit-time operation. American flag sort iterates through the bits of the objects, considering several bits of each object at a time. For each set of bits, American flag sort makes two passes through the array of objects: first to count the number of objects that will fall in each bin, and second to place each object in its bucket. This works especially well when sorting a byte at a time, using 256 buckets. With some optimizations, it is twice as fast as quicksort for large sets of strings.\nThe name comes by analogy with the Dutch national flag problem in the last step: efficiently partition the array into many \"stripes\".", "links": ["ASCII", "Adaptive sort", "Analogy", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket (computing)", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Dictionary of Algorithms and Data Structures", "Dutch national flag problem", "Flag of the United States", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "Multi-key quicksort", "National Institute of Standards and Technology", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Partition of a set", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "String (computer science)", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Sorting algorithms"], "title": "American flag sort"}
{"summary": "Bead sort, also called gravity sort is a natural sorting algorithm, developed by Joshua J. Arulanandham, Cristian S. Calude and Michael J. Dinneen in 2002, and published in The Bulletin of the European Association for Theoretical Computer Science. Both digital and analog hardware implementations of bead sort can achieve a sorting time of O(n); however, the implementation of this algorithm tends to be significantly slower in software and can only be used to sort lists of positive integers. Also, it would seem that even in the best case, the algorithm requires O(n2) space.", "links": ["Abacus", "Adaptive sort", "American flag sort", "Analog computer", "Batcher odd\u2013even mergesort", "Big O Notation", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cristian S. Calude", "Cycle sort", "Digital data", "Digital hardware", "European Association for Theoretical Computer Science", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "Implementation", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Joshua J. Arulanandham", "Kibibyte", "Library sort", "List (computing)", "Logarithm", "Merge sort", "Michael J. Dinneen", "Natural algorithm", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Positive integer", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Software", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Sorting algorithms"], "title": "Bead sort"}
{"summary": "Binary Prioritization is a sorting algorithm which prioritizes to-do tasks.\nUnlike other binary sort methods (e.g. binary search) this method assumes that the deferred work will be prioritized in a later process, but their order is not relevant in the first iteration. The faster processing of classified and important tasks is achieved by reducing the cost of sorting by not sorting the subset of the less important tasks. In each iteration, the cost is reduced by the sorted elements.", "links": ["Algorithm", "Binary search", "Delimiter", "Iteration", "Sorting algorithm"], "categories": ["All articles lacking sources", "All orphaned articles", "Articles lacking sources from October 2013", "Data processing", "Orphaned articles from October 2013", "Sorting algorithms"], "title": "Binary prioritization"}
{"summary": "Bitonic mergesort is a parallel algorithm for sorting. It is also used as a construction method for building a sorting network. The algorithm was devised by Ken Batcher. The resulting sorting networks consist of  comparators and have a delay of , where  is the number of items to be sorted.\nA sorted sequence is a monotonically non-decreasing (or non-increasing) sequence. A bitonic sequence is a sequence with  for some , or a circular shift of such a sequence.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Butterfly network", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Java (programming language)", "Ken Batcher", "Library sort", "List (computing)", "Merge sort", "NIST", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Parallel algorithm", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Python (programming language)", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Sorting algorithms"], "title": "Bitonic sorter"}
{"summary": "Block sort, or block merge sort, is a sorting algorithm combining at least two merge operations with an insertion sort to arrive at O(n log n) in-place stable sorting. It gets its name from the observation that merging two sorted lists, A and B, is equivalent to breaking A into evenly sized blocks, inserting each A block into B under special rules, and merging AB pairs.\nOne practical algorithm for block sort was proposed by Pok-Son Kim and Arne Kutzner in 2008.", "links": ["1/2 + 1/4 + 1/8 + 1/16 + \u00b7 \u00b7 \u00b7", "64-bit computing", "Adaptive sort", "Addison-Wesley", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Binary search algorithm", "Bitonic sorter", "Bitwise operation", "Block-sorting compression", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Digital object identifier", "Fixed-point arithmetic", "Flashsort", "Floor and ceiling functions", "Gnome sort", "Hacker's Delight", "Heapsort", "Hybrid algorithm", "In-place", "In-place algorithm", "Increment and decrement operators", "Information Processing Letters", "Insertion sort", "Integer overflow", "Integer sorting", "International Standard Serial Number", "Interval (mathematics)", "Introsort", "JSort", "Lecture Notes in Computer Science", "Library sort", "Linear search", "List (computing)", "Memory management", "Merge algorithm", "Merge sort", "Modulo operation", "Non-recursive function", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Programming Pearls", "Proxmap sort", "Quicksort", "Radix sort", "SIAM Journal on Computing", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stack (abstract data type)", "Stooge sort", "Strand sort", "Swap (computer science)", "Technical Report", "Theoretical Computer Science", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Transdichotomous model", "Tree sort"], "categories": ["Articles with example pseudocode", "Comparison sorts", "Sorting algorithms", "Stable sorts"], "title": "Block sort"}
{"summary": "In computer science, bogosort (also stupid sort, slowsort, random sort, shotgun sort or monkey sort) is a particularly ineffective sorting algorithm based on the generate and test paradigm. It is not useful for sorting, but may be used for educational purposes, to contrast it with other more realistic algorithms; it has also been used as an example in logic programming. If bogosort were used to sort a deck of cards, it would consist of checking if the deck were in order, and if it were not, throwing the deck into the air, picking the cards up at random, and repeating the process until the deck is sorted. Its name comes from the word bogus.", "links": ["Adaptive sort", "Almost surely", "American flag sort", "Array data structure", "Asymptotic analysis", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Bitonic sorter", "Block sort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer science", "Counting sort", "Cycle sort", "Deck of cards", "Digital object identifier", "Expected value", "Flashsort", "Gnome sort", "Google Code Jam", "Heapsort", "Heat death of the universe", "Hybrid algorithm", "In-place algorithm", "Infinite monkey theorem", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Las Vegas algorithm", "Lecture Notes in Computer Science", "Library sort", "List (computing)", "Logic programming", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Pseudocode", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sort (Unix)", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "Trial and error", "Unix-like", "WikiWikiWeb"], "categories": ["Accuracy disputes from November 2015", "All accuracy disputes", "Articles to be expanded from November 2015", "Comparison sorts", "Computer humor", "Sorting algorithms", "Use dmy dates from June 2011"], "title": "Bogosort"}
{"summary": "Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list to be sorted, compares each pair of adjacent items and swaps them if they are in the wrong order. The pass through the list is repeated until no swaps are needed, which indicates that the list is sorted. The algorithm, which is a comparison sort, is named for the way smaller elements \"bubble\" to the top of the list. Although the algorithm is simple, it is too slow and impractical for most problems even when compared to insertion sort. It can be practical if the input is usually in sort order but may occasionally have some out-of-order elements nearly in position.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Average-case complexity", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Big o notation", "Bitonic sorter", "Block sort", "Bogosort", "Branch predictor", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "Clifford Stein", "Cocktail shaker sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer science", "Counting sort", "Cycle sort", "Dictionary of Algorithms and Data Structures", "Donald Knuth", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introduction to Algorithms", "Introsort", "Inversion (discrete mathematics)", "Iteration", "JSort", "Jargon File", "Library sort", "List (computing)", "Merge sort", "National Institute of Standards and Technology", "Odd-even sort", "Odd\u2013even sort", "On-Line Encyclopedia of Integer Sequences", "Oscillating merge sort", "Owen Astrachan", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Pseudocode", "Quicksort", "Radix sort", "Ronald L. Rivest", "Sartaj Sahni", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Swap (computer science)", "The Art of Computer Programming", "The Tortoise and the Hare", "Thomas H. Cormen", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["All articles with unsourced statements", "Articles with example pseudocode", "Articles with unsourced statements from August 2015", "Commons category with local link same as on Wikidata", "Comparison sorts", "Pages with syntax highlighting errors", "Sorting algorithms", "Stable sorts", "Wikipedia articles needing clarification from October 2014"], "title": "Bubble sort"}
{"summary": "Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a distribution sort, and is a cousin of radix sort in the most to least significant digit flavour. Bucket sort is a generalization of pigeonhole sort. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity estimates involve the number of buckets.\nBucket sort works as follows:\nSet up an array of initially empty \"buckets\".\nScatter: Go over the original array, putting each object in its bucket.\nSort each non-empty bucket.\nGather: Visit the buckets in order and put all elements back into the original array.", "links": ["Adaptive sort", "American flag sort", "Analysis of algorithms", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket (computing)", "Burstsort", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "Clifford Stein", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Dictionary of Algorithms and Data Structures", "Distribution sort", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introduction to Algorithms", "Introsort", "JSort", "J sort", "Library sort", "List (computing)", "Merge sort", "Mergesort", "National Institute of Standards and Technology", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Post office", "Proxmap sort", "Quicksort", "Radix sort", "Ronald L. Rivest", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Thomas H. Cormen", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["All articles needing expert attention", "Articles needing expert attention from November 2008", "Articles needing expert attention with no reason or talk parameter", "Articles with example pseudocode", "Computer science articles needing expert attention", "Sorting algorithms", "Stable sorts"], "title": "Bucket sort"}
{"summary": "Burstsort and its variants are cache-efficient algorithms for sorting strings and are faster than radix sort for large data sets of common strings, first published in 2003.\nBurstsort algorithms use a trie to store prefixes of strings, with growable arrays of pointers as end nodes containing sorted, unique, suffixes (referred to as buckets). Some variants copy the string tails into the buckets. As the buckets grow beyond a predetermined threshold, the buckets are \"burst\", giving the sort its name. A more recent variant uses a bucket index with smaller sub-buckets to reduce memory usage. Most implementations delegate to multikey quicksort, an extension of three-way radix quicksort, to sort the contents of the buckets. By dividing the input into buckets with common prefixes, the sorting can be done in a cache-efficient manner.\nBurstsort was introduced as a sort that is similar to MSD Radix Sort, but is faster due to being aware of caching and related radixes being stored closer to each other due to specifics of trie structure. It exploits specifics of strings that are usually encountered in real world. And even though asymptotically it is the same as radix sort, with time complexity of O(wn) (w - word length and n - number of strings to be sorted), but due to more optimal memory distribution it tends to be twice as fast on big data sets of strings.", "links": ["Adaptive sort", "Algorithm", "American flag sort", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Data set", "Data structure", "Digital object identifier", "Dynamic array", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "String (computer science)", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "Trie"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Sorting algorithms"], "title": "Burstsort"}
{"summary": "The cache-oblivious distribution sort is a comparison-based sorting algorithm. It was introduced in 1999 in the context of the cache oblivious model. In the external memory model, the number of memory transfers it needs to perform a sort of  items on a machine with cache of size  and cache lines of length  is , under the tall cache assumption that . This number of memory transfers has been shown to be asymptotically optimal for comparison sorts. This distribution sort also achieves the asymptotically optimal runtime complexity of .", "links": ["Cache-oblivious algorithm", "Comparison sort", "Median of medians", "Sorting algorithm"], "categories": ["All articles lacking reliable references", "All articles needing expert attention", "All articles that are too technical", "All orphaned articles", "Analysis of algorithms", "Articles lacking reliable references from May 2014", "Articles needing expert attention from May 2014", "Cache (computing)", "Comparison sorts", "External memory algorithms", "Models of computation", "Orphaned articles from May 2014", "Sorting algorithms", "Wikipedia articles that are too technical from May 2014"], "title": "Cache-oblivious distribution sort"}
{"summary": "In computer science, a Cartesian tree is a binary tree derived from a sequence of numbers; it can be uniquely defined from the properties that it is heap-ordered and that a symmetric (in-order) traversal of the tree returns the original sequence. Introduced by Vuillemin (1980) in the context of geometric range searching data structures, Cartesian trees have also been used in the definition of the treap and randomized binary search tree data structures for binary search problems. The Cartesian tree for a sequence may be constructed in linear time using a stack-based algorithm for finding all nearest smaller values in a sequence.", "links": ["(a,b)-tree", "2\u20133 tree", "2\u20133\u20134 tree", "AA tree", "AVL tree", "Adaptive sort", "All nearest smaller values", "American flag sort", "Associative array", "B*-tree", "B+ tree", "B-tree", "BK-tree", "BSP tree", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Binary heap", "Binary search", "Binary search tree", "Binary tree", "Binomial heap", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Bx-tree", "C-trie", "Cartesian coordinate", "Cartesian plane", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer science", "Counting sort", "Cover tree", "Ctrie", "Cycle sort", "Dancing tree", "Data structure", "Digital object identifier", "Erik Demaine", "Euler tour", "Exponential tree", "Fenwick tree", "Fibonacci heap", "Finger tree", "Flashsort", "Fusion tree", "Gnome sort", "HTree", "Hash calendar", "Hash tree (persistent data structure)", "Heap (data structure)", "Heap sort", "Heapsort", "Hilbert R-tree", "Hybrid algorithm", "IDistance", "Implicit k-d tree", "In-place algorithm", "Insertion sort", "Integer sorting", "International Standard Book Number", "Interval tree", "Introsort", "JSTOR", "JSort", "Jon Bentley", "K-ary tree", "K-d tree", "Lecture Notes in Computer Science", "Left-child right-sibling binary tree", "Left-leaning red\u2013black tree", "Leftist tree", "Library sort", "Linear time", "Link/cut tree", "List (computing)", "Log-structured merge-tree", "Logarithm", "Lowest common ancestor", "M-tree", "MVP tree", "Martin Farach-Colton", "Mathematical Reviews", "Merge sort", "Merkle tree", "Metric tree", "Minimum spanning tree", "Octree", "Odd\u2013even sort", "Optimal binary search tree", "Order statistic tree", "Oscillating merge sort", "PQ tree", "Pairing heap", "Pairwise sorting network", "Pancake sorting", "Parallel algorithm", "Path graph", "Patience sorting", "Permutation", "Pigeonhole sort", "Polyphase merge sort", "Potential method", "Priority R-tree", "Priority queue", "Proxmap sort", "Quadtree", "Quicksort", "R* tree", "R+ tree", "R-tree", "Radix sort", "Radix tree", "Random binary search tree", "Randomized binary search tree", "Range Minimum Query", "Range searching", "Range tree", "Red\u2013black tree", "Robert Tarjan", "SIAM Journal on Computing", "SPQR tree", "Scapegoat tree", "Search tree", "Segment tree", "Selection algorithm", "Selection sort", "Sequential search", "Set (abstract data type)", "Shellsort", "Skew heap", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Spatial index", "Splay tree", "Splaysort", "Spreadsort", "Stack (data structure)", "Stooge sort", "Strand sort", "Suffix tree", "Symposium on Theory of Computing", "T-tree", "Ternary search tree", "Timsort", "Top tree", "Topological sorting", "Total order", "Tournament sort", "Treap", "Tree (data structure)", "Tree rotation", "Tree sort", "Tree traversal", "Trie", "UB-tree", "Ultrametric space", "Uzi Vishkin", "Van Emde Boas tree", "Vantage-point tree", "Weight-balanced tree", "Widest path problem", "X-fast trie", "X-tree", "Y-fast trie"], "categories": ["Binary trees", "CS1 French-language sources (fr)", "Sorting algorithms"], "title": "Cartesian tree"}
{"summary": "Cocktail sort, also known as bidirectional bubble sort, cocktail shaker sort, shaker sort (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is a variation of bubble sort that is both a stable sorting algorithm and a comparison sort. The algorithm differs from a bubble sort in that it sorts in both directions on each pass through the list. This sorting algorithm is only marginally more difficult to implement than a bubble sort, and solves the problem of turtles in bubble sorts. It provides only marginal performance improvements, and does not improve asymptotic performance; like the bubble sort, it is not of practical interest (insertion sort is preferred for simple sorts), though it finds some use in education.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stable sort", "Stooge sort", "Strand sort", "The Art of Computer Programming", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Articles with example pseudocode", "Comparison sorts", "Sorting algorithms", "Stable sorts"], "title": "Cocktail sort"}
{"summary": "Comb sort is a relatively simple sorting algorithm originally designed by W\u0142odzimierz Dobosiewicz in 1980. Later it was rediscovered by Stephen Lacey and Richard Box in 1991. Comb sort improves on bubble sort.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bubblesort", "Bucket sort", "Burstsort", "Byte Magazine", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Digital object identifier", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Information Processing Letters", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shell sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Swap (computer science)", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["All articles needing additional references", "Articles needing additional references from March 2011", "Articles with example pseudocode", "Comparison sorts", "Sorting algorithms"], "title": "Comb sort"}
{"summary": "A comparison sort is a type of sorting algorithm that only reads the list elements through a single abstract comparison operation (often a \"less than or equal to\" operator or a three-way comparison) that determines which of two elements should occur first in the final sorted list. The only requirement is that the operator obey two of the properties of a total order:\nif a \u2264 b and b \u2264 c then a \u2264 c (transitivity)\nfor all a and b, either a \u2264 b or b \u2264 a (totalness or trichotomy).\nIt is possible that both a \u2264 b and b \u2264 a; in this case either may come first in the sorted list. In a stable sort, the input order determines the sorted order in this case.\nA metaphor for thinking about comparison sorts is that someone has a set of unlabelled weights and a balance scale. Their goal is to line up the weights in order by their weight without any information except that obtained by placing two weights on the scale and seeing which one is heavier (or if they weigh the same).", "links": ["Adaptive sort", "American flag sort", "Asymptotic computational complexity", "Asymptotically optimal", "Balance scale", "Balanced ternary", "Batcher odd\u2013even mergesort", "Bead sort", "Big-O notation", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "Clifford Stein", "Cocktail sort", "Comb sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Digital object identifier", "Donald Knuth", "Factorial", "Flashsort", "Floating-point number", "Gnome sort", "Heap sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Information theory", "Insertion sort", "Integer sorting", "International Standard Book Number", "Intro sort", "Introduction to Algorithms", "Introsort", "JSort", "Lexicographic order", "Library sort", "Linearithmic", "List (computing)", "Merge insertion (Ford-Johnson) sort", "Merge sort", "Odd-even sort", "Odd\u2013even sort", "On-Line Encyclopedia of Integer Sequences", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quick sort", "Quicksort", "Radix sort", "Random Access Memory", "Ron Rivest", "Selection algorithm", "Selection sort", "Shannon entropy", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stirling's approximation", "Stooge sort", "Strand sort", "The Art of Computer Programming", "Thomas H. Cormen", "Three-way comparison", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "Trichotomy (mathematics)", "Tuple", "X + Y sorting"], "categories": ["Sorting algorithms"], "title": "Comparison sort"}
{"summary": "In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small integers; that is, it is an integer sorting algorithm. It operates by counting the number of objects that have each distinct key value, and using arithmetic on those counts to determine the positions of each key value in the output sequence. Its running time is linear in the number of items and the difference between the maximum and minimum key values, so it is only suitable for direct use in situations where the variation in keys is not significantly greater than the number of items. However, it is often used as a subroutine in another sorting algorithm, radix sort, that can handle larger keys more efficiently.\nBecause counting sort uses key values as indexes into an array, it is not a comparison sort, and the \u03a9(n log n) lower bound for comparison sorting does not apply to it. Bucket sort may be used for many of the same tasks as counting sort, with a similar time analysis; however, compared to counting sort, bucket sort requires linked lists, dynamic arrays or a large amount of preallocated memory to hold the sets of items within each bucket, whereas counting sort instead stores a single number (the count of items) per bucket.", "links": ["Adaptive sort", "Algorithm", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bit vector", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "Clifford Stein", "Cocktail sort", "Collection (computing)", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer science", "Cycle sort", "Digital object identifier", "Donald Knuth", "Dynamic array", "Flashsort", "Gnome sort", "Guy Blelloch", "Harold H. Seward", "Heapsort", "Histogram", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer", "Integer sorting", "International Standard Book Number", "Introduction to Algorithms", "Introsort", "JSort", "John Reif", "Library sort", "Linked list", "List (computing)", "Lower bound", "MIT Press", "Massachusetts Institute of Technology", "McGraw-Hill", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Parallel algorithm", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Prefix sum", "Proxmap sort", "Quicksort", "Radix sort", "Robert Sedgewick (computer scientist)", "Ron Rivest", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stable sort", "Stooge sort", "Strand sort", "Symposium on Foundations of Computer Science", "The Art of Computer Programming", "Thomas H. Cormen", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Sorting algorithms", "Stable sorts"], "title": "Counting sort"}
{"summary": "Cubesort is a parallel sorting algorithm that builds a self-balancing multi-dimensional array from the keys to be sorted. As the axes are of similar length the structure resembles a cube. After each key is inserted the cube can be rapidly converted to an array.\nA cubesort implementation written in C was published in 2014.", "links": ["Adaptive sort", "Algorithm", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Binary search", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Data structure", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Algorithms and data structures stubs", "All articles lacking reliable references", "All articles with topics of unclear notability", "All stub articles", "Articles lacking reliable references from September 2014", "Articles with topics of unclear notability from September 2014", "Comparison sorts", "Computer science stubs", "Online sorts", "Sorting algorithms", "Stable sorts"], "title": "Cubesort"}
{"summary": "Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm. It is based on the idea that the permutation to be sorted can be factored into cycles, which can individually be rotated to give a sorted result.\nUnlike nearly every other sort, items are never written elsewhere in the array simply to push them out of the way of the action. Each value is either written zero times, if it's already in its correct position, or written one time to its correct position. This matches the minimal number of overwrites required for a completed in-place sort.\nMinimizing the number of writes is useful when making writes to some huge data set is very expensive, such as with EEPROMs like Flash memory where each write reduces the lifespan of the memory.", "links": ["Adaptive sort", "Algorithm", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cyclic permutation", "EEPROM", "Flash memory", "Flashsort", "Gnome sort", "Heapsort", "Histogram", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "Linear search", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Perfect hash function", "Permutation", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Time complexity", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Articles with example pseudocode", "Comparison sorts", "Online sorts", "Sorting algorithms"], "title": "Cycle sort"}
{"summary": "The Dutch national flag problem (DNF) is a computer science programming problem proposed by Edsger Dijkstra. The flag of the Netherlands consists of three colours: red, white and blue. Given balls of these three colours arranged randomly in a line (the actual number of balls does not matter), the task is to arrange them such that all balls of the same colour are together and their collective colour groups are in the correct order.\nThe solution to this problem is of interest for designing sorting algorithms. In particular, variants of the quicksort algorithm that must be robust to repeated elements need a three-way partitioning function that groups items less than a given key (red), equal to the key (white) and greater than the key (blue). Several solutions exist that have varying performance characteristics, tailored to sorting arrays with either small or large numbers of repeated elements.\n\n", "links": ["Algorithm", "American flag sort", "Array data structure", "Computer science", "Dictionary of Algorithms and Data Structures", "Digital object identifier", "Edsger Dijkstra", "Flag of the Netherlands", "Loop invariant", "Multi-key quicksort", "National Institute of Standards and Technology", "Pseudocode", "Quicksort", "Sorting algorithm", "String (computer science)"], "categories": ["All articles needing additional references", "Articles needing additional references from July 2014", "Computational problems", "Sorting algorithms"], "title": "Dutch national flag problem"}
{"summary": "The elevator algorithm (also SCAN) is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.\nThis algorithm is named after the behavior of a building elevator, where the elevator continues to travel in its current direction (up or down) until empty, stopping only to let individuals off or to pick up new individuals heading in the same direction.\nFrom an implementation perspective, the drive maintains a buffer of pending read/write requests, along with the associated cylinder number of the request. Lower cylinder numbers indicate that the cylinder is closest to the spindle, and higher numbers indicate the cylinder is farther away.", "links": ["Cylinder (disk drive)", "Data buffer", "Disk drive", "Elevator", "FIFO (computing and electronics)", "FSCAN", "Hard disk", "I/O scheduling", "LOOK algorithm", "N-Step-SCAN", "Resource starvation", "Shortest seek first"], "categories": ["All articles needing additional references", "Articles needing additional references from November 2007", "Disk scheduling algorithms", "Sorting algorithms"], "title": "Elevator algorithm"}
{"summary": "External sorting is a term for a class of sorting algorithms that can handle massive amounts of data. External sorting is required when the data being sorted do not fit into the main memory of a computing device (usually RAM) and instead they must reside in the slower external memory (usually a hard drive). External sorting typically uses a hybrid sort-merge strategy. In the sorting phase, chunks of data small enough to fit in main memory are read, sorted, and written out to a temporary file. In the merge phase, the sorted subfiles are combined into a single larger file.", "links": ["Algorithm", "Asynchronous I/O", "Big O notation", "Bucket sort", "Data", "Donald Knuth", "Duality (mathematics)", "Ellis Horowitz", "External memory", "Hard drive", "Hybrid algorithm", "In-place algorithm", "J. S. Vitter", "Jim Gray (computer scientist)", "Main memory", "Megabyte", "Merge algorithm", "Merge sort", "Quicksort", "RAM", "Radix sort", "Sartaj Sahni", "Solid-state drives", "Sorting", "The Art of Computer Programming", "Thread (computer science)"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from October 2015", "External memory algorithms", "Sorting algorithms"], "title": "External sorting"}
{"summary": "Flashsort is a distribution sorting algorithm showing linear computational complexity  for uniformly distributed data sets and relatively little additional memory requirement. The original work was published in 1998 by Karl-Dietrich Neubert.", "links": ["Adaptive sort", "American flag sort", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "O notation", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Probability distribution", "Proxmap sort", "Quantile", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stable sort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Sorting algorithms"], "title": "Flashsort"}
{"summary": "Funnelsort is a comparison-based sorting algorithm. It was introduced by Frigo, Leiserson, Prokop, and Ramachandran in 1999 in the context of the cache oblivious model.\nIn the external memory model, the number of memory transfers it needs to perform a sort of  items on a machine with cache of size  and cache lines of length  is , under the tall cache assumption that . This number of memory transfers has been shown to be asymptotically optimal for comparison sorts. Funnelsort also achieves the asymptotically optimal runtime complexity of .\n^ M. Frigo, C.E. Leiserson, H. Prokop, and S. Ramachandran. Cache-oblivious algorithms. In Proceedings of the 40th IEEE Symposium on Foundations of Computer Science (FOCS 99), pp. 285-297. 1999. Extended abstract at IEEE, at Citeseer.\n^ Harald Prokop. Cache-Oblivious Algorithms. Masters thesis, MIT. 1999.", "links": ["Cache-oblivious algorithm", "Circular buffer", "Comparison sort", "Digital object identifier", "FIFO (computing and electronics)", "Lecture Notes in Computer Science", "Merge sort", "Queue (data structure)", "Sorting algorithm", "Springer Science+Business Media"], "categories": ["All articles lacking in-text citations", "All articles lacking reliable references", "All articles needing expert attention", "All articles that are too technical", "Analysis of algorithms", "Articles lacking in-text citations from May 2014", "Articles lacking reliable references from May 2014", "Articles needing expert attention from May 2014", "CS1 errors: chapter ignored", "Cache (computing)", "Comparison sorts", "External memory algorithms", "Models of computation", "Sorting algorithms", "Wikipedia articles that are too technical from May 2014"], "title": "Funnelsort"}
{"summary": "Gnome sort (or Stupid sort) is a sorting algorithm originally proposed by Dr. Hamid Sarbazi-Azad (Professor of Computer Engineering at Sharif University of Technology) in 2000 and called \"stupid sort\" (not to be confused with bogosort), and then later on described by Dick Grune and named \"gnome sort\" from the observation that it is \"how a gnome sorts a line of flower pots.\" It is a sorting algorithm which is similar to insertion sort, except that moving an element to its proper place is accomplished by a series of swaps, as in bubble sort. It is conceptually simple, requiring no nested loops. The average, or expected, running time is O(n2), but tends towards O(n) if the list is initially almost sorted.\nThe algorithm always finds the first place where two adjacent elements are in the wrong order, and swaps them. It takes advantage of the fact that performing a swap can introduce a new out-of-order adjacent pair only next to the two swapped elements. It does not assume that elements forward of the current position are sorted, so it only needs to check the position directly previous to the swapped elements.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Array data type", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Dick Grune", "Flashsort", "Hamid Sarbazi-Azad", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Pseudocode", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Sharif University of Technology", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Teleportation", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["All articles needing additional references", "Articles needing additional references from August 2010", "Articles needing additional references from November 2015", "Comparison sorts", "Sorting algorithms", "Stable sorts"], "title": "Gnome sort"}
{"summary": "In computer science, heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.\nAlthough somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(n log n) runtime. Heapsort is an in-place algorithm, but it is not a stable sort.\nHeapsort was invented by J. W. J. Williams in 1964. This was also the birth of the heap, presented already by Williams as a useful data structure in its own right. In the same year, R. W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Binary heap", "Binary tree", "Bitonic sorter", "Block sort", "Bogosort", "Branch prediction", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "Clifford Stein", "Cocktail sort", "Comb sort", "Communications of the ACM", "Comparison of programming languages (array)", "Comparison sort", "Computational complexity theory", "Computer science", "Counting sort", "Cycle sort", "Data cache", "Digital object identifier", "Donald Knuth", "Edsger W. Dijkstra", "External sorting", "Flashsort", "Fundamenta Informaticae", "Gnome sort", "Heap (data structure)", "Hybrid algorithm", "In-place algorithm", "Ingo Wegener", "Insertion sort", "Integer sorting", "International Standard Book Number", "Introduction to Algorithms", "Introsort", "J. W. J. Williams", "JSort", "Kurt Mehlhorn", "Library sort", "Linear speedup", "Linked list", "List (computing)", "Locality of reference", "Loop invariant", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Parallel algorithm", "Patience sorting", "Peter Sanders (computer scientist)", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Pseudocode", "Quicksort", "Radix sort", "Robert Floyd", "Robert W. Floyd", "Ronald L. Rivest", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stable sort", "Steven Skiena", "Stooge sort", "Strand sort", "Svante Carlsson", "Ternary heapsort", "The Art of Computer Programming", "Thomas H. Cormen", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "Treesort"], "categories": ["All articles with unsourced statements", "Articles with example pseudocode", "Articles with unsourced statements from June 2012", "Articles with unsourced statements from September 2014", "Comparison sorts", "Heaps (data structures)", "Sorting algorithms", "Use dmy dates from July 2012"], "title": "Heapsort"}
{"summary": "Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages:\nSimple implementation: Bentley shows a three-line C version, and a five-line optimized version\nEfficient for (quite) small data sets, much like other quadratic sorting algorithms\nMore efficient in practice than most other simple quadratic (i.e., O(n2)) algorithms such as selection sort or bubble sort\nAdaptive, i.e., efficient for data sets that are already substantially sorted: the time complexity is O(nk) when each element in the input is no more than k places away from its sorted position\nStable; i.e., does not change the relative order of elements with equal keys\nIn-place; i.e., only requires a constant amount O(1) of additional memory space\nOnline; i.e., can sort a list as it receives it\nWhen people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Binary search algorithm", "Binary tree", "Binary tree sort", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "C (programming language)", "Canada", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "Clifford Stein", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Data structure", "Digital object identifier", "Divide-and-conquer algorithm", "Donald Knuth", "Donald Shell", "EEPROM", "Farach-Colton", "Flash memory", "Flashsort", "Gnome sort", "Heap (data structure)", "Heap sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Integer sorting", "International Standard Book Number", "Introduction to Algorithms", "Introsort", "Iteration", "JSort", "Jon Bentley", "Library sort", "Linked list", "List (computing)", "Martin Farach-Colton", "Mathematical Reviews", "Merge sort", "Mergesort", "Odd\u2013even sort", "Online algorithm", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Pseudocode", "Quicksort", "Radix sort", "Robert Sedgewick (computer scientist)", "Ron Rivest", "Selection algorithm", "Selection sort", "Shellsort", "Skip list", "Smoothsort", "Sorted array", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stable sort", "Stooge sort", "Strand sort", "The Art of Computer Programming", "Thomas H. Cormen", "Time complexity", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "United Kingdom", "Zero-based numbering"], "categories": ["All articles with unsourced statements", "Articles with example pseudocode", "Articles with unsourced statements from September 2011", "Articles with unsourced statements from September 2014", "Commons category with local link same as on Wikidata", "Comparison sorts", "Online sorts", "Sorting algorithms", "Stable sorts"], "title": "Insertion sort"}
{"summary": "In computer science, integer sorting is the algorithmic problem of sorting a collection of data values by numeric keys, each of which is an integer. Algorithms designed for integer sorting may also often be applied to sorting problems in which the keys are floating point numbers or text strings. The ability to perform integer arithmetic on the keys allows integer sorting algorithms to be faster than comparison sorting algorithms in many cases, depending on the details of which operations are allowed in the model of computing and how large the integers to be sorted are.\nThe classical integer sorting algorithms of bucket sort, counting sort, and radix sort are widely used and practical. Much of the subsequent research on integer sorting algorithms has focused less on practicality and more on theoretical improvements in their worst case analysis, and the algorithms that come from this line of research are not believed to be practical for current 64-bit computer architectures, although experiments have shown that some of these methods may be an improvement on radix sorting for data with 128 or more bits per key. Additionally, for large data sets, the near-random memory access patterns of many integer sorting algorithms can handicap them compared to comparison sorting algorithms that have been designed with the memory hierarchy in mind.\nInteger sorting provides one of the six benchmarks in the DARPA High Productivity Computing Systems Discrete Mathematics benchmark suite, and one of eleven benchmarks in the NAS Parallel Benchmarks suite.", "links": ["64-bit", "Algorithm", "B-tree", "Benchmark (computing)", "Bitonic sorter", "Bitvector", "Bucket sort", "Charles E. Leiserson", "Clifford Stein", "Collection (computing)", "Comparison sort", "Computer science", "Counting sort", "DARPA", "Dan Willard", "Data structure", "David G. Kirkpatrick", "Digital object identifier", "Fusion tree", "Hash table", "Heap sort", "High Productivity Computing Systems", "Integer", "International Standard Book Number", "Introduction to Algorithms", "Iterated logarithm", "John Reif", "J\u00e1nos Koml\u00f3s (mathematician)", "Ken Batcher", "Linked list", "MIT Press", "Mathematical Reviews", "Max Planck Institute for Computer Science", "McGraw-Hill", "Memory hierarchy", "Merge algorithm", "Merge sort", "Michael Fredman", "Michael T. Goodrich", "Mikkel Thorup", "Mikl\u00f3s Ajtai", "NAS Parallel Benchmarks", "Parallel random access machine", "Pentium", "Pointer machine", "Positional notation", "Power of two", "Prefix sum", "Priority queue", "Radix", "Radix sort", "Random access machine", "Roberto Tamassia", "Ron Rivest", "Selection sort", "Sorting algorithm", "Susanne Albers", "Symposium on Foundations of Computer Science", "Thomas H. Cormen", "Transdichotomous model", "Trie", "Van Emde Boas tree", "Worst case", "Worst case analysis", "Y-fast trie"], "categories": ["Sorting algorithms"], "title": "Integer sorting"}
{"summary": "An internal sort is any data sorting process that takes place entirely within the main memory of a computer. This is possible whenever the data to be sorted is small enough to all be held in the main memory. For sorting larger datasets, it may be necessary to hold only a chunk of data in memory at a time, since it won\u2019t all fit. The rest of the data is normally held on some larger, but slower medium, like a hard-disk. Any reading or writing of data to and from this slower media can slow the sortation process considerably. This issue has implications for different sort algorithms.\nSome common internal sorting algorithms include:\nBubble Sort\nInsertion Sort\nQuick Sort\nHeap Sort\nRadix Sort\nSelection sort\nConsider a Bubblesort, where adjacent records are swapped in order to get them into the right order, so that records appear to \u201cbubble\u201d up and down through the dataspace. If this has to be done in chunks, then when we have sorted all the records in chunk 1, we move on to chunk 2, but we find that some of the records in chunk 1 need to \u201cbubble through\u201d chunk 2, and vice versa (i.e., there are records in chunk 2 that belong in chunk 1, and records in chunk 1 that belong in chunk 2 or later chunks). This will cause the chunks to be read and written back to disk many times as records cross over the boundaries between them, resulting in a considerable degradation of performance. If the data can all be held in memory as one large chunk, then this performance hit is avoided.\nOn the other hand, some algorithms handle external sorting rather better. A Merge sort breaks the data up into chunks, sorts the chunks by some other algorithm (maybe bubblesort or Quick sort) and then recombines the chunks two by two so that each recombined chunk is in order. This approach minimises the number or reads and writes of data-chunks from disk, and is a popular external sort method.", "links": ["Bubble Sort", "Bubblesort", "External sorting", "Heap Sort", "Insertion Sort", "Main memory", "Merge sort", "Quick Sort", "Quick sort", "Radix Sort", "Selection sort", "Sort algorithms"], "categories": ["All articles to be merged", "Articles to be merged from March 2012", "Sorting algorithms"], "title": "Internal sort"}
{"summary": "J sort is an in-place sort algorithm that uses strand sort to sort fewer than about 40 items and shuffle sort to sort more. John Cohen claimed to have invented this algorithm.\n^ A revolutionary new sort from John Cohen", "links": ["Adaptive sort", "American flag sort", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer science", "Counting sort", "Cycle sort", "Dictionary of Algorithms and Data Structures", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "In-place sort algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "National Institute of Standards and Technology", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["All stub articles", "Articles for deletion", "Computer science stubs", "Sorting algorithms"], "title": "J sort"}
{"summary": "JSort is an in-place sort algorithm that uses build heap twice to largely order the array then finishes with an insertion sort. JSort is attributed to Jason Morrison.\nThe first build heap pass converts the array to a heap, with the least item in the root, which is in the first position of the array. The second build heap pass works in reverse, with the greatest item in root, which is in the last position for this pass. The largely ordered array is finally sorted with insertion sort. Since insertion sort would do all the sorting by itself, the two passes with build heap only save it work, which could be significant.\nBuild heap partially orders the array very fast, since items may be moved a long way, up to half the length of the array. Items nearer the root are more likely to be in order, since few items were compared with each other. The farther from the root, the more likely items are significantly out of order, since they are not compared with each other, only with their parents. Thus items at the leaves are likely quite unordered, which would cause insertion sort to take a long time.\nThe second pass reverses the heap and puts the root at the last position in the array. It also reverses the heap sense, so the largest item is at the root. Thus the second pass follows the same general order as the first pass, lesser items near the first position and greater items near the last position, but works more at the last positions. The second pass, then, does most of its work exactly where the first pass did little. Together the two passes mostly order the array. The final insertion sort can run relatively quickly. Complexity is still O(n2).\n^ Code for a JSort visualization, in Java.", "links": ["Adaptive sort", "American flag sort", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Dictionary of Algorithms and Data Structures", "Flashsort", "Gnome sort", "Heap (data structure)", "Heapsort", "Hybrid algorithm", "In-place algorithm", "In-place sort algorithm", "Insertion sort", "Integer sorting", "Introsort", "J sort", "Leaf node", "Library sort", "List (computing)", "Merge sort", "National Institute of Standards and Technology", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Root node", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Articles for deletion", "Sorting algorithms"], "title": "JSort"}
{"summary": "Library sort, or gapped insertion sort is a sorting algorithm that uses an insertion sort, but with gaps in the array to accelerate subsequent insertions. The name comes from an analogy:\n\nSuppose a librarian were to store his books alphabetically on a long shelf, starting with the A's at the left end, and continuing to the right along the shelf with no spaces between the books until the end of the Z's. If the librarian acquired a new book that belongs to the B section, once he finds the correct space in the B section, he will have to move every book over, from the middle of the B's all the way down to the Z's in order to make room for the new book. This is an insertion sort. However, if he were to leave a space after every letter, as long as there was still space after B, he would only have to move a few books to make room for the new one. This is the basic principle of the Library Sort.\n\nThe algorithm was proposed by Michael A. Bender, Mart\u00edn Farach-Colton, and Miguel Mosteiro in 2004 and was published in 2006.\nLike the insertion sort it is based on, library sort is a stable comparison sort and can be run as an online algorithm; however, it was shown to have a high probability of running in O(n log n) time (comparable to quicksort), rather than an insertion sort's O(n2). The mechanism used for this improvement is very similar to that of a skip list. There is no full implementation given in the paper, nor the exact algorithms of important parts, such as insertion and rebalancing. Further information would be needed to discuss how the efficiency of library sort compares to that of other sorting methods in reality.\nCompared to basic insertion sort, the drawback of library sort is that it requires extra space for the gaps. The amount and distribution of that space would be implementation dependent. In the paper the size of the needed array is (1 + \u03b5)n, but with no further recommendations on how to choose \u03b5.\nOne weakness of insertion sort is that it may require a high number of swap operations and be costly if memory write is expensive. Library sort may improve that somewhat in the insertion step, as fewer elements need to move to make room, but is also adding an extra cost in the rebalancing step. In addition, locality of reference will be poor compared to mergesort as each insertion from a random data set may access memory that is no longer in cache, especially with large data sets.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Binary search", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Digital object identifier", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "List (computing)", "Martin Farach-Colton", "Mart\u00edn Farach-Colton", "Merge sort", "Mergesort", "Michael A. Bender", "Miguel Mosteiro", "Odd\u2013even sort", "Online algorithm", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Skip list", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stable sort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Comparison sorts", "Online sorts", "Sorting algorithms", "Stable sorts"], "title": "Library sort"}
{"summary": "Median cut is an algorithm to sort data of an arbitrary number of dimensions into series of sets by recursively cutting each set of data at the median point along the longest dimension. Median cut is typically used for color quantization. For example, to reduce a 64k-colour image to 256 colours, median cut is used to find 256 colours that match the original data well.", "links": ["Bucket sort", "Color quantization", "K-d tree", "Median", "Palette (computing)", "Pixel", "Power of two", "RGB color model", "Recursion", "Sorting algorithm"], "categories": ["Sorting algorithms"], "title": "Median cut"}
{"summary": "Merge algorithms are a family of algorithms that run sequentially over multiple sorted lists, typically producing more sorted lists as output. This is well-suited for machines with tape drives.\nThe general merge algorithm has a set of pointers p0..n that point to positions in a set of lists L0..n. Initially they point to the first item in each list. The algorithm is as follows:\nWhile any of p0..n still point to data inside of L0..n instead of past the end:\ndo something with the data items p0..n point to in their respective lists\nfind out which of those pointers points to the item with the lowest key; advance one of those pointers to the next item in its list", "links": ["Algorithm", "All nearest smaller values", "Array data structure", "Big O notation", "C++", "Charles E. Leiserson", "Clifford Stein", "Collections", "Computer language", "Digital object identifier", "Donald Knuth", "Heap (data structure)", "International Standard Book Number", "Introduction to Algorithms", "Iterator", "Join (SQL)", "Join (Unix)", "Join (relational algebra)", "Merge (revision control)", "Merge sort", "Multi-core processor", "Parallel computing", "Pointer (computer programming)", "Priority queue", "Python (programming language)", "Ronald L. Rivest", "Sorting algorithm", "Standard Template Library", "Tape drive", "The Art of Computer Programming", "Thomas H. Cormen", "Uzi Vishkin"], "categories": ["All articles needing additional references", "All articles needing expert attention", "Articles needing additional references from August 2008", "Articles needing expert attention from August 2009", "Articles needing expert attention with no reason or talk parameter", "Articles with example pseudocode", "Computer science articles needing expert attention", "Sorting algorithms"], "title": "Merge algorithm"}
{"summary": "In computer science, merge sort (also commonly spelled mergesort) is an O(n log n) comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the implementation preserves the input order of equal elements in the sorted output. Mergesort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and Neumann as early as 1948.", "links": ["Adaptive sort", "American flag sort", "Android (operating system)", "Array data structure", "Art of Computer Programming", "Average performance", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Binary logarithm", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cache (computing)", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "CiteSeer", "Clifford Stein", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer science", "Counting sort", "Cycle sort", "Digital object identifier", "Disk storage", "Divide and conquer algorithm", "Donald Knuth", "External sorting", "Flashsort", "Fork\u2013join model", "GNU Octave", "Gnome sort", "Heapsort", "Herman Goldstine", "Hybrid algorithm", "IBM 729", "In-place algorithm", "Insertion sort", "Integer sorting", "International Standard Book Number", "International Standard Serial Number", "Introduction to Algorithms", "Introsort", "JSort", "Java 7", "Java platform", "John von Neumann", "Library sort", "Linked list", "Lisp programming language", "List (computing)", "Locality of reference", "Magnetic tape", "Master theorem", "Memory hierarchy", "Merge algorithm", "Novosibirsk", "Odd\u2013even sort", "Oscillating merge sort", "Page (computer memory)", "Pairwise sorting network", "Pancake sorting", "Parallel Random Access Machine", "Patience sorting", "Perl", "Pigeonhole sort", "Polyphase merge sort", "Primary storage", "Proxmap sort", "Pseudocode", "Python (programming language)", "Quasilinear time", "Queue (abstract data type)", "Quicksort", "Radix sort", "Ron Rivest", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Software optimization", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stack (abstract data type)", "Stooge sort", "Strand sort", "Tape drive", "The Art of Computer Programming", "Thomas H. Cormen", "Tim Peters (software engineer)", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "Worst-case performance"], "categories": ["All articles with dead external links", "All articles with unsourced statements", "Articles with dead external links from June 2013", "Articles with example pseudocode", "Articles with inconsistent citation formats", "Articles with unsourced statements from April 2014", "Articles with unsourced statements from June 2008", "Articles with unsourced statements from March 2014", "Comparison sorts", "Sorting algorithms", "Stable sorts"], "title": "Merge sort"}
{"summary": "Multi-key quicksort, also known as three-way radix quicksort, is an algorithm for sorting strings. This hybrid of quicksort and radix sort was originally suggested by P. Shackleton, as reported in one of C.A.R. Hoare's seminal papers on quicksort; its modern incarnation was developed by Jon Bentley and Robert Sedgewick in the mid-1990s. The algorithm is designed to exploit the property that in many problems, strings tend to have shared prefixes.\nOne of the algorithm's uses is the construction of suffix arrays, for which it was one of the fastest algorithms as of 2004.", "links": ["Algorithm", "American flag sort", "Array slicing", "CiteSeer", "Dictionary of Algorithms and Data Structures", "Digital object identifier", "Dr. Dobb's Journal", "Dutch national flag problem", "Insertion sort", "International Standard Book Number", "Isomorphic", "Jon Bentley", "Lexicographical order", "Median", "National Institute of Standards and Technology", "Pointer (computer programming)", "Quicksort", "Radix sort", "Robert Sedgewick (computer scientist)", "Sorting algorithm", "String (computer science)", "Substring", "Suffix array", "Ternary search tree", "The Computer Journal", "Tony Hoare"], "categories": ["Algorithms on strings", "Articles with example pseudocode", "Comparison sorts", "Sorting algorithms"], "title": "Multi-key quicksort"}
{"summary": "In computing, an odd\u2013even sort or odd\u2013even transposition sort (also known as brick sort) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections. It is a comparison sort related to bubble sort, with which it shares many characteristics. It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, if a pair is in the wrong order (the first is larger than the second) the elements are switched. The next step repeats this for even/odd indexed pairs (of adjacent elements). Then it alternates between odd/even and even/odd steps until the list is sorted.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bubblesort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer science", "Counting sort", "Cycle sort", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "International Standard Book Number", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "Zero-based"], "categories": ["Accuracy disputes from July 2014", "All stub articles", "Articles containing proofs", "Articles with example pseudocode", "Comparison sorts", "Computer science stubs", "Sorting algorithms", "Stable sorts"], "title": "Odd\u2013even sort"}
{"summary": "Pancake sorting is the colloquial term for the mathematical problem of sorting a disordered stack of pancakes in order of size when a spatula can be inserted at any point in the stack and used to flip all pancakes above it. A pancake number is the maximum number of flips required for a given number of pancakes. In this form, the problem was first discussed by American geometer Jacob E. Goodman. It is a variation of the sorting problem in which the only allowed operation is to reverse the elements of some prefix of the sequence. Unlike a traditional sorting algorithm, which attempts to sort with the fewest comparisons possible, the goal is to sort the sequence in as few reversals as possible. A variant of the problem is concerned with burnt pancakes, where each pancake has a burnt side and all pancakes must, in addition, end up with the burnt side on bottom.", "links": ["Adaptive sort", "American flag sort", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bill Gates", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Carnegie Mellon University", "Cartesian tree", "Cascade merge sort", "Christos Papadimitriou", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "DNA computing", "David X. Cohen", "Digital object identifier", "Discrete Mathematics (journal)", "Eric W. Weisstein", "Escherichia coli", "Flashsort", "Futurama", "Geometer", "Gnome sort", "Hal Sudborough", "Harvard University", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "International Standard Book Number", "Introsort", "JSort", "Jacob E. Goodman", "Library sort", "List (computing)", "Manuel Blum", "Marek Karpinski", "MathWorld", "Mathematical Reviews", "Merge sort", "Microsoft", "NP-complete", "NP-hard", "Neil Sloane", "Odd\u2013even sort", "On-Line Encyclopedia of Integer Sequences", "Oscillating merge sort", "Pairwise sorting network", "Patience sorting", "Permutation", "Pigeonhole sort", "Polyphase merge sort", "Prefix (computer science)", "Proxmap sort", "PubMed Central", "PubMed Identifier", "Quicksort", "Radix sort", "Robert Tarjan", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Spatula", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "The Guardian", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "United States of America", "University of California, Berkeley", "University of Texas at Dallas"], "categories": ["Articles with inconsistent citation formats", "Sorting algorithms", "Use mdy dates from October 2014"], "title": "Pancake sorting"}
{"summary": "Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) and the number of possible key values (N) are approximately the same. It requires O(n + N) time. It is similar to counting sort), but \"Pigeonhole sort moves items twice: once to the bucket array and again to the final destination [whereas] counting sort builds an auxiliary array then uses the array to compute each item's final destination and move the item there.\"\nThe pigeonhole algorithm works as follows:\nGiven an array of values to be sorted, set up an auxiliary array of initially empty \"pigeonholes,\" one pigeonhole for each key through the range of the original array.\nGoing over the original array, put each value into the pigeonhole corresponding to its key, such that each pigeonhole eventually contains a list of all values with that key.\nIterate over the pigeonhole array in order, and put elements from non-empty pigeonholes back into the original array.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole principle", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Range (computer science)", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Sorting algorithms", "Stable sorts"], "title": "Pigeonhole sort"}
{"summary": "A polyphase merge sort is an algorithm which decreases the number of runs at every iteration of the main loop by merging runs into larger runs. It is used for external sorting.", "links": ["Adaptive sort", "American flag sort", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Donald Knuth", "External sorting", "Fibonacci number", "Fibonacci numbers of higher order", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "International Standard Book Number", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Tape drive", "The Art of Computer Programming", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Comparison sorts", "Online sorts", "Sorting algorithms"], "title": "Polyphase merge sort"}
{"summary": "ProxmapSort, or Proxmap sort, is a sorting algorithm that works by partitioning an array of data items, or keys, into a number of \"subarrays\" (termed buckets, in similar sorts). The name is short for computing a \"proximity map,\" which indicates for each key K the beginning of a subarray where K will reside in the final sorted order. Keys are placed into each subarray using insertion sort. If keys are \"well distributed\" among the subarrays, sorting occurs in linear time. The computational complexity estimates involve the number of subarrays and the proximity mapping function, the \"map key,\" used. It is a form of bucket and radix sort.\nOnce a ProxmapSort is complete, ProxmapSearch can be used to find keys in the sorted array in  time if the keys were well distributed during the sort.\nBoth algorithms were invented in the late 1980s by Prof. Thomas A. Standish at the University of California, Irvine.", "links": ["Adaptive sort", "American flag sort", "Analysis of algorithms", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Binary search tree", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket (computing)", "Bucket sort", "Bucket sorting", "Burstsort", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "Clifford Stein", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Digital object identifier", "Donald Bren School of Information and Computer Sciences", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introduction to Algorithms", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "Non-comparison sorting", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Quicksort", "Radix sort", "Ronald L. Rivest", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Thomas H. Cormen", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "University of California, Irvine"], "categories": ["All articles that may contain original research", "All articles with unsourced statements", "Articles that may contain original research from May 2015", "Articles with example pseudocode", "Articles with unsourced statements from May 2015", "Sorting algorithms", "Stable sorts"], "title": "Proxmap sort"}
{"summary": "qsort is a C standard library function that implements a polymorphic sorting algorithm for arrays of arbitrary objects according to a user-provided comparison function. It is named after the \"quicker sort\" algorithm (a quicksort variant due to R. S. Scowen), which was originally used to implement it in the Unix C library, although the C standard does not require it to implement quicksort.\nImplementations of the qsort function achieve polymorphism, the ability to sort different kinds of data, by taking a function pointer to a three-way comparison function, as well as a parameter that specifies the size of its individual input objects. The C standard requires the comparison function to implement a total order on the items in the input array.\nA qsort function was in place in Version 3 Unix of 1973, but was then an assembler subroutine. A C version, with roughly the interface of the standard C version, was in-place in Version 6 Unix. It was rewritten in 1983 at Berkeley. The function was standardized in ANSI C (1989).", "links": ["ANSI C", "Assembly language", "Berkeley Software Distribution", "C standard library", "Digital object identifier", "Function pointer", "ISO C", "Polymorphism (computer science)", "Quicksort", "Research Unix", "Software engineering", "Sorting algorithm", "Three-way comparison", "Total order", "Unix", "Version 6 Unix"], "categories": ["All stub articles", "C standard library", "Software engineering stubs", "Sorting algorithms"], "title": "Qsort"}
{"summary": "A quantum sort is any sorting algorithm that runs on a quantum computer. Any comparison-based quantum sorting algorithm would take at least  steps, which is already achievable by classical algorithms. Thus, for this task, quantum computers are no better than classical ones. However, in space-bounded sorts, quantum algorithms outperform their classical counterparts.", "links": ["Adaptive sort", "Adiabatic quantum computation", "Algorithmic cooling", "American flag sort", "BQP", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cavity quantum electrodynamics", "Charge qubit", "Circuit quantum electrodynamics", "Classical capacity", "Cluster state", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Deutsch\u2013Jozsa algorithm", "EQP (complexity)", "Entanglement-assisted classical capacity", "Entanglement-assisted stabilizer formalism", "Entanglement distillation", "Flashsort", "Flux qubit", "Gnome sort", "Grover's algorithm", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Kane quantum computer", "LOCC", "Library sort", "Linear optical quantum computing", "List (computing)", "Loss\u2013DiVincenzo quantum computer", "Merge sort", "Nitrogen-vacancy center", "Nuclear magnetic resonance quantum computer", "Odd\u2013even sort", "One-way quantum computer", "Optical lattice", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Phase qubit", "Pigeonhole sort", "Polyphase merge sort", "PostBQP", "Proxmap sort", "QMA", "Quantum Fourier transform", "Quantum Turing machine", "Quantum algorithm", "Quantum annealing", "Quantum capacity", "Quantum channel", "Quantum circuit", "Quantum complexity theory", "Quantum computer", "Quantum convolutional code", "Quantum cryptography", "Quantum decoherence", "Quantum energy teleportation", "Quantum error correction", "Quantum gate", "Quantum information", "Quantum information science", "Quantum key distribution", "Quantum network", "Quantum optics", "Quantum phase estimation algorithm", "Quantum programming", "Quantum teleportation", "Qubit", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Shor's algorithm", "Simon's problem", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Spin (physics)", "Splaysort", "Spreadsort", "Stabilizer code", "Stooge sort", "Strand sort", "Superconducting quantum computing", "Superdense coding", "Theoretical computer science", "Timeline of quantum computing", "Timsort", "Topological quantum computer", "Topological sorting", "Total order", "Tournament sort", "Trapped ion quantum computer", "Tree sort", "Ultracold atom", "Universal quantum simulator"], "categories": ["All stub articles", "Quantum information science", "Sorting algorithms", "Theoretical computer science stubs"], "title": "Quantum sort"}
{"summary": "Quicksort (sometimes called partition-exchange sort) is an efficient sorting algorithm, serving as a systematic method for placing the elements of an array in order. Developed by Tony Hoare in 1959, with his work published in 1961, it is still a commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than its main competitors, merge sort and heapsort.\nQuicksort is a comparison sort, meaning that it can sort items of any type for which a \"less-than\" relation (formally, a total order) is defined. In efficient implementations it is not a stable sort, meaning that the relative order of equal sort items is not preserved. Quicksort can operate in-place on an array, requiring small additional amounts of memory to perform the sorting.\nMathematical analysis of quicksort shows that, on average, the algorithm takes O(n log n) comparisons to sort n items. In the worst case, it makes O(n2) comparisons, though this behavior is rare.", "links": ["ALGOL", "Adaptive sort", "American flag sort", "Analysis of algorithms", "ArXiv", "Array data structure", "Association for Computing Machinery", "Autocode", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Binary search tree", "Binary tree sort", "Binomial coefficient", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "C standard library", "Call stack", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "CiteSeer", "Clifford Stein", "Cocktail sort", "Comb sort", "Communications of the ACM", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "David Musser", "Digital object identifier", "Disk storage", "Divide and conquer algorithm", "Donald Knuth", "Douglas McIlroy", "Dutch national flag problem", "Expected value", "Faron Moller", "Flashsort", "GNU libc", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer overflow", "Integer sorting", "International Standard Book Number", "International Standard Serial Number", "Introduction to Algorithms", "Introsort", "JSort", "Java (programming language)", "Java version history", "Jon Bentley", "Library sort", "Linked list", "List (computing)", "MIT Press", "Machine translation", "Magnetic tape data storage", "Main memory", "Master theorem", "McGraw-Hill", "Median", "Median of medians", "Merge sort", "Mergesort", "Moscow State University", "Multi-key quicksort", "National Physical Laboratory, UK", "Network attached storage", "Ninther", "Object (computer science)", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Parallel Random Access Machine", "Parallel algorithm", "Partial sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Prefix sum", "Primitive data type", "Programming Pearls", "Proxmap sort", "Pseudocode", "Qsort", "Quickselect", "Radix sort", "Recurrence relation", "Recursion (computer science)", "Robert Sedgewick (computer scientist)", "Ron Rivest", "Ronald L. Rivest", "SIAM Journal on Computing", "Samplesort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Soviet Union", "Spaghetti sort", "Splaysort", "Spreadsort", "Stable sort", "Steven Skiena", "Stirling's approximation", "Stooge sort", "Strand sort", "Swansea University", "Tail call", "Tail recursion", "Task parallelism", "The Computer Journal", "Thomas H. Cormen", "Timsort", "Tony Hoare", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "Uniform distribution (discrete)", "Unix", "Version 7 Unix"], "categories": ["1961 in science", "Accuracy disputes from August 2015", "Accuracy disputes from July 2015", "Articles with example pseudocode", "Comparison sorts", "Pages with DOIs inactive since 2015", "Sorting algorithms", "Use dmy dates from January 2012"], "title": "Quicksort"}
{"summary": "In computer science, radix sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position and value. A positional notation is required, but because integers can represent strings of characters (e.g., names or dates) and specially formatted floating point numbers, radix sort is not limited to integers. Radix sort dates back as far as 1887 to the work of Herman Hollerith on tabulating machines.\nMost digital computers internally represent all of their data as electronic representations of binary numbers, so processing the digits of integer representations by groups of binary digit representations is most convenient. Two classifications of radix sorts are least significant digit (LSD) radix sorts and most significant digit (MSD) radix sorts. LSD radix sorts process the integer representations starting from the least digit and move towards the most significant digit. MSD radix sorts work the other way around.\nLSD radix sorts typically use the following sorting order: short keys come before longer keys, and keys of the same length are sorted lexicographically. This coincides with the normal order of integer representations, such as the sequence 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11.\nMSD radix sorts use lexicographic order, which is suitable for sorting strings, such as words, or fixed-length integer representations. A sequence such as \"b, c, d, e, f, g, h, i, j, ba\" would be lexicographically sorted as \"b, ba, c, d, e, f, g, h, i, j\". If lexicographic ordering is used to sort variable-length integer representations, then the representations of the numbers from 1 to 10 would be output as 1, 10, 2, 3, 4, 5, 6, 7, 8, 9, as if the shorter keys were left-justified and padded on the right with blank characters to make the shorter keys as long as the longest key for the purpose of determining sorted order.", "links": ["Adaptive sort", "American flag sort", "Arithmetic sequence", "Array data type", "Base (exponentiation)", "Batcher", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Binary search", "Bitonic sorter", "Block sort", "Bogosort", "Breadth-first search", "Bubble sort", "Bucket (computing)", "Bucket sort", "Burstsort", "Byte", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "Clifford Stein", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer science", "Concatenate", "Counting sort", "Cycle sort", "Depth-first search", "Directed acyclic graph", "Donald Knuth", "Dr Dobb's Journal", "Fanout", "Flashsort", "Gnome sort", "Harold H. Seward", "Heapsort", "Herman Hollerith", "Hybrid algorithm", "IBM 80 series Card Sorters", "IEEE floating-point standard", "In-place algorithm", "Insertion sort", "Integer sorting", "Internal node", "Introduction to Algorithms", "Introsort", "JSort", "JavaScript", "Leaf node", "Least significant digit", "Lexicographic order", "Library sort", "Linear search", "Linked list", "List (computing)", "Massachusetts Institute of Technology", "Maze solving algorithm", "Merge sort", "Mergesort", "Most significant digit", "Novosibirsk", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Parallel Random Access Machine", "Parallel computing", "Parent node", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Positional notation", "Post-order traversal", "Pre-order traversal", "Proxmap sort", "Punched card", "Queue (data structure)", "Quicksort", "Radix", "Radix sort", "Random-access machine", "Recursion", "Ronald L. Rivest", "Root node", "Selection algorithm", "Selection sort", "Set (mathematics)", "Shellsort", "Significant figures", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stable sort", "Stooge sort", "Strand sort", "String (computer science)", "Tabulating machines", "Thomas H. Cormen", "Threaded binary tree", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "Trie", "Wojciech Rytter", "Word size"], "categories": ["Articles with example C code", "Sorting algorithms", "Stable sorts", "Use dmy dates from January 2012"], "title": "Radix sort"}
{"summary": "Samplesort is a sorting algorithm that is a divide and conquer algorithm often used in parallel processing systems. Conventional divide and conquer sorting algorithms partitions the array into sub-intervals or buckets. The buckets are then sorted individually and then concatenated together. However, if the array is non-uniformly distributed, the performance of these sorting algorithms can be significantly throttled. Samplesort addresses this issue by selecting a sample of size s from the n-element sequence, and determining the range of the buckets by sorting the sample and choosing m -1 elements from the result. These elements (called splitters) then divide the sample into m equal-sized buckets. Samplesort is described in the 1970 paper, \"Samplesort: A Sampling Approach to Minimal Storage Tree Sorting\", by W D Frazer and A C McKellar. In recent years, the algorithm has been adapted to implement randomized sorting on parallel computers.", "links": ["Buckets", "Divide and conquer algorithm", "Flashsort", "Parallel computer", "Quicksort", "Randomized sorting", "Sorting algorithm"], "categories": ["All articles needing expert attention", "Articles needing expert attention from April 2009", "Articles needing expert attention with no reason or talk parameter", "Computer science articles needing expert attention", "Sorting algorithms"], "title": "Samplesort"}
{"summary": "The Schwartzian transform is a computer science programming idiom used to improve the efficiency of sorting a list of items. This idiom is appropriate for comparison-based sorting when the ordering is actually based on the ordering of a certain property (the key) of the elements, where computing that property is an intensive operation that should be performed a minimal number of times. The Schwartzian transform is notable in that it does not use named temporary arrays.\nThe idiom is named after Randal L. Schwartz, who first demonstrated it in Perl shortly after the release of Perl 5 in 1994. The term \"Schwartzian transform\" applied solely to Perl programming for a number of years, but it has later been adopted by some users of other languages, such as Python, to refer to similar idioms in those languages. However, the algorithm was already in use in other languages (under no specific name) before it was popularized among the Perl community in the form of that particular idiom by Schwartz. The term \"Schwartzian transform\" indicates a specific idiom, and not the algorithm in general.\nFor example, to sort the word list (\"aaaa\",\"a\",\"aa\") according to word length: first build the list ([\"aaaa\",4],[\"a\",1],[\"aa\",2]), then sort it according to the numeric values getting ([\"a\",1],[\"aa\",2],[\"aaaa\",4]), then strip off the numbers and you get (\"a\",\"aa\",\"aaaa\"). That was the algorithm in general, so it does not count as a transform. To make it a true Schwartzian transform, it would be done in Perl like this:\n\nThe Schwartzian transform is a version of a Lisp idiom known as decorate-sort-undecorate, which avoids recomputing the sort keys by temporarily associating them with the input items. This approach is similar to memoization, which avoids repeating the calculation of the key corresponding to a specific input value. By comparison, this idiom assures that each input item's key is calculated exactly once, which may still result in repeating some calculations if the input data contains duplicate items.\n\n", "links": ["Big o notation", "Comp.lang.perl", "Comp.unix.shell", "Comparison sort", "Computer science", "D (programming language)", "Lisp (programming language)", "Lisp programming language", "Mac times", "Memoization", "Perl", "Programming idiom", "Programming language", "Python (programming language)", "Racket (programming language)", "Randal L. Schwartz", "Ruby (programming language)", "Sorting"], "categories": ["All articles needing style editing", "Articles with example Perl code", "Articles with example Racket code", "Perl", "Programming idioms", "Sorting algorithms", "Wikipedia articles needing style editing from September 2014"], "title": "Schwartzian transform"}
{"summary": "In computer science, selection sort is a sorting algorithm, specifically an in-place comparison sort. It has O(n2) time complexity, making it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity, and it has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.\nThe algorithm divides the input list into two parts: the sublist of items already sorted, which is built up from left to right at the front (left) of the list, and the sublist of items remaining to be sorted that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right.", "links": ["Adaptive sort", "American flag sort", "Arithmetic progression", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer science", "Counting sort", "Cycle sort", "Data structure", "Dictionary of Algorithms and Data Structures", "Divide and conquer algorithm", "Donald Knuth", "EEPROM", "Flash memory", "Flashsort", "Gnome sort", "Heap (data structure)", "Heapsort", "Hybrid algorithm", "Implicit data structure", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "Linked list", "List (computing)", "Merge sort", "Mergesort", "National Institute of Standards and Technology", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Pascal (programming language)", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Pseudocode", "Quicksort", "Radix sort", "Real-time computing", "Robert Sedgewick (computer scientist)", "Selection algorithm", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "The Art of Computer Programming", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Comparison sorts", "Sorting algorithms"], "title": "Selection sort"}
{"summary": "Smoothsort is a comparison-based sorting algorithm. It is a variation of heapsort developed by Edsger Dijkstra in 1981. Like heapsort, smoothsort is an in-place algorithm with an upper bound of O(n log n), but it is not a stable sort. The advantage of smoothsort is that it comes closer to O(n) time if the input is already sorted to some degree, whereas heapsort averages O(n log n) regardless of the initial sorted state.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Binary heap", "Bit vector", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Edsger Dijkstra", "Edsger W. Dijkstra", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Leonardo numbers", "Library sort", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stable sort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Transdichotomous model", "Tree sort", "University of Texas at Austin"], "categories": ["Articles with example Java code", "Comparison sorts", "Dutch inventions", "Heaps (data structures)", "Sorting algorithms"], "title": "Smoothsort"}
{"summary": "sort is a function in C++ Standard Library that takes two random-access iterators, the start and the end, as arguments and performs a comparison sort on the range of elements between the two iterators, front-inclusive and end-exclusive: [start, end).\nThe specific sorting algorithm is not mandated and may vary across implementations. However, the worst-case complexity must be O(n log n). In previous versions of C++, such as C++03, only average complexity was required to be O(n log n). This was to allow the use of algorithms like (median-of-3) quicksort, which are fast in the average case, indeed significantly faster than other algorithms like heap sort with optimal worst-case complexity, and where the worst-case quadratic complexity rarely occurs. The introduction of hybrid algorithms such as introsort allowed both fast average performance and optimal worst-case performance, and thus the complexity requirements were tightened in later standards.\nDifferent implementations use different algorithms. The GNU Standard C++ library, for example, uses a 3-part hybrid sorting algorithm: introsort is performed first (introsort itself being a hybrid of quicksort and heap sort), to a maximum depth given by 2\u00d7log2 n, where n is the number of elements, followed by an insertion sort on the result.", "links": ["Algorithm (C++)", "Argument (computer science)", "C++03", "C++ Standard Library", "Comparison sort", "David Musser", "GNU Compiler Collection", "Heap sort", "Hybrid algorithm", "ISO/IEC 14882", "In-place merge sort", "Inline expansion", "Insertion sort", "International Electrotechnical Commission", "International Organization for Standardization", "International Standard Book Number", "Introsort", "Linearithmic function", "Linearithmic time", "Partial sorting", "Qsort (C Standard Library)", "Quasilinear time", "Quickselect", "Quicksort", "Scott Meyers", "Selection algorithm", "Sorting algorithm", "Standard Template Library", "Stdlib.h"], "categories": ["C++ Standard Library", "Sorting algorithms", "Use dmy dates from January 2012"], "title": "Sort (C++)"}
{"summary": "In computer science, comparator networks are abstract devices built up of a fixed number of \"wires\", carrying values, and comparator modules that connect pairs of wires, swapping the values on the wires if they are not in a desired order. Such networks are typically designed to perform sorting on fixed numbers of values, in which case they are called sorting networks.\nSorting networks differ from general comparison sorts in that they are not capable of handling arbitrarily large inputs, and in that their sequence of comparisons is set in advance, regardless of the outcome of previous comparisons. This independence of comparison sequences is useful for parallel execution and for implementation in hardware. Despite the simplicity of sorting nets, their theory is surprisingly deep and complex. Sorting networks were first studied circa 1954 by Armstrong, Nelson and O'Connor, who subsequently patented the idea.\nSorting networks can be implemented either in hardware or in software. Donald Knuth describes how the comparators for binary integers can be implemented as simple, three-state electronic devices. Batcher, in 1968, suggested using them to construct switching networks for computer hardware, replacing both buses and the faster, but more expensive, crossbar switches. Since the 2000s, sorting nets (especially bitonic mergesort) are used by the GPGPU community for constructing sorting algorithms to run on graphics processing units.", "links": ["Adaptive sort", "American flag sort", "ArXiv", "Batcher odd\u2013even mergesort", "Bead sort", "Big-O notation", "Big O notation", "Bitonic sort", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Bus (computing)", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "Co-NP", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer hardware", "Computer science", "Contraposition", "Counting sort", "Crossbar switch", "Cycle sort", "Digital object identifier", "Divide and conquer algorithm", "Donald Knuth", "Endre Szemer\u00e9di", "Expander graph", "Flashsort", "General-purpose computing on graphics processing units", "Genetic algorithm", "Gnome sort", "Graphics processing unit", "Heapsort", "Hybrid algorithm", "If and only if", "In-place algorithm", "Insertion sort", "Integer sorting", "International Standard Book Number", "Introduction to Algorithms", "Introsort", "JSort", "J\u00e1nos Koml\u00f3s (mathematician)", "Ken Batcher", "Lemma (mathematics)", "Library sort", "List (computing)", "Mathematical induction", "Merge sort", "Michael S. Paterson", "Michael T. Goodrich", "Mikl\u00f3s Ajtai", "Monotonic function", "Odd\u2013even sort", "Oscillating merge sort", "P vs NP", "Pairwise sorting network", "Pancake sorting", "Parallel computing", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proof by contradiction", "Proxmap sort", "Quicksort", "Radix sort", "Richard J. Lipton", "Robert Floyd", "Ron Rivest", "Selection algorithm", "Selection sort", "Shell sort", "Shellsort", "Smoothsort", "Software", "Sorting algorithm", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Switch", "Symposium on Theory of Computing", "The Art of Computer Programming", "Thomas H. Cormen", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Computer engineering", "Pages using duplicate arguments in template calls", "Sorting algorithms"], "title": "Sorting network"}
{"summary": "Spaghetti sort is a linear-time, analog algorithm for sorting a sequence of items, by Alexander Dewdney in his column, Scientific American. This algorithm sorts a sequence of items requiring O(n) stack space in a stable manner. It requires a parallel processor.", "links": ["Adaptive sort", "Alexander Dewdney", "Algorithm", "American flag sort", "Analog computer", "Andrew Adamatzky", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Dietrich Stauffer", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "International Standard Book Number", "Introsort", "JSort", "Library sort", "Linear-time", "List (computing)", "Luniver Press", "Merge sort", "Natural number", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Parallel computing", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Scientific American", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort", "World Scientific"], "categories": ["Accuracy disputes from July 2013", "All Wikipedia articles needing clarification", "All accuracy disputes", "All articles with unsourced statements", "Articles with unsourced statements from April 2015", "Sorting algorithms", "Wikipedia articles needing clarification from July 2013"], "title": "Spaghetti sort"}
{"summary": "In computer science, splaysort is an adaptive comparison sorting algorithm based on the splay tree data structure.", "links": ["Adaptive sort", "Algorithm", "American flag sort", "Amortized analysis", "ArXiv", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer science", "Counting sort", "Cycle sort", "Data structure", "Digital object identifier", "Entropy (information theory)", "Flashsort", "Gnome sort", "Heap sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Inorder", "Insertion sort", "Integer sorting", "Introsort", "Inversion (discrete mathematics)", "JSort", "Lecture Notes in Computer Science", "Library sort", "List (computing)", "Mathematical Reviews", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "SIAM Journal on Computing", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splay tree", "Spreadsort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Sorting algorithms"], "title": "Splaysort"}
{"summary": "Spreadsort is a sorting algorithm invented by Steven J. Ross in 2002. It combines concepts from distribution-based sorts, such as radix sort and bucket sort, with partitioning concepts from comparison sorts such as quicksort and mergesort. In experimental results it was shown to be highly efficient, often outperforming traditional algorithms such as quicksort, particularly on distributions exhibiting structure.\nQuicksort identifies a pivot element in the list and then partitions the list into two sublists, those elements less than the pivot and those greater than the pivot. Spreadsort generalizes this idea by partitioning the list into n/c partitions at each step, where n is the total number of elements in the list and c is a small constant (in practice usually between 4 and 8 when comparisons are slow, or much larger in situations where they are fast). It uses distribution-based techniques to accomplish this, first locating the minimum and maximum value in the list, and then dividing the region between them into n/c equal-sized bins. Where caching is an issue, it can help to have a maximum number of bins in each recursive division step, causing this division process to take multiple steps. Though this causes more iterations, it reduces cache misses and can make the algorithm run faster overall.\nIn the case where the number of bins is at least the number of elements, spreadsort degenerates to bucket sort and the sort completes. Otherwise, each bin is sorted recursively. The algorithm uses heuristic tests to determine whether each bin would be more efficiently sorted by spreadsort or some other classical sort algorithm, then recursively sorts the bin.\nLike other distribution-based sorts, spreadsort has the weakness that the programmer is required to provide a means of converting each element into a numeric key, for the purpose of identifying which bin it falls in. Although it is possible to do this for arbitrary-length elements such as strings by considering each element to be followed by an infinite number of minimum values, and indeed for any datatype possessing a total order, this can be more difficult to implement correctly than a simple comparison function, especially on complex structures. Poor implementation of this value function can result in clustering that harms the algorithm's relative performance.", "links": ["Adaptive sort", "American flag sort", "Batcher odd\u2013even mergesort", "Bead sort", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "Mergesort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Sorting algorithms"], "title": "Spreadsort"}
{"summary": "Stooge sort is a recursive sorting algorithm with a time complexity of O(nlog 3 / log 1.5 ) = O(n2.7095...). The running time of the algorithm is thus slower compared to efficient sorting algorithms, such as Merge sort, and is even slower than Bubble sort, a canonical example of a fairly inefficient and simple sort.\nThe algorithm is defined as follows:\nIf the value at the end is smaller than the value at the start, swap them.\nIf there are 3 or more elements in the list, then:\nStooge sort the initial 2/3 of the list\nStooge sort the final 2/3 of the list\nStooge sort the initial 2/3 of the list again\n\nelse: exit the procedure\nIt is important to get the integer sort size used in the recursive calls by rounding the 2/3 upwards, e.g. rounding 2/3 of 5 should give 4 rather than 3, as otherwise the sort can fail on certain data. However, if the code is written to end on a base case of size 1, rather than terminating on either size 1 or size 2, rounding the 2/3 of 2 upwards gives an infinite number of calls.\nThe algorithm gets its name from slapstick routines of The Three Stooges, in which each stooge hits the other two.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Charles E. Leiserson", "Clifford Stein", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Computer science", "Counting sort", "Cycle sort", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "International Standard Book Number", "Introduction to Algorithms", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "National Institute of Standards and Technology", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Recursion", "Ron Rivest", "Selection algorithm", "Selection sort", "Shellsort", "Slapstick", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Strand sort", "The Three Stooges", "Thomas H. Cormen", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["All articles with unsourced statements", "All stub articles", "Articles with example pseudocode", "Articles with unsourced statements from March 2010", "Comparison sorts", "Computer science stubs", "Sorting algorithms", "Use dmy dates from October 2010"], "title": "Stooge sort"}
{"summary": "Strand sort is a sorting algorithm. It works by repeatedly pulling sorted sublists out of the list to be sorted and merging them with a result array. Each iteration through the unsorted list pulls out a series of elements which were already sorted, and merges those series together.\nThe name of the algorithm comes from the \"strands\" of sorted data within the unsorted list which are removed one at a time. It is a comparison sort due to its use of comparisons when removing strands and when merging them into the sorted array.\nThe strand sort algorithm is O(n2) in the average case. In the best case (a list which is already sorted) the algorithm is linear, or O(n). In the worst case (a list which is sorted in reverse order) the algorithm is O(n2).\nStrand sort is most useful for data which is stored in a linked list, due to the frequent insertions and removals of data. Using another data structure, such as an array, would greatly increase the running time and complexity of the algorithm due to lengthy insertions and deletions. Strand sort is also useful for data which already has large amounts of sorted data, because such data can be removed in a single strand.", "links": ["Adaptive sort", "American flag sort", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big-O notation", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Dictionary of Algorithms and Data Structures", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "Linked list", "List (computing)", "Merge sort", "Mergesort", "National Institute of Standards and Technology", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Pseudocode", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["Articles for deletion", "Articles with example pseudocode", "Comparison sorts", "Sorting algorithms"], "title": "Strand sort"}
{"summary": "Timsort is a hybrid stable sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data. It was invented by Tim Peters in 2002 for use in the Python programming language. The algorithm finds subsets of the data that are already ordered, and uses that knowledge to sort the remainder more efficiently. This is done by merging an identified subset, called a run, with existing runs until certain criteria are fulfilled. Timsort has been Python's standard sorting algorithm since version 2.3. It is also used to sort arrays of non-primitive type in Java SE 7, on the Android platform, and in GNU Octave.", "links": ["Adaptive sort", "Algorithm", "American flag sort", "Android (operating system)", "Array data structure", "Array data type", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Binary search", "Binary search algorithm", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "C++", "CPU cache", "CPython", "C (programming language)", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Flashsort", "Formal verification", "Function call", "GNU Octave", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "In-place merge sort", "Information theory", "Insertion sort", "Integer sorting", "International Standard Book Number", "Introsort", "JSort", "Java 7", "KeY", "Library sort", "Linear search", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Python (programming language)", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stack (data structure)", "Stooge sort", "Strand sort", "Tim Peters software engineer", "Topological sorting", "Total order", "Tournament sort", "Tree sort"], "categories": ["All articles with dead external links", "Articles with dead external links from June 2013", "Comparison sorts", "Sorting algorithms", "Stable sorts"], "title": "Timsort"}
{"summary": "Tournament sort is a sorting algorithm. It improves upon the naive selection sort by using a priority queue to find the next element in the sort. In the naive selection sort, it takes O(n) operations to select the next element of n elements; in a tournament sort, it takes O(log n) operations (after building the initial tournament in O(n)). Tournament sort is a variation of heapsort.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Donald Knuth", "Flashsort", "Gnome sort", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Priority queue", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Shellsort", "Single-elimination tournament", "Smoothsort", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "The Art of Computer Programming", "Timsort", "Topological sorting", "Total order", "Tree sort"], "categories": ["All articles needing additional references", "Articles needing additional references from July 2012", "Sorting algorithms"], "title": "Tournament sort"}
{"summary": "A tree sort is a sort algorithm that builds a binary search tree from the keys to be sorted, and then traverses the tree (in-order) so that the keys come out in sorted order. Its typical use is sorting elements adaptively: after each insertion, the set of elements seen so far is available in sorted order.", "links": ["Adaptive sort", "American flag sort", "Array data structure", "Batcher odd\u2013even mergesort", "Bead sort", "Best, worst and average case", "Big O notation", "Binary Tree", "Binary heap", "Binary search tree", "Bitonic sorter", "Block sort", "Bogosort", "Bubble sort", "Bucket sort", "Burstsort", "Cartesian tree", "Cascade merge sort", "Cocktail sort", "Comb sort", "Comparison sort", "Computational complexity theory", "Counting sort", "Cycle sort", "Flashsort", "Functional programming", "Gnome sort", "Haskell (programming language)", "Heapsort", "Hybrid algorithm", "In-place algorithm", "Insertion sort", "Integer sorting", "Introsort", "JSort", "Library sort", "Linked list", "List (computing)", "Merge sort", "Odd\u2013even sort", "Oscillating merge sort", "Pairwise sorting network", "Pancake sorting", "Patience sorting", "Pigeonhole sort", "Polyphase merge sort", "Proxmap sort", "Quicksort", "Radix sort", "Selection algorithm", "Selection sort", "Self-balancing binary search tree", "Shellsort", "Smoothsort", "Sort algorithm", "Sorting algorithm", "Sorting network", "Spaghetti sort", "Splay tree", "Splaysort", "Spreadsort", "Stooge sort", "Strand sort", "Timsort", "Topological sorting", "Total order", "Tournament sort", "Tree traversal"], "categories": ["All articles lacking sources", "All articles with unsourced statements", "Articles lacking sources from September 2014", "Articles with unsourced statements from September 2014", "Sorting algorithms"], "title": "Tree sort"}
{"summary": "In computer science, X + Y sorting is the problem of sorting pairs of numbers by their sum. Given two finite sets X and Y, the problem is to order all pairs (x, y) in the Cartesian product X \u00d7 Y by the key x + y. The problem is attributed to Elwyn Berlekamp.\nThis problem can be solved using a straightforward comparison sort on the Cartesian product, taking time O(nm log(nm)) for sets of sizes n and m. When it is assumed that m = n, the complexity is O(n2 log n2) = O(n2 log n), which is also the best known bound on the problem, but whether X + Y sorting can be done strictly faster than sorting n\u22c5m arbitrary numbers is an open problem. The number of required comparisons is certainly lower than for ordinary comparison sorting: Fredman showed, in 1976, that X + Y sorting can be done using only O(n2) comparisons, though he did not show an algorithm. The first actual algorithm that achieves this number of comparisons and O(n2 log n) total complexity was only published sixteen years later.\nOn a RAM machine with word size w and integer inputs 0 \u2264 {x, y} < n = 2w, the problem can be solved in O(n log n) operations by means of the fast Fourier transform.\nSkiena recounts a practical application in transit fare minimisation, an instance of the shortest path problem: given fares x and y for trips from departure A to some intermediate destination B and from B to final destination C, determine the least expensive combined trip from A to C.\n\n", "links": ["3SUM", "Cartesian product", "Communications of the ACM", "Comparison sort", "Computer science", "Digital object identifier", "Elwyn Berlekamp", "Erik Demaine", "Fare", "Fast Fourier transform", "Integer sorting", "List of unsolved problems in computer science", "Michael Fredman", "Non-constructive algorithm existence proofs", "Ordered pair", "RAM machine", "Shortest path problem", "Sorting algorithm", "Steven Skiena", "Time complexity", "Word size"], "categories": ["Sorting algorithms", "Unsolved problems in computer science"], "title": "X + Y sorting"}
{"summary": "In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. This includes the cases of finding the minimum, maximum, and median elements. There are O(n) (worst-case linear time) selection algorithms, and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. Selection is a subproblem of more complex problems like the nearest neighbor and shortest path problems. Many selection algorithms are derived by generalizing a sorting algorithm, and conversely some sorting algorithms can be derived as repeated application of selection.\nThe simplest case of a selection algorithm is finding the minimum (or maximum) element by iterating through the list, keeping track of the running minimum \u2013 the minimum so far \u2013 (or maximum) and can be seen as related to the selection sort. Conversely, the hardest case of a selection algorithm is finding the median, and this necessarily takes n/2 storage. In fact, a specialized median-selection algorithm can be used to build a general selection algorithm, as in median of medians. The best-known selection algorithm is quickselect, which is related to quicksort; like quicksort, it has (asymptotically) optimal average performance, but poor worst-case performance, though it can be modified to give optimal worst-case performance as well.", "links": ["Algorithm", "Almost certain", "Amortized analysis", "Array data structure", "C++", "CPAN", "Charles E. Leiserson", "Clifford Stein", "Computer science", "Counting sort", "Decrease and conquer", "Descriptive statistics", "Dictionary of Algorithms and Data Structures", "Digital object identifier", "Donald E. Knuth", "Donald Knuth", "Frequency tables", "Geometric series", "Hash table", "Heap (data structure)", "Introduction to Algorithms", "Introselect", "Introsort", "Lazy evaluation", "List (abstract data type)", "Manuel Blum", "Maximum", "Median", "Median of medians", "Minimum", "National Institute of Standards and Technology", "Nearest neighbor problem", "Odds algorithm", "Online algorithm", "Order statistic", "Order statistic tree", "Ordinal optimization", "Partial sorting", "Perl", "Python (programming language)", "Quickselect", "Quicksort", "Radix sort", "Random access", "Range Queries", "Reduction (complexity)", "Robert Floyd", "Robert Tarjan", "Robert W. Floyd", "Ron Rivest", "Ronald L. Rivest", "Secretary problem", "Selection (genetic algorithm)", "Selection sort", "Self-balancing binary search tree", "Shortest path", "Sorting algorithm", "Sublinear time", "The Art of Computer Programming", "Thomas H. Cormen", "Vaughan Pratt"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from April 2014", "Selection algorithms"], "title": "Selection algorithm"}
{"summary": "In computer science, the median of medians algorithm is a selection algorithm based on the quickselect algorithm, and is optimal, having worst-case linear time complexity for selecting the kth largest element. The algorithm finds an approximate median in linear time \u2013 this is the key step \u2013 which is then used as a pivot in quickselect. In other words, it uses an (asymptotically) optimal approximate median-selection algorithm to build an (asymptotically) optimal general selection algorithm.\nThe approximate median-selection algorithm can also be used as a pivot strategy in quicksort, yielding an optimal algorithm, with worst-case complexity O(n log n). Although this approach optimizes quite well, it is typically outperformed in practice by instead choosing random pivots, which has average linear time for selection and average log-linear time for sorting, and avoids the overhead of computing the pivot. Median of medians is used in the hybrid introselect algorithm as a fallback, to ensure worst-case linear performance: introselect starts with quickselect, to obtain good average performance, and then falls back to median of medians if progress is too slow.\nThe algorithm was published in Blum et al. (Tarjan), and thus is sometimes called BFPRT after the last names of the authors. In the original paper the algorithm was referred to as PICK, referring to quickselect as \"FIND\".", "links": ["Akra\u2013Bazzi method", "Array data structure", "Best, worst and average case", "Charles E. Leiserson", "Clifford Stein", "Computer science", "Decile", "Decrease and conquer", "Digital object identifier", "Geometric series", "Insertion sort", "International Standard Book Number", "Introduction to Algorithms", "Introselect", "Manuel Blum", "Mutual recursion", "Percentile", "Pseudocode", "Quickselect", "Quicksort", "Recursion (computer science)", "Robert Floyd", "Robert Tarjan", "Ron Rivest", "Selection algorithm", "Thomas H. Cormen", "Triangular number", "Vaughan Pratt"], "categories": ["Selection algorithms"], "title": "Median of medians"}
{"summary": "In computer science, the Floyd-Rivest algorithm is a selection algorithm developed by Robert W. Floyd and Ronald L. Rivest that has an optimal expected number of comparisons within lower-order terms. It is functionally equivalent to quickselect, but runs faster in practice on average.  It has an expected running time of O(n) and an expected number of comparisons of n + min(k, n - k) + O(n1/2).\nThe algorithm was originally presented in a Stanford University technical report containing two papers, where it was referred to as SELECT and paired with PICK, or median of medians.  It was subsequently published in Communications of the ACM, Volume 18: Issue 3.", "links": ["Algorithm", "Array data structure", "Best, worst and average case", "Communications of the ACM", "Computer science", "Data structure", "Digital object identifier", "Divide and conquer algorithm", "Lower-order terms", "Median of medians", "Pseudocode", "Quickselect", "Robert W. Floyd", "Ron Rivest", "Ronald L. Rivest", "Sampling (statistics)", "Selection algorithm"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Selection algorithms"], "title": "Floyd\u2013Rivest algorithm"}
{"summary": "In computer science, introselect (short for \"introspective selection\") is a selection algorithm that is a hybrid of quickselect and median of medians which has fast average performance and optimal worst-case performance. Introselect is related to the introsort sorting algorithm: these are analogous refinements of the basic quickselect and quicksort algorithms, in that they both start with the quick algorithm, which has good average performance and low overhead, but fall back to an optimal worst-case algorithm (with higher overhead) if the quick algorithm does not progress rapidly enough. Both algorithms were introduced by David Musser in (Musser 1997), with the purpose of providing generic algorithms for the C++ Standard Library which had both fast average performance and optimal worst-case performance, thus allowing the performance requirements to be tightened.", "links": ["Array data structure", "Best, worst and average case", "C++ Standard Library", "Computer science", "David Musser", "Digital object identifier", "Generic algorithm", "Heapsort", "Hybrid algorithm", "Introsort", "Median of medians", "Quickselect", "Quicksort", "Selection algorithm", "Sorting algorithm"], "categories": ["Pages with DOIs inactive since 2015", "Selection algorithms"], "title": "Introselect"}
{"summary": "In computer science, the median of medians algorithm is a selection algorithm based on the quickselect algorithm, and is optimal, having worst-case linear time complexity for selecting the kth largest element. The algorithm finds an approximate median in linear time \u2013 this is the key step \u2013 which is then used as a pivot in quickselect. In other words, it uses an (asymptotically) optimal approximate median-selection algorithm to build an (asymptotically) optimal general selection algorithm.\nThe approximate median-selection algorithm can also be used as a pivot strategy in quicksort, yielding an optimal algorithm, with worst-case complexity O(n log n). Although this approach optimizes quite well, it is typically outperformed in practice by instead choosing random pivots, which has average linear time for selection and average log-linear time for sorting, and avoids the overhead of computing the pivot. Median of medians is used in the hybrid introselect algorithm as a fallback, to ensure worst-case linear performance: introselect starts with quickselect, to obtain good average performance, and then falls back to median of medians if progress is too slow.\nThe algorithm was published in Blum et al. (Tarjan), and thus is sometimes called BFPRT after the last names of the authors. In the original paper the algorithm was referred to as PICK, referring to quickselect as \"FIND\".", "links": ["Akra\u2013Bazzi method", "Array data structure", "Best, worst and average case", "Charles E. Leiserson", "Clifford Stein", "Computer science", "Decile", "Decrease and conquer", "Digital object identifier", "Geometric series", "Insertion sort", "International Standard Book Number", "Introduction to Algorithms", "Introselect", "Manuel Blum", "Mutual recursion", "Percentile", "Pseudocode", "Quickselect", "Quicksort", "Recursion (computer science)", "Robert Floyd", "Robert Tarjan", "Ron Rivest", "Selection algorithm", "Thomas H. Cormen", "Triangular number", "Vaughan Pratt"], "categories": ["Selection algorithms"], "title": "Median of medians"}
{"summary": "In computer science, quickselect is a selection algorithm to find the kth smallest element in an unordered list. It is related to the quicksort sorting algorithm. Like quicksort, it was developed by Tony Hoare, and thus is also known as Hoare's selection algorithm. Like quicksort, it is efficient in practice and has good average-case performance, but has poor worst-case performance. Quickselect and variants is the selection algorithm most often used in efficient real-world implementations.\nQuickselect uses the same overall approach as quicksort, choosing one element as a pivot and partitioning the data in two based on the pivot, accordingly as less than or greater than the pivot. However, instead of recursing into both sides, as in quicksort, quickselect only recurses into one side \u2013 the side with the element it is searching for. This reduces the average complexity from O(n log n) (in quicksort) to O(n) (in quickselect).\nAs with quicksort, quickselect is generally implemented as an in-place algorithm, and beyond selecting the k'th element, it also partially sorts the data. See selection algorithm for further discussion of the connection with sorting.", "links": ["Almost certain", "Array data structure", "Best, worst and average case", "Big-O notation", "Communications of the ACM", "Computer science", "David Eppstein", "David Musser", "Digital object identifier", "Floyd\u2013Rivest algorithm", "Geometric series", "In-place algorithm", "Introselect", "Median of medians", "Quicksort", "Selection algorithm", "Tail recursion", "Tony Hoare"], "categories": ["All articles needing additional references", "Articles needing additional references from August 2013", "Selection algorithms"], "title": "Quickselect"}
{"summary": "The Geohash-36 geocode is an opensource compression algorithm for world coordinate data. It was developed as a variation of the OpenPostcode format developed as a candidate geolocation postcode for the Republic of Ireland. It is similar in function to the original public domain Geohash code. It is calculated differently and uses a more accurate base 36 (or rather radix 36) representation rather than the original base 32 representation.", "links": ["Base 36", "Checksum", "Country code top-level domain", "Decimal data type", "Edge cases", "FIPS county code", "FIPS place code", "Federal Information Processing Standard state code", "Geocoding", "Geohash", "Geolocation", "Georef", "Geotude", "Hierarchical administrative subdivision codes", "ISO 3166-1", "ISO 3166-1 alpha-2", "ISO 3166-1 alpha-3", "ISO 3166-1 numeric", "International Air Transport Association airport code", "International Civil Aviation Organization airport code", "Irish Geocodes", "List of FIPS country codes", "List of IOC country codes", "List of ITU letter codes", "List of country calling codes", "List of geocoding systems", "MARC standards", "Maidenhead Locator System", "Marsden square", "Mobile country code", "Modulus", "Munich Orientation Convention", "Natural Area Code", "Nomenclature of Territorial Units for Statistics", "North America", "ONS coding system", "Opensource", "Postal Index Number", "Postal code", "Postal codes in the Republic of Ireland", "Public domain", "QDGC", "QRA locator", "Radix", "SALB", "SQL", "Shard London Bridge", "Standard Geographical Classification code (Canada)", "Statue of Liberty", "UN/LOCODE", "UN M.49", "URL", "Universal Transverse Mercator coordinate system", "Vowel", "World Meteorological Organization squares", "ZIP code"], "categories": ["All articles needing additional references", "Articles needing additional references from June 2012", "Compression algorithms", "Geocodes"], "title": "Geohash-36"}
{"summary": "A power quality compression algorithm is an algorithm used in power quality analysis. To provide high quality electric power service, it is essential to monitor the quality of the electric signals also termed as power quality (PQ) at different locations along an electrical power network. Electrical utilities carefully monitor waveforms and currents at various network locations constantly, to understand what lead up to any unforeseen events such as a power outage and blackouts. This is particularly critical at sites where the environment and public safety are at risk (institutions such as hospitals, sewage treatment plants, mines, etc.).", "links": ["Algorithm", "Alternating current", "Bzip", "Electric current", "Electrical network", "Harmonics", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lossy compression", "Power factor", "Power outage", "Power quality", "Total harmonic distortion", "Utility frequency", "Voltage"], "categories": ["All articles needing cleanup", "All articles to be merged", "All articles with topics of unclear notability", "Articles needing cleanup from November 2010", "Articles to be merged from March 2013", "Articles with topics of unclear notability from December 2012", "Cleanup tagged articles without a reason field from November 2010", "Compression algorithms", "Power engineering", "Wikipedia pages needing cleanup from November 2010"], "title": "Power quality compression algorithm"}
{"summary": "Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast, lossy compression permits reconstruction only of an approximation of the original data, though this usually improves compression rates (and therefore reduces file sizes).\nLossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by the LAME MP3 encoder and other lossy audio encoders).\nLossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data could be deleterious. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods. Lossless audio formats are most often used for archiving or production purposes, while smaller lossy audio files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary.", "links": ["7-Zip", "7zip", "A-law algorithm", "Adaptive Huffman coding", "Adaptive Transform Acoustic Coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Algorithmic complexity theory", "Amiga", "Apple Lossless", "Apt-X", "Arithmetic coding", "Audio Lossless Coding", "Audio codec", "Audio compression (data)", "Autoregressive", "Average bitrate", "Benchmark (computing)", "Bijection", "Bit rate", "Brotli", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Bzip2", "C10n.info", "CCM (software)", "Calgary Corpus", "Canonical Huffman code", "Ccmx", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Comp.compression", "Companding", "Comparison of file archivers", "Compress", "Compression algorithm", "Compression artifact", "Constant bitrate", "Context-mixing", "Context mixing", "Context tree weighting", "Convolution", "Counting argument", "Cryptanalysis", "Cryptosystem", "DEFLATE", "DEFLATE (algorithm)", "DTS-HD Master Audio", "Data-compression.com", "Data compression", "Data compression ratio", "David A. Huffman", "Deblocking filter", "Delta (letter)", "Delta encoding", "Demo (computer programming)", "Dictionary coder", "Differential pulse-code modulation", "Digital object identifier", "Discrete cosine transform", "Discrete wavelet transform", "Display resolution", "Dolby TrueHD", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Executable compression", "Exponential-Golomb coding", "FAQ", "Fibonacci coding", "Film frame", "Flashzip", "Fourier transform", "Fractal compression", "Frame rate", "FreeArc", "Free Lossless Audio Codec", "Function (mathematics)", "GIF", "GNU", "GNU General Public License", "Genetic algorithm", "Genetics", "Golomb coding", "Grammar induction", "Graphics Interchange Format", "Gzip", "HTTP", "HapMap", "Heuristics", "Huffman coding", "Hutter Prize", "ILBM", "Image compression", "Image resolution", "Indexed color", "Indexed image", "Information entropy", "Information theory", "Interchange File Format", "Interlaced video", "Intuition (knowledge)", "JBIG2", "JPEG2000", "JPEG 2000", "JPEG XR", "JavaScript", "Joint (audio engineering)", "Karhunen\u2013Lo\u00e8ve theorem", "Kilobyte", "Kolmogorov complexity", "LAME", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZ77 and LZ78 (algorithms)", "LZ78", "LZJB", "LZMA", "LZRW", "LZW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "List of codecs", "Log area ratio", "Lossless", "Lossless JPEG", "Lossless Transform Audio Compression", "Lossy compression", "MP3", "MPEG-4 SLS", "Macroblock", "Matt Mahoney (computer scientist)", "Meridian Lossless Packing", "Modified Huffman coding", "Modified discrete cosine transform", "Monkey's Audio", "Motion compensation", "Move-to-front transform", "Multiple-image Network Graphics", "NanoZip", "Newsgroup", "Normal number", "Nyquist\u2013Shannon sampling theorem", "OpenCTM", "OptimFROG", "Original Sound Quality", "PAQ", "PKWARE, Inc.", "PPMd", "Peak signal-to-noise ratio", "Pi", "Pigeonhole principle", "Pixel", "Point-to-Point Protocol", "Portable Network Graphics", "Precompressor", "Prediction by partial matching", "Progressive Graphics File", "Psychoacoustics", "PubMed Central", "PubMed Identifier", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Random", "Range encoding", "Rate\u2013distortion theory", "RealPlayer", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Secure Shell", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Shorten (file format)", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel Ziv", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Super Audio CD", "TIFF", "TTA (codec)", "Tagged Image File Format", "Timeline of information theory", "Tunstall coding", "UTF-8", "Unary coding", "Unicity distance", "United States", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "WavPack", "Wavelet compression", "WebP", "Wikipedia", "WinRK", "Windows Media Audio 9 Lossless", "XML", "Xz", "ZIP (file format)", "\u039c-law algorithm"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from August 2011", "Articles with unsourced statements from December 2007", "Articles with unsourced statements from November 2012", "Articles with unsourced statements from November 2015", "Data compression", "Lossless compression algorithms"], "title": "Lossless compression"}
{"summary": "7z is a compressed archive file format that supports several different data compression, encryption and pre-processing algorithms. The 7z format initially appeared as implemented by the 7-Zip archiver. The 7-Zip program is publicly available under the terms of the GNU Lesser General Public License. The LZMA SDK 4.62 was placed in the public domain in December 2008. The latest stable version of 7-Zip and LZMA SDK is version 9.20.\nThe official 7z file format specification is distributed with 7-Zip's source code. The specification can be found in plain text format in the 'doc' sub-directory of the source code distribution.", "links": ["7-Zip", "7Z (disambiguation)", "7z (file format)", "ACE (compression file format)", "ARC (file format)", "ARJ", "ARM Thumb", "ARM architecture", "Access control list", "AdvanceCOMP", "Advanced Encryption Standard", "Android application package", "Apple Disk Image", "Ar (Unix)", "Archive file format", "Archive format", "B1 (archive format)", "BCJ2", "BagIt", "Binary tree", "Brute-force search", "Burrows\u2013Wheeler transform", "Bzip2", "CFS (file format)", "Cabinet (file format)", "Compact Pro", "Comparison of archive formats", "Compress", "Cpio", "DEFLATE", "DGCA (computing)", "DOS", "Data compression", "Data compression ratio", "Deb (file format)", "Delta encoding", "Doom WAD", "EAR (file format)", "EGG (file format)", "EPUB", "Encryption", "Entropy coding", "Exbibyte", "File format", "File spanning", "Filename extension", "Filesystem permissions", "Free file format", "GNU Lesser General Public License", "Gzip", "Header (computing)", "Huffman coding", "Igor Pavlov (programmer)", "International Standard Book Number", "Internet media type", "Itanium", "JAR (file format)", "Java EE Connector Architecture", "KGB Archiver", "Key stretching", "LBR (file format)", "LHA (file format)", "LZ77 and LZ78", "LZMA", "LZX (algorithm)", "Lempel-Ziv-Markov chain algorithm", "Lempel-Ziv-Storer-Szymanski", "Lempel\u2013Ziv\u2013Markov chain algorithm", "List of archive formats", "Lzip", "Lzop", "MPQ", "Markov chain", "Move to front", "Multiple-image Network Graphics", "NTFS", "Open Packaging Conventions", "Open eBook", "Open format", "PAQ", "PDF", "PPM compression algorithm", "Package (OS X)", "Package format", "Passphrase", "PeaZip", "Portable Network Graphics", "PowerPC", "Prediction by Partial Matching", "Public domain", "Quadruple D", "RAR (file format)", "RPM Package Manager", "Range encoding", "Rzip", "SHA-256", "SQX", "SQ (program)", "Shar", "Solid compression", "Sourceforge", "StuffIt", "Tar (computing)", "Tar (file format)", "UHARC", "UNIX", "UPX", "Unicode", "Uniform Type Identifier", "WAR (file format)", "Windows Installer", "X86", "Xar (archiver)", "Xz", "ZIP (file format)", "ZPAQ", "Zip (file format)", "Zlib", "Zoo (file format)"], "categories": ["1999 introductions", "All articles with unsourced statements", "Archive formats", "Articles with unsourced statements from June 2014", "Lossless compression algorithms", "Russian inventions", "Wikipedia articles needing clarification from October 2015"], "title": "7z"}
{"summary": "Adam7 is an interlacing algorithm for raster images, best known as the interlacing scheme optionally used in PNG images. An Adam7 interlaced image is broken into seven subimages, which are defined by replicating this 8\u00d78 pattern across the full image.\n\nThe subimages are then stored in the image file in numerical order.\nAdam7 uses seven passes and operates in both dimensions, compared to only four passes in the vertical dimension used by GIF. This means that an approximation of the entire image can be perceived much more quickly in the early passes, particularly if interpolation algorithms such as bicubic interpolation are used.", "links": ["Adam7 (disambiguation)", "Algorithm", "Bicubic interpolation", "Decimation (signal processing)", "Discrete wavelet transform", "Downsample", "GIF", "Haar wavelet", "Interlacing (bitmaps)", "Lee Daniel Crocker", "Low-pass filter", "Peano curve", "Pixelation", "Portable Network Graphics", "Raster image"], "categories": ["Image compression", "Lossless compression algorithms"], "title": "Adam7 algorithm"}
{"summary": "Adaptive coding refers to variants of entropy encoding methods of lossless data compression. They are particularly suited to streaming data, as they adapt to localized changes in the characteristics of the data, and don't require a first pass over the data to calculate a probability model. The cost paid for these advantages is that the encoder and decoder must be more complex to keep their states synchronized, and more computational power is needed to keep adapting the encoder/decoder state.\nAlmost all data compression methods involve the use of a model, a prediction of the composition of the data. When the data matches the prediction made by the model, the encoder can usually transmit the content of the data at a lower information cost, by making reference to the model. This general statement is a bit misleading as general data compression algorithm would include the popular LZW and LZ77 algorithms, which are hardly comparable to compression techniques typically called adaptive. Run length encoding and the typical JPEG compression with run length encoding and predefined Huffman codes do not transmit a model. A lot of other methods adapt their model to the current file and need to transmit it in addition to the encoded data, because both the encoder and the decoder need to use the model.\nIn adaptive coding, the encoder and decoder are instead equipped with a predefined meta-model about how they will alter their models in response to the actual content of the data, and otherwise start with a blank slate, meaning that no initial model needs to be transmitted. As the data is transmitted, both encoder and decoder adapt their models, so that unless the character of the data changes radically, the model becomes better-adapted to the data its handling and compresses it more efficiently approaching the efficiency of the static coding.", "links": ["Beacon mode service", "Binary Golay code", "CCSDS 122.0-B-1", "Cassini-Huygens", "Command Loss Timer Reset", "Concatenated error correction code", "Consultative Committee for Space Data Systems", "Data compression", "Entropy encoding", "GMSK", "ICER", "JPEG", "JPEG 2000", "K band", "Ka band", "Ku band", "LZ77", "LZW", "Lossless data compression", "Low-density parity-check code", "Message Abstraction Layer", "OQPSK", "Performance Enhancing Proxy", "Phase-shift keying", "Proximity-1 Space Link Protocol", "QPSK", "Run length encoding", "S band", "Saturn", "Service-oriented architecture", "Solid-state drive", "Space Communications Protocol Specifications", "Spacecraft Monitoring & Control", "Turbo code", "X band"], "categories": ["All articles lacking sources", "Articles lacking sources from June 2009", "Lossless compression algorithms"], "title": "Adaptive coding"}
{"summary": "The Algorithm BSTW is a data compression algorithm, named after its designers, Bentley, Sleator, Tarjan and Wei in 1986. BSTW is a dictionary-based algorithm that uses a move-to-front transform to keep recently seen dictionary entries at the front of the dictionary. Dictionary references are then encoded using any of a number of encoding methods, usually Elias delta coding or Elias gamma coding.", "links": ["Algorithm", "Data compression", "Data structure", "Elias delta coding", "Elias gamma coding", "Move-to-front transform"], "categories": ["Algorithms and data structures stubs", "All articles needing additional references", "All stub articles", "Articles needing additional references from May 2008", "Computer science stubs", "Lossless compression algorithms"], "title": "Algorithm BSTW"}
{"summary": "Brotli is an open source data compression library developed by Jyrki Alakuijala and Zoltan Szabadka. Brotli is based on a modern variant of the LZ77 algorithm, Huffman coding and 2nd order context modeling. Replacing deflate with brotli typically gives an increase of 20% in compression density for text files, while compression and decompression speeds are roughly unchanged.\nThe first release of brotli in 2013 was built for off-line compression of web fonts. The version of brotli released in September 2015 has been extended to perform competitively in generic lossless data compression, with particular emphasis on use for HTTP compression. The encoder has been partly rewritten, the compression ratio improved, both the encoder and the decoder have been sped up, the streaming API was improved, more compression quality levels have been added, performance has been improved across platforms, decoding memory use has been reduced, and more use cases are taken into account.\nBrotli uses a pre-defined static dictionary of more than 13,000 strings to \"warm up\" its internal state. The dictionary contains common words, phrases and other substrings derived from a large corpus of text and HTML documents.\nStreams compressed with Brotli have the proposed content encoding type \"br\".\nLike zopfli, another compression algorithm from Google, brotli is named after a Swiss bakery product, br\u00f6tli.", "links": ["Algorithm", "Content encoding type", "Context modeling", "Data compression", "Data structure", "Deflate", "Google", "HTTP compression", "Huffman coding", "LZ77 and LZ78", "Lossless data compression", "Open source", "SDCH", "Spanisch Br\u00f6tli", "Web Open Font Format", "Zopfli"], "categories": ["Algorithms and data structures stubs", "All articles lacking reliable references", "All stub articles", "Articles lacking reliable references from September 2015", "Computer science stubs", "Free computer libraries", "Lossless compression algorithms", "Pages containing cite templates with deprecated parameters"], "title": "Brotli"}
{"summary": "The Burrows\u2013Wheeler transform (BWT, also called block-sorting compression) rearranges a character string into runs of similar characters. This is useful for compression, since it tends to be easy to compress a string that has runs of repeated characters by techniques such as move-to-front transform and run-length encoding. More importantly, the transformation is reversible, without needing to store any additional data. The BWT is thus a \"free\" method of improving the efficiency of text compression algorithms, costing only some extra computation.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "ArXiv", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Base pair", "Best, worst and average case", "Bijection", "Bijective", "Bit rate", "Bowtie (sequence analysis)", "Byte pair encoding", "Bzip2", "Cambridge University Press", "Canonical Huffman code", "ChIP-Seq", "Chain code", "Character string (computer science)", "Chen\u2013Fox\u2013Lyndon theorem", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "DEC Systems Research Center", "DEFLATE", "DNA", "DNA sequencing", "Data compression", "David Wheeler (British computer scientist)", "David Wheeler (computer scientist)", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Digital object identifier", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Eland (software)", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "End-of-file", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Genome", "Golomb coding", "Hash function", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "International Standard Book Number", "International Standard Serial Number", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Lexicographic order", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossy compression", "Lyndon word", "M. Lothaire", "Macroblock", "Maq", "Michael Burrows", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Next-generation sequencing", "Null character", "Nyquist\u2013Shannon sampling theorem", "Optimization (computer science)", "PAQ", "Palo Alto", "Peak signal-to-noise ratio", "Permutation", "Pixel", "Prediction by partial matching", "Pseudocode", "Psychoacoustics", "PubMed Central", "PubMed Identifier", "Pyramid (image processing)", "Python (programming language)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sorting", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Suffix array", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "Zentralblatt MATH", "\u039c-law algorithm"], "categories": ["Articles with example Python code", "Articles with example pseudocode", "Lossless compression algorithms", "Transforms"], "title": "Burrows\u2013Wheeler transform"}
{"summary": "Byte pair encoding or digram coding is a simple form of data compression in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur within that data. A table of the replacements is required to rebuild the original data. The algorithm was first described publicly by Philip Gage in a February 1994 article \"A New Algorithm for Data Compression\" in the C Users Journal.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossy compression", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Recursion", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "\u039c-law algorithm"], "categories": ["Lossless compression algorithms"], "title": "Byte pair encoding"}
{"summary": "bzip2 is a free and open-source file compression program that uses the Burrows\u2013Wheeler algorithm. It only compresses single files and is not a file archiver. It is developed and maintained by Julian Seward. Seward made the first public release of bzip2, version 0.15, in July 1996. The compressor's stability and popularity grew over the next several years, and Seward released version 1.0 in late 2000.", "links": [".NET Compact Framework", ".NET Framework", "3ivx", "7-Zip", "7z (file format)", "ACE (compression file format)", "ALZip", "ARC (file format)", "ARJ", "AROS Research Operating System", "ASCII", "Alternative terms for free software", "Android (operating system)", "Android application package", "Apache License", "Apache Software Foundation", "Apache Spark", "Apple Disk Image", "Apple Lossless", "Apple Public Source License", "Ar (Unix)", "Archive Utility", "Archive format", "Arithmetic coding", "Ark (software)", "Artistic License", "Audio Lossless Coding", "Audio compression (data)", "B1 (archive format)", "BSD-like license", "BSD license", "BSD licenses", "BZIP domain", "BagIt", "Base 1", "Basic For Qt", "Beerware", "Berkeley Software Distribution", "BetterZip", "Big data", "Bijective numeration", "Binary blob", "Bit array", "Blender Foundation", "Blu-code", "Boost Software License", "BulkZip", "Burrows\u2013Wheeler transform", "Bzip", "C++", "CC0", "CELT", "CFS (file format)", "Cabinet (file format)", "Canonical Huffman code", "CineForm", "Cinepak", "Commercial software", "Common Development and Distribution License", "Compact Pro", "Comparison of archive formats", "Comparison of audio coding formats", "Comparison of file archivers", "Comparison of free and open-source software licenses", "Comparison of open-source operating systems", "Comparison of open-source wireless drivers", "Comparison of open source and closed source", "Comparison of source code hosting facilities", "Comparison of video codecs", "Compress", "Compression program", "Computer file", "Contiki", "Contributor License Agreement", "Copyfree", "Copyleft", "CoreAVC", "Cpio", "Cross-platform", "DEFLATE", "DGCA (computing)", "DNxHD codec", "Daala", "Darwin (operating system)", "Data compression", "Deb (file format)", "Debian Free Software Guidelines", "Definition of Free Cultural Works", "Delta encoding", "Digital rights management", "Dirac (video compression format)", "DivX", "Doom WAD", "EAR (file format)", "ECos", "EGG (file format)", "EPUB", "Eclipse (software)", "Eclipse Foundation", "Eclipse Public License", "Executable compression", "FAAC", "FFV1", "FFmpeg", "FLAC", "File Roller", "File archiver", "File format", "Filename extension", "Filzip", "Fork (software development)", "Fraunhofer FDK AAC", "FreeArc", "FreeBASIC", "FreeBSD Foundation", "FreeDOS", "Free Pascal", "Free Software Foundation", "Free Software Foundation Europe", "Free Software Foundation Latin America", "Free Software Foundation of India", "Free Software Movement of India", "Free and open-source", "Free and open-source graphics device driver", "Free and open-source software", "Free license", "Free software", "Free software license", "Free software movement", "Freedesktop.org", "Freeware", "GNOME Foundation", "GNU", "GNU Compiler Collection", "GNU General Public License", "GNU Lesser General Public License", "GNU Privacy Guard", "GNU Project", "Gambas", "Gratis versus libre", "Gzip", "H.264/MPEG-4 AVC", "HDX4", "Hadoop", "Haiku (operating system)", "Haiku Applications", "Hardware restriction", "Helix (multimedia project)", "High Efficiency Video Coding", "History of Firefox", "History of Haiku (operating system)", "History of Linux", "History of Mozilla Application Suite", "History of Mozilla Thunderbird", "History of free and open-source software", "Huffman coding", "Huffyuv", "IOS", "ISC license", "Igor Pavlov (Computer programmer)", "Indeo", "Indonesia, Go Open Source", "Inferno (operating system)", "Info-ZIP", "Internet media type", "JAR (file format)", "JAR (software)", "Java (programming language)", "Java EE Connector Architecture", "Julia (programming language)", "Julian Seward", "KDE e.V.", "KGB Archiver", "Kilobyte", "L3enc", "LAME", "LBR (file format)", "LGPL", "LHA (file format)", "LLVM", "LZX (algorithm)", "Lagarith", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Welch", "Libavcodec", "License proliferation", "Linux", "Linux Foundation", "Linux distribution", "List of Unix programs", "List of archive formats", "List of file archivers", "List of formerly proprietary software", "List of free-software events", "List of free and open-source Android applications", "List of free and open-source iOS applications", "List of free and open-source software packages", "List of free software project directories", "List of free software web applications", "List of software categories", "Long-term support", "Lossless compression", "Lossy compression", "Lua (programming language)", "Lzip", "Lzop", "MINIX", "MIT License", "MIT license", "MPEG-4 Part 2", "MPQ", "MSU Lossless Video Codec", "Ma3bar", "MacBinary", "Mac OS", "Mac OS X", "Mach (kernel)", "Message Passing Interface", "Microsoft Open Specification Promise", "Monkey's Audio", "Mono (software)", "Move-to-front transform", "Move to front", "Mozilla", "Mozilla Corporation software rebranded by the Debian project", "Mozilla Foundation", "Mozilla Public License", "Musepack", "Nero AAC Codec", "Nero Digital", "NetBeans", "Open-source license", "Open-source software", "Open-source software development", "Open-source software security", "Open64", "OpenBSD Foundation", "OpenH264", "OpenSSH", "OpenSolaris", "Open Knowledge Foundation", "Open Packaging Conventions", "Open Source Geospatial Foundation", "Open Source Initiative", "Open eBook", "Open format", "Operating system", "OptimFROG", "Opus (audio format)", "Outline of free software", "PAQ", "PHP", "PKZIP", "POSIX Threads", "Pack (compression)", "Package (OS X)", "Package format", "PeaZip", "Perl", "Permissive free software licence", "Plan 9 from Bell Labs", "PowerArchiver", "Proprietary software", "Public domain", "Python (programming language)", "Qt (software)", "Quadruple D", "QuickTime", "RAR (file format)", "ROSE (compiler framework)", "RPM Package Manager", "ReactOS", "Revolution OS", "Rob Landley", "Ruby (programming language)", "Run-length encoding", "Rzip", "SCO\u2013Linux controversies", "SQX", "SQ (program)", "Shar", "Shared source", "Shorten (file format)", "Silverlight", "Smalltalk", "Smart Bitrate Control", "Snappy (software)", "Software Freedom Conservancy", "Software Freedom Law Center", "Software Package Data Exchange", "Software developer", "Software in the Public Interest", "Software license", "Software patent", "Software patents and free software", "Software release life cycle", "Sorenson codec", "Speex", "StuffIt", "StuffIt Expander", "Symbian Foundation", "Symmetric multiprocessing", "TTA (codec)", "TUGZip", "Tar (computing)", "Tar (file format)", "Tcl", "The Cathedral and the Bazaar", "The Document Foundation", "The Free Software Definition", "The Open Source Definition", "The Unarchiver", "Theora", "Thread (computer science)", "TooLAME", "Trusted Computing", "Type code", "UEFI Secure Boot", "UHARC", "UPX", "Ubuntu Foundation", "Uniform Type Identifier", "Unix philosophy", "VP7", "VP8", "VideoLAN", "Video compression", "Viral license", "Vorbis", "WAR (file format)", "WTFPL", "WavPack", "Wayback Machine", "Wikimedia Foundation", "WinAce", "WinRAR", "WinZip", "Windows Installer", "Windows Media Encoder", "Windows Phone", "X.Org Foundation", "X264", "X265", "XMPP Standards Foundation", "XZ Utils", "Xamarin", "Xar (archiver)", "Xarchiver", "Xbox 360", "Xiph.Org Foundation", "Xvid", "Xz", "YULS", "ZIP (file format)", "ZPAQ", "ZipGenius", "Zip (file format)", "Zipeg", "Zlib License", "Zoo (file format)"], "categories": ["1996 software", "All articles containing potentially dated statements", "All articles with specifically marked weasel-worded phrases", "All articles with unsourced statements", "Archive formats", "Articles containing potentially dated statements from May 2010", "Articles with specifically marked weasel-worded phrases from February 2014", "Articles with unsourced statements from February 2014", "Cross-platform software", "Free data compression software", "Lossless compression algorithms", "Unix archivers and compression-related utilities", "Use dmy dates from August 2012"], "title": "Bzip2"}
{"summary": "A canonical Huffman code is a particular type of Huffman code with unique properties which allow it to be described in a very compact manner.\nData compressors generally work in one of two ways. Either the decompressor can infer what codebook the compressor has used from previous context, or the compressor must tell the decompressor what the codebook is. Since a canonical Huffman codebook can be stored especially efficiently, most compressors start by generating a \"normal\" Huffman codebook, and then convert it to canonical Huffman before using it.\nIn order for a symbol code scheme such as the Huffman code to be decompressed, the same model that the encoding algorithm used to compress the source data must be provided to the decoding algorithm so that it can use it to decompress the encoded data. In standard Huffman coding this model takes the form of a tree of variable-length codes, with the most frequent symbols located at the top of the structure and being represented by the fewest number of bits.\nHowever, this code tree introduces two critical inefficiencies into an implementation of the coding scheme. Firstly, each node of the tree must store either references to its child nodes or the symbol that it represents. This is expensive in memory usage and if there is a high proportion of unique symbols in the source data then the size of the code tree can account for a significant amount of the overall encoded data. Secondly, traversing the tree is computationally costly, since it requires the algorithm to jump randomly through the structure in memory as each bit in the encoded data is read in.\nCanonical Huffman codes address these two issues by generating the codes in a clear standardized format; all the codes for a given length are assigned their values sequentially. This means that instead of storing the structure of the code tree for decompression only the lengths of the codes are required, reducing the size of the encoded data. Additionally, because the codes are sequential, the decoding algorithm can be dramatically simplified so that it is computationally efficient.", "links": ["8-bit", "A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Alphabetical", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Binary numeral system", "Bit", "Bit-length", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Codebook", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Huffman code", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "JPEG File Interchange Format", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "Kraft's inequality", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Logical shift", "Lossless compression", "Lossy compression", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Pseudo code", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Radix point", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Symbol code", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Value (computer science)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "\u039c-law algorithm"], "categories": ["All Wikipedia articles needing context", "All articles lacking in-text citations", "All articles needing expert attention", "All articles that are too technical", "All pages needing cleanup", "Articles lacking in-text citations from March 2014", "Articles needing expert attention from June 2011", "Coding theory", "Lossless compression algorithms", "Wikipedia articles needing context from June 2011", "Wikipedia articles that are too technical from June 2011", "Wikipedia introduction cleanup from June 2011"], "title": "Canonical Huffman code"}
{"summary": "A chain code is a lossless compression algorithm for monochrome images. The basic principle of chain codes is to separately encode each connected component, or \"blob\", in the image.\nFor each such region, a point on the boundary is selected and its coordinates are transmitted. The encoder then moves along the boundary of the region and, at each step, transmits a symbol representing the direction of this movement.\nThis continues until the encoder returns to the starting position, at which point the blob has been completely described, and encoding continues with the next blob in the image.\nThis encoding method is particularly effective for images consisting of a reasonably small number of large connected components.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression algorithm", "Compression artifact", "Connected component (topology)", "Constant bitrate", "Context tree weighting", "Convolution", "Crack code (Image Processing)", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Digital object identifier", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Herbert Freeman", "Huffman coding", "Image", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless", "Lossless compression", "Lossy compression", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Monochrome", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "\u039c-law algorithm"], "categories": ["Image compression", "Lossless compression algorithms"], "title": "Chain code"}
{"summary": "The context tree weighting method (CTW) is a lossless compression and prediction algorithm by Willems, Shtarkov & Tjalkens 1995. The CTW algorithm is among the very few such algorithms that offer both theoretical guarantees and good practical performance (see, e.g. Begleiter, El-Yaniv & Yona 2004). The CTW algorithm is an \u201censemble method,\u201d mixing the predictions of many underlying variable order Markov models, where each such model is constructed using zero-order conditional probability estimators.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Computer science", "Constant bitrate", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Journal of Artificial Intelligence Research", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossy compression", "Macroblock", "Markov model", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "\u039c-law algorithm"], "categories": ["All stub articles", "Computer science stubs", "Lossless compression algorithms"], "title": "Context tree weighting"}
{"summary": "Context-adaptive binary arithmetic coding (CABAC) is a form of entropy encoding used in the H.264/MPEG-4 AVC and High Efficiency Video Coding (HEVC) standards. It is a lossless compression technique, although the video coding standards in which it is used are typically for lossy compression applications. CABAC is notable for providing much better compression than most other entropy encoding algorithms used in video encoding, and it is one of the key elements that provides the H.264/AVC encoding scheme with better compression capability than its predecessors.\nIn H.264/MPEG-4 AVC, CABAC is only supported in the Main and higher profiles of the standard, as it requires a larger amount of processing to decode than the simpler scheme known as context-adaptive variable-length coding (CAVLC) that is used in the standard's Baseline profile. CABAC is also difficult to parallelize and vectorize, so other forms of parallelism (such as spatial region parallelism) may be coupled with its use. In HEVC, CABAC is used in all profiles of the standard.", "links": ["Arithmetic coding", "Binary numeral system", "Bit rate", "CAVLC", "Context-adaptive variable-length coding", "Data compression", "Entropy encoding", "H.264/MPEG-4 AVC", "High Efficiency Video Coding", "Lossless compression", "Lossy compression", "Probability", "Quantization (signal processing)"], "categories": ["Lossless compression algorithms", "MPEG", "Video compression"], "title": "Context-adaptive binary arithmetic coding"}
{"summary": "In computing, deflate is a data compression algorithm that uses a combination of the LZ77 algorithm and Huffman coding. It was originally defined by Phil Katz for version 2 of his PKZIP archiving tool and was later specified in RFC 1951.\nThe original algorithm as designed by Katz was patented as U.S. Patent 5,051,745 and assigned to PKWARE, Inc. As stated in the RFC document, Deflate is widely thought to be implementable in a manner not covered by patents. This has led to its widespread use, for example in gzip compressed files, PNG image files and the .ZIP file format for which Katz originally designed it.", "links": [".NET Compact Framework", ".NET Framework", "7-Zip", "A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "AdvanceCOMP", "Algebraic code-excited linear prediction", "Algorithm", "Android (operating system)", "Apache HTTP Server", "Apache license", "Application-specific integrated circuit", "Arithmetic coding", "Assembly language", "Audio codec", "Audio compression (data)", "Average bitrate", "BSD license", "Bit", "Bit rate", "Bitstream", "Borland", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Bzip2", "C++", "C (programming language)", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Commodore 128", "Commodore 64", "Common Lisp", "Companding", "Comparison of file archivers", "Compression artifact", "Computing", "Constant bitrate", "Context tree weighting", "Convolution", "Crypto++", "Data compression", "Debian Free Software Guidelines", "Deblocking filter", "DeflOpt", "Deflate", "Deflation (disambiguation)", "Delta encoding", "Device driver", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "FPGA", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "GIMP", "GNU General Public License", "GNU Lesser General Public License", "Golomb coding", "Gzip", "Huffman coding", "IOS", "Image compression", "Image resolution", "Information theory", "Interlaced video", "International Standard Book Number", "Internet Engineering Task Force", "Java (programming language)", "Javascript (programming language)", "Karhunen\u2013Lo\u00e8ve theorem", "Ken Silverman", "Kolmogorov complexity", "L. Peter Deutsch", "LGPL", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Linux (kernel)", "List of archive formats", "List of file archivers", "Load balancer", "Log area ratio", "Lossless compression", "Lossy compression", "Lua (programming language)", "M68000", "MIT License", "MOS Technology 6502", "MSX", "Mac OS", "Macroblock", "Microsoft .NET Framework", "Microsoft Windows", "Modified Huffman coding", "Modified discrete cosine transform", "Mono (software)", "Motion compensation", "Move-to-front transform", "Multiple-image Network Graphics", "Nyquist\u2013Shannon sampling theorem", "OpenSolaris", "PAQ", "PCI-X", "PCI Local Bus", "PCIe", "PDP-11 architecture", "PKWARE, Inc.", "PKZIP", "PKZip", "PNGOUT", "Pascal (programming language)", "Peak signal-to-noise ratio", "Phil Katz", "Pixel", "Plan 9 from Bell Labs", "Portable Network Graphics", "Prediction by partial matching", "Psychoacoustics", "PuTTY", "Public Domain", "Pyramid (image processing)", "Python (programming language)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Red Gate Software", "Redundancy (information theory)", "Reference (computer science)", "Run-length encoding", "SAM Coup\u00e9", "Sampling (signal processing)", "Seed7", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Silverlight", "Sliding scale", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Storage area network", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Virtex (FPGA)", "Vulnerability (computing)", "Warped linear predictive coding", "Wavelet compression", "Windows Phone", "Xamarin", "Xbox 360", "Xilinx", "Z80", "ZIP (file format)", "ZIP file format", "Zlib", "Zlib License", "Zopfli", "\u039c-law algorithm"], "categories": ["All articles needing additional references", "Articles needing additional references from January 2009", "Lossless compression algorithms"], "title": "DEFLATE"}
{"summary": "A dictionary coder, also sometimes known as a substitution coder, is a class of lossless data compression algorithms which operate by searching for matches between the text to be compressed and a set of strings contained in a data structure (called the 'dictionary') maintained by the encoder. When the encoder finds such a match, it substitutes a reference to the string's position in the data structure.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Circular buffer", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Concordance (publishing)", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Data structure", "Deblocking filter", "Delta encoding", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 (algorithm)", "LZ77 and LZ78", "LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossless data compression", "Lossy compression", "Macroblock", "Mobile app", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Personal digital assistant", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "String (computer science)", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "\u039c-law algorithm"], "categories": ["All articles needing additional references", "Articles needing additional references from September 2014", "Lossless compression algorithms"], "title": "Dictionary coder"}
{"summary": "Dynamic Markov compression (DMC) is a lossless data compression algorithm developed by Gordon Cormack and Nigel Horspool. It uses predictive arithmetic coding similar to prediction by partial matching (PPM), except that the input is predicted one bit at a time (rather than one byte at a time). DMC has a good compression ratio and moderate speed, similar to PPM, but requires somewhat more memory and is not widely implemented. Some recent implementations include the experimental compression programs hook by Nania Francesco Antonio, ocamyd by Frank Schwellinger, and as a submodel in paq8l by Matt Mahoney. These are based on the 1993 implementation in C by Gordon Cormack.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Claude Shannon", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context mixing", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Gordon Cormack", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossy compression", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nigel Horspool", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "\u039c-law algorithm"], "categories": ["Lossless compression algorithms", "Markov models"], "title": "Dynamic Markov compression"}
{"summary": "Embedded Zerotrees of Wavelet transforms (EZW) is a lossy image compression algorithm. At low bit rates, i.e. high compression ratios, most of the coefficients produced by a subband transform (such as the wavelet transform) will be zero, or very close to zero. This occurs because \"real world\" images tend to contain mostly low frequency information (highly correlated). However where high frequency information does occur (such as edges in the image) this is particularly important in terms of human perception of the image quality, and thus must be represented accurately in any high quality coding scheme.\nBy considering the transformed coefficients as a tree (or trees) with the lowest frequency coefficients at the root node and with the children of each tree node being the spatially related coefficients in the next higher frequency subband, there is a high probability that one or more subtrees will consist entirely of coefficients which are zero or nearly zero, such subtrees are called zerotrees. Due to this, we use the terms node and coefficient interchangeably, and when we refer to the children of a coefficient, we mean the child coefficients of the node in the tree where that coefficient is located. We use children to refer to directly connected nodes lower in the tree and descendants to refer to all nodes which are below a particular node in the tree, even if not directly connected.\nIn zerotree based image compression scheme such as EZW and SPIHT, the intent is to use the statistical properties of the trees in order to efficiently code the locations of the significant coefficients. Since most of the coefficients will be zero or close to zero, the spatial locations of the significant coefficients make up a large portion of the total size of a typical compressed image. A coefficient (likewise a tree) is considered significant if its magnitude (or magnitudes of a node and all its descendants in the case of a tree) is above a particular threshold. By starting with a threshold which is close to the maximum coefficient magnitudes and iteratively decreasing the threshold, it is possible to create a compressed representation of an image which progressively adds finer detail. Due to the structure of the trees, it is very likely that if a coefficient in a particular frequency band is insignificant, then all its descendants (the spatially related higher frequency band coefficients) will also be insignificant.\nEZW uses four symbols to represent (a) a zerotree root, (b) an isolated zero (a coefficient which is insignificant, but which has significant descendants), (c) a significant positive coefficient and (d) a significant negative coefficient. The symbols may be thus represented by two binary bits. The compression algorithm consists of a number of iterations through a dominant pass and a subordinate pass, the threshold is updated (reduced by a factor of two) after each iteration. The dominant pass encodes the significance of the coefficients which have not yet been found significant in earlier iterations, by scanning the trees and emitting one of the four symbols. The children of a coefficient are only scanned if the coefficient was found to be significant, or if the coefficient was an isolated zero. The subordinate pass emits one bit (the most significant bit of each coefficient not so far emitted) for each coefficient which has been found significant in the previous significance passes. The subordinate pass is therefore similar to bit-plane coding.\nThere are several important features to note. Firstly, it is possible to stop the compression algorithm at any time and obtain an approximation of the original image, the greater the number of bits received, the better the image. Secondly, due to the way in which the compression algorithm is structured as a series of decisions, the same algorithm can be run at the decoder to reconstruct the coefficients, but with the decisions being taken according to the incoming bit stream. In practical implementations, it would be usual to use an entropy code such as arithmetic code to further improve the performance of the dominant pass. Bits from the subordinate pass are usually random enough that entropy coding provides no further coding gain.\nThe coding performance of EZW has since been exceeded by SPIHT and its many derivatives.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Huffman coding", "IEEE", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossy compression", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "SPIHT", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tree (graph theory)", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "Wavelet transform", "\u039c-law algorithm"], "categories": ["Commons category with local link same as on Wikidata", "Image compression", "Lossless compression algorithms", "Trees (data structures)", "Wavelets"], "title": "Embedded Zerotrees of Wavelet transforms"}
{"summary": "FELICS, which stands for Fast Efficient & Lossless Image Compression System, is a lossless image compression algorithm that performs 5-times faster than the original lossless JPEG codec and achieves a similar compression ratio.", "links": ["Binary code", "Causal", "Comparison of graphics file formats", "Data compression ratio", "Decorrelation", "Entropy encoding", "Exchangeable image file format", "Exif", "Felix (disambiguation)", "GIMP", "Geometric distribution", "HiRISE", "Image compression", "Image file formats", "JPEG-LS", "Jeffrey Vitter", "Lossless JPEG", "Lossless data compression", "Portable Network Graphics", "Rice code"], "categories": ["All articles needing additional references", "Articles needing additional references from August 2011", "Lossless compression algorithms", "Lossy compression algorithms", "Use dmy dates from July 2013"], "title": "FELICS"}
{"summary": "In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Ph.D. student at MIT, and published in the 1952 paper \"A Method for the Construction of Minimum-Redundancy Codes\".\nThe output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in linear time to the number of input weights if these weights are sorted. However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods.", "links": ["A-law algorithm", "ASCII", "A Mathematical Theory of Communication", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Alan Tucker", "Algebraic code-excited linear prediction", "Arithmetic coding", "Array data type", "Audio codec", "Audio compression (data)", "Average bitrate", "Bernoulli process", "Big O notation", "Binary search tree", "Binary tree", "Bit rate", "Block code", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Charles E. Leiserson", "Chroma subsampling", "Claude Shannon", "Clifford Stein", "Code-excited linear prediction", "Codec", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Computer science", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "DEFLATE (algorithm)", "Data compression", "David A. Huffman", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Digital object identifier", "Discrete cosine transform", "Display resolution", "Doctor of Philosophy", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exam", "Expected value", "Exponential-Golomb coding", "Fax machines", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Frequency table", "Golomb coding", "Greedy algorithm", "Group 4 compression", "Huffyuv", "Image compression", "Image resolution", "Independent and identically distributed", "Information entropy", "Information theory", "Interlaced video", "Internal node", "Introduction to Algorithms", "JPEG", "JSTOR", "Jan van Leeuwen", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Leaf node", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear-time", "Linear predictive coding", "Linear time", "Linearithmic", "Log area ratio", "Lossless compression", "Lossless data compression", "Lossy compression", "MIT", "MP3", "Macroblock", "Massachusetts Institute of Technology", "Modified Huffman coding", "Modified discrete cosine transform", "Monoid", "Morse code", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "PKZIP", "Package-merge algorithm", "Patent", "Peak signal-to-noise ratio", "Pixel", "Polynomial time", "Power of two", "Prediction by partial matching", "Prefix code", "Priority queue", "Probability mass function", "Proceedings of the IRE", "Proportionality (mathematics)", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Queue (data structure)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Robert M. Fano", "Ronald L. Rivest", "Run-length encoding", "Sampling (signal processing)", "Scientific American", "Set partitioning in hierarchical trees", "Shannon's source coding theorem", "Shannon-Fano coding", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "T. C. Hu", "Term paper", "Thomas H. Cormen", "Timeline of information theory", "Total order", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable-length code", "Variable bitrate", "Varicode", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "Weighted path length from the root", "\u039c-law algorithm"], "categories": ["1952 in computer science", "All articles lacking in-text citations", "Articles lacking in-text citations from January 2011", "Binary trees", "Commons category with local link same as on Wikidata", "Lossless compression algorithms", "Wikipedia articles needing clarification from February 2012"], "title": "Huffman coding"}
{"summary": "Huffyuv (or HuffYUV) is a lossless video codec created by Ben Rudiak-Gould which is meant to replace uncompressed YCbCr as a video capture format. The codec can also compress in the RGB color space.\n\"Lossless\" means that the output from the decompressor is bit-for-bit identical with the original input to the compressor. Lossless only occurs when the compression color space matches the input and output color space. When the color spaces do not match, a low loss compression is performed.\nHuffyuv's algorithm is similar to that of lossless JPEG, in that it predicts each sample and then Huffman-encodes the error.", "links": [".m2ts", "3GP and 3G2", "3ivx", "7-Zip", "A-law algorithm", "AAC-LD", "ALZip", "AMV video format", "APNG", "ARC (file format)", "ARJ", "Adaptive Huffman coding", "Adaptive Multi-Rate Wideband", "Adaptive Multi-Rate audio codec", "Adaptive Transform Acoustic Coding", "Advanced Audio Coding", "Advanced Systems Format", "Algebraic code-excited linear prediction", "Alliance for Open Media", "Apple Lossless", "Apple Video", "Archive Utility", "Ark (software)", "Asao (codec)", "Au file format", "Audio Interchange File Format", "Audio Lossless Coding", "Audio Video Interleave", "Audio Video Standard", "Audio compression (data)", "Avid Audio", "BMP file format", "Ben Rudiak-Gould", "BetterZip", "Better Portable Graphics", "Bink Video", "Blu-code", "BulkZip", "Bzip2", "CELT", "CineForm", "Cinema Craft Encoder", "Cinepak", "Code-excited linear prediction", "Codec2", "Commercial software", "Comparison of audio coding formats", "Comparison of file archivers", "Comparison of video codecs", "Compress", "CoreAVC", "DNxHD codec", "DTS (sound system)", "DV", "Daala", "Data compression", "Digital Item", "Digital Video Interactive", "Digital container format", "Dirac (video compression format)", "DivX", "DjVu", "Dolby Digital", "Dynamic Resolution Adaptation", "Enhanced VOB", "Enhanced Variable Rate Codec", "Enhanced Variable Rate Codec B", "Enhanced full rate", "Executable compression", "Extended Adaptive Multi-Rate \u2013 Wideband", "FAAC", "FFV1", "FFmpeg", "FLAC", "Ffdshow", "File Roller", "Filzip", "Flash Video", "Fork (software development)", "Fraunhofer FDK AAC", "FreeArc", "Free Lossless Image Format", "Free software", "Freeware", "Full Rate", "G.711", "G.718", "G.719", "G.722", "G.722.1", "G.723", "G.723.1", "G.726", "G.728", "G.729", "G.729.1", "GIF", "GNU General Public License", "General Exchange Format", "Group 4 compression", "Gzip", "H.120", "H.261", "H.262/MPEG-2 Part 2", "H.263", "H.264/MPEG-4 AVC", "HDX4", "Haiku Applications", "Half Rate", "Harmonic Vector Excitation Coding", "Helix (multimedia project)", "High-Efficiency Advanced Audio Coding", "High Efficiency Video Coding", "Huffman coding", "ICER", "IETF", "ISO base media file format", "ITU-T", "Image compression", "Indeo", "Info-ZIP", "Interchange File Format", "International Electrotechnical Commission", "International Organization for Standardization", "Internet Low Bitrate Codec", "Internet Speech Audio Codec", "JAR (software)", "JBIG", "JBIG2", "JPEG", "JPEG 2000", "JPEG XR", "KGB Archiver", "L3enc", "LAME", "LHA (file format)", "Lagarith", "Libavcodec", "Linux", "List of codecs", "Lossless", "Lossless JPEG", "Lossless compression", "Lossy compression", "Lzip", "Lzop", "MOD and TOD", "MP3", "MPEG-1", "MPEG-1 Audio Layer I", "MPEG-1 Audio Layer II", "MPEG-2", "MPEG-4", "MPEG-4 Part 14", "MPEG-4 Part 2", "MPEG-4 SLS", "MPEG-H", "MPEG-H 3D Audio", "MPEG-LA", "MPEG Multichannel", "MPEG Surround", "MPEG media transport", "MPEG program stream", "MPEG transport stream", "MPlayer", "MSU Lossless Video Codec", "MT9", "MacBinary", "Mac OS X", "Material Exchange Format", "Matroska", "Microsoft Video 1", "Monkey's Audio", "Motion JPEG", "Motion JPEG 2000", "Multimedia", "Multiple-image Network Graphics", "Musepack", "NETVC", "Nero AAC Codec", "Nero Digital", "OMS Video", "Ogg", "OpenEXR", "OpenH264", "OptimFROG", "Opus (audio format)", "Original Sound Quality", "PAQ", "PKZIP", "PPM compression algorithm", "Pack (compression)", "PeaZip", "Pixlet", "Portable Network Graphics", "PowerArchiver", "ProRes 422", "ProRes 4444", "Progressive Graphics File", "Qualcomm code-excited linear prediction", "QuickTime", "QuickTime Animation", "QuickTime File Format", "QuickTime Graphics", "QuickTime VR", "RGB", "RTAudio", "RTVideo", "RatDVD", "RealAudio", "RealMedia", "RealVideo", "Relaxed code-excited linear prediction", "Resource Interchange File Format", "Rzip", "SILK", "SVOPC", "Selectable Mode Vocoder", "SheerVideo", "Shorten (file format)", "Siren (codec)", "Smacker video", "Smart Bitrate Control", "Snappy (software)", "Sorenson codec", "Speex", "StuffIt", "StuffIt Expander", "Super Audio CD", "TIFF/EP", "TTA (codec)", "TUGZip", "Tagged Image File Format", "Tar (computing)", "The Unarchiver", "Theora", "TooLAME", "TwinVQ", "UPX", "Unified Speech and Audio Coding", "VC-1", "VLC media player", "VOB", "VP3", "VP6", "VP7", "VP8", "VP9", "Variable-Rate Multimode Wideband", "Vector sum excited linear prediction", "Video codec", "Video compression", "Video for Windows", "Vorbis", "WAV", "WavPack", "WebM", "WebP", "WinAce", "WinRAR", "WinZip", "Windows Media Audio", "Windows Media Encoder", "Windows Media Video (video compression format)", "Wireless Application Protocol Bitmap Format", "X264", "X265", "XZ Utils", "Xarchiver", "Xvid", "YCbCr", "YULS", "ZPAQ", "ZipGenius", "Zipeg", "\u039c-law algorithm"], "categories": ["Free video codecs", "Lossless compression algorithms"], "title": "Huffyuv"}
{"summary": "An incompressible string is one that cannot be compressed because it lacks sufficient repeating sequences. Whether a string is compressible will often depend on the algorithm being used. Some strings are incompressible by any algorithm \u2014 see Kolmogorov complexity.", "links": ["Algorithm", "Data compression", "Kolmogorov complexity", "Lookup table", "Redundancy (information theory)", "String (computer science)"], "categories": ["All articles lacking sources", "Articles lacking sources from December 2009", "Lossless compression algorithms", "String (computer science)"], "title": "Incompressible string"}
{"summary": "Incremental encoding, also known as front compression, back compression, or front coding, is a type of delta encoding compression algorithm whereby common prefixes or suffixes and their lengths are recorded so that they need not be duplicated. This algorithm is particularly well-suited for compressing sorted data, e.g., a list of words from a dictionary.\nFor example:\nThe encoding used to store the common prefix length itself varies from application to application. Typical techniques are storing the value as a single byte; delta encoding, which stores only the change in the common prefix length; and various universal codes. It may be combined with other general lossless data compression techniques such as entropy encoding and dictionary coders to compress the remaining suffixes.", "links": ["Affix", "Bigram", "Compression algorithm", "Computer storage", "Data", "Delta encoding", "Dictionary", "Dictionary coder", "Entropy encoding", "GNU locate", "Index (search engine)", "Lossless data compression", "Prefix (linguistics)", "Software", "Sorting", "Universal code (data compression)", "Word"], "categories": ["All stub articles", "Database index techniques", "Lossless compression algorithms", "Storage software stubs"], "title": "Incremental encoding"}
{"summary": "The Lempel\u2013Ziv\u2013Markov chain algorithm (LZMA) is an algorithm used to perform lossless data compression. It has been under development either since 1996 or 1998 and was first used in the 7z format of the 7-Zip archiver. This algorithm uses a dictionary compression scheme somewhat similar to the LZ77 algorithm published by Abraham Lempel and Jacob Ziv in 1977 and features a high compression ratio (generally higher than bzip2)) and a variable compression-dictionary size (up to 4 GB), while still maintaining decompression speed similar to other commonly used compression algorithms.\nLZMA2 is a simple container format that can include both uncompressed data and LZMA data, possibly with multiple different LZMA encoding parameters. LZMA2 supports arbitrarily scalable multithreaded compression and decompression and efficient compression of data which is partially incompressible.", "links": [".NET Compact Framework", ".NET Framework", "7-Zip", "7-zip", "7z", "7z (file format)", "A-law algorithm", "ACE (compression file format)", "ARC (file format)", "ARJ", "Abraham Lempel", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Android (operating system)", "Android application package", "Apple Disk Image", "Ar (Unix)", "Archive format", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "B1 (archive format)", "BagIt", "Binary tree", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Bzip2", "C++", "CFS (file format)", "C (programming language)", "C Sharp (programming language)", "Cabinet (file format)", "Canonical Huffman code", "Central processing unit", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Common Public License", "Compact Pro", "Companding", "Comparison of archive formats", "Compress", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "Cpio", "DEFLATE", "DGCA (computing)", "Data compression", "Deb (file format)", "Debian", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Doom WAD", "Dpkg", "Dynamic Markov compression", "Dynamic programming", "Dynamic range", "EAR (file format)", "EGG (file format)", "EPUB", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Embedded system", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fedora", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "GHz", "GNU Lesser General Public License", "Gigabyte", "Go (programming language)", "Golomb coding", "Gzip", "Hash chain", "Huffman coding", "IOS", "Igor Pavlov (programmer)", "Image compression", "Image resolution", "Information theory", "Interlaced video", "International Standard Book Number", "JAR (file format)", "Jacob Ziv", "Java (programming language)", "Java EE Connector Architecture", "KGB Archiver", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LBR (file format)", "LHA (file format)", "LZ4 (compression algorithm)", "LZ77", "LZ77 and LZ78", "LZHAM", "LZJB", "LZMA", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Linux kernel", "List of archive formats", "Log area ratio", "Lossless compression", "Lossless data compression", "Lossy compression", "Lzip", "Lzop", "MPQ", "Mac OS", "Macroblock", "Markov chains", "Martin Airport (Slovakia)", "Megabyte per second", "Modified Huffman coding", "Modified discrete cosine transform", "Mono (software)", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "Open Packaging Conventions", "Open eBook", "Open source", "PAQ", "Package (OS X)", "Package format", "Pascal (programming language)", "Patricia trie", "PeaZip", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Public domain", "Pyramid (image processing)", "Python (programming language)", "Quadruple D", "Quantization (image processing)", "Quantization (signal processing)", "RAR (file format)", "RPM", "RPM Package Manager", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Rzip", "SQX", "SQ (program)", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Shar", "Silverlight", "Sliding window", "Sound quality", "SourceForge", "Sourceforge", "Speech coding", "Springer Publishing", "Standard test image", "Statistical Lempel\u2013Ziv", "StuffIt", "Sub-band coding", "Tar (computing)", "Thread (computer science)", "Timeline of information theory", "Tunstall coding", "UHARC", "Unary coding", "Universal code (data compression)", "Unix-like", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "WAR (file format)", "Warped linear predictive coding", "Wavelet compression", "Windows Installer", "Windows Phone", "XZ Utils", "Xamarin", "Xar (archiver)", "Xbox 360", "Xz", "ZIP (file format)", "ZPAQ", "Zip (file format)", "Zoo (file format)", "\u039c-law algorithm"], "categories": ["All articles covered by WikiProject Wikify", "All articles needing additional references", "All articles needing style editing", "All articles that may contain original research", "All articles with unsourced statements", "All pages needing cleanup", "Articles covered by WikiProject Wikify from July 2014", "Articles needing additional references from July 2010", "Articles that may contain original research from April 2012", "Articles with unsourced statements from June 2013", "Lossless compression algorithms", "Wikipedia articles needing style editing from July 2014", "Wikipedia introduction cleanup from July 2014"], "title": "Lempel\u2013Ziv\u2013Markov chain algorithm"}
{"summary": "Lempel\u2013Ziv\u2013Oberhumer (LZO) is a lossless data compression algorithm that is focused on decompression speed.", "links": ["A-law algorithm", "ANSI C", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Arithmetic coding", "Atari TOS", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Btrfs", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "Copyright", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Free software", "GNU General Public License", "GitHub", "Golomb coding", "Huffman coding", "IBM AIX (operating system)", "IRIX", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Java (programming language)", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Linux", "Log area ratio", "Lossless", "Lossless compression", "Lossy compression", "Lzop", "Mac OS", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nintendo 64", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Palm OS", "Peak signal-to-noise ratio", "Perl", "Pixel", "PlayStation", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Python (programming language)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Software", "Solaris (operating system)", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "SunOS", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "VxWorks", "Warped linear predictive coding", "Wavelet compression", "Wayback Machine", "Wii", "Win32", "Zfs", "\u039c-law algorithm"], "categories": ["All articles lacking reliable references", "All articles needing additional references", "All stub articles", "Articles lacking reliable references from March 2015", "Articles needing additional references from July 2014", "C libraries", "Free data compression software", "Lossless compression algorithms", "Software stubs"], "title": "Lempel\u2013Ziv\u2013Oberhumer"}
{"summary": "Lempel\u2013Ziv\u2013Stac (LZS, or Stac compression) is a lossless data compression algorithm that uses a combination of the LZ77 sliding-window compression algorithm and fixed Huffman coding. It was originally developed by Stac Electronics for tape compression, and subsequently adapted for hard disk compression and sold as the Stacker disk compression software. It was later specified as a compression algorithm for various network protocols. LZS is specified in the Cisco IOS stack.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Cisco IOS", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "DoubleSpace", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Hard disk compression", "Hifn", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossless data compression", "Lossy compression", "MS-DOS 6.0", "Macroblock", "Microsoft Point-to-Point Compression", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Stac Electronics", "Stac v. Microsoft", "Stacker (disk compression)", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "\u039c-law algorithm"], "categories": ["Lossless compression algorithms"], "title": "Lempel\u2013Ziv\u2013Stac"}
{"summary": "Lempel\u2013Ziv\u2013Storer\u2013Szymanski (LZSS) is a lossless data compression algorithm, a derivative of LZ77, that was created in 1982 by James Storer and Thomas Szymanski. LZSS was described in article \"Data compression via textual substitution\" published in Journal of the ACM (pp. 928\u2013951).\nLZSS is a dictionary encoding technique. It attempts to replace a string of symbols with a reference to a dictionary location of the same string.\nThe main difference between LZ77 and LZSS is that in LZ77 the dictionary reference could actually be longer than the string it was replacing. In LZSS, such references are omitted if the length is less than the \"break even\" point. Furthermore, LZSS uses one-bit flags to indicate whether the next chunk of data is a literal (byte) or a reference to an offset/length pair.", "links": ["A-law algorithm", "ARJ", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Allegro library", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Digital object identifier", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Game Boy Advance", "Golomb coding", "Green Eggs and Ham", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Journal of the ACM", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LHarc", "LZ4 (compression algorithm)", "LZ77", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossless data compression", "Lossy compression", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "PKZip", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "RAR (file format)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Thomas Szymanski", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "Zoo (file format)", "\u039c-law algorithm"], "categories": ["Lossless compression algorithms"], "title": "Lempel\u2013Ziv\u2013Storer\u2013Szymanski"}
{"summary": "Lempel\u2013Ziv\u2013Welch (LZW) is a universal lossless data compression algorithm created by Abraham Lempel, Jacob Ziv, and Terry Welch. It was published by Welch in 1984 as an improved implementation of the LZ78 algorithm published by Lempel and Ziv in 1978. The algorithm is simple to implement, and has the potential for very high throughput in hardware implementations. It is the algorithm of the widely used Unix file compression utility compress, and is used in the GIF image format.", "links": ["A-law algorithm", "Abraham Lempel", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Adobe Acrobat Reader", "Algebraic code-excited linear prediction", "Algorithm", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Binary-to-text encoding", "Bit", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compress", "Compression artifact", "Concatenation", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Data compression ratio", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Digital object identifier", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "English language", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "GIF", "Golomb coding", "Graphics Interchange Format", "Gzip", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "International Business Machines", "Jacob Ziv", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel-Ziv-Markov chain algorithm", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossless data compression", "Lossy compression", "Macroblock", "Mark N. Wegman", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "PDF", "Patent", "Peak signal-to-noise ratio", "Pixel", "Portable Network Graphics", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Sperry Corporation", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "TIFF", "Terry Welch", "Timeline of information theory", "Tunstall coding", "Unary coding", "Uncompress", "Unisys", "United States", "Universal code (data compression)", "Unix", "Variable bitrate", "Victor S. Miller", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "\u039c-law algorithm"], "categories": ["Articles with example pseudocode", "Lossless compression algorithms", "Wikipedia articles needing clarification from October 2012"], "title": "Lempel\u2013Ziv\u2013Welch"}
{"summary": "liblzg is a compression library for performing lossless data compression. It implements an algorithm that is a variation of the LZ77 algorithm, called the LZG algorithm, with the primary focus of providing a very simple and fast decoding method. One of the key features of the algorithm is that it requires no memory during decompression. The software library is free software, distributed under the zlib license.", "links": ["8-bit", "A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Arithmetic coding", "Assembly language", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "C (programming language)", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "Cross-platform", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fletcher's checksum", "Fourier transform", "Fractal compression", "Frame rate", "Free software", "Golomb coding", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "JavaScript", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "List of software categories", "Log area ratio", "Lossless compression", "Lossless data compression", "Lossy compression", "Lua (programming language)", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "Operating system", "PAQ", "Pascal (programming language)", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Reference (computer science)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Software license", "Software release life cycle", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "Zlib license", "\u039c-law algorithm"], "categories": ["All articles lacking reliable references", "All articles with topics of unclear notability", "Articles lacking reliable references from March 2015", "Articles with topics of unclear notability from March 2015", "Free data compression software", "Lossless compression algorithms", "Software using the zlib license"], "title": "Liblzg"}
{"summary": "Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast, lossy compression permits reconstruction only of an approximation of the original data, though this usually improves compression rates (and therefore reduces file sizes).\nLossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by the LAME MP3 encoder and other lossy audio encoders).\nLossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data could be deleterious. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods. Lossless audio formats are most often used for archiving or production purposes, while smaller lossy audio files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary.", "links": ["7-Zip", "7zip", "A-law algorithm", "Adaptive Huffman coding", "Adaptive Transform Acoustic Coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Algorithmic complexity theory", "Amiga", "Apple Lossless", "Apt-X", "Arithmetic coding", "Audio Lossless Coding", "Audio codec", "Audio compression (data)", "Autoregressive", "Average bitrate", "Benchmark (computing)", "Bijection", "Bit rate", "Brotli", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Bzip2", "C10n.info", "CCM (software)", "Calgary Corpus", "Canonical Huffman code", "Ccmx", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Comp.compression", "Companding", "Comparison of file archivers", "Compress", "Compression algorithm", "Compression artifact", "Constant bitrate", "Context-mixing", "Context mixing", "Context tree weighting", "Convolution", "Counting argument", "Cryptanalysis", "Cryptosystem", "DEFLATE", "DEFLATE (algorithm)", "DTS-HD Master Audio", "Data-compression.com", "Data compression", "Data compression ratio", "David A. Huffman", "Deblocking filter", "Delta (letter)", "Delta encoding", "Demo (computer programming)", "Dictionary coder", "Differential pulse-code modulation", "Digital object identifier", "Discrete cosine transform", "Discrete wavelet transform", "Display resolution", "Dolby TrueHD", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Executable compression", "Exponential-Golomb coding", "FAQ", "Fibonacci coding", "Film frame", "Flashzip", "Fourier transform", "Fractal compression", "Frame rate", "FreeArc", "Free Lossless Audio Codec", "Function (mathematics)", "GIF", "GNU", "GNU General Public License", "Genetic algorithm", "Genetics", "Golomb coding", "Grammar induction", "Graphics Interchange Format", "Gzip", "HTTP", "HapMap", "Heuristics", "Huffman coding", "Hutter Prize", "ILBM", "Image compression", "Image resolution", "Indexed color", "Indexed image", "Information entropy", "Information theory", "Interchange File Format", "Interlaced video", "Intuition (knowledge)", "JBIG2", "JPEG2000", "JPEG 2000", "JPEG XR", "JavaScript", "Joint (audio engineering)", "Karhunen\u2013Lo\u00e8ve theorem", "Kilobyte", "Kolmogorov complexity", "LAME", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZ77 and LZ78 (algorithms)", "LZ78", "LZJB", "LZMA", "LZRW", "LZW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "List of codecs", "Log area ratio", "Lossless", "Lossless JPEG", "Lossless Transform Audio Compression", "Lossy compression", "MP3", "MPEG-4 SLS", "Macroblock", "Matt Mahoney (computer scientist)", "Meridian Lossless Packing", "Modified Huffman coding", "Modified discrete cosine transform", "Monkey's Audio", "Motion compensation", "Move-to-front transform", "Multiple-image Network Graphics", "NanoZip", "Newsgroup", "Normal number", "Nyquist\u2013Shannon sampling theorem", "OpenCTM", "OptimFROG", "Original Sound Quality", "PAQ", "PKWARE, Inc.", "PPMd", "Peak signal-to-noise ratio", "Pi", "Pigeonhole principle", "Pixel", "Point-to-Point Protocol", "Portable Network Graphics", "Precompressor", "Prediction by partial matching", "Progressive Graphics File", "Psychoacoustics", "PubMed Central", "PubMed Identifier", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Random", "Range encoding", "Rate\u2013distortion theory", "RealPlayer", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Secure Shell", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Shorten (file format)", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel Ziv", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Super Audio CD", "TIFF", "TTA (codec)", "Tagged Image File Format", "Timeline of information theory", "Tunstall coding", "UTF-8", "Unary coding", "Unicity distance", "United States", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "WavPack", "Wavelet compression", "WebP", "Wikipedia", "WinRK", "Windows Media Audio 9 Lossless", "XML", "Xz", "ZIP (file format)", "\u039c-law algorithm"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from August 2011", "Articles with unsourced statements from December 2007", "Articles with unsourced statements from November 2012", "Articles with unsourced statements from November 2015", "Data compression", "Lossless compression algorithms"], "title": "Lossless compression"}
{"summary": "LZ4 is a lossless data compression algorithm that is focused on compression and decompression speed. It belongs to the LZ77 family of byte-oriented compression schemes.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "C (programming language)", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Computing platform", "Constant bitrate", "Context tree weighting", "Convolution", "Cross-platform", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "FreeBSD", "GitHub", "Golomb coding", "Gzip", "Huffman coding", "Illumos", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LWN.net", "LZ77", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Linux kernel", "List of software categories", "Log area ratio", "Lossless", "Lossless compression", "Lossy compression", "Macroblock", "Michael Larabel", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "New BSD", "Nyquist\u2013Shannon sampling theorem", "OpenZFS", "Operating system", "PAQ", "Peak signal-to-noise ratio", "Phoronix", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Software developer", "Software license", "Software release life cycle", "Sound quality", "Speech coding", "Squashfs", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "\u039c-law algorithm"], "categories": ["C libraries", "Free data compression software", "Lossless compression algorithms"], "title": "LZ4 (compression algorithm)"}
{"summary": "LZ77 and LZ78 are the two lossless data compression algorithms published in papers by Abraham Lempel and Jacob Ziv in 1977 and 1978. They are also known as LZ1 and LZ2 respectively. These two algorithms form the basis for many variations including LZW, LZSS, LZMA and others. Besides their academic influence, these algorithms formed the basis of several ubiquitous compression schemes, including GIF and the DEFLATE algorithm used in PNG.\nThey are both theoretically dictionary coders. LZ77 maintains a sliding window during compression. This was later shown to be equivalent to the explicit dictionary constructed by LZ78\u2014however, they are only equivalent when the entire data is intended to be decompressed. LZ78 decompression allows random access to the input as long as the entire dictionary is available, while LZ77 decompression must always start at the beginning of the input.\nThe algorithms were named an IEEE Milestone in 2004.", "links": ["A-law algorithm", "Abraham Lempel", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "CiteSeer", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Digital object identifier", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Electronic Arts", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Endianness", "Entropy (information theory)", "Entropy encoding", "Explicit dictionary", "Exponential-Golomb coding", "Faculty of Electrical Engineering and Computing, University of Zagreb", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "GIF", "Golomb coding", "Huffman coding", "IEEE Transactions on Information Theory", "Image compression", "Image resolution", "Information theory", "Institute of Electrical and Electronics Engineers", "Interlaced video", "Jacob Ziv", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZJB", "LZRW", "LZSS", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel-Ziv-Markov chain algorithm", "Lempel-Ziv-Stac", "Lempel-Ziv-Storer-Szymanski", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "List of IEEE milestones", "Log area ratio", "Lossless compression", "Lossless data compression", "Lossy compression", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "PalmDoc", "Peak signal-to-noise ratio", "Peter Shor", "Pixel", "Portable Network Graphics", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sliding window", "Sliding window compression", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Usenet newsgroup", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "\u039c-law algorithm"], "categories": ["All accuracy disputes", "All articles containing potentially dated statements", "Articles containing potentially dated statements from 2008", "Articles with disputed statements from November 2010", "Lossless compression algorithms", "Use dmy dates from August 2012"], "title": "LZ77 and LZ78"}
{"summary": "LZJB is a lossless data compression algorithm invented by Jeff Bonwick to compress crash dumps and data in ZFS. It includes a number of improvements to the LZRW1 algorithm, a member of the Lempel\u2013Ziv family of compression algorithms. The name LZJB is derived from its parent algorithm and its creator\u2014Lempel Ziv Jeff Bonwick. Bonwick is also one of two architects of ZFS, and the creator of the Slab Allocator.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Computer science", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Jeff Bonwick", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossless data compression", "Lossy compression", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Slab allocation", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "ZFS", "\u039c-law algorithm"], "categories": ["All stub articles", "Computer science stubs", "Lossless compression algorithms", "Sun Microsystems software"], "title": "LZJB"}
{"summary": "Lempel\u2013Ziv Ross Williams (LZRW) refers to variants of the LZ77 lossless data compression algorithms with an emphasis on improving compression speed through the use of hash tables and other techniques. This family was explored by Ross Williams, who published a series of algorithms beginning with LZRW1 in 1991.\nThe variants are:\nLZRW1\nLZRW1-A\nLZRW2\nLZRW3\nLZRW3-A\nLZRW4\nLZRW5\nThe LZJB algorithm used in ZFS is derived from LZRW1.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithms", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Computer science", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Hash tables", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77", "LZ77 and LZ78", "LZJB", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossless data compression", "Lossy compression", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Ross Williams (computer scientist)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "ZFS", "\u039c-law algorithm"], "categories": ["All stub articles", "Computer science stubs", "Free data compression software", "Lossless compression algorithms"], "title": "LZRW"}
{"summary": "LZWL is a syllable-based variant of the character-based LZW compression algorithm that can work with syllables obtained by all algorithms of decomposition into syllables. The algorithm can be used for words too.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZW", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossy compression", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "\u039c-law algorithm"], "categories": ["All articles needing additional references", "All articles needing style editing", "Articles needing additional references from January 2013", "Lossless compression algorithms", "Wikipedia articles needing style editing from August 2009"], "title": "LZWL"}
{"summary": "LZX is the name of an LZ77 family compression algorithm. It is also the name of a file archiver with the same name. Both were invented by Jonathan Forbes and Tomi Poutanen.", "links": ["80x86", "A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Amiga", "Archive formats", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Cabinet (file format)", "Canada", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Comparison of file archivers", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Jonathan Forbes (programmer)", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "List of archive formats", "Log area ratio", "Lossless compression", "Lossy compression", "Macroblock", "Microsoft", "Microsoft Compressed HTML Help", "Microsoft Reader", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "Nyquist\u2013Shannon sampling theorem", "OpenLaszlo", "Operand", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Prediction by partial matching", "Preprocessor", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Shareware", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tomi Poutanen", "Tunstall coding", "Unary coding", "Universal code (data compression)", "University of Waterloo", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "Windows 7", "Windows Imaging Format", "Windows Vista", "Xbox Live Avatars", "\u039c-law algorithm"], "categories": ["Amiga", "Lossless compression algorithms"], "title": "LZX (algorithm)"}
{"summary": "Microsoft Point-to-Point Compression (MPPC; described in RFC 2118) is a streaming data compression algorithm based on an implementation of Lempel\u2013Ziv using a sliding window buffer. According to Hifn's IP statement, MPPC is patent-encumbered.\nWhere V.44 or V.42bis operate at layer 1 on the OSI model, MPPC operates on layer 2, giving it a significant advantage in terms of computing resources available to it. The dialup modem's in-built compression (V.44 or V.42bis) can only occur after the data has been serially transmitted to the modem, typically at a maximum rate of 115,200 bit/s. MPPC, as it is controlled by the operating system, can receive as much data as it wishes to compress, before forwarding it on to the modem.\nThe modem's hardware must not delay data too much, while waiting for more to compress in one packet, otherwise an unacceptable latency level will result. It also cannot afford to, as this would require both sizable computing resources (on the scale of a modem) as well as significant buffer RAM. Software compression such as MPPC is free to use the host computer's resources which will typically include a CPU of several hundred Megahertz and several hundred Megabytes of RAM - greater computing power than that of the modem by several orders of magnitude. This allows it to keep a much larger buffer to work on at any one time, and it processes through a given amount of data much faster.\nThe end result is that where V.44 may achieve a maximum of 4:1 compression (230 kbit/s) but is usually limited to 115.2 kbit/s, MPPC is capable of a maximum of 8:1 compression (460 kbit/s). MPPC also, given the far greater computing power at its disposal, is more effective on data than V.44 and achieves higher compression ratios when 8:1 isn't achievable.", "links": ["CPU", "Data compression", "Dialup", "Hifn", "LZ77", "Lag", "Lempel\u2013Ziv", "Lempel\u2013Ziv\u2013Stac", "Modem", "OSI model", "Operating system", "Patent", "Random access memory", "Sliding window", "Stac Electronics", "Streaming data", "V.42bis", "V.44"], "categories": ["All articles lacking sources", "All articles needing expert attention", "Articles lacking sources from August 2009", "Articles needing expert attention from February 2009", "Articles needing expert attention with no reason or talk parameter", "Computing articles needing expert attention", "Lossless compression algorithms", "Microsoft initiatives", "Modems"], "title": "Microsoft Point-to-Point Compression"}
{"summary": "The move-to-front (MTF) transform is an encoding of data (typically a stream of bytes) designed to improve the performance of entropy encoding techniques of compression. When efficiently implemented, it is fast enough that its benefits usually justify including it as an extra step in data compression algorithms.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithm", "Arithmetic coding", "Array data structure", "Audio codec", "Audio compression (data)", "Average bitrate", "Big O notation", "Bit rate", "Burrows\u2013Wheeler transform", "Byte", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context tree weighting", "Convolution", "DEFLATE", "Data", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Digital object identifier", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Huffman coding", "Image compression", "Image resolution", "Information theory", "Interlaced video", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Linked list", "List (computing)", "Log area ratio", "Lossless compression", "Lossy compression", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Plain text", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Python (programming language)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "To be, or not to be", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "\u039c-law algorithm"], "categories": ["All Wikipedia articles needing clarification", "All articles lacking in-text citations", "All articles needing additional references", "Articles lacking in-text citations from May 2011", "Articles needing additional references from May 2011", "Data compression", "Lossless compression algorithms", "Transforms", "Wikipedia articles needing clarification from February 2011", "Wikipedia articles needing clarification from February 2012", "Wikipedia articles needing clarification from July 2015"], "title": "Move-to-front transform"}
{"summary": "The package-merge algorithm is an O(nL)-time algorithm for finding an optimal length-limited Huffman code for a given distribution on a given alphabet of size n, where no code word is longer than L. It is a greedy algorithm, and a generalization of Huffman's original algorithm. Package-merge works by reducing the code construction problem to the binary coin collector's problem.", "links": ["ArXiv", "Big O notation", "Canonical Huffman code", "Code word", "Dan Hirschberg", "Data compression", "Digital object identifier", "Graph theory", "Greedy algorithm", "Huffman coding", "IEEE Transactions on Communications", "Lawrence L. Larmore", "Numismatics", "SIAM Journal on Computing"], "categories": ["Coding theory", "Lossless compression algorithms"], "title": "Package-merge algorithm"}
{"summary": "Prediction by partial matching (PPM) is an adaptive statistical data compression technique based on context modeling and prediction. PPM models use a set of previous symbols in the uncompressed symbol stream to predict the next symbol in the stream. PPM algorithms can also be used to cluster data into predicted groupings in cluster analysis.", "links": ["A-law algorithm", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Cluster analysis", "Code-excited linear prediction", "Coding tree unit", "Color space", "Companding", "Compression artifact", "Constant bitrate", "Context modeling", "Context tree weighting", "Convolution", "DEFLATE", "Dasher (software)", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Digital object identifier", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Entropy (information theory)", "Entropy encoding", "Escape sequence", "Exponential-Golomb coding", "Fibonacci coding", "Film frame", "Fourier transform", "Fractal compression", "Frame rate", "Golomb coding", "Huffman coding", "Huffman encoding", "IEEE Computer Society Press", "IEEE Transactions on Communications", "Image compression", "Image resolution", "Information theory", "Interlaced video", "International Standard Book Number", "Karhunen\u2013Lo\u00e8ve theorem", "Kevin Nanney", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Language model", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Log area ratio", "Lossless compression", "Lossy compression", "Macroblock", "Modified Huffman coding", "Modified discrete cosine transform", "Motion compensation", "Move-to-front transform", "N-gram", "Natural language", "Nyquist\u2013Shannon sampling theorem", "PAQ", "Peak signal-to-noise ratio", "Pixel", "Prediction", "Pseudocount", "Psychoacoustics", "PubMed Identifier", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Random Access Memory", "Range encoding", "Ranking", "Rate\u2013distortion theory", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Speech coding", "Standard test image", "Statistical Lempel\u2013Ziv", "Statistics", "Sub-band coding", "Symbol", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet compression", "Zero-frequency problem", "\u039c-law algorithm"], "categories": ["Articles with Russian-language external links", "Lossless compression algorithms"], "title": "Prediction by partial matching"}
{"summary": "A prefix code is a type of code system (typically a variable-length code) distinguished by its possession of the \"prefix property\", which requires that there is no code word in the system that is a prefix (initial segment) of any other code word in the system. For example, a code with code words {9, 55} has the prefix property; a code consisting of {9, 5, 59, 55} does not, because \"5\" is a prefix of \"59\" and also of \"55\". A prefix code is a uniquely decodable code: a receiver can identify each word without requiring a special marker between words.\nPrefix codes are also known as prefix-free codes, prefix condition codes and instantaneous codes. Although Huffman coding is just one of many algorithms for deriving prefix codes, prefix codes are also widely referred to as \"Huffman codes\", even when the code was not produced by a Huffman algorithm. The term comma-free code is sometimes also applied as a synonym for prefix-free codes but in most mathematical books and articles (e.g.) a comma-free code is used to mean a self-synchronizing code, a subclass of prefix codes.\nUsing prefix codes, a message can be transmitted as a sequence of concatenated code words, without any out-of-band markers or (alternatively) special markers between words to frame the words in the message. The recipient can decode the message unambiguously, by repeatedly finding and removing sequences that form valid code words. This is not generally possible with codes that lack the prefix property, for example {0, 1, 10, 11}: a receiver reading a \"1\" at the start of a code word would not know whether that was the complete code word \"1\", or merely the prefix of the code word \"10\" or \"11\"; so the string \"10\" could be interpreted either as a single codeword or as the concatenation of the words \"1\" then \"0\".\nThe variable-length Huffman codes, country calling codes, the country and publisher parts of ISBNs, the Secondary Synchronization Codes used in the UMTS W-CDMA 3G Wireless Standard, and the instruction sets (machine language) of most computer microarchitectures are prefix codes.\nPrefix codes are not error-correcting codes. In practice, a message might first be compressed with a prefix code, and then encoded again with channel coding (including error correction) before transmission.\nKraft's inequality characterizes the sets of code word lengths that are possible in a uniquely decodable code.", "links": ["Asynchronous Transfer Mode", "Basil Gordon", "Block code", "Cambridge University Press", "Channel coding", "Charles E. Leiserson", "Clifford Stein", "Code", "Code word", "Copyright status of work by the U.S. government", "Country calling codes", "Digital object identifier", "Elias delta coding", "Elias gamma coding", "Elias omega coding", "Entropy encoding", "Error-correcting code", "Error-correcting codes", "Federal Standard 1037C", "Fibonacci coding", "Frame synchronization", "Framing (telecommunication)", "General Services Administration", "Golomb Rice code", "Huffman coding", "ISBN", "ISO 8859-15", "Instruction set", "International Standard Book Number", "International Standard Serial Number", "Introduction to Algorithms", "Kraft's inequality", "Levenshtein coding", "Lossless data compression", "Morse code", "Out-of-band data", "Peter Elias", "Prefix (computer science)", "Ronald L. Rivest", "Scientific American", "Self-synchronizing code", "Shannon-Fano coding", "Solomon W. Golomb", "Straddling checkerboard", "Thomas H. Cormen", "Truncated binary encoding", "UMTS", "UTF-32/UCS-4", "UTF-8", "Unary coding", "Unicode", "Uniquely decodable code", "Universal code (data compression)", "VCR Plus", "Variable-length code", "W-CDMA", "Zentralblatt MATH"], "categories": ["Coding theory", "Data compression", "Lossless compression algorithms", "Prefixes", "Wikipedia articles incorporating text from the Federal Standard 1037C"], "title": "Prefix code"}
{"summary": "QUAD is a high-performance data compressor based on the LZ algorithms (LZ77, LZ78, LZW). It's designed to produce small files but still decompress fast and with little memory. QUAD is licensed under the LGPL.", "links": ["Computer storage", "Data compression", "GNU Lesser General Public License", "LZ77 and LZ78", "Lempel-Ziv-Welch", "Software"], "categories": ["All articles lacking reliable references", "All articles with topics of unclear notability", "All stub articles", "Archive formats", "Articles lacking reliable references from May 2013", "Articles with topics of unclear notability from May 2013", "Free data compression software", "Lossless compression algorithms", "Storage software stubs"], "title": "QUAD (compressor)"}
{"summary": "Sequitur (or Nevill-Manning algorithm) is a recursive algorithm developed by Craig Nevill-Manning and Ian H. Witten in 1997 that infers a hierarchical structure (context-free grammar) from a sequence of discrete symbols. The algorithm operates in linear space and time. It can be used in data compression software applications.", "links": ["ArXiv", "Context-free grammar", "Craig Nevill-Manning", "Data compression", "Digital object identifier", "Digram", "Ian H. Witten", "Lossless compression", "Nonterminal symbol", "Straight-line grammar", "Terminal symbol"], "categories": ["Lossless compression algorithms"], "title": "Sequitur algorithm"}
{"summary": "UT Video Codec Suite is a fast, lossless video codec, developed by Takeshi Umezawa and released under the free GNU General Public License.\nThe algorithm of UT video builds on the Huffman code to.\nUT video was as an alternative to HuffYUV developed which allows for better compression and has appeared both as x86 and x64 than build. It can handle the color spaces YUV422 (ULY2), RGB (ULRG), RGBA (Ulra) and, most recently, YUV420 bypass (ULY0). Due to its support for multithreading this codec is also capable of HDTV material to encode in real time. However, this is a good support for the SSE2 -Befehlssatzes on the main processor required because Umezawa reblogged this instruction set used for optimization.\nUT video uses the following FOURCC codes: ULY0, ULY2, ULRA, ULRG.", "links": ["C++", "FOURCC", "File size", "GNU General Public License", "HDTV", "HuffYUV", "Huffman code", "List of software categories", "Lossless", "Multithreading", "OS X", "Operating system", "RGB", "RGBA", "SSE2", "Software developer", "Software license", "Software release life cycle", "Takeshi Umezawa", "Video codec", "Windows"], "categories": ["2008 software", "All articles lacking sources", "All orphaned articles", "All stub articles", "Articles lacking sources from April 2015", "Free video codecs", "Lossless compression algorithms", "Orphaned articles from August 2015", "Television technology stubs"], "title": "Ut Video Codec Suite"}
{"summary": "Zopfli is a data compression algorithm that encodes data into DEFLATE, gzip and zlib formats. Zopfli is regarded as the most size-efficient DEFLATE encoder available. In February 2013, a reference implementation of the Zopfli algorithm was released by Google as a free software programming library under the Apache License, Version 2.0. The name Z\u00f6pfli is the Swiss German diminutive of \u201cZopf\u201d, a special type of Hefezopf.", "links": ["7-Zip", "AdvanceCOMP", "Android application package", "Apache License", "Brotli", "C (programming language)", "C Sharp (programming language)", "DEFLATE", "Data compression", "Diminutive", "Free software", "GitHub", "Google", "Graph (mathematics)", "Gzip", "HTTP compression", "Hefezopf", "JAR (file format)", "Library (computing)", "PHP", "PNGOUT", "Portable Network Graphics", "Reference implementation", "Shortest path problem", "Swiss German", "Web Open Font Format", "World Wide Web", "Wrapper library", "Zip (file format)", "Zlib", "Zopf"], "categories": ["All articles needing additional references", "Articles needing additional references from September 2015", "Free computer libraries", "Lossless compression algorithms"], "title": "Zopfli"}
{"summary": "ZPAQ is an open source (GPL) command line archiver for Windows and Linux. It uses a journaling or append-only format which can be rolled back to an earlier state to retrieve older versions of files and directories. It supports fast incremental update by adding only files whose last-modified date has changed since the previous update. It compresses using deduplication and several algorithms (LZ77, BWT, and context mixing) depending on the data type and the selected compression level. To preserve forward and backward compatibility between versions as the compression algorithm is improved, it stores the decompression algorithm in the archive. The ZPAQ source code includes a public domain API, libzpaq, which provides compression and decompression services to C++ applications. The format is believed to be unencumbered by patents.", "links": ["3ivx", "7-Zip", "ALZip", "API", "ARC (file format)", "ARJ", "AROS Research Operating System", "Alternative terms for free software", "Apache License", "Apache Software Foundation", "Apple Lossless", "Apple Public Source License", "Archive Utility", "Archiver", "Arithmetic coding", "Ark (software)", "Artistic License", "Assembly language", "Audio Lossless Coding", "Audio compression (data)", "BSD licenses", "BWT", "Basic For Qt", "Beerware", "Berkeley Software Distribution", "BetterZip", "Binary blob", "Blender Foundation", "Blu-code", "Boost Software License", "BulkZip", "Bzip2", "C++", "CC0", "CELT", "C (programming language)", "CineForm", "Cinepak", "Codec", "Command line", "Commercial software", "Common Development and Distribution License", "Comparison of audio coding formats", "Comparison of file archivers", "Comparison of free and open-source software licenses", "Comparison of open-source operating systems", "Comparison of open-source wireless drivers", "Comparison of open source and closed source", "Comparison of source code hosting facilities", "Comparison of video codecs", "Compress", "Computing platform", "Context mixing", "Contiki", "Contributor License Agreement", "Copyfree", "Copyleft", "CoreAVC", "DNxHD codec", "Daala", "Darwin (operating system)", "Data compression", "Data deduplication", "Debian Free Software Guidelines", "Definition of Free Cultural Works", "Digital rights management", "Dirac (video compression format)", "DivX", "ECos", "Eclipse (software)", "Eclipse Foundation", "Eclipse Public License", "Executable compression", "FAAC", "FASTQ format", "FFV1", "FFmpeg", "FLAC", "File Roller", "File archiver", "Filzip", "Fork (software development)", "Fraunhofer FDK AAC", "FreeArc", "FreeBASIC", "FreeBSD Foundation", "FreeDOS", "Free Pascal", "Free Software Foundation", "Free Software Foundation Europe", "Free Software Foundation Latin America", "Free Software Foundation of India", "Free Software Movement of India", "Free and open-source graphics device driver", "Free and open-source software", "Free license", "Free software", "Free software license", "Free software movement", "Freedesktop.org", "Freeware", "GNOME Foundation", "GNU", "GNU/Linux", "GNU Compiler Collection", "GNU General Public License", "GNU Lesser General Public License", "GNU Project", "GPL", "Gambas", "Graphical user interface", "Gratis versus libre", "Gzip", "H.264/MPEG-4 AVC", "HDX4", "Haiku (operating system)", "Haiku Applications", "Hardware restriction", "Helix (multimedia project)", "High Efficiency Video Coding", "History of Firefox", "History of Haiku (operating system)", "History of Linux", "History of Mozilla Application Suite", "History of Mozilla Thunderbird", "History of free and open-source software", "Huffyuv", "IA-32", "ISC license", "Indeo", "Indonesia, Go Open Source", "Inferno (operating system)", "Info-ZIP", "JAR (software)", "Java (programming language)", "Julia (programming language)", "KDE e.V.", "KGB Archiver", "L3enc", "LAME", "LHA (file format)", "LLVM", "LZ77", "Lagarith", "Libavcodec", "License proliferation", "Linux", "Linux Foundation", "Linux distribution", "List of formerly proprietary software", "List of free-software events", "List of free and open-source Android applications", "List of free and open-source iOS applications", "List of free and open-source software packages", "List of free software project directories", "List of free software web applications", "List of software categories", "Long-term support", "Lossless compression", "Lossy compression", "Lua (programming language)", "Lzip", "Lzop", "MINIX", "MIT License", "MPEG-4 Part 2", "MSU Lossless Video Codec", "Ma3bar", "MacBinary", "Mach (kernel)", "Matt Mahoney (computer scientist)", "Microsoft Open Specification Promise", "Microsoft Windows", "Monkey's Audio", "Mozilla", "Mozilla Corporation software rebranded by the Debian project", "Mozilla Foundation", "Mozilla Public License", "Musepack", "Nero AAC Codec", "Nero Digital", "NetBeans", "Open-source license", "Open-source software", "Open-source software development", "Open-source software security", "Open64", "OpenBSD Foundation", "OpenH264", "OpenSSH", "OpenSolaris", "Open Knowledge Foundation", "Open Source Geospatial Foundation", "Open Source Initiative", "Open source", "Operating system", "OptimFROG", "Opus (audio format)", "Outline of free software", "PAQ", "PHP", "PKZIP", "Pack (compression)", "Patent", "PeaZip", "Perl", "Permissive free software licence", "Plan 9 from Bell Labs", "PowerArchiver", "Proprietary software", "Public domain", "Python (programming language)", "Qt (software)", "QuickTime", "ROSE (compiler framework)", "Rabin fingerprint", "ReactOS", "Revolution OS", "Rolling hash", "Ruby (programming language)", "Rzip", "SCO\u2013Linux controversies", "SHA-1", "Shared source", "Shorten (file format)", "Smalltalk", "Smart Bitrate Control", "Snappy (software)", "Software Freedom Conservancy", "Software Freedom Law Center", "Software Package Data Exchange", "Software developer", "Software in the Public Interest", "Software license", "Software patents and free software", "Software release life cycle", "Sorenson codec", "Speex", "StuffIt", "StuffIt Expander", "Symbian Foundation", "TTA (codec)", "TUGZip", "Tar (computing)", "Tcl", "The Cathedral and the Bazaar", "The Document Foundation", "The Free Software Definition", "The Open Source Definition", "The Unarchiver", "Theora", "TooLAME", "Trusted Computing", "UEFI Secure Boot", "UPX", "Ubuntu Foundation", "VP7", "VP8", "VideoLAN", "Video compression", "Viral license", "Vorbis", "WTFPL", "WavPack", "Wikimedia Foundation", "WinAce", "WinRAR", "WinZip", "Windows Media Encoder", "X.Org Foundation", "X264", "X265", "X86-64", "XMPP Standards Foundation", "XZ Utils", "Xarchiver", "Xiph.Org Foundation", "Xvid", "YULS", "ZipGenius", "Zipeg", "Zlib License"], "categories": ["All articles lacking reliable references", "All articles with topics of unclear notability", "Archive formats", "Articles lacking reliable references from October 2013", "Articles with topics of unclear notability from October 2013", "Free data compression software", "Lossless compression algorithms", "Open formats", "Wikipedia articles with possible conflicts of interest from October 2013"], "title": "ZPAQ"}
{"summary": "3Dc (FourCC : ATI2), also known as DXN, BC5, or Block Compression 5 is a lossy data compression algorithm for normal maps invented and first implemented by ATI. It builds upon the earlier DXT5 algorithm and is an open standard. 3Dc is now implemented by both ATI and Nvidia.", "links": ["ATI Technologies", "Bump mapping", "DXT5", "Data compression", "FourCC", "Geometric", "Lossy data compression", "Normal mapping", "Nvidia", "Open standard", "Texture mapping"], "categories": ["3D graphics software", "Lossy compression algorithms", "Open formats", "Texture compression"], "title": "3Dc"}
{"summary": "Adaptive Scalable Texture Compression (ASTC) is a lossy block-based texture compression algorithm developed by J\u00f8rn Nystad et al. of ARM Ltd.\nFull details of ASTC were first presented publicly at the High Performance Graphics 2012 conference, in a paper by Olson et al. entitled \"Adaptive Scalable Texture Compression\"\nASTC was adopted as an official extension for both OpenGL and OpenGL ES by the Khronos Group on 6 August 2012. It's also a part of Direct3D 11.3 and Direct3D 12 in Windows 10.", "links": ["3Dc", "ARM Holdings", "Algorithm", "Color Cell Compression", "Direct3D 11.3", "Direct3D 12", "Ericsson Texture Compression", "Lossy compression", "PVRTC", "S3TC", "Texture compression", "Windows 10"], "categories": ["Lossy compression algorithms", "Texture compression"], "title": "Adaptive Scalable Texture Compression"}
{"summary": "Apple Video is a lossy video compression and decompression algorithm (codec) developed by Apple Inc. and first released as part of QuickTime 1.0 in 1991. The codec is also known as QuickTime Video, by its FourCC RPZA and the name Road Pizza. When used in the AVI container, the FourCC AZPR is also used. The bit-stream format of Apple Video has been reverse-engineered and a decoder has been implemented in the projects XAnim and libavcodec.", "links": [".m2ts", "3GP and 3G2", "A-law algorithm", "AAC-LD", "AMV video format", "APNG", "Adaptive Multi-Rate Wideband", "Adaptive Multi-Rate audio codec", "Adaptive Transform Acoustic Coding", "Advanced Audio Coding", "Advanced Systems Format", "Algebraic code-excited linear prediction", "Alliance for Open Media", "Apple Inc.", "Apple Lossless", "Asao (codec)", "Au file format", "Audio Interchange File Format", "Audio Lossless Coding", "Audio Video Interleave", "Audio Video Standard", "Avid Audio", "BMP file format", "Better Portable Graphics", "Bink Video", "Block truncation coding", "CELT", "CineForm", "Cinepak", "Code-excited linear prediction", "Codec", "Codec2", "Color Cell Compression", "Color depth", "Color quantization", "Conditional replenishment", "DNxHD codec", "DTS (sound system)", "DV", "Daala", "Data compression", "Digital Item", "Digital Video Interactive", "Digital container format", "Dirac (video compression format)", "DivX", "DjVu", "Dolby Digital", "Dynamic Resolution Adaptation", "Enhanced VOB", "Enhanced Variable Rate Codec", "Enhanced Variable Rate Codec B", "Enhanced full rate", "Extended Adaptive Multi-Rate \u2013 Wideband", "FFV1", "FLAC", "Flash Video", "FourCC", "Free Lossless Image Format", "Full Rate", "G.711", "G.718", "G.719", "G.722", "G.722.1", "G.723", "G.723.1", "G.726", "G.728", "G.729", "G.729.1", "GIF", "General Exchange Format", "Group 4 compression", "H.120", "H.261", "H.262/MPEG-2 Part 2", "H.263", "H.264/MPEG-4 AVC", "Half Rate", "Harmonic Vector Excitation Coding", "High-Efficiency Advanced Audio Coding", "HighColor", "High Efficiency Video Coding", "Huffyuv", "ICER", "IETF", "ISO base media file format", "ITU-T", "Image compression", "Indeo", "Indexed color", "Interchange File Format", "International Electrotechnical Commission", "International Organization for Standardization", "Internet Low Bitrate Codec", "Internet Speech Audio Codec", "JBIG", "JBIG2", "JPEG", "JPEG 2000", "JPEG XR", "Lagarith", "Libavcodec", "Linear interpolation", "Lossless JPEG", "Lossy", "MOD and TOD", "MP3", "MPEG", "MPEG-1", "MPEG-1 Audio Layer I", "MPEG-1 Audio Layer II", "MPEG-2", "MPEG-4", "MPEG-4 Part 14", "MPEG-4 Part 2", "MPEG-4 SLS", "MPEG-H", "MPEG-H 3D Audio", "MPEG-LA", "MPEG Multichannel", "MPEG Surround", "MPEG media transport", "MPEG program stream", "MPEG transport stream", "MSU Lossless Video Codec", "MT9", "Material Exchange Format", "Matroska", "Microsoft Video 1", "Monkey's Audio", "Motion JPEG", "Motion JPEG 2000", "Motion compensation", "Multimedia", "Multiple-image Network Graphics", "Musepack", "NETVC", "OMS Video", "Ogg", "OpenEXR", "OptimFROG", "Opus (audio format)", "Original Sound Quality", "Palette (computing)", "Pixlet", "Portable Network Graphics", "ProRes 422", "ProRes 4444", "Progressive Graphics File", "Pulse-code modulation", "Qualcomm code-excited linear prediction", "QuickTime", "QuickTime Animation", "QuickTime File Format", "QuickTime Graphics", "QuickTime VR", "RGB", "RTAudio", "RTVideo", "RatDVD", "RealAudio", "RealMedia", "RealVideo", "Relaxed code-excited linear prediction", "Resource Interchange File Format", "Run-length encoding", "S3 Texture Compression", "SILK", "SVOPC", "Selectable Mode Vocoder", "SheerVideo", "Shorten (file format)", "Siren (codec)", "Smacker video", "Sorenson codec", "Speex", "Super Audio CD", "TIFF/EP", "TTA (codec)", "Tagged Image File Format", "Theora", "Transform coding", "TwinVQ", "Unified Speech and Audio Coding", "VC-1", "VOB", "VP3", "VP6", "VP7", "VP8", "VP9", "Variable-Rate Multimode Wideband", "Vector quantization", "Vector sum excited linear prediction", "Vorbis", "WAV", "WavPack", "WebM", "WebP", "Windows Media Audio", "Windows Media Video (video compression format)", "Wireless Application Protocol Bitmap Format", "XAnim", "YULS", "\u039c-law algorithm"], "categories": ["Apple Inc. software", "Lossy compression algorithms", "Video codecs"], "title": "Apple Video"}
{"summary": "Block Truncation Coding, or BTC, is a type of lossy image compression technique for greyscale images. It divides the original images into blocks and then uses a quantiser to reduce the number of grey levels in each block whilst maintaining the same mean and standard deviation. It is an early predecessor of the popular hardware DXTC technique, although BTC compression method was first adapted to colour long before DXTC using a very similar approach called Color Cell Compression. BTC has also been adapted to video compression \nBTC was first proposed by E.J Delp and O.R. Mitchell  at Purdue University. Another variation of BTC is Absolute Moment Block Truncation Coding or AMBTC, in which instead of using the standard deviation the first absolute moment is preserved along with the mean. AMBTC is computationally simpler than BTC and also typically results in a lower Mean Squared Error (MSE). AMBTC was proposed by Maximo Lema and Robert Mitchell.\nUsing sub-blocks of 4x4 pixels gives a compression ratio of 4:1 assuming 8-bit integer values are used during transmission or storage. Larger blocks allow greater compression (\"a\" and \"b\" values spread over more pixels) however quality also reduces with the increase in block size due to the nature of the algorithm.\nThe BTC algorithm was used for compressing Mars Pathfinder's rover images.", "links": ["Arithmetic mean", "Color Cell Compression", "Digital object identifier", "Grey levels", "Greyscale", "International Standard Book Number", "Lossy compression", "Mars Pathfinder", "Mean", "Pixel", "S3 Texture Compression", "Standard deviation"], "categories": ["Image compression", "Lossy compression algorithms"], "title": "Block Truncation Coding"}
{"summary": "A Bloom filter is a space-efficient probabilistic data structure, conceived by Burton Howard Bloom in 1970, that is used to test whether an element is a member of a set. False positive matches are possible, but false negatives are not, thus a Bloom filter has a 100% recall rate. In other words, a query returns either \"possibly in set\" or \"definitely not in set\". Elements can be added to the set, but not removed (though this can be addressed with a \"counting\" filter). The more elements that are added to the set, the larger the probability of false positives.\nBloom proposed the technique for applications where the amount of source data would require an impractically large amount of memory if \"conventional\" error-free hashing techniques were applied. He gave the example of a hyphenation algorithm for a dictionary of 500,000 words, out of which 90% follow simple hyphenation rules, but the remaining 10% require expensive disk accesses to retrieve specific hyphenation patterns. With sufficient core memory, an error-free hash could be used to eliminate all unnecessary disk accesses; on the other hand, with limited core memory, Bloom's technique uses a smaller hash area but still eliminates most unnecessary accesses. For example, a hash area only 15% of the size needed by an ideal error-free hash still eliminates 85% of the disk accesses, an 85\u201315 form of the Pareto principle (Bloom (1970)).\nMore generally, fewer than 10 bits per element are required for a 1% false positive probability, independent of the size or number of elements in the set (Bonomi et al. (2006)).", "links": ["Aggregate function", "Algorithm", "Andrei Broder", "Apache Cassandra", "Apache HBase", "ArXiv", "Arithmetic overflow", "Array data structure", "Associative array", "Azuma\u2013Hoeffding inequality", "Bernard Chazelle", "BigTable", "Bit array", "Bitcoin", "Bitwise operation", "Bloom (shader effect)", "Cache misses", "Calvin Mooers", "Cascading (software)", "Chemical similarity", "CiteSeer", "Communications of the ACM", "Count\u2013min sketch", "Cuckoo hashing", "Data structure", "Data structures", "Data synchronization", "David Eppstein", "Digital object identifier", "Double hashing", "Edge-notched card", "Element (mathematics)", "Enhanced double hashing", "Exim", "False positive", "Feature hashing", "Golomb coding", "Google Chrome", "Hash compaction", "Hash function", "Hash table", "Hyphenation algorithm", "IEEE/ACM Transactions on Networking", "Information content", "International Standard Book Number", "Intersection (set theory)", "Jaccard index", "Lattice (order)", "Lecture Notes in Computer Science", "Linked list", "Map (mathematics)", "Medium", "Michael T. Goodrich", "Mihai P\u0103tra\u015fcu", "MinHash", "PRNG", "Pareto principle", "Perl", "Peter Sanders (computer scientist)", "Precision and recall", "Probabilistic", "PubMed Identifier", "Quotient filter", "Random binary tree", "Random tree (disambiguation)", "Randomized algorithm", "Rapidly exploring random tree", "SPIN model checker", "Self-balancing binary search tree", "Set (computer science)", "Skip list", "Squid (software)", "Subgraph isomorphism problem", "Superimposed code", "Treap", "Trie", "Triple hashing", "Type I and type II errors", "Union (set theory)", "University of Wisconsin\u2013Madison", "Venti", "Web cache", "Workshop on Algorithms and Data Structures", "World Wide Web", "YouTube", "Zatocoding"], "categories": ["All articles lacking in-text citations", "All articles with dead external links", "Articles lacking in-text citations from November 2009", "Articles with dead external links from June 2010", "Commons category with local link same as on Wikidata", "Hashing", "Lossy compression algorithms", "Probabilistic data structures"], "title": "Bloom filter"}
{"summary": "Color Cell Compression is an early lossy image compression algorithm first described by Campbell et al. in 1986. It is a variant of Block Truncation Coding. The encoding process works on small blocks of pixels. For each block, it first partitions the pixels in that block into two sets based on their luminance values, then generates representative colour values for each of these sets, and a bitmap that specifies which pixels belong to which set. The two colour values and the bitmap for each block are then output directly without any further quantization or entropy coding.\nThe decoding process is simple; each pixel of an output block is generated by choosing one of the two representative colours for that block, based on that block's bitmap.\nIn spite of its very simple mechanism, the algorithm yields surprisingly good results on photographic images, and it has the advantage of being very fast to decode with limited hardware. Although far surpassed in compression ratio by later block-transform coding methods such as JPEG, it had the advantage of very simple decompression and fast random access into the compressed image, and it can be regarded as a forerunner of modern texture compression algorithms.", "links": ["Adaptive Scalable Texture Compression", "Bitmap", "Block Truncation Coding", "Computer graphics", "Digital object identifier", "Entropy coding", "Image compression", "Indexed color", "International Standard Book Number", "JPEG", "Lossy compression", "Luminance", "Pixel", "Quantization (signal processing)", "S3 Texture Compression", "Texture compression"], "categories": ["All stub articles", "Computer graphics stubs", "Lossy compression algorithms"], "title": "Color Cell Compression"}
{"summary": "Fractal compression is a lossy compression method for digital images, based on fractals. The method is best suited for textures and natural images, relying on the fact that parts of an image often resemble other parts of the same image. Fractal algorithms convert these parts into mathematical data called \"fractal codes\" which are used to recreate the encoded image.", "links": ["A-law algorithm", "ACM SIGGRAPH", "Adaptive Huffman coding", "Adaptive differential pulse-code modulation", "Algebraic code-excited linear prediction", "Algorithms", "Arithmetic coding", "Audio codec", "Audio compression (data)", "Average bitrate", "Bicubic interpolation", "Bilinear interpolation", "Binary image", "Bit rate", "Burrows\u2013Wheeler transform", "Byte pair encoding", "CD-ROM", "Canonical Huffman code", "Chain code", "Chroma subsampling", "Code-excited linear prediction", "Coding tree unit", "Color space", "Compact space", "Companding", "Compression artifact", "Constant bitrate", "Context tree weighting", "Contraction mapping", "Convolution", "DEFLATE", "Data compression", "Deblocking filter", "Delta encoding", "Dictionary coder", "Differential pulse-code modulation", "Digital image", "Discrete cosine transform", "Display resolution", "Dynamic Markov compression", "Dynamic range", "Elias gamma coding", "Embedded Zerotrees of Wavelet transforms", "Encarta", "Entropy (information theory)", "Entropy encoding", "Exponential-Golomb coding", "Fiasco (image format)", "Fibonacci coding", "Film frame", "Fixed point iteration", "Fourier transform", "Fractal", "Fractal transform", "Frame rate", "Genuine Fractals", "Golomb coding", "Graph of a function", "Grayscale", "Huffman coding", "Hutchinson operator", "IEEE Geoscience and Remote Sensing Society", "Image compression", "Image resolution", "Information theory", "Interlaced video", "International Standard Book Number", "Interpolant", "Iterated function system", "JPEG", "Karhunen\u2013Lo\u00e8ve theorem", "Kolmogorov complexity", "LZ4 (compression algorithm)", "LZ77 and LZ78", "LZJB", "LZRW", "LZWL", "LZX (algorithm)", "Lapped transform", "Latency (audio)", "Lempel\u2013Ziv\u2013Markov chain algorithm", "Lempel\u2013Ziv\u2013Oberhumer", "Lempel\u2013Ziv\u2013Stac", "Lempel\u2013Ziv\u2013Storer\u2013Szymanski", "Lempel\u2013Ziv\u2013Welch", "Levenshtein coding", "Line spectral pairs", "Linear predictive coding", "Linux Journal", "Log area ratio", "Lossless compression", "Lossy compression", "Macroblock", "Michael Barnsley", "Microsoft", "Mitsubishi", "Modified Huffman coding", "Modified discrete cosine transform", "Morgan Kaufmann Publishers", "Motion compensation", "Move-to-front transform", "Netpbm", "Nyquist\u2013Shannon sampling theorem", "OnOne Software", "PAQ", "Patent", "Peak signal-to-noise ratio", "Photoshop", "Pixel", "Prediction by partial matching", "Psychoacoustics", "Pyramid (image processing)", "Quantization (image processing)", "Quantization (signal processing)", "Range encoding", "Rate\u2013distortion theory", "RealVideo", "Redundancy (information theory)", "Run-length encoding", "Sampling (signal processing)", "Set partitioning in hierarchical trees", "Shannon coding", "Shannon\u2013Fano coding", "Shannon\u2013Fano\u2013Elias coding", "Sound quality", "Spectrum Holobyte", "Speech coding", "Standard test image", "Star Trek: The Next Generation A Final Unity", "Statistical Lempel\u2013Ziv", "Sub-band coding", "Timeline of information theory", "Tunstall coding", "Unary coding", "Universal code (data compression)", "University of South Carolina", "Variable bitrate", "Video", "Video codec", "Video compression", "Video compression picture types", "Video quality", "Warped linear predictive coding", "Wavelet", "Wavelet compression", "Windows Media Player", "\u039c-law algorithm"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from August 2009", "Articles with unsourced statements from March 2008", "Fractals", "Image compression", "Lossy compression algorithms", "Pages containing cite templates with deprecated parameters"], "title": "Fractal compression"}
{"summary": "GWIC (GNU Wavelet Image Codec) is a lossy image compression algorithm.", "links": ["Codec", "Graphics software"], "categories": ["All articles lacking sources", "All articles with dead external links", "All articles with topics of unclear notability", "All orphaned articles", "All stub articles", "Articles lacking sources from February 2014", "Articles with dead external links from February 2014", "Articles with topics of unclear notability from November 2015", "Graphics software stubs", "Lossy compression algorithms", "Orphaned articles from February 2009"], "title": "GWIC"}
{"summary": "Microsoft Video 1 or MS-CRAM is an early lossy video compression and decompression algorithm (codec) that was released with version 1.0 of Microsoft's Video for Windows in November 1992. It is based on MotiVE, a vector quantization codec which Microsoft licensed from Media Vision. In 1993, Media Vision marketed the Pro Movie Spectrum, an ISA board that captured video in both raw and MSV1 formats (the MSV1 processing was done in hardware on the board).", "links": [".m2ts", "3GP and 3G2", "A-law algorithm", "AAC-LD", "AMV video format", "APNG", "Adaptive Multi-Rate Wideband", "Adaptive Multi-Rate audio codec", "Adaptive Transform Acoustic Coding", "Advanced Audio Coding", "Advanced Systems Format", "Algebraic code-excited linear prediction", "Alliance for Open Media", "Apple Lossless", "Apple Video", "Asao (codec)", "Au file format", "Audio Interchange File Format", "Audio Lossless Coding", "Audio Video Interleave", "Audio Video Standard", "Avid Audio", "BMP file format", "Better Portable Graphics", "Bink Video", "Block truncation coding", "CELT", "CineForm", "Cinepak", "Code-excited linear prediction", "Codec", "Codec2", "Color Cell Compression", "Color quantization", "Conditional replenishment", "DNxHD codec", "DTS (sound system)", "DV", "Daala", "Data compression", "Digital Item", "Digital Video Interactive", "Digital container format", "Dirac (video compression format)", "DivX", "DjVu", "Dolby Digital", "Dynamic Resolution Adaptation", "Enhanced VOB", "Enhanced Variable Rate Codec", "Enhanced Variable Rate Codec B", "Enhanced full rate", "Extended Adaptive Multi-Rate \u2013 Wideband", "FFV1", "FLAC", "Flash Video", "Free Lossless Image Format", "Full Rate", "G.711", "G.718", "G.719", "G.722", "G.722.1", "G.723", "G.723.1", "G.726", "G.728", "G.729", "G.729.1", "GIF", "General Exchange Format", "Group 4 compression", "H.120", "H.261", "H.262/MPEG-2 Part 2", "H.263", "H.264/MPEG-4 AVC", "Half Rate", "Harmonic Vector Excitation Coding", "High-Efficiency Advanced Audio Coding", "High Efficiency Video Coding", "Huffyuv", "ICER", "IETF", "ISO base media file format", "ITU-T", "Image compression", "Image scaling", "Indeo", "Indexed color", "Interchange File Format", "International Electrotechnical Commission", "International Organization for Standardization", "Internet Low Bitrate Codec", "Internet Speech Audio Codec", "JBIG", "JBIG2", "JPEG", "JPEG 2000", "JPEG XR", "Lagarith", "List of monochrome and RGB palettes", "List of software palettes", "Lossless JPEG", "Lossy", "MOD and TOD", "MP3", "MPEG-1", "MPEG-1 Audio Layer I", "MPEG-1 Audio Layer II", "MPEG-2", "MPEG-4", "MPEG-4 Part 14", "MPEG-4 Part 2", "MPEG-4 SLS", "MPEG-H", "MPEG-H 3D Audio", "MPEG-LA", "MPEG Multichannel", "MPEG Surround", "MPEG media transport", "MPEG program stream", "MPEG transport stream", "MSU Lossless Video Codec", "MT9", "Material Exchange Format", "Matroska", "Media Vision", "Microsoft's", "Monkey's Audio", "Motion JPEG", "Motion JPEG 2000", "Multimedia", "Multiple-image Network Graphics", "Musepack", "NETVC", "NetShow", "OMS Video", "Ogg", "OpenEXR", "OptimFROG", "Opus (audio format)", "Original Sound Quality", "Palette (computing)", "Pixlet", "Portable Network Graphics", "ProRes 422", "ProRes 4444", "Progressive Graphics File", "Qualcomm code-excited linear prediction", "QuickTime Animation", "QuickTime File Format", "QuickTime Graphics", "QuickTime VR", "RTAudio", "RTVideo", "RatDVD", "RealAudio", "RealMedia", "RealVideo", "Relaxed code-excited linear prediction", "Resource Interchange File Format", "S3 Texture Compression", "SILK", "SVOPC", "Selectable Mode Vocoder", "SheerVideo", "Shorten (file format)", "Siren (codec)", "Smacker video", "Software", "Sorenson codec", "Speex", "Super Audio CD", "TIFF/EP", "TTA (codec)", "Tagged Image File Format", "Theora", "TwinVQ", "Unified Speech and Audio Coding", "VC-1", "VOB", "VP3", "VP6", "VP7", "VP8", "VP9", "Variable-Rate Multimode Wideband", "Vector quantization", "Vector sum excited linear prediction", "Video for Windows", "Vorbis", "WAV", "WavPack", "WebM", "WebP", "Windows Media Audio", "Windows Media Encoder", "Windows Media Video", "Windows Media Video (video compression format)", "Wireless Application Protocol Bitmap Format", "YULS", "\u039c-law algorithm"], "categories": ["All articles with topics of unclear notability", "All stub articles", "Articles with topics of unclear notability from September 2011", "Film and video technology", "Lossy compression algorithms", "Microsoft", "Multimedia software stubs", "Video codecs"], "title": "Microsoft Video 1"}
{"summary": "Opus is a lossy audio coding format developed by the Internet Engineering Task Force (IETF) that is particularly suitable for interactive real-time applications over the Internet.\nOpus incorporates technology from two other audio coding formats: the speech-oriented SILK and the low-latency CELT. Opus can be adjusted seamlessly between high and low bitrates, and internally, it transitions between linear predictive coding at lower bitrates and transform coding at higher bitrates (as well as a hybrid for a short overlap). Opus has a very low algorithmic delay (26.5 ms by default), which is a necessity for use as part of a low audio latency communication link, which can permit natural conversation, networked music performances, or lip sync at live events. Opus permits trading-off quality or bitrate to achieve an even smaller algorithmic delay, down to 5 ms. Its delay is very low compared to well over 100 ms for popular music formats such as MP3, Ogg Vorbis and HE-AAC; yet Opus performs very competitively with these formats in terms of quality per bitrate. Unlike Ogg Vorbis, Opus does not require the definition of large codebooks for each individual file, making it preferable to Vorbis for short clips of audio.\nAs an open format standardized through RFC 6716, a reference implementation audio codec called opus-tools is available under the New BSD License. All known software patents which cover Opus are licensed under royalty-free terms.", "links": [".m2ts", "3-clause BSD license", "3GP and 3G2", "3ivx", "7-Zip", "A-law algorithm", "AAC-LD", "AIMP", "ALZip", "AMV video format", "ANSI C", "APNG", "ARC (file format)", "ARJ", "Acrobits Softphone", "Adaptive Multi-Rate Wideband", "Adaptive Multi-Rate audio codec", "Adaptive Transform Acoustic Coding", "Advanced Audio Coding", "Advanced Systems Format", "Airtime", "Algebraic code-excited linear prediction", "Alliance for Open Media", "Amarok (software)", "Android (operating system)", "Annodex", "Apple Inc.", "Apple Lossless", "Apple Video", "Archive Utility", "Archos", "Ark (software)", "Ars Technica", "Asao (codec)", "Asterisk (PBX)", "Asunder (software)", "Au file format", "Audio Interchange File Format", "Audio Lossless Coding", "Audio Video Interleave", "Audio Video Standard", "Audio codec", "Audio coding format", "Audio coding formats", "Audio compression (data)", "Avid Audio", "BMP file format", "BSD licenses", "BetterZip", "Better Portable Graphics", "Bink Video", "Bit rate", "Bitstream", "Blink (layout engine)", "Blu-code", "Broadcom", "BulkZip", "Bzip2", "CDBurnerXP", "CELT", "CSipSimple", "C (programming language)", "Cdparanoia", "Channel (communications)", "Chris Montgomery", "Christopher Montgomery", "Chromecast", "Chromium (web browser)", "CineForm", "Cinepak", "Cmus", "Code-excited linear prediction", "Codebook", "Codec2", "Commercial software", "Comparison of VoIP software", "Comparison of audio coding formats", "Comparison of file archivers", "Comparison of layout engines (HTML5 media)", "Comparison of video codecs", "Compress", "Computing platform", "Constant bitrate", "Continuous Media Markup Language", "CoreAVC", "Cross-platform", "DNxHD codec", "DTS (sound system)", "DV", "Daala", "Data compression", "Digital Item", "Digital Radio Mondiale", "Digital Video Interactive", "Digital container format", "Dirac (video compression format)", "DirectShow", "DivX", "DjVu", "Dolby Digital", "Dynamic Resolution Adaptation", "Empathy (software)", "Enhanced VOB", "Enhanced Variable Rate Codec", "Enhanced Variable Rate Codec B", "Enhanced full rate", "Ericsson", "Executable compression", "Extended Adaptive Multi-Rate \u2013 Wideband", "FAAC", "FFV1", "FFmpeg", "FLAC", "File Roller", "Filename extension", "Filzip", "Firefox", "Flash Video", "Floating-point unit", "Floating point unit", "Foobar2000", "France T\u00e9l\u00e9com", "Fraunhofer FDK AAC", "FreeArc", "Free Lossless Image Format", "Free software", "Freeware", "Full Rate", "G.711", "G.718", "G.719", "G.722", "G.722.1", "G.723", "G.723.1", "G.726", "G.728", "G.729", "G.729.1", "GIF", "GNU General Public License", "GStreamer", "GatesAir", "General Exchange Format", "GitHub", "Google", "Google Chrome", "Grandstream", "Group 4 compression", "Gzip", "H.120", "H.261", "H.262/MPEG-2 Part 2", "H.263", "H.264/MPEG-4 AVC", "HDX4", "HE-AAC", "HTML5", "Haas effect", "Haiku Applications", "Half Rate", "Harmonic Vector Excitation Coding", "Hearing range", "Helix (multimedia project)", "High-Efficiency Advanced Audio Coding", "High Efficiency Video Coding", "Huawei", "Huffyuv", "ICER", "IETF", "IOS", "IP phone", "IPod", "ISO base media file format", "ITU-T", "Icecast", "Image compression", "Indeo", "Info-ZIP", "Interchange File Format", "International Electrotechnical Commission", "International Organization for Standardization", "International Standard Book Number", "International Standard Serial Number", "International standard", "Internet Engineering Task Force", "Internet Explorer", "Internet Low Bitrate Codec", "Internet Speech Audio Codec", "Internet media type", "Iriver", "JAR (software)", "JBIG", "JBIG2", "JPEG", "JPEG 2000", "JPEG XR", "JavaScript", "Jitsi", "Joint (audio engineering)", "KGB Archiver", "Kilobit", "L3enc", "LAME", "LAV Filters", "LHA (file format)", "Lagarith", "Latency (audio)", "Libav", "Libavcodec", "Line2", "Linear predictive coding", "Linphone", "Lip sync", "Lip sync error", "Liquidsoap", "List of software categories", "Lossless JPEG", "Lossless compression", "Lossy compression", "Low-frequency effects", "Lzip", "Lzop", "MKVToolNix", "MOD and TOD", "MP3", "MP4", "MPEG-1", "MPEG-1 Audio Layer I", "MPEG-1 Audio Layer II", "MPEG-2", "MPEG-4", "MPEG-4 Part 14", "MPEG-4 Part 2", "MPEG-4 SLS", "MPEG-H", "MPEG-H 3D Audio", "MPEG-LA", "MPEG-TS", "MPEG Multichannel", "MPEG Surround", "MPEG media transport", "MPEG program stream", "MPEG transport stream", "MSU Lossless Video Codec", "MT9", "MacBinary", "Material Exchange Format", "Matroska", "MediaCoder", "Microsoft", "Microsoft Video 1", "Microsoft Windows", "Modified discrete cosine transform", "Monkey's Audio", "Motion JPEG", "Motion JPEG 2000", "Mozilla Corporation", "Mpxplay", "Multimedia", "Multimedia framework", "Multiple-image Network Graphics", "Mumble (software)", "Musepack", "MusicBee", "NETVC", "Nero AAC Codec", "Nero Digital", "Networked music performance", "New BSD License", "Noise shaping", "Nyquist\u2013Shannon sampling theorem", "OMS Video", "Ogg", "OggPCM", "Ogg Squish", "Ogg Writ", "Ogg page", "OpenCodecs", "OpenEXR", "OpenH264", "Open format", "Opera (web browser)", "Operating system", "OptimFROG", "Orange Labs", "Original Sound Quality", "PAQ", "PKZIP", "Pack (compression)", "PeaZip", "Phoner", "PhonerLite", "Pixlet", "Polycom", "Portable Network Graphics", "Portable media players", "PowerArchiver", "ProRes 422", "ProRes 4444", "Progressive Graphics File", "Qualcomm", "Qualcomm code-excited linear prediction", "QuickTime", "QuickTime Animation", "QuickTime File Format", "QuickTime Graphics", "QuickTime VR", "RTAudio", "RTVideo", "Range encoding", "RatDVD", "Real-time Transport Protocol", "Real-time computing", "RealAudio", "RealMedia", "RealVideo", "Reference implementation", "Reference implementation (computing)", "Relaxed code-excited linear prediction", "Request for Comments", "Resource Interchange File Format", "Rockbox", "Royalty-free", "Rzip", "SFLphone", "SILK", "SMplayer", "SVOPC", "Sample rate", "Sampling rate", "Sandisk", "Selectable Mode Vocoder", "Session Initiation Protocol", "SheerVideo", "Shorten (file format)", "Siren (codec)", "Skype Technologies", "Smacker video", "Smart Bitrate Control", "Snappy (software)", "Software-defined radio", "Software developer", "Software license", "Software patent", "Software release life cycle", "Sorenson codec", "Source code", "Spectral band replication", "Spectrogram", "Speex", "SteamOS", "Streaming media", "StuffIt", "StuffIt Expander", "Super Audio CD", "Surround sound", "TIFF/EP", "TTA (codec)", "TUGZip", "Tagged Image File Format", "Tar (computing)", "TeamSpeak 3", "Telephony", "The Unarchiver", "Theora", "Tieline", "TooLAME", "Tox (software)", "Transform coding", "Tremor (software)", "TrueConf", "Tuenti", "TwinVQ", "UPX", "Unified Speech and Audio Coding", "Unix-like", "Use of Ogg formats in HTML5", "VC-1", "VLC media player", "VOB", "VP3", "VP6", "VP7", "VP8", "VP9", "Variable-Rate Multimode Wideband", "Variable bitrate", "Vector sum excited linear prediction", "Video compression", "Videoconferencing", "VoIP", "Voice over IP", "Vorbis", "Vorbis comment", "WAV", "WavPack", "WebM", "WebP", "WebRTC", "Wikimedia", "Wikimedia Foundation", "WinAce", "WinRAR", "WinZip", "Winamp", "Windows Media Audio", "Windows Media Encoder", "Windows Media Video (video compression format)", "Wireless Application Protocol Bitmap Format", "Working group", "X-Lite", "X264", "X265", "XML Shareable Playlist Format", "XZ Utils", "Xarchiver", "Xiph.Org Foundation", "Xiph QuickTime Components", "Xmplay", "Xvid", "YULS", "ZPAQ", "ZipGenius", "Zipeg", "\u039c-law algorithm"], "categories": ["All articles with unsourced statements", "Articles with hAudio microformats", "Articles with unsourced statements from March 2015", "Articles with unsourced statements from May 2014", "CS1 Russian-language sources (ru)", "CS1 errors: external links", "Free audio codecs", "Lossy compression algorithms", "Software using the BSD license", "Speech codecs", "Xiph.Org projects"], "title": "Opus (audio format)"}
{"summary": "QuickTime Graphics is a lossy video compression and decompression algorithm (codec) developed by Apple Inc. and first released as part of QuickTime 1.x in the early 1990s. The codec is also known by the name Apple Graphics and its FourCC SMC. The codec operates on 8-bit palettized RGB data. The bit-stream format of QuickTime Graphics has been reverse-engineered and a decoder has been implemented in the projects XAnim and libavcodec.", "links": [".m2ts", "3GP and 3G2", "A-law algorithm", "AAC-LD", "AMV video format", "APNG", "Adaptive Multi-Rate Wideband", "Adaptive Multi-Rate audio codec", "Adaptive Transform Acoustic Coding", "Advanced Audio Coding", "Advanced Systems Format", "Algebraic code-excited linear prediction", "Alliance for Open Media", "Apple Inc.", "Apple Lossless", "Apple Video", "Asao (codec)", "Au file format", "Audio Interchange File Format", "Audio Lossless Coding", "Audio Video Interleave", "Audio Video Standard", "Avid Audio", "BMP file format", "Better Portable Graphics", "Bink Video", "Block truncation coding", "CELT", "CineForm", "Cinepak", "Code-excited linear prediction", "Codec", "Codec2", "Color Cell Compression", "Color quantization", "Conditional replenishment", "DNxHD codec", "DTS (sound system)", "DV", "Daala", "Data compression", "Digital Item", "Digital Video Interactive", "Digital container format", "Dirac (video compression format)", "DivX", "DjVu", "Dolby Digital", "Dynamic Resolution Adaptation", "Enhanced VOB", "Enhanced Variable Rate Codec", "Enhanced Variable Rate Codec B", "Enhanced full rate", "Extended Adaptive Multi-Rate \u2013 Wideband", "FFV1", "FLAC", "Flash Video", "FourCC", "Free Lossless Image Format", "Full Rate", "G.711", "G.718", "G.719", "G.722", "G.722.1", "G.723", "G.723.1", "G.726", "G.728", "G.729", "G.729.1", "GIF", "General Exchange Format", "Group 4 compression", "H.120", "H.261", "H.262/MPEG-2 Part 2", "H.263", "H.264/MPEG-4 AVC", "Half Rate", "Harmonic Vector Excitation Coding", "High-Efficiency Advanced Audio Coding", "High Efficiency Video Coding", "Huffyuv", "ICER", "IETF", "ISO base media file format", "ITU-T", "Image compression", "Indeo", "Indexed color", "Interchange File Format", "International Electrotechnical Commission", "International Organization for Standardization", "Internet Low Bitrate Codec", "Internet Speech Audio Codec", "JBIG", "JBIG2", "JPEG", "JPEG 2000", "JPEG XR", "Lagarith", "Libavcodec", "Lossless JPEG", "Lossy", "MOD and TOD", "MP3", "MPEG", "MPEG-1", "MPEG-1 Audio Layer I", "MPEG-1 Audio Layer II", "MPEG-2", "MPEG-4", "MPEG-4 Part 14", "MPEG-4 Part 2", "MPEG-4 SLS", "MPEG-H", "MPEG-H 3D Audio", "MPEG-LA", "MPEG Multichannel", "MPEG Surround", "MPEG media transport", "MPEG program stream", "MPEG transport stream", "MSU Lossless Video Codec", "MT9", "Material Exchange Format", "Matroska", "Microsoft Video 1", "Monkey's Audio", "Motion JPEG", "Motion JPEG 2000", "Motion compensation", "Multimedia", "Multiple-image Network Graphics", "Musepack", "NETVC", "OMS Video", "Ogg", "OpenEXR", "OptimFROG", "Opus (audio format)", "Original Sound Quality", "Palette (computing)", "Pixlet", "Portable Network Graphics", "ProRes 422", "ProRes 4444", "Progressive Graphics File", "Pulse-code modulation", "Qualcomm code-excited linear prediction", "QuickTime", "QuickTime Animation", "QuickTime File Format", "QuickTime VR", "RGB", "RTAudio", "RTVideo", "RatDVD", "RealAudio", "RealMedia", "RealVideo", "Relaxed code-excited linear prediction", "Resource Interchange File Format", "Run-length encoding", "S3 Texture Compression", "SILK", "SVOPC", "Selectable Mode Vocoder", "SheerVideo", "Shorten (file format)", "Siren (codec)", "Smacker video", "Sorenson codec", "Speex", "Super Audio CD", "TIFF/EP", "TTA (codec)", "Tagged Image File Format", "Theora", "Transform coding", "TwinVQ", "Unified Speech and Audio Coding", "VC-1", "VOB", "VP3", "VP6", "VP7", "VP8", "VP9", "Variable-Rate Multimode Wideband", "Vector quantization", "Vector sum excited linear prediction", "Vorbis", "WAV", "WavPack", "WebM", "WebP", "Windows Media Audio", "Windows Media Video (video compression format)", "Wireless Application Protocol Bitmap Format", "XAnim", "YULS", "\u039c-law algorithm"], "categories": ["Apple Inc. software", "Lossy compression algorithms", "Video codecs"], "title": "QuickTime Graphics"}
{"summary": "S2TC (short for Super Simple Texture Compression) is a texture compression algorithm designed to be compatible with existing patented S3TC decompressors while avoiding any need for patent licensing fees. According to the authors, compressed textures produced by a good S2TC implementation are similar in quality to compressed textures produced by a bad S3TC implementation. The S2TC reference implementation is also capable of decompressing S3TC compressed textures, but instead of implementing the patented aspects of the algorithm, the S2TC decompressor picks colors at random. S2TC was created to provide an alternative to S3TC for open source OpenGL implementations which are legally constrained from implementing patented algorithms.", "links": ["Algorithm", "Data compression", "OpenGL", "Open source software", "Patent", "S3TC", "Texture compression", "Texture mapping"], "categories": ["Lossy compression algorithms", "Texture compression"], "title": "S2TC"}
{"summary": "S3 Texture Compression (S3TC) (sometimes also called DXTn or DXTC) is a group of related lossy texture compression algorithms originally developed by Iourcha et al. of S3 Graphics, Ltd. for use in their Savage 3D computer graphics accelerator. The method of compression is strikingly similar to the previously published Color Cell Compression, which is in turn an adaptation of Block Truncation Coding published in the late 1970s. Unlike some image compression algorithms (e.g. JPEG), S3TC's fixed-rate data compression coupled with the single memory access (cf. Color Cell Compression and some VQ-based schemes) made it well-suited for use in compressing textures in hardware-accelerated 3D computer graphics. Its subsequent inclusion in Microsoft's DirectX 6.0 and OpenGL 1.3 (via the GL_EXT_texture_compression_s3tc extension) led to widespread adoption of the technology among hardware and software makers. While S3 Graphics is no longer a competitor in the graphics accelerator market, license fees are still levied and collected for the use of S3TC technology, for example in game consoles and graphics cards. The wide use of S3TC has led to a de facto requirement for OpenGL drivers to support it, but the patent-encumbered status of S3TC presents a major obstacle to open source implementations.", "links": ["3D computer graphics", "3Dc", "ATI Technologies", "Adaptive Scalable Texture Compression", "Algorithm", "Alpha channel", "Alpha compositing", "Apple Video", "Bit", "Block Truncation Coding", "Color Cell Compression", "Compression artifacts", "De facto", "DirectDraw Surface", "DirectX", "Doom 3", "Ericsson Texture Compression", "FXT1", "FourCC", "Game console", "Id Software", "JPEG", "Lossy compression", "Lossy data compression", "MIT license", "Microsoft", "Normal mapping", "OpenGL", "Open source", "PVRTC", "Pixel shader", "RGBA color space", "RGB color space", "S3 Graphics", "Savage 3D", "Texture compression", "Texture mapping", "Vector quantization", "ZLIB license"], "categories": ["3D computer graphics", "Lossy compression algorithms", "Texture compression"], "title": "S3 Texture Compression"}
{"summary": "Vector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression. It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms.\nThe density matching property of vector quantization is powerful, especially for identifying the density of large and high-dimensioned data. Since data points are represented by the index of their closest centroid, commonly occurring data have low error, and rare data high error. This is why VQ is suitable for lossy data compression. It can also be used for lossy data correction and density estimation.\nVector quantization is based on the competitive learning paradigm, so it is closely related to the self-organizing map model and to sparse coding models used in deep learning algorithms such as autoencoder.", "links": [".VQA", "AMR-WB+", "Adaptive resonance theory", "Apple Video", "Autoencoder", "Bink video", "CELP", "Centroid", "Centroidal Voronoi tessellation", "Cinepak", "Cluster analysis", "Codebook", "Competitive learning", "Coordinate vector", "DTS Coherent Acoustics", "Daala", "Data clustering", "Data compression", "Deep Learning", "Deep learning", "Density estimation", "Digital Video Interactive", "Digital object identifier", "Dynamic time warping", "Entropy code", "Free On-line Dictionary of Computing", "G.729", "Hidden Markov model", "ILBC", "Image segmentation", "Indeo", "Internet Engineering Task Force", "K-means", "K-means clustering", "Learning vector quantization", "Linde\u2013Buzo\u2013Gray algorithm", "Linear subspace", "Lloyd's algorithm", "Lossy data compression", "MPEG", "MPEG-4", "Microsoft Video 1", "Motion compensation", "Neural gas", "Ogg Vorbis", "Opus (codec)", "Pattern recognition", "Prefix code", "Projection (mathematics)", "Quantization (signal processing)", "QuickTime", "QuickTime Graphics Codec", "Rate-distortion function", "Self-organizing map", "Signal processing", "Simulated annealing", "Smacker video", "Sorenson codec", "Sparse coding", "Speech coding", "Transform coding", "TwinVQ", "Vector space", "Voronoi diagram"], "categories": ["Articles to be expanded from February 2009", "Incomplete lists from August 2008", "Lossy compression algorithms"], "title": "Vector quantization"}
{"summary": "A vocoder (/\u02c8vo\u028ako\u028ad\u0259r/, short for voice encoder) is a category of voice codec that analyzes and synthesizes the human voice signal for audio data compression, multiplexing, voice encryption, voice transformation, etc.\nThe earliest type of vocoder, channel vocoder was originally developed as a speech coder for telecommunications applications in the 1930s, the idea being to code a speech for reducing bandwidth (i.e. audio data compression) for multiplexing transmission. On the channel vocoder algorithm, among the two components of analytic signal, only consider the amplitude component and simply ignore the phase component, and it tend to result in the unclear voice. For the improvement of this issue, see phase vocoder.\nIn the encoder, the input is passed through a multiband filter, each band is passed through an envelope follower, and the control signals from the envelope followers are transmitted to the decoder. The decoder applies these (amplitude) control signals to corresponding filters for re\u2013synthesis. Since the control signals change only slowly compared to the original speech waveform, the bandwidth required to transmit speech can be reduced. This allows more speech channels to share the single communication channel such as a radio channel or a submarine cable (i.e. multiplexing).\nBy encrypting the control signals, voice transmission can be secured against interception. Its primary use in this fashion is for secure radio communication. The advantage of this method of encryption is that none of the original signal is sent, but rather envelopes of the bandpass filters. The receiving unit needs to be set up in the same filter configuration to re\u2013synthesise a version of the original signal spectrum.\nThe vocoder has also been used extensively as an electronic musical instrument (see #Uses in music). The decoder portion of the vocoder, called a voder, can be used independently for speech synthesis (see #History).", "links": ["2001: A Space Odyssey", "A Clockwork Orange (film)", "Adaptive Differential Pulse Code Modulation", "Alan Parsons", "Algebraic code-excited linear prediction", "Amplitude", "Analytic signal", "Animals (Pink Floyd album)", "Around the World (Daft Punk song)", "Arthur C. Clarke", "Atari Teenage Riot", "Audio Engineering Society", "Audio codec", "Audio data compression", "Audio decoder", "Audio filter", "Audio timescale-pitch modification", "Auto-Tune", "Band-pass filter", "Bandpass filter", "Bandwidth (signal processing)", "Battlestar Galactica (1978 TV series)", "Bell Laboratories", "Bell Labs", "Bell Telephone Laboratories", "Bruce Haack", "Captain Scarlet and the Mysterons", "Channel (communications)", "Chris Martin", "Classical music", "Cochlear implant", "Code", "Code-excited linear prediction", "Coldplay", "Comb filter", "Computational neuroscience", "Consonant", "Continuously variable slope delta modulation", "Cylon (1978)", "Daft Punk", "Daisy Bell", "Deutsches Museum", "Digital filter", "Digital object identifier", "Disco", "Discovery (Electric Light Orchestra album)", "Doctor Who", "Doctor Who theme", "Dogs", "Electric Light Orchestra", "Electronic Music Studios", "Electronic music", "Electronic musical instrument", "Electronic oscillator", "Electronic rock", "Electropop", "Elsevier", "Embedded system", "Encoder", "Encryption", "Envelope follower", "Expression pedal", "Federal Information Processing Standard", "Filmmaking", "Five Miles Out", "Formant", "From Here to Eternity (Giorgio Moroder album)", "Fundamental frequency", "Get Lucky (Daft Punk song)", "Ghost Stories (Coldplay album)", "Giorgio Moroder", "Glottis", "Harald Bode", "Harmonic", "Haskins Laboratories", "Herbie Hancock", "Homer Dudley", "Homework (Daft Punk album)", "Human voice", "Hurts Like Heaven", "IBM 704", "ITU Telecommunication Standardization Sector", "I Robot (album)", "In the Air Tonight", "Infinite impulse response", "Inflection", "Instant Crush", "Instantaneous phase", "International Standard Book Number", "International Telecommunication Union", "Isao Tomita", "James Flanagan (engineer)", "Jean Michel Jarre", "Jeff Lynne", "John Larry Kelly, Jr.", "Julian Casablancas", "KY-57", "Kraftwerk", "Le Figaro", "Linear prediction", "Linear predictive coding", "Linguistics", "List of vocoders", "Live at Brixton Academy (Atari Teenage Riot album)", "Louis Gerstman", "Ludwig van Beethoven", "Main Street Electrical Parade", "Major Minus", "Mark Jenkins (musician)", "Max Mathews", "Michael Boddicker", "Michael Faraday", "Michael Jackson", "Microphone", "Midnight (Coldplay song)", "Mike Oldfield", "Mixed-excitation linear prediction", "Modular synthesizer", "Mr. Blue Sky", "Mr. Roboto", "Multi-band excitation", "Multiplexing", "Music", "Mylo Xyloto", "NPR Music", "NSA encryption systems", "National Security Agency", "Neil Young", "New-age music", "Noise (audio)", "Noise generator", "Nyquist rate", "O (Coldplay song)", "Out of the Blue (Electric Light Orchestra album)", "P.Y.T. (Pretty Young Thing)", "Peter Howell (musician)", "Phase vocoder", "Phil Collins", "Pink Floyd", "Pitch (sound)", "Pitch control", "Pop music", "Progressive rock", "Psychophysics", "Pulse-code modulation", "QE2 (album)", "Queen (band)", "Radio Ga Ga", "Random Access Memories", "Resonance", "Ring modulation", "Robert Moog", "Robotic voice effects", "Rock music", "Roger Meddows Taylor", "SIGSALY", "STU-III", "Sawtooth waveform", "Secure Communications Interoperability Protocol", "Secure Terminal Equipment", "Sibilance", "Sibilant consonant", "Siemens", "Silent speech interface", "Snowflakes are Dancing", "Solid state (electronics)", "Sonovox", "Sound synthesis", "Soundtrack", "Soundwave (Transformers)", "Special effect", "Speech codec", "Speech coding", "Speech communication", "Speech synthesis", "Stanley Kubrick", "Stevie Wonder", "Stop consonant", "Studio for Electronic Music (WDR)", "Styx (band)", "Submarine communications cable", "Supermarionation", "Sweet Talkin' Woman", "Synthesizer", "Synthpop", "T-Pain effect", "Tales of Mystery and Imagination", "Talk box", "Telecommunications", "Television production", "The Alan Parsons Project", "The Diary of Horace Wimp", "The Electric Lucifer", "The Raven (song)", "The Voder", "The Works (Queen album)", "Thriller (Michael Jackson album)", "Time (Electric Light Orchestra album)", "Trans (album)", "Transformers", "United States", "University at Buffalo, The State University of New York", "University of California, Santa Barbara", "Variable-gain amplifier", "VoIP", "Vocal folds", "Vocal tract", "Vocoders", "Voder", "Voice codec", "Voice encryption", "Voiceless consonant", "Vowel", "Wendy Carlos", "Werner Meyer-Eppler", "Westdeutscher Rundfunk", "Wireless local loop", "World War II", "Zoolook"], "categories": ["All articles needing additional references", "All articles with unsourced statements", "Articles needing additional references from May 2013", "Articles with hAudio microformats", "Articles with specifically marked weasel-worded phrases from July 2014", "Articles with unsourced statements from July 2012", "Articles with unsourced statements from July 2014", "Articles with unsourced statements from June 2011", "Articles with unsourced statements from March 2015", "Articles with unsourced statements from October 2008", "Audio effects", "CS1 German-language sources (de)", "Electronic musical instruments", "Lossy compression algorithms", "Music hardware", "Robotics", "Speech codecs"], "title": "Vocoder"}
{"summary": "The Wavelet Scalar Quantization algorithm (WSQ) is a compression algorithm used for gray-scale fingerprint images. It is based on wavelet theory and has become a standard for the exchange and storage of fingerprint images. WSQ was developed by the FBI, the Los Alamos National Laboratory, and the National Institute of Standards and Technology (NIST).\nThis compression method is preferred over standard compression algorithms like JPEG because at the same compression ratios WSQ doesn't present the \"blocking artifacts\" and loss of fine-scale features that are not acceptable for identification in financial environments and criminal justice. \nMost American law enforcement agencies use Wavelet Scalar Quantization (WSQ) - for efficient storage of compressed fingerprint images at 500 pixels per inch (ppi). For fingerprints recorded at 1000 ppi spatial resolution, law enforcement (including the FBI) uses JPEG 2000 instead of WSQ.", "links": ["FBI", "Fingerprint", "IAFIS", "International Standard Book Number", "JPEG", "Los Alamos National Laboratory", "National Institute of Standards and Technology", "Wavelet"], "categories": ["Fingerprints", "Graphics file formats", "Lossy compression algorithms"], "title": "Wavelet scalar quantization"}
{"summary": "In mathematics and computer science, optimal addition-chain exponentiation is a method of exponentiation by positive integer powers that requires a minimal number of multiplications. It works by creating the shortest addition chain that generates the desired exponent. Each exponentiation in the chain can be evaluated by multiplying two of the earlier exponentiation results. More generally, addition-chain exponentiation may also refer to exponentiation by non-minimal addition chains constructed by a variety of algorithms (since a shortest addition chain is very difficult to find).\nThe shortest addition-chain algorithm requires no more multiplications than binary exponentiation and usually less. The first example of where it does better is for a15, where the binary method needs six multiplications but a shortest addition chain requires only five:\n (binary, 6 multiplications)\n (shortest addition chain, 5 multiplications).\nOn the other hand, the determination of a shortest addition chain is hard: no efficient optimal methods are currently known for arbitrary exponents, and the related problem of finding a shortest addition chain for a given set of exponents has been proven NP-complete. Even given a shortest chain, addition-chain exponentiation requires more memory than the binary method, because it must potentially store many previous exponents from the chain. So in practice, shortest addition-chain exponentiation is primarily used for small fixed exponents for which a shortest chain can be precomputed and is not too large.\nThere are also several methods to approximate a shortest addition chain, and which often require fewer multiplications than binary exponentiation; binary exponentiation itself is a suboptimal addition-chain algorithm. The optimal algorithm choice depends on the context (such as the relative cost of the multiplication and the number of times a given exponent is re-used).\nThe problem of finding the shortest addition chain cannot be solved by dynamic programming, because it does not satisfy the assumption of optimal substructure. That is, it is not sufficient to decompose the power into smaller powers, each of which is computed minimally, since the addition chains for the smaller powers may be related (to share computations). For example, in the shortest addition chain for a15 above, the subproblem for a6 must be computed as (a3)2 since a3 is re-used (as opposed to, say, a6 = a2(a2)2, which also requires three multiplies).", "links": ["Addition-subtraction chain", "Addition chain", "Algorithm", "Binary exponentiation", "Computer science", "Digital object identifier", "Donald E. Knuth", "Dynamic programming", "Elliptic curve", "Exponentiation", "Integer", "Mathematics", "NP-complete", "Negative number", "Optimal substructure"], "categories": ["Addition chains", "Computer arithmetic algorithms", "Exponentials"], "title": "Addition-chain exponentiation"}
{"summary": "In mathematics, the AGM method (for arithmetic\u2013geometric mean) makes it possible to construct fast algorithms for calculation of exponential and trigonometric functions, and some mathematical constants and in particular, to quickly compute .", "links": ["Algorithm", "Arithmetic\u2013geometric mean", "Computing \u03c0", "Digital object identifier", "Elementary function", "Elliptic integral", "Eugene Salamin (mathematician)", "Exponential function", "Fast algorithms", "Gauss", "Gauss\u2013Legendre algorithm", "International Standard Book Number", "Jonathan Borwein", "Mathematical Reviews", "Mathematical constant", "Mathematics", "Peter Borwein", "Pi", "Richard Brent (scientist)", "Transcendental function", "Trigonometric functions"], "categories": ["All articles to be merged", "Articles to be merged from September 2012", "Computer arithmetic algorithms"], "title": "AGM method"}
{"summary": "In mathematics, binary splitting is a technique for speeding up numerical evaluation of many types of series with rational terms. In particular, it can be used to evaluate hypergeometric series at rational points. Given a series\n\nwhere pn and qn are integers, the goal of binary splitting is to compute integers P(a, b) and Q(a, b) such that\n\nThe splitting consists of setting m = [(a + b)/2] and recursively computing P(a, b) and Q(a, b) from P(a, m), P(m, b), Q(a, m), and Q(m, b). When a and b are sufficiently close, P(a, b) and Q(a, b) can be computed directly from pa...pb and qa...qb.\nBinary splitting requires more memory than direct term-by-term summation, but is asymptotically faster since the sizes of all occurring subproducts are reduced. Additionally, whereas the most naive evaluation scheme for a rational series uses a full-precision division for each term in the series, binary splitting requires only one final division at the target precision; this is not only faster, but conveniently eliminates rounding errors. To take full advantage of the scheme, fast multiplication algorithms such as Toom\u2013Cook and Sch\u00f6nhage\u2013Strassen must be used; with ordinary O(n2) multiplication, binary splitting may render no speedup at all or be slower.\nSince all subdivisions of the series can be computed independently of each other, binary splitting lends well to parallelization and checkpointing.\nIn a less specific sense, binary splitting may also refer to any divide and conquer algorithm that always divides the problem in two halves.", "links": ["Checkpointing", "Class Library for Numbers", "Divide and conquer algorithm", "Hypergeometric series", "Mathematics", "Parallelization", "Sch\u00f6nhage\u2013Strassen algorithm", "Series (mathematics)", "Toom\u2013Cook multiplication"], "categories": ["Computer arithmetic algorithms"], "title": "Binary splitting"}
{"summary": "The following tables list the running time of various algorithms for common mathematical operations.\nHere, complexity refers to the time complexity of performing computations on a multitape Turing machine. See big O notation for an explanation of the notation used.\nNote: Due to the variety of multiplication algorithms, M(n) below stands in for the complexity of the chosen multiplication algorithm.", "links": ["ACM Symposium on Theory of Computing", "Addition", "Alfred Aho", "Algorithm", "Analysis of algorithms", "Analytic function", "ArXiv", "Arithmetic-geometric mean", "Bal\u00e1zs Szegedy", "Bareiss algorithm", "Big O notation", "Binary GCD algorithm", "Binary splitting", "Bit burst", "Chris Umans", "Computational number theory", "Coppersmith\u2013Winograd algorithm", "Determinant", "Digital object identifier", "Divide and conquer algorithm", "Division (mathematics)", "Donald Knuth", "E (mathematical constant)", "Elementary function", "Euclidean algorithm", "Euler\u2013Mascheroni constant", "Exponential function", "Exponential integral", "Exponentiation by squaring", "Factorial", "Fast Fourier transform", "Floating-point arithmetic", "F\u00fcrer's algorithm", "Gamma function", "Gauss\u2013Jordan elimination", "Golden ratio", "Greatest common divisor", "Henry Cohn", "Horner's method", "Hypergeometric function", "ISSAC", "Incomplete gamma function", "International Standard Book Number", "Invertible matrix", "Iterated logarithm", "Jacobi symbol", "Jeffrey Ullman", "John Hopcroft", "Karatsuba algorithm", "LU decomposition", "Laplace expansion", "Long division", "Machin's formula", "Mathematical operation", "Matrix inversion", "Matrix multiplication", "Modular exponentiation", "Montgomery reduction", "Multiplication", "Multiplication algorithm", "Multitape Turing machine", "Natural logarithm", "Newton's method", "Newton\u2013Raphson division", "Number theory", "Paul Zimmermann", "Pi", "Polynomial", "Ran Raz", "Richard P. Brent", "Robert Kleinberg", "Salamin\u2013Brent algorithm", "Sch\u00f6nhage controlled Euclidean descent algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Square root", "Square root of 2", "Stehl\u00e9\u2013Zimmermann algorithm", "Strassen algorithm", "Subtraction", "Taylor series", "The Art of Computer Programming", "Time complexity", "Toom\u2013Cook multiplication", "Triangular matrix", "Trigonometric function"], "categories": ["All articles needing additional references", "Articles needing additional references from April 2015", "Computational complexity theory", "Computer arithmetic algorithms", "Mathematics-related lists", "Number theoretic algorithms", "Unsolved problems in computer science"], "title": "Computational complexity of mathematical operations"}
{"summary": "A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.\nDivision algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton\u2013Raphson and Goldschmidt fall into this category.\nDiscussion will refer to the form , where\nN = Numerator (dividend)\nD = Denominator (divisor)\nis the input, and\nQ = Quotient\nR = Remainder\nis the output.\n\n", "links": ["AMD", "Algorithm", "Analysis of algorithms", "Approximation", "Barrett reduction", "Binomial theorem", "Chunking (division)", "Cryptography", "Digital object identifier", "Division (mathematics)", "Double precision", "Equioscillation theorem", "Euclid's Elements", "Euclidean division", "Extended precision", "Fixed point arithmetic", "Floating point", "Fused multiply\u2013add", "Greatest common divisor", "Hexadecimal", "Integer (computer science)", "International Standard Book Number", "Karatsuba algorithm", "Long division", "Lookup table", "Microprocessor", "Modular arithmetic", "Multiplication algorithm", "Multiplicative inverse", "Newton's method", "OCLC", "Original Intel Pentium (P5 microarchitecture)", "Output-sensitive algorithm", "Pentium FDIV bug", "Power of two", "Precision (computer science)", "Quotient", "Radix", "Rate of convergence", "Relative error", "Remainder", "Remez algorithm", "Round-off error", "Sch\u00f6nhage\u2013Strassen algorithm", "Short division", "Single precision", "Toom\u2013Cook multiplication"], "categories": ["All articles to be expanded", "All articles with unsourced statements", "All pages needing factual verification", "Articles to be expanded from September 2012", "Articles with example pseudocode", "Articles with unsourced statements from February 2012", "Articles with unsourced statements from February 2014", "Binary arithmetic", "Computer arithmetic", "Computer arithmetic algorithms", "Division (mathematics)", "Wikipedia articles needing clarification from July 2015", "Wikipedia articles needing factual verification from June 2015"], "title": "Division algorithm"}
{"summary": "In computer science, the double dabble algorithm is used to convert binary numbers into binary-coded decimal (BCD) notation. It is also known as the shift and add 3 algorithm, and can be implemented using a small number of gates in computer hardware, but at the expense of high latency. The algorithm operates as follows:\nSuppose the original number to be converted is stored in a register that is n bits wide. Reserve a scratch space wide enough to hold both the original number and its BCD representation; 4\u00d7ceil(n/3) bits will be enough. It takes a maximum of 4 bits in binary to store each decimal digit.\nThen partition the scratch space into BCD digits (on the left) and the original register (on the right). For example, if the original number to be converted is eight bits wide, the scratch space would be partitioned as follows:\n\n100s Tens Ones   Original\n0010 0100 0011   11110011\n\nThe diagram above shows the binary representation of 24310 in the original register, and the BCD representation of 243 on the left.\nThe scratch space is initialized to all zeros, and then the value to be converted is copied into the \"original register\" space on the right.\n\n0000 0000 0000   11110011\n\nThe algorithm then iterates n times. On each iteration, the entire scratch space is left-shifted one bit. However, before the left-shift is done, any BCD digit which is greater than 4 is incremented by 3. The increment ensures that a value of 5, incremented and left-shifted, becomes 16, thus correctly \"carrying\" into the next BCD digit.\nThe double-dabble algorithm, performed on the value 24310, looks like this:\n\n0000 0000 0000   11110011   Initialization\n0000 0000 0001   11100110   Shift\n0000 0000 0011   11001100   Shift\n0000 0000 0111   10011000   Shift\n0000 0000 1010   10011000   Add 3 to ONES, since it was 7\n0000 0001 0101   00110000   Shift\n0000 0001 1000   00110000   Add 3 to ONES, since it was 5\n0000 0011 0000   01100000   Shift\n0000 0110 0000   11000000   Shift\n0000 1001 0000   11000000   Add 3 to TENS, since it was 6\n0001 0010 0001   10000000   Shift\n0010 0100 0011   00000000   Shift\n   2    4    3\n       BCD\n\nNow eight shifts have been performed, so the algorithm terminates. The BCD digits to the left of the \"original register\" space display the BCD encoding of the original value 243.\nAnother example for the double dabble algorithm - value 6524410.\n\n 104  103  102   101  100    Original binary\n0000 0000 0000 0000 0000   1111111011011100   Initialization\n0000 0000 0000 0000 0001   1111110110111000   Shift left (1st)\n0000 0000 0000 0000 0011   1111101101110000   Shift left (2nd)\n0000 0000 0000 0000 0111   1111011011100000   Shift left (3rd)\n0000 0000 0000 0000 1010   1111011011100000   Add 3 to 100, since it was 7\n0000 0000 0000 0001 0101   1110110111000000   Shift left (4th)\n0000 0000 0000 0001 1000   1110110111000000   Add 3 to 100, since it was 5\n0000 0000 0000 0011 0001   1101101110000000   Shift left (5th)\n0000 0000 0000 0110 0011   1011011100000000   Shift left (6th)\n0000 0000 0000 1001 0011   1011011100000000   Add 3 to 101, since it was 6\n0000 0000 0001 0010 0111   0110111000000000   Shift left (7th)\n0000 0000 0001 0010 1010   0110111000000000   Add 3 to 100, since it was 7\n0000 0000 0010 0101 0100   1101110000000000   Shift left (8th)\n0000 0000 0010 1000 0100   1101110000000000   Add 3 to 101, since it was 5\n0000 0000 0101 0000 1001   1011100000000000   Shift left (9th)\n0000 0000 1000 0000 1001   1011100000000000   Add 3 to 102, since it was 5\n0000 0000 1000 0000 1100   1011100000000000   Add 3 to 100, since it was 9\n0000 0001 0000 0001 1001   0111000000000000   Shift left (10th)\n0000 0001 0000 0001 1100   0111000000000000   Add 3 to 100, since it was 9\n0000 0010 0000 0011 1000   1110000000000000   Shift left (11th)\n0000 0010 0000 0011 1011   1110000000000000   Add 3 to 100, since it was 8\n0000 0100 0000 0111 0111   1100000000000000   Shift left (12th)\n0000 0100 0000 1010 0111   1100000000000000   Add 3 to 101, since it was 7\n0000 0100 0000 1010 1010   1100000000000000   Add 3 to 100, since it was 7\n0000 1000 0001 0101 0101   1000000000000000   Shift left (13th)\n0000 1011 0001 0101 0101   1000000000000000   Add 3 to 103, since it was 8\n0000 1011 0001 1000 0101   1000000000000000   Add 3 to 101, since it was 5\n0000 1011 0001 1000 1000   1000000000000000   Add 3 to 100, since it was 5\n0001 0110 0011 0001 0001   0000000000000000   Shift left (14th)\n0001 1001 0011 0001 0001   0000000000000000   Add 3 to 103, since it was 6\n0011 0010 0110 0010 0010   0000000000000000   Shift left (15th)\n0011 0010 1001 0010 0010   0000000000000000   Add 3 to 102, since it was 6\n0110 0101 0010 0100 0100   0000000000000000   Shift left (16th)\n   6    5    2    4    4\n            BCD\n\nSixteen shifts have been performed, so the algorithm terminates. The BCD digits is: 6*104 + 5*103 + 2*102 + 4*101 + 4*100 = 65244.", "links": ["Algorithm", "Binary-coded decimal", "Binary numbers", "C (programming language)", "Computer science", "Digital object identifier", "International Standard Book Number", "Latency (engineering)", "Malloc", "Pedagogical", "Processor register"], "categories": ["Articles with example C code", "Binary arithmetic", "Computer arithmetic algorithms"], "title": "Double dabble"}
{"summary": "In mathematics and computer programming, exponentiating by squaring is a general method for fast computation of large positive integer powers of a number, or more generally of an element of a semigroup, like a polynomial or a square matrix. Some variants are commonly referred to as square-and-multiply algorithms or binary exponentiation. These can be of quite general use, for example in modular arithmetic or powering of matrices. For semigroups for which additive notation is commonly used, like elliptic curves used in cryptography, this method is also referred to as double-and-add.", "links": ["Abelian group", "Addition-chain exponentiation", "Addition chain", "Big-O notation", "Binary expansion", "Brauer", "Commutative", "Compiler", "Computer programming", "Cryptography", "Elliptic curve", "Floor function", "Group (mathematics)", "Hamming weight", "Heuristic", "Identity matrix", "Mathematics", "Matrix (math)", "Modular arithmetic", "Modular exponentiation", "Montgomery reduction", "Non-adjacent form", "Number", "Peter Montgomery (mathematician)", "Polynomial", "Positive integer", "Precomputation", "Public-key cryptography", "Recursion (computer science)", "Remainder", "Result", "Ring (mathematics)", "Ruby (programming language)", "Semigroup", "Side-channel attack", "Signed-digit representation", "Square matrix", "Static typing", "Tail call", "Timing attack", "Up to", "Vectorial addition chain"], "categories": ["Computer arithmetic", "Computer arithmetic algorithms", "Exponentials"], "title": "Exponentiation by squaring"}
{"summary": "In mathematics, the FEE method is the method of fast summation of series of a special form. It was constructed in 1990 by E. A. Karatsuba and was called FEE\u2014Fast E-function Evaluation\u2014because it makes it possible fast computations of the Siegel  -functions, and in particular, \nA class of functions, which are 'similar to the exponential function' was given the name 'E-functions' by Siegel. Among these functions are such special functions as the hypergeometric function, cylinder, spherical functions and so on.\nUsing the FEE, it is possible to prove the following theorem\nTheorem: Let  be an elementary Transcendental function, that is the exponential function, or a trigonometric function, or an elementary algebraic function, or their superposition, or their inverse, or a superposition of the inverses. Then\n\nHere  is the complexity of computation (bit) of the function  with accuracy up to  digits,  is the complexity of multiplication of two -digit integers.\nThe algorithms based on the method FEE include the algorithms for fast calculation of any elementary Transcendental function for any value of the argument, the classical constants e,  the Euler constant  the Catalan and the Ap\u00e9ry constants, such higher transcendental functions as the Euler gamma function and its derivatives, the hypergeometric, spherical, cylinder (including the Bessel) functions and some other functions for algebraic values of the argument and parameters, the Riemann zeta function for integer values of the argument and the Hurwitz zeta function for integer argument and algebraic values of the parameter, and also such special integrals as the integral of probability, the Fresnel integrals, the integral exponential function, the trigonometric integrals, and some other integrals for algebraic values of the argument with the complexity bound which is close to the optimal one, namely\n\nAt present, only the FEE makes it possible to calculate fast the values of the functions from the class of higher transcendental functions, certain special integrals of mathematical physics and such classical constants as Euler's, Catalan's and Ap\u00e9ry's constants. An additional advantage of the method FEE is the possibility of parallelizing the algorithms based on the FEE.", "links": ["AGM method", "Algebraic function", "Algebraic number", "Analysis of algorithms", "Ap\u00e9ry's constant", "Bessel function", "Carl Ludwig Siegel", "Catalan's constant", "Complexity of computation (bit)", "E (mathematical constant)", "Error function", "Euler\u2013Mascheroni constant", "Exponential function", "Exponential integral", "Fast algorithms", "Fresnel integral", "Gamma function", "Hurwitz zeta function", "Hypergeometric function", "Inverse function", "Pi", "Riemann zeta function", "Special functions", "Spherical harmonics", "Transcendental function", "Trigonometric functions", "Trigonometric integral"], "categories": ["Computer arithmetic algorithms", "Numerical analysis", "Pi algorithms", "Vague or ambiguous time from August 2011"], "title": "FEE method"}
{"summary": "F\u00fcrer's algorithm is an integer multiplication algorithm for very large numbers possessing a very low asymptotic complexity. It was created in 2007 by Swiss mathematician Martin F\u00fcrer of Pennsylvania State University as an asymptotically faster (when analysed on a multitape Turing machine) algorithm than its predecessor, the Sch\u00f6nhage\u2013Strassen algorithm published in 1971.\nThe predecessor to the F\u00fcrer algorithm, the Sch\u00f6nhage\u2013Strassen algorithm, used fast Fourier transforms to compute integer products in time  (in big O notation) and its authors, Arnold Sch\u00f6nhage and Volker Strassen, also conjectured a lower bound for the problem of . Here,  denotes the total number of bits in the two input numbers. F\u00fcrer's algorithm reduces the gap between these two bounds: it can be used to multiply binary integers of length  in time  (or by a circuit with that many logic gates), where log*n represents the iterated logarithm operation. However, the difference between the  and  factors in the time bounds of the Sch\u00f6nhage\u2013Strassen algorithm and the F\u00fcrer algorithm for realistic values of  is very small.\nIn 2008, Anindya De, Chandan Saha, Piyush Kurur and Ramprasad Saptharishi gave a similar algorithm that relies on modular arithmetic instead of complex arithmetic to achieve the same running time.\nIn 2014, David Harvey, Joris van der Hoeven, and Gr\u00e9goire Lecerf showed that an optimized version of F\u00fcrer's algorithm achieves a running time of , making the implied constant in the  exponent explicit. They also gave a modification to F\u00fcrer's algorithm that achieves \nIn 2015 Svyatoslav Covanov and Emmanuel Thom\u00e9 provided another modifications that achieves same running time. Yet, as all the other implementation, it's still not practical.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "ArXiv", "Arnold Sch\u00f6nhage", "Asymptotic complexity", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Big O notation", "Binary GCD algorithm", "Boolean circuit", "Chakravala method", "Cipolla's algorithm", "Continued fraction factorization", "Cornacchia's algorithm", "Data structure", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fast Fourier transform", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "General number field sieve", "Generating primes", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer multiplication algorithm", "Integer square root", "Iterated logarithm", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lower bound", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Martin F\u00fcrer", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Multiplication algorithm", "Number-theoretic transform", "Number theory", "Pennsylvania State University", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Switzerland", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Turing machine", "Volker Strassen", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Algorithms and data structures stubs", "All articles with unsourced statements", "All stub articles", "Articles with unsourced statements from June 2015", "Computer arithmetic algorithms", "Computer science stubs"], "title": "F\u00fcrer's algorithm"}
{"summary": "The Karatsuba algorithm is a fast multiplication algorithm. It was discovered by Anatolii Alexeevitch Karatsuba in 1960 and published in 1962. It reduces the multiplication of two n-digit numbers to at most  single-digit multiplications in general (and exactly  when n is a power of 2). It is therefore faster than the classical algorithm, which requires n2 single-digit products. For example, the Karatsuba algorithm requires 310 = 59,049 single-digit multiplications to multiply two 1024-digit numbers (n = 1024 = 210), whereas the classical algorithm requires (210)2 = 1,048,576.\nThe Karatsuba algorithm was the first multiplication algorithm asymptotically faster than the quadratic \"grade school\" algorithm. The Toom\u2013Cook algorithm is a faster generalization of Karatsuba's method, and the Sch\u00f6nhage\u2013Strassen algorithm is even faster, for sufficiently large n.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Anatolii Alexeevitch Karatsuba", "Ancient Egyptian multiplication", "Andrey Kolmogorov", "Asymptotically optimal", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Big-O notation", "Big O notation", "Binary GCD algorithm", "Carry-save adder", "Chakravala method", "Charles Babbage", "Cipolla's algorithm", "Computational complexity theory", "Computer platform", "Continued fraction factorization", "Cornacchia's algorithm", "Cybernetics", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Eric W. Weisstein", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer square root", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Master theorem", "MathWorld", "Miller\u2013Rabin primality test", "Modular exponentiation", "Moscow State University", "Multiplication ALU", "Multiplication algorithm", "Number theory", "Physics-Doklady", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proceedings of the USSR Academy of Sciences", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Radix", "Rational sieve", "Recurrence relation", "Recursion", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "The Art of Computer Programming", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm", "Yuri Petrovich Ofman"], "categories": ["Computer arithmetic algorithms", "Multiplication", "Pages with syntax highlighting errors"], "title": "Karatsuba algorithm"}
{"summary": "In numerical analysis, a branch of mathematics, there are several square root algorithms or methods of computing the principal square root of a nonnegative real number. For the square roots of a negative or complex number, see below.\nFinding  is the same as solving the equation . Therefore, any general numerical root-finding algorithm can be used. Newton's method, for example, reduces in this case to the so-called Babylonian method:\n\nGenerally, these methods yield approximate results. To get a higher precision for the root, a higher precision for the square is required and a larger number of steps must be calculated.", "links": ["Absolute value", "Algorithm", "Alpha max plus beta min algorithm", "Arithmetic mean", "Babylonian mathematics", "Bakhshali manuscript", "Binary numeral system", "Bitwise operation", "Brahmagupta", "CORDIC", "Calculator", "CiteSeer", "Complex number", "Continued fraction", "David Wheeler (computer scientist)", "Division algorithm", "Electronic Delay Storage Automatic Calculator", "Eric W. Weisstein", "Exponent bias", "Exponential function", "Fast inverse square root", "Floating point", "Fortran", "Fused multiply\u2013add", "Generalized continued fraction", "Geometric mean", "Halley's method", "Hero of Alexandria", "Heron's formula", "Heron's method", "Hexadecimal", "Householder's method", "IEEE floating-point standard", "Inequality of arithmetic and geometric means", "Integer square root", "International Standard Book Number", "Logarithm table", "Long division", "MathWorld", "Maurice Wilkes", "Mental calculation", "Methods of computing square roots", "Multiplicative inverse", "Napier's bones", "Natural logarithm", "Natural number", "Newton's method", "Nonnegative", "Normalized number", "Nth root algorithm", "Numeral system", "Numerical analysis", "On-Line Encyclopedia of Integer Sequences", "Order of convergence", "P-adic number", "Pell's equation", "Periodic continued fraction", "Quadratic convergence", "Quadratic integer", "Quadratic irrational", "Rate of convergence", "Real number", "Recurrence relation", "Relative error", "Root-finding algorithm", "SGI Indigo", "Scientific notation", "Seed value", "Shifting nth-root algorithm", "Shifting nth root algorithm", "Slide rule", "Space-time tradeoff", "Square number", "Square root", "Square root of 2", "Stanley Gill", "Taylor series", "Vedic Mathematics (book)"], "categories": ["All articles needing expert attention", "All articles that are too technical", "All articles that may contain original research", "Articles needing expert attention from September 2012", "Articles that may contain original research from January 2012", "Computer arithmetic algorithms", "Pages using web citations with no URL", "Root-finding algorithms", "Wikipedia articles that are too technical from September 2012"], "title": "Methods of computing square roots"}
{"summary": "Multiple Precision Integers and Rationals (MPIR) is an open-source software multiprecision integer library forked from the GNU Multiple Precision Arithmetic Library (GMP) project. It consists of much code from past GMP releases, and some original contributed code.\nAccording to the MPIR developers, some of the main goals of the MPIR project are:\nDeveloping parallel algorithms for multiprecision arithmetic including support for graphics processing units (GPU) and other multi-core processors.\nMaintaining compatibility with GMP - so that MPIR can be used as a replacement for GMP.\nProviding build support for Linux, Mac OS, Solaris and Windows systems.\nSupporting building MPIR using Microsoft based build tools for use in 32- and 64-bit versions of Windows.\nMPIR is optimised for many processors (CPUs). Assembly language code exists for these as of 2012: ARM, DEC Alpha 21064, 21164, and 21264, AMD K6, K6-2, Athlon, K8 and K10, Intel Pentium, Pentium Pro-II-III, Pentium 4, generic x86, Intel IA-64, Core 2, i7, Atom, Motorola-IBM PowerPC 32 and 64, MIPS R3000, R4000, SPARCv7, SuperSPARC, generic SPARCv8, UltraSPARC.", "links": [".NET Framework", "Abstract data type", "Algebraic data type", "Arbitrary-precision arithmetic", "Array data type", "Assembly language", "Associative array", "Bit", "Boolean data type", "Bottom type", "Byte", "C++", "C (programming language)", "C Sharp (programming language)", "Character (computing)", "Class (computer programming)", "Class Library for Numbers", "Complex data type", "Composite data type", "Computer compatibility", "Container (abstract data type)", "Cross-platform", "Data structure", "Data type", "Decimal data type", "Dependent type", "Double-precision floating-point format", "Enumerated type", "Exception handling", "Extended precision", "F Sharp (programming language)", "Fixed-point arithmetic", "Floating point", "Fork (software development)", "Function type", "GNU Lesser General Public License", "GNU Multiple Precision Arithmetic Library", "Generalized algebraic data type", "Generic programming", "GiNaC", "Graphics processing unit", "Half-precision floating-point format", "Inductive type", "Integer (computer science)", "Interface (computing)", "Interval arithmetic", "Intuitionistic type theory", "Kind (type theory)", "Library (computing)", "Linux", "List (abstract data type)", "List of software categories", "MPFR", "Mac OS", "Mathematical software", "Memory address", "Metaclass", "Metaobject", "Microsoft Windows", "Minifloat", "Multi-core processor", "Null-terminated string", "Object (computer science)", "Octuple-precision floating-point format", "Opaque data type", "Open-source software", "Operating system", "Option type", "Parallel algorithm", "Parametric polymorphism", "Physical address", "Plain text", "Pointer (computer programming)", "Primitive data type", "Product type", "Protocol (object-oriented programming)", "Quadruple-precision floating-point format", "Rational data type", "Record (computer science)", "Recursive data type", "Reference (computer science)", "Semaphore (programming)", "Set (abstract data type)", "Signedness", "Single-precision floating-point format", "Software build", "Software developer", "Software license", "Software release life cycle", "Solaris (operating system)", "Stream (computing)", "String (computer science)", "Subtyping", "Tagged union", "Ternary numeral system", "Top type", "Type class", "Type constructor", "Type conversion", "Type system", "Union type", "Unit type", "Virtual address space", "Void type", "Word (computer architecture)"], "categories": ["All articles containing potentially dated statements", "All articles with unsourced statements", "Articles containing potentially dated statements from 2012", "Articles with unsourced statements from April 2014", "C libraries", "Computer arithmetic", "Computer arithmetic algorithms", "Free software programmed in C", "Numerical software"], "title": "MPIR (mathematics software)"}
{"summary": "A multiplication algorithm is an algorithm (or method) to multiply two numbers. Depending on the size of the numbers, different algorithms are in use. Efficient multiplication algorithms have existed since the advent of the decimal system.", "links": ["ACC0", "AKS primality test", "Abacus", "Addison-Wesley", "Addition", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Analog computer", "Analog signal", "Ancient Egyptian multiplication", "Arbitrary-precision arithmetic", "Arnold Sch\u00f6nhage", "BBC News", "Baby-step giant-step", "Babylonian mathematics", "Baillie\u2013PSW primality test", "Big O notation", "Bignum", "Binary GCD algorithm", "Binary multiplier", "Booth encoding", "Branching program", "Chakravala method", "Cipolla's algorithm", "Computer algebra system", "Computer hardware", "Continued fraction factorization", "Cornacchia's algorithm", "Digital data", "Digital object identifier", "Discrete Fourier transform", "Discrete logarithm", "Distributivity", "Divide and conquer algorithm", "Division algorithm", "Dixon's factorization method", "Donald Knuth", "Elementary school", "Elliptic curve primality", "Elliptic curve primality proving", "Enderun", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "FL (complexity)", "Fast Fourier transform", "Fermat's factorization method", "Fermat primality test", "Fibonacci", "Floating-point unit", "Floating point", "Floor and ceiling functions", "Function field sieve", "F\u00fcrer's algorithm", "Gauss", "General number field sieve", "Generating primes", "Greatest common divisor", "Grid method multiplication", "Hardware multiplier", "Horner scheme", "Index calculus algorithm", "Integer factorization", "Integer overflow", "Integer square root", "Integrated circuit", "International Standard Book Number", "International Standard Serial Number", "JSTOR", "Joachim von zur Gathen", "Karatsuba algorithm", "Kronecker substitution", "Lattice multiplication", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Liber Abaci", "Linear map", "List of unsolved problems in computer science", "Logarithm", "Logical AND", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "MOS Technology 6502", "Matrak\u00e7\u0131 Nasuh", "Mental calculation", "Mersenne Prime", "Microcode", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Muhammad ibn Musa al-Khwarizmi", "Multiplication", "Multiplication table", "Napier's bones", "Napier's rods", "Number-theoretic transform", "Number theory", "Numeral system", "Operational amplifier", "Partial products algorithm", "Peasant multiplication", "Piecewise linear function", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Poker chip", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Polynomial", "Primality test", "Primary school", "Prosthaphaeresis", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Radix", "Rational sieve", "Recursion", "Ring (mathematics)", "Rob Eastaway", "Rounding error", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Slide rule", "Solovay\u2013Strassen primality test", "Special number field sieve", "The Art of Computer Programming", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trachtenberg system", "Trial division", "Twiddle factor", "Volker Strassen", "Voltage", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["All articles needing additional references", "All articles to be expanded", "Articles needing additional references from January 2013", "Articles needing additional references from May 2013", "Articles needing additional references from September 2012", "Articles to be expanded from October 2008", "Computer arithmetic algorithms", "Multiplication", "Pages with citations having bare URLs", "Pages with citations lacking titles", "Unsolved problems in computer science"], "title": "Multiplication algorithm"}
{"summary": "Quote notation is a number system for representing rational numbers which was designed to be attractive for use in computer architecture. In a typical computer architecture, the representation and manipulation of rational numbers is a complex topic. In quote notation, arithmetic operations take particularly simple, consistent forms, and can produce exact answers with no roundoff error.\nQuote notation\u2019s arithmetic algorithms work with a typical right-to-left direction, in which the addition, subtraction, and multiplication algorithms have the same complexity for natural numbers, and division is easier than a typical division algorithm.\nThe notation was invented by Eric Hehner of the University of Toronto and Nigel Horspool, then at McGill University, and published in the SIAM Journal on Computing, v.8, n.2, May 1979, pp. 124\u2013134. The construction of this system follows the approach of Kurt Hensel's p-adic numbers.", "links": ["Bit", "Computer architecture", "Divisor", "Eric Hehner", "Integer", "Kurt Hensel", "McGill University", "Multiplication table", "Natural number", "Negative number", "Nigel Horspool", "Number base", "Numerical digit", "P-adic number", "Prime number", "Radix point", "Rational number", "Round-off error", "Roundoff error", "SIAM Journal on Computing", "Scientific notation", "Society for Industrial and Applied Mathematics", "Subtraction", "Two's complement", "University of Toronto"], "categories": ["CS1 errors: external links", "Computer arithmetic algorithms"], "title": "Quote notation"}
{"summary": "The Sch\u00f6nhage\u2013Strassen algorithm is an asymptotically fast multiplication algorithm for large integers. It was developed by Arnold Sch\u00f6nhage and Volker Strassen in 1971. The run-time bit complexity is, in Big O notation, O(n log n log log n) for two n-digit numbers. The algorithm uses recursive Fast Fourier transforms in rings with 22n + 1 elements, a specific type of number theoretic transform.\nThe Sch\u00f6nhage\u2013Strassen algorithm was the asymptotically fastest multiplication method known from 1971 until 2007, when a new method, F\u00fcrer's algorithm, was announced with lower asymptotic complexity; however, F\u00fcrer's algorithm currently only achieves an advantage for astronomically large values and is not used in practice.\nIn practice the Sch\u00f6nhage\u2013Strassen algorithm starts to outperform older methods such as Karatsuba and Toom\u2013Cook multiplication for numbers beyond 2215 to 2217 (10,000 to 40,000 decimal digits). The GNU Multi-Precision Library uses it for values of at least 1728 to 7808 64-bit words (33,000 to 150,000 decimal digits), depending on architecture. There is a Java implementation of Sch\u00f6nhage\u2013Strassen which uses it above 74,000 decimal digits.\nApplications of the Sch\u00f6nhage\u2013Strassen algorithm include mathematical empiricism, such as the Great Internet Mersenne Prime Search and computing approximations of \u03c0, as well as practical applications such as Kronecker substitution, in which multiplication of polynomials with integer coefficients can be efficiently reduced to large integer multiplication; this is used in practice by GMP-ECM for Lenstra elliptic curve factorization.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Approximations of \u03c0", "Arnold Sch\u00f6nhage", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Big O notation", "Binary GCD algorithm", "Bit complexity", "Chakravala method", "Cipolla's algorithm", "Continued fraction factorization", "Convolution theorem", "Cooley\u2013Tukey FFT algorithm", "Cornacchia's algorithm", "Cyclic convolution", "Discrete Fourier transform", "Discrete Fourier transform (general)", "Discrete logarithm", "Discrete weighted transform", "Dixon's factorization method", "Donald Knuth", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fast Fourier transform", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "GNU Multi-Precision Library", "General number field sieve", "Generating primes", "Great Internet Mersenne Prime Search", "Greatest common divisor", "Index calculus algorithm", "Integer", "Integer factorization", "Integer square root", "Karatsuba algorithm", "Karatsuba multiplication", "Kronecker substitution", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Locality of reference", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Mathematical empiricism", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Negacyclic convolution", "Number-theoretic transform", "Number theory", "Order (group theory)", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Positional notation", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Ring (mathematics)", "Root of unity", "Schoof's algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "The Art of Computer Programming", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Volker Strassen", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Computer arithmetic algorithms", "Multiplication"], "title": "Sch\u00f6nhage\u2013Strassen algorithm"}
{"summary": "The shifting nth root algorithm is an algorithm for extracting the nth root of a positive real number which proceeds iteratively by shifting in n digits of the radicand, starting with the most significant, and produces one digit of the root on each iteration, in a manner similar to long division.\n\n", "links": ["Algorithm", "Binary search", "Decimal precision", "Integer", "Invariant (computer science)", "Long division", "Nth root", "Numerical digit", "Radix", "Real number"], "categories": ["All articles lacking sources", "Articles lacking sources from May 2010", "Computer arithmetic algorithms", "Root-finding algorithms"], "title": "Shifting nth root algorithm"}
{"summary": "A spigot algorithm is an algorithm for computing the value of a mathematical constant such as \u03c0 or e which generates output digits left to right, with limited intermediate storage.\nThe name comes from a \"spigot\", meaning a tap or valve controlling the flow of a liquid.\nInterest in such algorithms was spurred in the early days of computational mathematics by extreme constraints on memory, and an algorithm for calculating the digits of e appears in a paper by Sale in 1968. The name \"Spigot algorithm\" appears to have been coined by Stanley Rabinowitz and Stan Wagon, whose algorithm for calculating the digits of \u03c0 is sometimes referred to as \"the spigot algorithm for \u03c0\".\nThe spigot algorithm of Rabinowitz and Wagon is bounded, in the sense that the number of required digits must be specified in advance. Jeremy Gibbons (2004) uses the term \"streaming algorithm\" to mean one which can be run indefinitely, without a prior bound. A further refinement is an algorithm which can compute a single arbitrary digit, without first computing the preceding digits: an example is the Bailey-Borwein-Plouffe formula, a digit extraction algorithm for \u03c0 which produces hexadecimal digits.", "links": ["Algorithm", "Bailey-Borwein-Plouffe formula", "Digital object identifier", "E (mathematical constant)", "Eric W. Weisstein", "Jeremy Gibbons", "MathWorld", "Modular arithmetic", "Modular exponentiation", "Natural logarithm", "On-Line Encyclopedia of Integer Sequences", "Pi", "Precision (arithmetic)", "Single precision", "Tap (valve)"], "categories": ["Computer arithmetic algorithms"], "title": "Spigot algorithm"}
{"summary": "Toom\u2013Cook, sometimes known as Toom-3, named after Andrei Toom, who introduced the new algorithm with its low complexity, and Stephen Cook, who cleaned the description of it, is a multiplication algorithm, a method of multiplying two large integers.\nGiven two large integers, a and b, Toom\u2013Cook splits up a and b into k smaller parts each of length l, and performs operations on the parts. As k grows, one may combine many of the multiplication sub-operations, thus reducing the overall complexity of the algorithm. The multiplication sub-operations can then be computed recursively using Toom\u2013Cook multiplication again, and so on. Although the terms \"Toom-3\" and \"Toom\u2013Cook\" are sometimes incorrectly used interchangeably, Toom-3 is only a single instance of the Toom\u2013Cook algorithm, where k = 3.\nToom-3 reduces 9 multiplications to 5, and runs in \u0398(nlog(5)/log(3)), about \u0398(n1.465). In general, Toom-k runs in \u0398(c(k) ne), where e = log(2k \u2212 1) / log(k), ne is the time spent on sub-multiplications, and c is the time spent on additions and multiplication by small constants. The Karatsuba algorithm is a special case of Toom\u2013Cook, where the number is split into two smaller ones. It reduces 4 multiplications to 3 and so operates at \u0398(nlog(3)/log(2)), which is about \u0398(n1.585). Ordinary long multiplication is equivalent to Toom-1, with complexity \u0398(n2).\nAlthough the exponent e can be set arbitrarily close to 1 by increasing k, the function c unfortunately grows very rapidly. The growth rate for mixed-level Toom-Cook schemes was still an open research problem in 2005. An implementation described by Donald Knuth achieves the time complexity \u0398(n 2\u221a(2 log n) log n).\nDue to its overhead, Toom\u2013Cook is slower than long multiplication with small numbers, and it is therefore typically used for intermediate-size multiplications, before the asymptotically faster Sch\u00f6nhage\u2013Strassen algorithm (with complexity \u0398(n log n log log n)) becomes practical.\nToom first described this algorithm in 1963, and Cook published an improved (asymptotically equivalent) algorithm in his PhD thesis in 1966.\n\n", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Andrei Toom", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Chakravala method", "Cipolla's algorithm", "Continued fraction factorization", "Cornacchia's algorithm", "Discrete logarithm", "Dixon's factorization method", "Donald Knuth", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "Gaussian elimination", "General number field sieve", "Generating primes", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer square root", "Karatsuba algorithm", "Karatsuba multiplication", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Positional notation", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Stephen Cook", "The Art of Computer Programming", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Computer arithmetic algorithms", "Multiplication"], "title": "Toom\u2013Cook multiplication"}
{"summary": "The Bailey\u2013Borwein\u2013Plouffe formula (BBP formula) is a spigot algorithm for computing the nth binary digit of pi (symbol: \u03c0) using base 16 math. The formula can directly calculate the value of any given digit of \u03c0 without calculating the preceding digits. The BBP is a summation-style formula that was discovered in 1995 by Simon Plouffe and was named after the authors of the paper in which the formula was published, David H. Bailey, Peter Borwein, and Simon Plouffe. Before that paper, it had been published by Plouffe on his own site. The formula is\n.\nThe discovery of this formula came as a surprise. For centuries it had been assumed that there was no way to compute the nth digit of \u03c0 without calculating all of the preceding n \u2212 1 digits.\nSince this discovery, many formulas for other irrational constants have been discovered of the general form\n\nwhere \u03b1 is the constant and p and q are polynomials in integer coefficients and b \u2265 2 is an integer base.\nFormulas in this form are known as BBP-type formulas. Certain combinations of specific p, q and b result in well-known constants, but there is no systematic algorithm for finding the appropriate combinations; known formulas are discovered through experimental mathematics.", "links": ["Ap\u00e9ry's constant", "ArXiv", "Bellard's formula", "Binary digit", "Carry (arithmetic)", "Catalan's constant", "Computing \u03c0", "David H. Bailey", "Digital object identifier", "Eric W. Weisstein", "Experimental mathematics", "Feynman point", "Helaman Ferguson", "Hexadecimal", "Infinity", "Integer relation algorithm", "Inverse tangent", "Long multiplication", "MathWorld", "Mathematical Intelligencer", "Mathematical Reviews", "Mathematics of Computation", "Modular arithmetic", "Modular exponentiation", "Peter Borwein", "Pi", "Polylogarithm ladder", "Polynomial", "Radix", "Richard J. Lipton", "Riemann zeta function", "Series (mathematics)", "Simon Plouffe", "Spigot algorithm", "Summation", "Time complexity"], "categories": ["All articles needing additional references", "Articles needing additional references from March 2014", "Pi algorithms"], "title": "Bailey\u2013Borwein\u2013Plouffe formula"}
{"summary": "In mathematics, Borwein's algorithm is an algorithm devised by Jonathan and Peter Borwein to calculate the value of 1/\u03c0. They devised several other algorithms. They published a book: Jonathon M. Borwein, Peter B. Borwein, Pi and the AGM - A Study in Analytic Number Theory and Computational Complexity, Wiley, New York, 1987. Many of their results are available in: Jorg Arndt, Christoph Haenel, Pi Unleashed, Springer, Berlin, 2001, ISBN 3-540-66572-2.", "links": ["Algorithm", "Bailey\u2013Borwein\u2013Plouffe formula", "Gauss\u2013Legendre algorithm", "International Standard Book Number", "Jonathan Borwein", "Mathematics", "Peter Borwein", "Pi", "Ramanujan\u2013Sato series"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from June 2011", "Pi algorithms"], "title": "Borwein's algorithm"}
{"summary": "The Chudnovsky algorithm is a fast method for calculating the digits of \u03c0. It was published by the Chudnovsky brothers in 1989, and was used in the world record calculations of 2.7 trillion digits of \u03c0 in December 2009, 5 trillion digits of \u03c0 in August 2010, 10 trillion digits of \u03c0 in October 2011, and 12.1 trillion digits in December 2013.\nThe algorithm is based on the negated Heegner number , the j-function , and on the following rapidly convergent generalized hypergeometric series:\n\nNote that 545140134 = 163 x 3344418 and,\n\nThis identity is similar to some of Ramanujan's formulas involving \u03c0, and is an example of a Ramanujan\u2013Sato series.", "links": ["Australian Broadcasting Corporation", "Chronology of computation of \u03c0", "Chudnovsky brothers", "Digital object identifier", "Generalized hypergeometric series", "Heegner number", "International Standard Serial Number", "J-invariant", "JSTOR", "Mathematical Reviews", "NewScientist", "Number", "Numerical approximations of \u03c0", "Pi", "Proceedings of the National Academy of Sciences", "PubMed Central", "PubMed Identifier", "Ramanujan", "Ramanujan\u2013Sato series"], "categories": ["All stub articles", "Number stubs", "Pi algorithms"], "title": "Chudnovsky algorithm"}
{"summary": "The Gauss\u2013Legendre algorithm is an algorithm to compute the digits of \u03c0. It is notable for being rapidly convergent, with only 25 iterations producing 45 million correct digits of \u03c0. However, the drawback is that it is memory intensive and it is therefore sometimes not used over Machin-like formulas.\nThe method is based on the individual work of Carl Friedrich Gauss (1777\u20131855) and Adrien-Marie Legendre (1752\u20131833) combined with modern algorithms for multiplication and square roots. It repeatedly replaces two numbers by their arithmetic and geometric mean, in order to approximate their arithmetic-geometric mean.\nThe version presented below is also known as the Gauss\u2013Euler, Brent\u2013Salamin (or Salamin\u2013Brent) algorithm; it was independently discovered in 1975 by Richard Brent and Eugene Salamin. It was used to compute the first 206,158,430,000 decimal digits of \u03c0 on September 18 to 20, 1999, and the results were checked with Borwein's algorithm.", "links": ["Adrien-Marie Legendre", "Algorithm", "Arithmetic-geometric mean", "Arithmetic mean", "Arithmetic\u2013geometric mean", "Borwein's algorithm", "Carl Friedrich Gauss", "Elliptic integral", "Eugene Salamin (mathematician)", "Geometric mean", "International Standard Serial Number", "Machin-like formulas", "Numerical approximations of \u03c0", "Pi", "Richard Brent (scientist)", "Square root"], "categories": ["Pi algorithms"], "title": "Gauss\u2013Legendre algorithm"}
{"summary": "Liu Hui's \u03c0 algorithm was invented by Liu Hui (fl. 3rd century), a mathematician of Wei Kingdom. Before his time, the ratio of the circumference of a circle to diameter was often taken experimentally as three in China, while Zhang Heng (78\u2013139) rendered it as 3.1724 (from the proportion of the celestial circle to the diameter of the earth, 92/29) or as . Liu Hui was not satisfied with this value. He commented that it was too large and overshot the mark. Another mathematician Wan Fan (219\u2013257) provided \u03c0 \u2248 142/45 \u2248 3.156. All these empirical \u03c0 values were accurate to two digits (i.e. one decimal place). Liu Hui was the first Chinese mathematician to provide a rigorous algorithm for calculation of \u03c0 to any accuracy. Liu Hui's own calculation with a 96-gon provided an accuracy of five digits: \u03c0 \u2248 3.1416.\nLiu Hui remarked in his commentary to the The Nine Chapters on the Mathematical Art, that the ratio of the circumference of an inscribed hexagon to the diameter of the circle was three, hence \u03c0 must be greater than three. He went on to provide a detailed step-by-step description of an iterative algorithm to calculate \u03c0 to any required accuracy based on bisecting polygons; he calculated \u03c0 to between 3.141024 and 3.142708 with a 96-gon; he suggested that 3.14 was a good enough approximation, and expressed \u03c0 as 157/50; he admitted that this number was a bit small. Later he invented an ingenious quick method to improve on it, and obtained \u03c0 \u2248 3.1416 with only a 96-gon, with an accuracy comparable to that from a 1536-gon. His most important contribution in this area was his simple iterative \u03c0 algorithm.", "links": ["A History of Pi", "Approximations of \u03c0", "Area of a disk", "Aryabhata", "Basel problem", "Chronology of computation of \u03c0", "Chudnovsky brothers", "Circumference", "Counting rods", "Feynman point", "He Chengtian", "Indiana Pi Bill", "Jin Dynasty (265\u2013420)", "John Machin", "John Wrench", "Joseph Needham", "Lindemann\u2013Weierstrass theorem", "List of formulae involving \u03c0", "List of topics related to \u03c0", "Liu Hui", "Ludolph van Ceulen", "Madhava of Sangamagrama", "Method of exhaustion", "Pi", "Pi Day", "Piphilology", "Proof that 22/7 exceeds \u03c0", "Proof that \u03c0 is irrational", "Pythagorean theorem", "Qin Jiushao", "Rod calculus", "Seki Takakazu", "Squaring the circle", "Takebe Katahiro", "The Nine Chapters on the Mathematical Art", "Wan Fan", "Wei Kingdom", "William Jones (mathematician)", "William Shanks", "Yasumasa Kanada", "Yoshio Mikami", "Yuan dynasty", "Zhang Heng", "Zhao Youqin's \u03c0 algorithm", "Zu Chongzhi"], "categories": ["All articles that may contain original research", "Articles that may contain original research from March 2009", "Cao Wei", "Chinese mathematics", "Pi", "Pi algorithms"], "title": "Liu Hui's \u03c0 algorithm"}
{"summary": "In computer science, a concurrent algorithm is one that can be executed concurrently. Most standard computer algorithms are sequential algorithms, and assume that the algorithm is run from start to finish without any other processes executing. These often do not behave correctly when run concurrently, as demonstrated at right, and are often nondeterministic, as the actual sequence of computations is determined by the external scheduler. Concurrency often adds significant complexity to an algorithm, requiring concurrency control such as mutual exclusion to avoid problems such as race conditions.\nMany parallel algorithms are run concurrently, particularly distributed algorithms, though these are distinct concepts in general.", "links": ["Algorithm", "Computer science", "Concurrency (computer science)", "Concurrency control", "Concurrent computation", "Data structure", "Distributed algorithm", "Mutual exclusion", "Nondeterministic algorithm", "Parallel algorithm", "Race condition", "Sequential algorithm"], "categories": ["Algorithms and data structures stubs", "All articles lacking sources", "All stub articles", "Articles lacking sources from February 2014", "Computer science stubs", "Concurrent algorithms"], "title": "Concurrent algorithm"}
{"summary": "In computer science, the ostrich algorithm is a strategy of ignoring potential problems on the basis that they may be exceedingly rare. It is named for the ostrich effect which is defined as \"to stick one's head in the sand and pretend there is no problem.\" It is used when it is more cost-effective to allow the problem to occur than to attempt its prevention.", "links": ["Banker's algorithm", "Computer science", "Concurrent programming", "Deadlock", "Integer overflow", "Ostrich effect", "Simplex algorithm", "Standard ML", "UNIX", "Windows"], "categories": ["Concurrent algorithms"], "title": "Ostrich algorithm"}
{"summary": "In computer science, a parallel algorithm, as opposed to a traditional serial algorithm, is an algorithm which can be executed a piece at a time on many different processing devices, and then combined together again at the end to get the correct result.\nMany parallel algorithms are executed concurrently \u2013 though in general concurrent algorithms are a distinct concept \u2013 and thus these concepts are often conflated, with which aspect of an algorithm is parallel and which is concurrent not being clearly distinguished. Further, non-parallel, non-concurrent algorithms are often referred to as \"sequential algorithms\", by contrast with concurrent algorithms.", "links": ["Algorithm", "Amdahl's law", "Analysis of parallel algorithms", "Application checkpointing", "Application programming interface", "Associative array", "Asymmetric multiprocessing", "Ateji PX", "Barrier (computer science)", "Beowulf cluster", "Bit-level parallelism", "Boost (C++ libraries)", "C++ AMP", "CUDA", "Cache-only memory architecture", "Cache coherence", "Cache invalidation", "Carnegie Mellon University", "Charm++", "Cilk", "Cloud computing", "Cluster computing", "Coarray Fortran", "Computer cluster", "Computer hardware", "Computer programming", "Computer science", "Concurrency (computer science)", "Concurrent algorithm", "Concurrent computing", "Cost efficiency", "Crossbar switch", "Data parallelism", "Deadlock", "Deterministic algorithm", "Distributed algorithm", "Distributed computing", "Distributed memory", "Distributed shared memory", "Dryad (programming)", "Embarrassingly parallel", "Embarrassingly parallel problem", "Explicit parallelism", "Fiber (computer science)", "Flynn's taxonomy", "Frequency scaling", "Global Arrays", "Grid computing", "Gustafson's law", "High-performance computing", "Hyper-threading", "Implicit parallelism", "Instruction-level parallelism", "Instruction window", "Karp\u2013Flatt metric", "Load balancing (computing)", "Lock (computer science)", "MIMD", "MISD", "Massively parallel (computing)", "Memory-level parallelism", "Memory coherence", "Message Passing Interface", "Message passing", "Multi-core", "Multiple-agent system", "Multiprocessing", "Multiprocessor", "Multithreading (computer architecture)", "Newton's method", "Non-blocking algorithm", "Non-uniform memory access", "Numerical analysis", "OpenACC", "OpenCL", "OpenHMPP", "OpenMP", "POSIX Threads", "Parallel Extensions", "Parallel LINQ", "Parallel Virtual Machine", "Parallel computing", "Parallel programming model", "Parallel random-access machine", "Parallel slowdown", "Pi", "Prime number", "Process (computing)", "Race condition", "Resource starvation", "Rubik's Cube", "SIMD", "SISD", "SPMD", "Scalability", "Semiconductor memory", "Serial algorithm", "Shared memory", "Shared memory (interprocess communication)", "Simultaneous multithreading", "Software lockout", "Speedup", "Supercomputer", "Superscalar", "Symmetric multiprocessing", "Synchronization (computer science)", "Task parallelism", "Temporal multithreading", "Thread (computing)", "Threading Building Blocks", "Three-body problem", "Throughput", "Unified Parallel C", "Uniform memory access", "Vector processor"], "categories": ["All articles needing additional references", "All articles to be expanded", "Articles needing additional references from November 2012", "Articles to be expanded from February 2014", "Concurrent algorithms", "Distributed algorithms", "Parallel computing"], "title": "Parallel algorithm"}
{"summary": "In computer science, the prefix sum, scan, or cumulative sum of a sequence of numbers x0, x1, x2, ... is a second sequence of numbers y0, y1, y2, ..., the sums of prefixes (running totals) of the input sequence:\ny0 = x0\ny1 = x0 + x1\ny2 = x0 + x1+ x2\n...\nFor instance, the prefix sums of the natural numbers are the triangular numbers:\n\nPrefix sums are trivial to compute in sequential models of computation, by using the formula yi = yi \u2212 1 + xi to compute each output value in sequence order. However, despite their ease of computation, prefix sums are a useful primitive in certain algorithms such as counting sort, and they form the basis of the scan higher-order function in functional programming languages. When datasets are stored in Fenwick trees, prefix sums can be calculated in O(log) time. Prefix sums of large datasets can be computed in using Fenwick tree. Prefix sums have also been much studied in parallel algorithms, both as a test problem to be solved and as a useful primitive to be used as a subroutine in other parallel algorithms.\nAbstractly, a prefix sum requires only a binary associative operator \u2295, making it useful for many applications from calculating well-separated pair decompositions of points to string processing. \nMathematically, the operation of taking prefix sums can be generalized from finite to infinite sequences; in that context, a prefix sum is known as a partial sum of a series. Prefix summation or partial summation form linear operators on the vector spaces of finite or infinite sequences; their inverses are finite difference operators.", "links": ["Adder (electronics)", "Addition", "Array data structure", "Associative property", "C++", "Charles E. Leiserson", "Clifford Stein", "Computer science", "Counting sort", "Digital object identifier", "Divided difference", "Eric W. Weisstein", "Euler tour", "Exclusive or", "Factorial", "Fenwick tree", "Finite difference", "Fold (higher-order function)", "Functional programming", "GPU", "Gray code", "Guy E. Blelloch", "Haskell (programming language)", "Hermite interpolation", "Higher order function", "Histogram", "Integer sorting", "International Standard Book Number", "Introduction to Algorithms", "Journal of the ACM", "Linear operator", "Linked list", "List ranking", "MIT Press", "Majority function", "MathWorld", "Mathematical Reviews", "McGraw-Hill", "Message Passing Interface", "Michael J. Fischer", "Natural number", "Newton form", "Oxford University Press", "Parallel algorithm", "Parallel random access machine", "Partial sum", "Polynomial interpolation", "Prefix (computer science)", "Proceedings of the USSR Academy of Sciences", "Radix sort", "Robert E. Tarjan", "Ron Rivest", "Running total", "SIAM Journal on Computing", "Series (mathematics)", "Sorting network", "Summation", "Thomas H. Cormen", "Tree (graph theory)", "Triangular number", "Uzi Vishkin", "Vandermonde", "Vector space", "Well-separated pair decomposition", "Yuri Petrovich Ofman"], "categories": ["CS1 Russian-language sources (ru)", "CS1 errors: external links", "CS1 uses Russian-language script (ru)", "Concurrent algorithms", "Higher-order functions"], "title": "Prefix sum"}
{"summary": "The Banker's algorithm, sometimes referred to as the avoidance algorithm, is a resource allocation and deadlock avoidance algorithm developed by Edsger Dijkstra that tests for safety by simulating the allocation of predetermined maximum possible amounts of all resources, and then makes an \"s-state\" check to test for possible deadlock conditions for all other pending activities, before deciding whether allocation should be allowed to continue.\nThe algorithm was developed in the design process for the THE operating system and originally described (in Dutch) in EWD108. When a new process enters a system, it must declare the maximum number of instances of each resource type that it may ever claim; clearly, that number may not exceed the total number of resources in the system. Also, when a process gets all its requested resources it must return them in a finite amount of time.", "links": ["Algorithm", "Deadlock", "Deadly embrace", "Dutch language", "Edsger Dijkstra", "Edsger W. Dijkstra", "Interface (computer science)", "Memory (computers)", "Operating system", "Resource (computer science)", "Resource allocation", "Semaphore (programming)", "THE (operating system)", "University of Texas at Austin"], "categories": ["All articles with unsourced statements", "Articles with example pseudocode", "Articles with unsourced statements from October 2015", "Concurrency control algorithms", "Dutch inventions"], "title": "Banker's algorithm"}
{"summary": "Dekker's algorithm is the first known correct solution to the mutual exclusion problem in concurrent programming. The solution is attributed to Dutch mathematician Th. J. Dekker by Edsger W. Dijkstra in an unpublished paper on sequential process descriptions and his manuscript on cooperating sequential processes. It allows two threads to share a single-use resource without conflict, using only shared memory for communication.\nIt avoids the strict alternation of a na\u00efve turn-taking algorithm, and was one of the first mutual exclusion algorithms to be invented.", "links": ["Busy wait", "Busy waiting", "C++11", "CPU", "Concurrent programming", "Critical section", "Deadlock", "Dutch people", "Edsger W. Dijkstra", "Eisenberg & McGuire algorithm", "Infinite loop", "Lamport's bakery algorithm", "Loop-invariant code motion", "Mathematician", "Memory barrier", "Memory ordering", "Mutual exclusion", "Peterson's algorithm", "Pseudocode", "Resource starvation", "Semaphore (programming)", "Shared memory (interprocess communication)", "Symmetric multiprocessing", "Szymanski's Algorithm", "Test-and-set", "Theodorus Dekker", "University of Texas at Austin", "Volatile variable"], "categories": ["All articles needing additional references", "Articles needing additional references from May 2015", "Concurrency control algorithms", "Dutch inventions"], "title": "Dekker's algorithm"}
{"summary": "Lamport's bakery algorithm is a computer algorithm devised by computer scientist Leslie Lamport, which is intended to improve the safety in the usage of shared resources among multiple threads by means of mutual exclusion.\nIn computer science, it is common for multiple threads to simultaneously access the same resources. Data corruption can occur if two or more threads try to write into the same memory location, or if one thread reads a memory location before another has finished writing into it. Lamport's bakery algorithm is one of many mutual exclusion algorithms designed to prevent concurrent threads entering critical sections of code concurrently to eliminate the risk of data corruption.", "links": ["Algorithm", "Analogy", "Compare-and-swap", "Computer science", "Concurrency (computer science)", "Cooperative multitasking", "Critical section", "Critical sections", "Data corruption", "Dekker's algorithm", "Eisenberg & McGuire algorithm", "Lamport's Distributed Mutual Exclusion Algorithm", "Leslie Lamport", "Lexicographical order", "Memory (computers)", "Mutual exclusion", "Peterson's algorithm", "PlusCal", "Pseudocode", "Semaphore (programming)", "Szymanski's Algorithm", "Thread (computer science)"], "categories": ["All articles lacking in-text citations", "All articles to be merged", "Articles lacking in-text citations from December 2010", "Articles to be merged from October 2013", "Articles with example pseudocode", "Concurrency control algorithms", "Use dmy dates from December 2012"], "title": "Lamport's bakery algorithm"}
{"summary": "Lamport's Distributed Mutual Exclusion Algorithm is a contention-based algorithm for mutual exclusion on a distributed system.", "links": ["Computer science", "Distributed system", "Lamport's bakery algorithm", "Lamport timestamps", "Maekawa's Algorithm", "Mutual exclusion", "Naimi-Trehel's Algorithm", "Raymond's Algorithm", "Ricart-Agrawala algorithm", "Suzuki-Kasami algorithm"], "categories": ["All articles to be merged", "All stub articles", "Articles to be merged from October 2013", "Computer science stubs", "Concurrency control algorithms"], "title": "Lamport's distributed mutual exclusion algorithm"}
{"summary": "Maekawa's algorithm is an algorithm for mutual exclusion on a distributed system. The basis of this algorithm is a quorum like approach where any one site needs only to seek permissions from a subset of other sites.", "links": ["Distributed system", "Lamport's Distributed Mutual Exclusion Algorithm", "Lamport's bakery algorithm", "Logical clock", "Mutual exclusion", "Raymond's algorithm", "Ricart-Agrawala algorithm"], "categories": ["All articles lacking sources", "Articles lacking sources from December 2009", "Concurrency control algorithms"], "title": "Maekawa's algorithm"}
{"summary": "In computer science, an algorithm is called non-blocking if failure or suspension of any thread cannot cause failure or suspension of another thread; for some operations, these algorithms provide a useful alternative to traditional blocking implementations. A non-blocking algorithm is lock-free if there is guaranteed system-wide progress, and wait-free if there is also guaranteed per-thread progress.\nThe word \"non-blocking\" was traditionally used to describe telecommunications networks that could route a connection through a set of relays \"without having to re-arrange existing calls\", see Clos network. Also, if the telephone exchange \"is not defective, it can always make the connection\", see Nonblocking minimal spanning switch.", "links": ["Algorithm", "Circular buffer", "Clos network", "Compare-and-swap", "Computer science", "Critical section", "Data structure", "Deadlock", "Digital object identifier", "FIFO (computing and electronics)", "Faith Ellen", "Hash table", "International Conference on Distributed Computing Systems", "International Standard Book Number", "Interrupt handler", "Linearizability", "Livelock", "Liveness", "Load-link/store-conditional", "Lock (computer science)", "Lock (software engineering)", "Multi-core processor", "Mutual exclusion", "Non-blocking I/O", "Nonblocking minimal spanning switch", "Parallel computing", "Pre-emptive multitasking", "Priority inversion", "Producer\u2013consumer problem", "Queue (data structure)", "Read-copy-update", "Read-modify-write", "Real-time computing", "Resource (computer science)", "Resource starvation", "Scheduling (computing)", "Semaphore (programming)", "Set (computer science)", "Software transactional memory", "Stack (data structure)", "Telecommunications network", "Thread (computing)"], "categories": ["All articles needing additional references", "All articles needing style editing", "All articles with unsourced statements", "Articles needing additional references from August 2010", "Articles with unsourced statements from June 2014", "CS1 maint: Date and year", "Concurrency control", "Concurrency control algorithms", "Synchronization", "Wikipedia articles needing style editing from October 2012"], "title": "Non-blocking algorithm"}
{"summary": "Peterson's algorithm (AKA Peterson's solution) is a concurrent programming algorithm for mutual exclusion that allows two processes to share a single-use resource without conflict, using only shared memory for communication. It was formulated by Gary L. Peterson in 1981. While Peterson's original formulation worked with only two processes, the algorithm can be generalized for more than two, as shown below.", "links": ["Algorithm", "Atomic operation", "Bounded bypass", "Compare-and-swap", "Concurrent programming", "Critical section", "DEC Alpha", "Dekker's algorithm", "Eisenberg & McGuire algorithm", "Gary L. Peterson", "International Standard Book Number", "Lamport's bakery algorithm", "Liveness", "Load-link/store-conditional", "MIPS architecture", "Mathematical induction", "Maurice Herlihy", "Memory barrier", "Memory ordering", "Michel Raynal", "Mutual exclusion", "Nir Shavit", "PowerPC", "Semaphore (programming)", "Symmetric multiprocessing", "Szymanski's Algorithm", "Test-and-set", "X86", "Xbox 360"], "categories": ["All articles with unsourced statements", "Articles with example C code", "Articles with unsourced statements from May 2015", "Concurrency control algorithms"], "title": "Peterson's algorithm"}
{"summary": "Raymond's Algorithm is a lock based algorithm for mutual exclusion on a distributed system. It imposes a logical structure (a K-ary tree) on distributed resources. As defined, each node has only a single parent, to which all requests to attain the token are made.", "links": ["Critical section", "Distributed system", "FIFO (computing and electronics)", "K-ary tree", "Lamport's Distributed Mutual Exclusion Algorithm", "Lamport's bakery algorithm", "Maekawa's Algorithm", "Mutual exclusion", "Naimi-Trehel's Algorithm", "Ricart-Agrawala algorithm", "Suzuki-Kasami's Algorithm"], "categories": ["Concurrency control algorithms"], "title": "Raymond's algorithm"}
{"summary": "In software engineering, a spinlock is a lock which causes a thread trying to acquire it to simply wait in a loop (\"spin\") while repeatedly checking if the lock is available. Since the thread remains active but is not performing a useful task, the use of such a lock is a kind of busy waiting. Once acquired, spinlocks will usually be held until they are explicitly released, although in some implementations they may be automatically released if the thread being waited on (that which holds the lock) blocks, or \"goes to sleep\".\nBecause they avoid overhead from operating system process rescheduling or context switching, spinlocks are efficient if threads are likely to be blocked for only short periods. For this reason, operating-system kernels often use spinlocks. However, spinlocks become wasteful if held for longer durations, as they may prevent other threads from running and require rescheduling. The longer a thread holds a lock, the greater the risk that the thread will be interrupted by the OS scheduler while holding the lock. If this happens, other threads will be left \"spinning\" (repeatedly trying to acquire the lock), while the thread holding the lock is not making progress towards releasing it. The result is an indefinite postponement until the thread holding the lock can finish and release it. This is especially true on a single-processor system, where each waiting thread of the same priority is likely to waste its quantum (allocated time where a thread can run) spinning until the thread that holds the lock is finally finished.\nImplementing spin locks correctly offers challenges because programmers must take into account the possibility of simultaneous access to the lock, which could cause race conditions. Generally, such implementation is possible only with special assembly-language instructions, such as atomic test-and-set operations, and cannot be easily implemented in programming languages not supporting truly atomic operations. On architectures without such operations, or if high-level language implementation is required, a non-atomic locking algorithm may be used, e.g. Peterson's algorithm. But note that such an implementation may require more memory than a spinlock, be slower to allow progress after unlocking, and may not be implementable in a high-level language if out-of-order execution is allowed.", "links": ["80386", "Assembly language", "Atomic operation", "Bus (computing)", "Busy spin", "Busy waiting", "Computer memory", "Context switch", "Cyrix", "Deadlock", "FreeBSD", "Gert Boddaert", "I486", "IA-64", "Intel", "International Standard Book Number", "Interrupt", "Jeffrey Richter", "John M. Mellor-Crummey", "LWN.net", "Lock (computer science)", "MESI", "Mac OS X", "Memory barrier", "Memory ordering", "Michael L. Scott", "Mutual exclusion", "Non-blocking synchronization", "Operating system", "Operating system kernel", "Out-of-order execution", "Pentium (brand)", "Pentium Pro", "Peterson's algorithm", "Race condition", "Real-time operating system", "Resource starvation", "Scheduling (computing)", "Seqlock", "Software engineering", "Solaris (operating system)", "Symmetric multiprocessing", "Synchronization (computer science)", "Test-and-set", "Thomas E. Anderson", "Thread (computer science)", "Thread (computing)", "Ticket lock", "Wait (operating system)"], "categories": ["All articles needing additional references", "Articles needing additional references from October 2012", "Concurrency control algorithms", "Programming constructs"], "title": "Spinlock"}
{"summary": "Szymanski's Mutual Exclusion Algorithm is a mutual exclusion algorithm devised by computer scientist Dr. Boleslaw Szymanski, which has many favorable properties including linear wait, and which extension  solved the open problem posted by Leslie Lamport whether there is an algorithm with a constant number of communication bits per process that satisfies every reasonable fairness and failure-tolerance requirement that Lamport conceived of (Lamport's solution used n factorial communication variables vs. Szymanski's 5).", "links": ["Amir Pnueli", "Boleslaw Szymanski", "Cache (computing)", "Computer science", "Dekker's algorithm", "Digital object identifier", "Eisenberg & McGuire algorithm", "Formal verification", "International Standard Book Number", "Lamport's bakery algorithm", "Leslie Lamport", "Mutual exclusion", "Peterson's algorithm", "Semaphore (programming)", "Zohar Manna"], "categories": ["All stub articles", "Computer science stubs", "Concurrency control algorithms"], "title": "Szyma\u0144ski's algorithm"}
{"summary": "In computer science, a ticket lock is a synchronization mechanism, or locking algorithm, that is a type of spinlock that uses \"tickets\" to control which thread of execution is allowed to enter a critical section.", "links": ["Array-based queueing locks", "Atomicity (programming)", "Busy waiting", "Compare-and-swap", "Computer science", "Critical section", "FIFO (computing and electronics)", "Fetch-and-add", "Fetch and add", "First In First Out", "International Standard Book Number", "Lamport's bakery algorithm", "Linux kernel", "Load-link/store-conditional", "Lock (computer science)", "Non-uniform memory access", "O(1)", "Paravirtualization", "Resource starvation", "Spinlock", "Test-and-set", "Test and Test-and-set", "Thread (computer science)", "Thundering herd problem", "Ticket lock", "Unbounded nondeterminism"], "categories": ["All articles containing potentially dated statements", "Articles containing potentially dated statements from July 2010", "Concurrency control algorithms", "Pages using citations with accessdate and no URL"], "title": "Ticket lock"}
{"summary": "In computer science, a timestamp-based concurrency control algorithm is a non-lock concurrency control method. It is used in some databases to safely handle transactions, using timestamps.", "links": ["Computer science", "Database", "Multiversion concurrency control", "Non-lock concurrency control", "Object (computer science)", "Schedule (computer science)", "Thomas Write Rule", "Timestamp", "Timestamping (computing)", "Total order"], "categories": ["Accuracy disputes from April 2012", "All accuracy disputes", "All articles lacking sources", "All articles needing cleanup", "Articles lacking sources from June 2007", "Articles needing cleanup from April 2012", "Cleanup tagged articles without a reason field from April 2012", "Concurrency control", "Concurrency control algorithms", "Transaction processing", "Wikipedia pages needing cleanup from April 2012"], "title": "Timestamp-based concurrency control"}
{"summary": "A distributed algorithm is an algorithm designed to run on computer hardware constructed from interconnected processors. Distributed algorithms are used in many varied application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation.\nDistributed algorithms are a sub-type of parallel algorithm, typically executed concurrently, with separate parts of the algorithm being run simultaneously on independent processors, and having limited information about what the other parts of the algorithm are doing. One of the major challenges in developing and implementing distributed algorithms is successfully coordinating the behavior of the independent parts of the algorithm in the face of processor failures and unreliable communications links. The choice of an appropriate distributed algorithm to solve a given problem depends on both the characteristics of the problem, and characteristics of the system the algorithm will run on such as the type and probability of processor or link failures, the kind of inter-process communication that can be performed, and the level of timing synchronization between separate processes.", "links": ["Algorithm", "Atomic commit", "Central processing unit", "Computer hardware", "Concurrency (computer science)", "Consensus (computer science)", "Distributed computing", "Information processing", "International Standard Book Number", "Leader election", "Morgan Kaufmann Publishers", "Mutual exclusion", "Non-blocking data structures", "Parallel algorithm", "Paxos algorithm", "Process control", "Replication (computer science)", "Resource allocation", "Scientific computing", "Search algorithm", "Spanning tree", "Spanning tree (mathematics)", "Telecommunications", "Three-phase commit protocol", "Two-phase commit protocol", "Vertex coloring"], "categories": ["Distributed algorithms"], "title": "Distributed algorithm"}
{"summary": "The Berkeley algorithm is a method of clock synchronisation in distributed computing which assumes no machine has an accurate time source. It was developed by Gusella and Zatti at the University of California, Berkeley in 1989  and like Cristian's algorithm is intended for use within intranets.", "links": ["Chang and Roberts algorithm", "Clock synchronisation", "Cristian's algorithm", "Digital object identifier", "Distributed computing", "Intranets", "Leader election", "Make (software)", "Round-trip time"], "categories": ["Distributed algorithms"], "title": "Berkeley algorithm"}
{"summary": "The bully algorithm is a programming mechanism that applies a hierarchy to nodes on a system, making a process coordinator or slave. This is used as a method in distributed computing for dynamically electing a coordinator by process ID number. The process with the highest process ID number is selected as the coordinator.", "links": ["Chang and Roberts algorithm", "Distributed Computing", "Distributed computing"], "categories": ["All Wikipedia articles needing clarification", "All self-contradictory articles", "Distributed algorithms", "Self-contradictory articles from January 2015", "Wikipedia articles needing clarification from January 2015"], "title": "Bully algorithm"}
{"summary": "In computer science, Cannon's algorithm is a distributed algorithm for matrix multiplication for two-dimensional meshes first described in 1969 by Lynn Elliot Cannon.\nIt is especially suitable for computers laid out in an N \u00d7 N mesh. While Cannon's algorithm works well in homogeneous 2D grids, extending it to heterogeneous 2D grids has been shown to be difficult.\nThe main advantage of the algorithm is that its storage requirements remain constant and are independent of the number of processors.\nThe Scalable Universal Matrix Multiplication Algorithm (SUMMA) is a more practical algorithm that requires less workspace and overcomes the need for a square 2D grid. It is used by the ScaLAPACK, PLAPACK, and Elemental libraries.", "links": ["Algorithm for matrix multiplication", "Applied mathematics", "Basic Linear Algebra Subprograms", "CPU cache", "Cache-oblivious algorithm", "Comparison of linear algebra libraries", "Comparison of numerical analysis software", "Computer science", "Distributed algorithm", "Floating point", "Lynn Elliot Cannon", "Matrix decomposition", "Matrix multiplication", "Matrix multiplication algorithm", "Mesh networking", "Multiprocessing", "Numerical linear algebra", "Numerical stability", "PLAPACK", "SIMD", "ScaLAPACK", "Sparse matrix", "System of linear equations", "Systolic array", "Translation lookaside buffer"], "categories": ["All stub articles", "Applied mathematics stubs", "Distributed algorithms", "Matrix multiplication algorithms"], "title": "Cannon's algorithm"}
{"summary": "The Chandra\u2013Toueg consensus algorithm, published by Tushar Deepak Chandra and Sam Toueg in 1996, is an algorithm for solving consensus in a network of unreliable processes equipped with an eventually strong failure detector. The failure detector is an abstract version of timeouts; it signals to each process when other processes may have crashed. An eventually strong failure detector is one that never identifies some specific good process as having failed after some initial period of confusion, and at the same time eventually identifies all bad processes as failed. The algorithm itself is similar to the Paxos algorithm, which also relies on failure detectors. Both algorithms assume the number of faulty processes is less than n/2, where n is the total number of processes.", "links": ["Consensus (computer science)", "Failure detector", "Paxos algorithm", "Timeout (computing)", "Timestamp"], "categories": ["All articles needing additional references", "All articles needing style editing", "Articles needing additional references from October 2011", "Distributed algorithms", "Fault-tolerant computer systems", "Fault tolerance", "Wikipedia articles needing style editing from October 2011"], "title": "Chandra\u2013Toueg consensus algorithm"}
{"summary": "The Chang and Roberts algorithm is a ring-based coordinator election algorithm, employed in distributed computing.", "links": ["Bully algorithm", "Digital object identifier", "Distributed Computing", "Distributed computing", "HS algorithm", "Leader election", "Liveness", "Ring network", "Safety (computer science)", "Unidirectional ring"], "categories": ["Distributed algorithms"], "title": "Chang and Roberts algorithm"}
{"summary": "Cristian's Algorithm (introduced by Flaviu Cristian in 1989) is a method for clock synchronization which can be used in many fields of distributive computer science but is primarily used in low-latency intranets. Cristian observed that this simple algorithm is probabilistic, in that it only achieves synchronization if the round-trip time (RTT) of the request is short compared to required accuracy. It also suffers in implementations using a single server, making it unsuitable for many distributive applications where redundancy may be crucial.", "links": ["Allan variance", "Berkeley algorithm", "Clock synchronization", "DAYTIME", "Digital object identifier", "ICMP Timestamp", "ICMP Timestamp Reply", "International Atomic Time", "Intranets", "NTP pool", "NTP server misuse and abuse", "Ntpd", "Ntpdate", "OpenNTPD", "Precision Time Protocol", "Round-trip time", "Synchronization", "TIME protocol", "Time server", "UTC"], "categories": ["Distributed algorithms", "Synchronization"], "title": "Cristian's algorithm"}
{"summary": "The distributed minimum spanning tree (MST) problem involves the construction of a minimum spanning tree by a distributed algorithm, in a network where nodes communicate by message passing. It is radically different from the classical sequential problem, although the most basic approach resembles Bor\u016fvka's algorithm. One important application of this problem is to find a tree that can be used for broadcasting. In particular, if the cost for a message to pass through an edge in a graph is significant, a MST can minimize the total cost for a source process to communicate with all the other processes in the network.\nThe problem was first suggested and solved in  time in 1983 by Gallager et al., where  is the number of vertices in the graph. Later, the solution was improved to  and finally  where D is the network, or graph diameter. A lower bound on the time complexity of the solution has been eventually shown to be", "links": ["Baruch Awerbuch", "Bor\u016fvka's algorithm", "Broadcasting (computing)", "David Peleg (scientist)", "Distributed algorithm", "Distributed computing", "FIFO (computing and electronics)", "Graph theory", "Kruskal's algorithm", "Message passing", "Minimum spanning tree", "Planar graph", "Prim's algorithm", "Robert G. Gallager", "SIAM Journal on Computing", "Symposium on Foundations of Computer Science", "Symposium on Theory of Computing"], "categories": ["Distributed algorithms", "Spanning tree"], "title": "Distributed minimum spanning tree"}
{"summary": "In computer science, edge-chasing is an algorithm for deadlock detection in distributed systems.\nWhenever a process A is blocked for some resource, a probe message is sent to all processes A may depend on. The probe message contains the process id of A along with the path that the message has followed through the distributed system. If a blocked process receives the probe it will update the path information and forward the probe to all the processes it depends on. Non-blocked processes may discard the probe.\nIf eventually the probe returns to process A, there is a circular waiting loop of blocked processes, and a deadlock is detected. Efficiently detecting such cycles in the \u201cwait-for graph\u201d of blocked processes is an important implementation problem.", "links": ["Circular wait", "Computer science", "Deadlock", "Distributed systems", "Process (computer science)", "Wait-for graph"], "categories": ["All articles lacking sources", "Articles lacking sources from December 2009", "Distributed algorithms"], "title": "Edge chasing"}
{"summary": "The HS Algorithm is named after Dan Hirschberg and J. B. Sinclair. It is a distributed algorithm designed for the Leader Election problem in a Synchronous Ring.\nThe algorithm requires the use of unique IDs (UID) for each process. The algorithm works in phases and sends its UID out in both directions. The message goes out a distance of 2Phase Number hops and then the message heads back to the originating process. While the messages are heading \"out\" each receiving process will compare the incoming UID to its own. If the UID is greater than its own UID then it will continue the message on. Otherwise if the UID is less than its own UID, it will not pass the information on. At the end of a phase, a process can determine if it will send out messages in the next round by if it received both of its incoming messages. Phases continue until a process receives both of its out messages, from both of its neighbors. At this time the process knows it is the largest UID in the ring and declares itself the leader.", "links": ["Algorithm", "Dan Hirschberg", "Data structure", "Distributed algorithm", "J. B. Sinclair", "Leader election", "Synchronous Ring"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Distributed algorithms"], "title": "HS algorithm"}
{"summary": "The algorithm of Lamport timestamps is a simple algorithm used to determine the order of events in a distributed computer system. As different nodes or processes will typically not be perfectly synchronized, this algorithm is used to provide a partial ordering of events with minimal overhead, and conceptually provide a starting point for the more advanced vector clock method. They are named after their creator, Leslie Lamport.\nDistributed algorithms such as resource synchronization often depend on some method of ordering events to function. For example, consider a system with two processes and a disk. The processes send messages to each other, and also send messages to the disk requesting access. The disk grants access in the order the messages were sent. Now, imagine process 1 sends a message to the disk asking for access to write, and then sends a message to process 2 asking it to read. Process 2 receives the message, and as a result sends its own message to the disk. Now, due to some timing delay, the disk receives both messages at the same time: how does it determine which message happened-before the other? ( happens-before  if one can get from  to  by a sequence of moves of two types: moving forward while remaining in the same process, and following a message from its sending to its reception.) A logical clock algorithm provides a mechanism to determine facts about the order of such events.\nLamport invented a simple mechanism by which the happened-before ordering can be captured numerically. A Lamport logical clock is an incrementing software counter maintained in each process.\nIt follows some simple rules:\nA process increments its counter before each event in that process;\nWhen a process sends a message, it includes its counter value with the message;\nOn receiving a message, the receiver process sets its counter to be the maximum of the message counter and its own counter (already incremented due to rule 1), before it considers the message received.\nConceptually, this logical clock can be thought of as a clock that only has meaning in relation to messages moving between processes. When a process receives a message, it resynchronizes its logical clock with that sender.", "links": ["Clock synchronization", "Communications of the ACM", "Digital object identifier", "Distributed computer system", "Happened-before", "Leslie Lamport", "Partial order", "Partially ordered set", "Total ordering", "Vector clock", "Vector clocks"], "categories": ["Distributed algorithms"], "title": "Lamport timestamps"}
{"summary": "A local algorithm is a distributed algorithm that runs in constant time, independently of the size of the network.", "links": ["Algorithm", "Constant time"], "categories": ["All orphaned articles", "All stub articles", "Computing stubs", "Distributed algorithms", "Orphaned articles from September 2013"], "title": "Local algorithm"}
{"summary": "A logical clock is a mechanism for capturing chronological and causal relationships in a distributed system. Distributed system may have no physically synchronous global clock, so a logical clock allows global ordering on events from different processes in such systems. The first implementation, the Lamport timestamps, was proposed by Leslie Lamport in 1978 (Turing Award in 2013).\nIn logical clock systems each process has two data structures: logical local time and logical global time. Logical local time is used by the process to mark its own events, and logical global time is the local information about global time. A special protocol is used to update logical local time after each local event, and logical global time when processes exchange data.\nLogical clocks are useful in computation analysis, distributed algorithm design, individual event tracking, and exploring computational progress.\nSome noteworthy logical clock algorithms are:\nLamport timestamps, which are monotonically increasing software counters.\nVector clocks, that allow for partial ordering of events in a distributed system.\nVersion vectors, order replicas, according to updates, in an optimistic replicated system.\nMatrix clocks, an extension of vector clocks that also contains information about other processes' views of the system.", "links": ["Algorithm", "Data structure", "Distributed system", "Integrated Authority File", "Lamport timestamps", "Leslie Lamport", "Matrix clock", "Turing Award", "Vector clock", "Version vector"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Distributed algorithms", "Wikipedia articles with GND identifiers"], "title": "Logical clock"}
{"summary": "The parallel-TEBD is a version of the TEBD algorithm adapted to run on multiple hosts. The task of parallelizing TEBD could be achieved in various ways.\nAs a first option, one could use the OpenMP API (this would probably be the simplest way to do it), using preprocessor directives to decide which portion of the code should be parallelized. The drawback of this is that one is confined to Symmetric multiprocessing (SMP) architectures and the user has no control on how the code is parallelized. An Intel extension of OpenMP, called Cluster OpenMP [2], is a socket-based implementation of OpenMP which can make use of a whole cluster of SMP machines; this spares the user of explicitly writing messaging code while giving access to multiple hosts via a distributed shared-memory system. The OpenMP paradigm (hence its extension Cluster OpenMP as well) allows the user a straightforward parallelization of serial code by embedding a set of directives in it.\nThe second option is using the Message Passing Interface (MPI) API. MPI can treat each core of the multi-core machines as separate execution host, so a cluster of, let's say, 10 compute nodes with dual-core processors will appear as 20 compute nodes, on which the MPI application can be distributed. MPI offers the user more control over the way the program is parallelized. The drawback of MPI is that is not very easy to implement and the programmer has to have a certain understanding of parallel simulation systems.\nFor the determined programmer the third option would probably be the most appropriate: to write ones own routines, using a combination of threads and TCP/IP sockets to complete the task. The threads are necessary in order to make the socket-based communication between the programs non-blocking (the communication between programs has to take place in threads, so that the main thread doesn't have to wait for the communication to end and can execute other parts of the code). This option offers the programmer complete control over the code and eliminates any overhead which might come from the use of the Cluster OpenMP or MPI libraries.\nThis article introduces the conceptual basis of the implementation, using MPI-based pseudo-code for exemplification, while not restricting itself to MPI - the same basic schema could be implemented with the use of home-grown messaging routines.", "links": ["API", "Distributed shared memory", "Guifr\u00e9 Vidal", "Internet socket", "Message Passing Interface", "Non-blocking I/O", "OpenMP", "Parallel computing", "Symmetric multiprocessing", "TEBD", "Thread (computer science)"], "categories": ["Computational physics", "Distributed algorithms"], "title": "Parallel-TEBD"}
{"summary": "Paxos is a family of protocols for solving consensus in a network of unreliable processors. Consensus is the process of agreeing on one result among a group of participants. This problem becomes difficult when the participants or their communication medium may experience failures.\nConsensus protocols are the basis for the state machine replication approach to distributed computing, as suggested by Leslie Lamport and surveyed by Fred B. Schneider. State machine replication is a technique for converting an algorithm into a fault-tolerant, distributed implementation. Ad-hoc techniques may leave important cases of failures unresolved. The principled approach proposed by Lamport et al. ensures all cases are handled safely.\nThe Paxos protocol was first published in 1989 and named after a fictional legislative consensus system used on the Paxos island in Greece. It was later published as a journal article in 1998.\nThe Paxos family of protocols includes a spectrum of trade-offs between the number of processors, number of message delays before learning the agreed value, the activity level of individual participants, number of messages sent, and types of failures. Although no deterministic fault-tolerant consensus protocol can guarantee progress in an asynchronous network (a result proven in a paper by Fischer, Lynch and Paterson), Paxos guarantees safety (consistency), and the conditions that could prevent it from making progress are difficult to provoke.\nPaxos is usually used where durability is required (for example, to replicate a file or a database), in which the amount of durable state could be large. The protocol attempts to make progress even during periods when some bounded number of replicas are unresponsive. There is also a mechanism to drop a permanently failed replica or to add a new replica.", "links": ["ACM Symposium on Principles of Distributed Computing", "Amazon Web Services", "Apache Mesos", "Apache ZooKeeper", "Barbara Liskov", "BigTable", "Bitcoin", "Block chain (database)", "Byzantine fault tolerance", "Ceph (software)", "Chandra\u2013Toueg consensus algorithm", "Clustrix", "Communications of the ACM", "Commutative", "Consensus (computer science)", "Cynthia Dwork", "Dahlia Malkhi", "Digital object identifier", "Distributed algorithm", "Distributed lock manager", "Fred B. Schneider", "Gbcast", "Google Spanner", "IBM SAN Volume Controller", "If and only if", "International Conference on Dependable Systems and Networks", "Isis2 (programming library)", "Journal of the Association for Computing Machinery", "Ken Birman", "Larry Stockmeyer", "Lease (computer science)", "Leslie Lamport", "NAK (protocol message)", "Nancy Lynch", "Neo4j", "Nutanix", "Paxi", "Raft (computer science)", "State machine replication", "Storage virtualization", "Viewstamped replication", "Virtual synchrony", "WANdisco", "XtreemFS"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from September 2009", "Distributed algorithms", "Fault-tolerant computer systems"], "title": "Paxos (computer science)"}
{"summary": "Raft is a consensus algorithm designed as an alternative to Paxos. It was meant to be more understandable than Paxos by means of separation of logic, but it is also formally proven safe and offers some new features. Raft offers a generic way to distribute a state machine across a cluster of computing systems, ensuring that each node in the cluster agrees upon the same series of state transitions. It has a number of open-source reference implementations, with full-spec implementations in Go, C++, Java, and Scala.", "links": ["C++", "Computer cluster", "Computer science", "Consensus (computer science)", "Finite-state machine", "Go (programming language)", "Java (programming language)", "Paxos (computer science)", "Scala (programming language)"], "categories": ["All stub articles", "Computer science stubs", "Distributed algorithms", "Fault-tolerant computer systems"], "title": "Raft (computer science)"}
{"summary": "The Ricart-Agrawala Algorithm is an algorithm for mutual exclusion on a distributed system. This algorithm is an extension and optimization of Lamport's Distributed Mutual Exclusion Algorithm, by removing the need for  messages. It was developed by Glenn Ricart and Ashok Agrawala.", "links": ["Ashok Agrawala", "Distributed system", "Glenn Ricart", "Lamport's Distributed Mutual Exclusion Algorithm", "Lamport's bakery algorithm", "Logical clock", "Maekawa's Algorithm", "Mutual exclusion", "Naimi-Trehel's Algorithm", "Raymond's Algorithm", "Suzuki-Kasami algorithm"], "categories": ["All articles lacking sources", "Articles lacking sources from December 2009", "Distributed algorithms"], "title": "Ricart\u2013Agrawala algorithm"}
{"summary": "The snapshot algorithm is an algorithm used in distributed systems for recording a consistent global state of an asynchronous system. The algorithm discussed here is also known as the Chandy\u2013Lamport algorithm, after Leslie Lamport and K. Mani Chandy.", "links": ["Algorithm", "Asynchronous communication", "Distributed systems", "FIFO (computing and electronics)", "Internet protocol suite", "K. Mani Chandy", "Leslie Lamport", "University of Texas at Austin"], "categories": ["Distributed algorithms"], "title": "Snapshot algorithm"}
{"summary": "The Suzuki-Kasami algorithm is a token-based algorithm for achieving mutual exclusion in distributed systems. The process holding the token is the only process able to enter its critical section.\nThis is a modification to Ricart\u2013Agrawala algorithm in which a REQUEST and REPLY message are used for attaining the critical section. but in this algorithm they introduced a method in which a seniority vise and also by handing over the critical section to other node by sending a single PRIVILEGE message to other node. So, the node which has the privilege it can use the critical section and if it does not have one it cannot. If a process wants to enter its critical section and it does not have the token, it broadcasts a request message to all other processes in the system. The process that has the token, if it is not currently in a critical section, will then send the token to the requesting process. The algorithm makes use of increasing Request Numbers to allow messages to arrive out-of-order.", "links": ["Access token", "Algorithm", "Critical section", "Distributed systems", "Hypertext Transfer Protocol", "Lamport timestamps", "Mutual exclusion", "Node (networking)", "Ricart\u2013Agrawala algorithm"], "categories": ["All articles needing additional references", "All articles needing cleanup", "All articles needing expert attention", "All articles with topics of unclear notability", "Articles needing additional references from September 2014", "Articles needing cleanup from May 2009", "Articles needing expert attention from May 2009", "Articles needing expert attention with no reason or talk parameter", "Articles with topics of unclear notability from May 2009", "Cleanup tagged articles without a reason field from May 2009", "Computer science articles needing expert attention", "Distributed algorithms", "Wikipedia pages needing cleanup from May 2009"], "title": "Suzuki-Kasami algorithm"}
{"summary": "In computer science, a synchronizer is an algorithm that can be used to run a synchronous algorithm on top of an asynchronous processor network, so enabling the asynchronous system to run as a synchronous network.\nThe concept was originally proposed in (Awerbuch, 1985) along with three synchronizer algorithms named alpha, beta and gamma which provided different tradeoffs in terms of time and message complexity. Essentially, they are a solution to the problem of asynchronous algorithms (which operate in a network with no global clock) being harder to design and often less efficient than the equivalent synchronous algorithms. By using a synchronizer, algorithm designers can deal with the simplified \"ideal network\" and then later mechanically produce a version that operates in more realistic asynchronous cases.", "links": ["Algorithm", "Asynchronous system", "Baruch Awerbuch", "Computer science", "Synchronization (computer science)"], "categories": ["Distributed algorithms"], "title": "Synchronizer (algorithm)"}
{"summary": "Vector clocks is an algorithm for generating a partial ordering of events in a distributed system and detecting causality violations. Just as in Lamport timestamps, interprocess messages contain the state of the sending process's logical clock. A vector clock of a system of N processes is an array/vector of N logical clocks, one clock per process; a local \"smallest possible values\" copy of the global clock-array is kept in each process, with the following rules for clock updates:\n\nInitially all clocks are zero.\nEach time a process experiences an internal event, it increments its own logical clock in the vector by one.\nEach time a process prepares to send a message, it sends its entire vector along with the message being sent.\nEach time a process receives a message, it increments its own logical clock in the vector by one and updates each element in its vector by taking the maximum of the value in its own vector clock and the value in the vector in the received message (for every element).\nThe vector clocks algorithm was independently developed by Colin Fidge and Friedemann Mattern in 1988.", "links": ["Algorithm", "Antisymmetric relation", "Array data structure", "Causality", "Digital object identifier", "Distributed system", "Friedemann Mattern", "International Standard Book Number", "Lamport timestamps", "Logical clock", "Matrix clock", "Partial ordering", "Transitive relation", "Version vector"], "categories": ["Distributed algorithms"], "title": "Vector clock"}
{"summary": "Verification-based message-passing algorithms (VB-MPAs) in compressed sensing (CS), a branch of digital signal processing that deals with measuring sparse signals, are some methods to efficiently solve the recovery problem in compressed sensing. One of the main goal in compressed sensing is the recovery process. Generally speaking, recovery process in compressed sensing is a method by which the original signal is estimated using the knowledge of the compressed signal and the measurement matrix. Mathematically, the recovery process in Compressed Sensing is finding the sparsest possible solution of an under-determined system of linear equations. Based on the nature of the measurement matrix one can employ different reconstruction methods. If the measurement matrix is also sparse, one efficient way is to use Message Passing Algorithms for signal recovery. Although there are message passing approaches that deals with dense matrices, the nature of those algorithms are to some extent different from the algorithms working on sparse matrices.", "links": ["Algorithms", "Almost surely", "Analysis of algorithms", "Anna C. Gilbert", "Benchmark (computing)", "Big O notation", "Bipartite graph", "Change of variable", "Compressed sensing", "Continuous distribution", "Digital signal processing", "False alarm", "Graph theory", "Iteration", "Iterative method", "LOOP (programming language)", "Logical matrix", "Parallel computing", "Piotr Indyk", "Pseudo code", "Right hand side", "Sides of an equation", "Sparse", "Support (mathematics)", "System of linear equations", "Variable (mathematics)"], "categories": ["All articles covered by WikiProject Wikify", "All articles lacking in-text citations", "All articles needing references cleanup", "All orphaned articles", "Articles covered by WikiProject Wikify from February 2015", "Articles lacking in-text citations from February 2015", "Digital signal processing", "Distributed algorithms", "Inter-process communication", "Orphaned articles from January 2015", "Pages using web citations with no URL", "Wikipedia references cleanup from February 2015"], "title": "Verification-based message-passing algorithms in compressed sensing"}
{"summary": "The intersection algorithm is an agreement algorithm used to select sources for estimating accurate time from a number of noisy time sources, it forms part of the modern Network Time Protocol. It is a modified form of Marzullo's algorithm.\nWhile Marzullo's algorithm will return the smallest interval consistent with the largest number of sources, the returned interval does not necessarily include the center point (calculated offset) of all the sources in the intersection. The Intersection algorithm returns an interval that includes that returned by Marzullo's algorithm but may be larger since it will include the center points. This larger interval allows using additional statistical data to select a point within the interval, reducing the jitter in repeated execution.", "links": ["Agreement algorithm", "Confidence band", "Jitter", "Marzullo's algorithm", "Network Time Protocol", "Noise"], "categories": ["Agreement algorithms"], "title": "Intersection algorithm"}
{"summary": "Marzullo's algorithm, invented by Keith Marzullo for his Ph.D. dissertation in 1984, is an agreement algorithm used to select sources for estimating accurate time from a number of noisy time sources. A refined version of it, renamed the \"intersection algorithm\", forms part of the modern Network Time Protocol. The Marzullo's algorithm is also used to compute the relaxed intersection of n boxes (or more generally n subsets of Rn), as required by several robust set estimation methods.", "links": ["Agreement algorithm", "Asymptotic", "Big O notation", "Confidence interval", "Consistent", "Intersection algorithm", "Keith Marzullo", "Linear", "Network Time Protocol", "Noise", "Probabilistic model", "Relaxed intersection", "Set estimation", "Sorting algorithm", "Tuple"], "categories": ["Agreement algorithms"], "title": "Marzullo's algorithm"}
{"summary": "Huang's algorithm is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Shing-Tsaan Huang in 1989 in the Journal of Computers.", "links": ["Algorithm", "Dijkstra-Scholten algorithm", "Distributed system", "Journal of Computers", "Shing-Tsaan Huang", "Termination analysis"], "categories": ["All articles lacking sources", "Articles lacking sources from December 2009", "Termination algorithms"], "title": "Huang's algorithm"}
{"summary": "Bach's algorithm is a probabilistic polynomial time algorithm for generating random numbers along with their factorization, named after its discoverer, Eric Bach. It is of interest because no algorithm is known that efficiently factors numbers, so the straightforward method, namely generating a random number and then factoring it, is impractical.\nThe algorithm performs, in expectation, O(log n) primality tests.\nA simpler, but less efficient algorithm (performing, in expectation, O(log2 n) primality tests), is known and is due to Adam Kalai", "links": ["Adam Kalai", "Algorithm", "Eric Bach", "Factorization", "Polynomial time", "Primality tests", "Prime number", "Pseudorandom number generator", "Rejection sampling"], "categories": ["Cryptographic algorithms"], "title": "Bach's algorithm"}
{"summary": "In modular arithmetic, Barrett reduction is a reduction algorithm introduced in 1986 by P.D. Barrett. A naive way of computing\n\nwould be to use a fast division algorithm. Barrett reduction is an algorithm designed to optimize this operation assuming  is constant, and , replacing divisions by multiplications.", "links": ["Alfred J. Menezes", "Algorithm", "Digital object identifier", "Division algorithm", "Fixed point (mathematics)", "Floating point", "Floor function", "International Standard Book Number", "Modular arithmetic", "Montgomery reduction", "Scott A. Vanstone"], "categories": ["Computer arithmetic", "Cryptographic algorithms", "Modular arithmetic", "Wikipedia articles needing clarification from January 2014"], "title": "Barrett reduction"}
{"summary": "In cryptography, a mode of operation is an algorithm that uses a block cipher to provide an information service such as confidentiality or authenticity. A block cipher by itself is only suitable for the secure cryptographic transformation (encryption or decryption) of one fixed-length group of bits called a block. A mode of operation describes how to repeatedly apply a cipher's single-block operation to securely transform amounts of data larger than a block.\nMost modes require a unique binary sequence, often called an initialization vector (IV), for each encryption operation. The IV has to be non-repeating and, for some modes, random as well. The initialization vector is used to ensure distinct ciphertexts are produced even when the same plaintext is encrypted multiple times independently with the same key. Block ciphers have one or more block size(s), but during transformation the block size is always fixed. Block cipher modes operate on whole blocks and require that the last part of the data be padded to a full block if it is smaller than the current block size. There are, however, modes that do not require padding because they effectively use a block cipher as a stream cipher.\nHistorically, encryption modes have been studied extensively in regard to their error propagation properties under various scenarios of data modification. Later development regarded integrity protection as an entirely separate cryptographic goal. Some modern modes of operation combine confidentiality and authenticity in an efficient way, and are known as authenticated encryption modes.", "links": ["3-Way", "3-subset meet-in-the-middle attack", "AEAD block cipher modes of operation", "ANSI", "ARIA (cipher)", "Advanced Encryption Standard", "Advanced Encryption Standard process", "Akelarre (cipher)", "Anubis (cipher)", "Authenticated encryption", "Authentication", "Avalanche effect", "BATON", "BEAR and LION ciphers", "BLAKE (hash function)", "BaseKing", "BassOmatic", "Bcrypt", "Biclique attack", "Birthday attack", "Bit", "Bitmap image", "Block (data storage)", "Block cipher", "Block cipher modes of operation", "Block size (cryptography)", "Blowfish (cipher)", "Boomerang attack", "Bruce Schneier", "Brute-force attack", "Brute force attack", "CAST-128", "CAST-256", "CBC-MAC", "CCM mode", "CIKS-1", "CIPHERUNICORN-A", "CIPHERUNICORN-E", "CLEFIA", "CMAC", "COCONUT98", "CRYPTON", "CRYPTREC", "CS-Cipher", "CWC mode", "C (programming language)", "Camellia (cipher)", "Cellular Message Encryption Algorithm", "Chi-square test", "Chiasmus (cipher)", "Cipher block chaining", "Cipher security summary", "Ciphertext", "Ciphertext stealing", "Cobra ciphers", "Collision (computer science)", "Collision attack", "Confidentiality", "Crab (cipher)", "Crypt (C)", "Cryptanalysis", "Cryptographic hash function", "Cryptographic nonce", "Cryptographic protocol", "Cryptographically secure pseudorandom number generator", "Cryptography", "Cryptomeria cipher", "DEAL", "DES-X", "DES supplementary material", "DFC (cipher)", "Data Authentication Algorithm", "Data Encryption Standard", "Davies' attack", "Differential-linear attack", "Differential cryptanalysis", "Digital signature", "Disk encryption", "Disk encryption theory", "Distinguishing attack", "E2 (cipher)", "EAX mode", "EFF DES cracker", "Elliptic curve only hash", "Error-correcting code", "FEA-M", "FEAL", "FROG", "Fast Syndrome Based Hash", "Feistel cipher", "GCM mode", "GDES", "GOST (block cipher)", "GOST (hash function)", "Galois/Counter Mode", "Grand Cru (cipher)", "Gr\u00f8stl", "HAS-160", "HAVAL", "HMAC", "Hash function security summary", "Hasty Pudding cipher", "Hierocrypt", "Higher-order differential cryptanalysis", "History of cryptography", "IACBC mode", "IAPM (mode)", "IAPM mode", "ICE (cipher)", "IDEA NXT", "IEEE", "IETF", "Impossible differential cryptanalysis", "Information security", "Initialization vector", "Integral cryptanalysis", "Integrity protection", "Intel Cascade Cipher", "International Data Encryption Algorithm", "International Electrotechnical Commission", "International Organization for Standardization", "International Standard Book Number", "Interpolation attack", "Iraqi block cipher", "JH (hash function)", "KASUMI", "KHAZAD", "KN-Cipher", "KeeLoq", "Kendall tau rank correlation coefficient", "Kerberos (protocol)", "Key (cryptography)", "Key derivation function", "Key feedback mode", "Key schedule", "Key size", "Key stretching", "Key whitening", "Keystream", "Khufu and Khafre", "Known-key distinguishing attack", "Kupyna", "LM hash", "LOKI", "LOKI97", "LRW", "Ladder-DES", "Lai-Massey scheme", "Length extension attack", "Libelle (cipher)", "Linear cryptanalysis", "Literal string", "Lucifer (cipher)", "M6 (cipher)", "M8 (cipher)", "MAGENTA", "MARS (cryptography)", "MD2 (cryptography)", "MD4", "MD5", "MD6", "MDC-2", "MESH (cipher)", "MISTY1", "MMB", "MULTI2", "MacGuffin (cipher)", "Madryga", "Martin Hellman", "Meet-in-the-middle attack", "Mercy (cipher)", "Merkle\u2013Damg\u00e5rd construction", "Message authentication", "Message authentication code", "Message authentication codes", "Mod n cryptanalysis", "Mode of operation", "Modus operandi", "MultiSwap", "N-Hash", "NESSIE", "NIST", "NIST hash function competition", "NOEKEON", "NUSH", "National Institute of Standards and Technology", "NewDES", "New Data Seal", "Niels Ferguson", "Nimbus (cipher)", "Null character", "OCB mode", "One-key MAC", "One-way compression function", "Outline of cryptography", "PBKDF2", "PMAC (cryptography)", "POODLE", "PRESENT (cipher)", "Padding (cryptography)", "Padding oracle attack", "Partitioning cryptanalysis", "Permutation box", "Piling-up lemma", "Pixel", "Plaintext", "Poly1305", "Preimage attack", "Product cipher", "Public-key cryptography", "Q (cipher)", "RC2", "RC5", "RC6", "REDOC", "RIPEMD", "RadioGat\u00fan", "Rainbow table", "Rebound attack", "Red Pike (cipher)", "Related-key attack", "Replay attack", "Residual block termination", "Rotational cryptanalysis", "S-1 block cipher", "S-box", "SAFER", "SAVILLE", "SC2000", "SEED", "SHA-1", "SHA-2", "SHA-3", "SHACAL", "SHARK", "SMS4", "SWIFFT", "SXAL/MBAL", "Salt (cryptography)", "Scrypt", "Secrecy", "Serpent (cipher)", "Shift register", "Side-channel attack", "Simon (cipher)", "SipHash", "Skein (hash function)", "Skipjack (cipher)", "Slide attack", "Snefru", "Speck (cipher)", "Spectr-H64", "Square (cipher)", "Steganography", "Stream cipher", "Streebog", "Substitution-permutation network", "Symmetric-key algorithm", "Threefish", "Tiger (cryptography)", "Time/memory/data tradeoff attack", "Timing attack", "Tiny Encryption Algorithm", "Treyfer", "Triple DES", "Truncated differential cryptanalysis", "Twofish", "UES (cipher)", "UMAC", "VMAC", "Very smooth hash", "Virgil D. Gligor", "WASTE", "Weak key", "Whirlpool (cryptography)", "Whitening transformation", "Whitfield Diffie", "XCBC mode", "XOR", "XSL attack", "XTEA", "XTS-AES", "XTS mode", "XXTEA", "Xenon (cipher)", "Xmx", "Zodiac (cipher)"], "categories": ["Block cipher modes of operation", "Cryptographic algorithms"], "title": "Block cipher mode of operation"}
{"summary": "In cryptography, CDMF (Commercial Data Masking Facility) is an algorithm developed at IBM in 1992 to reduce the security strength of the 56-bit DES cipher to that of 40-bit encryption, at the time a requirement of U.S. restrictions on export of cryptography. Rather than a separate cipher from DES, CDMF constitutes a key generation algorithm, called key shortening. It is one of the cryptographic algorithms supported by S-HTTP.", "links": ["40-bit encryption", "56-bit encryption", "Bruce Schneier", "Brute force attack", "Cryptography", "Data Encryption Standard", "Digital object identifier", "Export of cryptography", "IBM", "International Standard Book Number", "John Wiley & Sons", "Key (cryptography)", "Key generation", "PDF", "S-HTTP", "XOR"], "categories": ["All stub articles", "Block ciphers", "Cryptographic algorithms", "Cryptography stubs", "Data Encryption Standard", "Key management"], "title": "CDMF"}
{"summary": "The Common Scrambling Algorithm (or CSA) is the encryption algorithm used in the DVB digital television broadcasting for encrypting video streams.\nCSA was specified by ETSI and adopted by the DVB consortium in May 1994. It is being succeeded by CSA3, based on a combination of 128-bit AES and a confidential block cipher, XRC. However, CSA3 is not yet in any significant use, so CSA continues to be the dominant cipher for protecting DVB broadcasts.", "links": ["AVX2", "Advanced Access Content System", "Advanced Encryption Standard", "Analog television", "Authorized domain", "B-CAS", "BD+", "Basic Interoperable Scrambling System", "Bit slicing", "Block cipher modes of operation", "Broadcast flag", "Broadcast syndication", "Brute force attack", "CableCARD", "Card sharing", "Common Interface", "Conax", "Conditional-access module", "Conditional access", "Content Scramble System", "Cryptanalyst", "Cryptoworks", "DVB-CPCM", "Data security", "DigiCipher 2", "Digital Video Broadcasting", "Digital rights management", "Digital television", "Digital terrestrial television", "Disassembler", "ETSI", "Encryption", "Encryption algorithm", "EuroCrypt", "Executable", "FPGA", "FTA receiver", "Free-to-view", "Free to view", "GPU", "Hardware restrictions", "High-bandwidth Digital Content Protection", "Initialization vector", "KeyFly", "Key exchange", "Known plaintext", "Mediaguard", "NDS Group", "Nagra France", "Nagravision", "Packetized elementary stream", "Pay television", "Pirate decryption", "PowerVu", "Programming language", "Rainbow table", "Renewable security", "Reusable Asset Specification", "Reverse engineering", "S-box", "S-boxes", "SIMD", "Smart card", "Software", "Television encryption", "Viaccess", "VideoCrypt", "VideoGuard", "Video coding", "Videocipher"], "categories": ["Cryptographic algorithms", "Digital Video Broadcasting"], "title": "Common Scrambling Algorithm"}
{"summary": "Crypto++ (also known as CryptoPP, libcrypto++, and libcryptopp) is a free and open source C++ class library of cryptographic algorithms and schemes written by Wei Dai. Crypto++ has been widely used in academia, student projects, open source and non-commercial projects, as well as businesses. Released in 1995, the library fully supports 32-bit and 64-bit architectures for many major operating systems and platforms, including Android (using STLport), Apple (Mac OS X and iOS), BSD, Cygwin, IBM AIX and S/390, Linux, MinGW, Solaris, Windows, Windows Phone and Windows RT. The project also supports compilation under C++03 and C++11, a variety of compilers and IDEs, including Borland Turbo C++, Borland C++ Builder, Clang, CodeWarrior Pro, GCC (including Apple's GCC), Intel C++ Compiler (ICC), Microsoft Visual C/C++, and Sun Studio.", "links": ["32-bit", "64-bit", "AES-NI", "ANSI X9.17", "Advanced Encryption Standard", "Advanced Encryption Standard process", "Algorithms", "Apple Inc.", "Authenticated encryption", "BSD", "Block cipher", "Block cipher mode of operation", "Block cipher modes of operation", "Blowfish (cipher)", "Blum Blum Shub", "Boost Software License", "Borland C++", "C++", "CAST-256", "CBC-MAC", "CCM mode", "CMAC", "Camellia (cipher)", "Ciphertext stealing", "CiteSeer", "Clang", "CodeWarrior", "Compilers", "Computer architecture", "Computer science", "Cross-platform", "Cryptographic hash function", "Cryptography", "Cygwin", "DHAES", "DLIES", "DMAC (cryptography)", "Diffie\u2013Hellman key exchange", "Digital Signature Algorithm", "EAX mode", "ECDH", "ECDSA", "ECIES", "ECMQV", "ECNR", "ECRYPT", "EMSA2", "EMSA5", "ESIGN", "ElGamal", "Elliptic curve cryptography", "Federal Information Processing Standard", "GCM mode", "GNU Compiler Collection", "Galois/Counter Mode", "HMAC", "Hash function", "IBM AIX", "IBM ESA/390", "IEEE P1363", "IETF", "Information dispersal algorithm", "Integrated development environment", "Intel C++ Compiler", "International Data Encryption Algorithm", "International Organization for Standardization", "Internet Draft", "Internet Engineering Task Force", "KDF2 (cryptography)", "Key-agreement protocol", "Key derivation function", "LUCDIF", "LUCELG", "Library (computer science)", "Library (computing)", "Linear congruential generator", "Linux", "List of software categories", "Lucas sequence", "MARS (cryptography)", "Menezes-Qu-Vanstone", "Message authentication code", "Message authentication codes", "Microsoft Windows", "MinGW", "NESSIE", "Nyberg-Rueppel", "OAEP", "Open source", "Operating system", "PBKDF1", "PBKDF2", "PKCS", "PKCS12", "PSSR", "PSS (cryptography)", "Padding (cryptography)", "Panama (cryptography)", "Pseudorandom number generator", "Public-Key Cryptography Standards", "Public-key cryptography", "Public key", "RC5", "RC6", "RIPEMD", "RSA (algorithm)", "Rabin-Williams", "Rijndael", "SEED", "SHA-1", "SHA-2", "SHA-3", "SHACAL-2", "SOSEMANUK", "STLport", "Salsa20", "Secure Hash Algorithm (disambiguation)", "Serpent (cipher)", "Shamir's Secret Sharing", "Skipjack (cipher)", "Software developer", "Software license", "Software release life cycle", "Solaris (operating system)", "Stream cipher", "Symmetric cipher", "Tiger (cryptography)", "Tiny Encryption Algorithm", "Triple-DES", "Two-Track-MAC", "Twofish", "VMAC", "Visual C++", "WHIRLPOOL", "Wei Dai", "Whirlpool (cryptography)", "Windows Phone", "Windows RT", "X86", "XSalsa20", "XTEA", "XTR-DH"], "categories": ["Articles with attributed pull quotes", "C++ libraries", "Cryptographic algorithms", "Cryptographic software", "Free computer libraries", "Pages using citations with accessdate and no URL", "Pages using web citations with no URL"], "title": "Crypto++"}
{"summary": "Cycles per byte (sometimes abbreviated cpb) is a unit of measurement which indicates the number of clock cycles a microprocessor will perform per byte (usually of octet size) of data processed in an algorithm. It is commonly used as a partial indicator of real-world performance in cryptographic functions.", "links": ["AIM Multiuser Benchmark", "ARM architecture", "Adjusted Peak Performance", "Algorithm", "Algorithmic efficiency", "AnTuTu", "Apache JMeter", "Arithmetic logic unit", "Average CPU power", "BAPCo consortium", "BRL-CAD", "Benchmark (computing)", "Block cipher", "BogoMips", "Bonnie++", "Browser speed test", "Byte", "CPU power dissipation", "Clock cycles", "Computer memory", "Computer network", "Coremark", "Cryptographic", "Cryptography", "Curl-loader", "DEISA Benchmark Suite", "Data center infrastructure efficiency", "Database transaction", "Dhrystone", "Digital signal processor", "EEMBC", "Embedded systems", "FLOPS", "Fhourstones", "Filesystems", "Floating point unit", "Free software", "Function (computer science)", "Futuremark", "Giga-updates per second", "Graphics processing unit", "HD Tach", "HPC Challenge Benchmark", "Hash function", "Hierarchical INTegration", "Httperf", "I/O Subsystem", "IBM iSeries benchmarks", "ICOMP (index)", "IGen", "IOzone", "Integer", "Iometer", "Java (software platform)", "Javascript", "LAPACK", "Linpack", "Linux", "Livermore loops", "Microprocessor", "Motorola 68k", "Multiuser", "NAS Parallel Benchmarks", "NBench", "NoSQL", "Novabench", "Nuclear weapon", "OS X", "Octet (computing)", "OpenSTA", "Parallel computing", "Performance Rating", "Performance per watt", "Peripherals", "Phoronix Test Suite", "Physics Abstraction Layer", "Physics engine", "Prime95", "Princeton Application Repository for Shared-Memory Computers", "Proprietary software", "RPerf", "Recursion", "SDET", "SPECfp", "SPECint", "SPECpower", "SPECvirt", "SUPS", "Server Efficiency Rating Tool", "Standard Performance Evaluation Corporation", "SuperPrime", "Super PI", "Surveillance performance index", "SysSpeed", "Sysinfo", "TATP Benchmark", "TPC-W", "Tak (function)", "Transaction Processing Performance Council", "Transaction Processing over XML", "Tsung", "Unit of measurement", "VMmark", "Video Compression", "Virtual machine", "Web server benchmarking", "Whetstone (benchmark)", "WorldBench", "X86", "X Windows", "Xmark93", "YCSB"], "categories": ["All stub articles", "Computer benchmarks", "Cryptographic algorithms", "Cryptography stubs"], "title": "Cycles per byte"}
{"summary": "In sequence design, a Feedback with Carry Shift Register (or FCSR) is the arithmetic or with carry analog of a Linear feedback shift register (LFSR). If  is an integer, then an N-ary FCSR of length  is a finite state device with a state  consisting of a vector of elements  in  and an integer . The state change operation is determined by a set of coefficients  and is defined as follows: compute . Express s as  with  in . Then the new state is . By iterating the state change an FCSR generates an infinite, eventually period sequence of numbers in .\nFCSRs have been used in the design of stream ciphers (such as the F-FCSR generator), in the cryptanalyis of the summation combiner stream cipher (the reason Goresky and Klapper invented them), and in generating pseudorandom numbers for quasi-Monte Carlo (under the name Multiply With Carry (MWC) generator - invented by Couture and L'Ecuyer,) generalizing work of Marsaglia and Zaman.\nFCSRs are analyzed using number theory. Associated with the FCSR is a connection integer . Associated with the output sequence is the N-adic number  The fundamental theorem of FCSRs says that there is an integer  so that , a rational number. The output sequence is strictly periodic if and only if  is between  and . It is possible to express u as a simple quadratic polynomial involving the initial state and the qi.\nThere is also an exponential representation of FCSRs: if  is the inverse of , and the output sequence is strictly periodic, then , where  is an integer. It follows that the period is at most the order of N in the multiplicative group of units modulo q. This is maximized when q is prime and N is a primitive element modulo q. In this case, the period is . In this case the output sequence is called an l-sequence (for \"long sequence\").\nl-sequences have many excellent statistical properties that make them candidates for use in applications, including near uniform distribution of sub-blocks, ideal arithmetic autocorrelations, and the arithmetic shift and add property. They are the with-carry analog of m-sequences or maximum length sequences.\nThere are efficient algorithms for FCSR synthesis. This is the problem: given a prefix of a sequence, construct a minimal length FCSR that outputs the sequence. This can be solved with a variant of Mahler and De Weger's lattice based analysis of N-adic numbers when ; by a variant of the Euclidean algorithm when N is prime; and in general by Xu's adaptation of the Berlekamp-Massey algorithm. If L is the size of the smallest FCSR that outputs the sequence (called the N-adic complexity of the sequence), then all these algorithms require a prefix of length about  to be successful and have quadratic time complexity. It follows that, as with LFSRs and linear complexity, any stream cipher whose N-adic complexity is low should not be used for cryptography.\nFCSRs and LFSRs are special cases of a very general algebraic construction of sequence generators called Algebraic Feedback Shift Registers (AFSRs) in which the integers are replaced by an arbitrary ring R and N is replaced by an arbitrary non-unit in R. A general reference on the subject of LFSRs, FCSRs, and AFSRs is the book.", "links": ["A5/1", "A5/2", "Achterbahn", "Algorithms", "Block cipher", "Block cipher modes of operation", "Correlation attack", "Correlation immunity", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "E0 (cipher)", "ESTREAM", "F-FCSR", "FISH (cipher)", "Grain (cipher)", "HC-256", "History of cryptography", "ISAAC (cipher)", "Initialization vector", "Linear feedback shift register", "MICKEY", "MUGI", "Maximum length sequence", "Message authentication code", "Multiply-with-carry", "NLFSR", "Number theory", "Outline of cryptography", "P-adic number", "Panama (cryptography)", "Phelix", "Pike (cipher)", "Primitive root modulo n", "Pseudorandomness", "Public-key cryptography", "Py (cipher)", "QUAD (cipher)", "Quasi-Monte Carlo method", "RC4", "Rabbit (cipher)", "SEAL (cipher)", "SNOW", "SOBER", "SOBER-128", "SOSEMANUK", "Salsa20", "Scream (cipher)", "Shift register", "Shrinking generator", "Steganography", "Stream cipher", "Stream ciphers", "Summation generator", "Symmetric-key algorithm", "T-function", "Trivium (cipher)", "VEST", "WAKE (cipher)"], "categories": ["Cryptographic algorithms", "Cryptography", "Digital registers", "Pseudorandom number generators", "Stream ciphers"], "title": "Feedback with Carry Shift Registers"}
{"summary": "In computational number theory, a variety of algorithms make it possible to generate prime numbers efficiently. These are used in various applications, for example hashing, public-key cryptography, and search of prime factors in large numbers.\nFor relatively small numbers, it is possible to just apply trial division to each successive odd number. Prime sieves are almost always faster.", "links": ["Algorithm", "Baillie-PSW primality test", "Big O notation", "CiteSeer", "Composite number", "Computational number theory", "Digital object identifier", "Fermat prime", "Formula for primes", "Hash table", "Mersenne prime", "Miller-Rabin primality test", "Odd number", "On-Line Encyclopedia of Integer Sequences", "Primality test", "Prime factor", "Prime factorization", "Prime number", "Probable prime", "Provable prime", "Public-key cryptography", "RAM", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Sublinear", "Trial division", "Wheel factorization"], "categories": ["Cryptographic algorithms", "Number theoretic algorithms", "Prime numbers"], "title": "Generating primes"}
{"summary": "HOTP is an HMAC-based one-time password (OTP) algorithm. It is a cornerstone of Initiative For Open Authentication (OATH).\nHOTP was published as an informational IETF RFC 4226 in December 2005, documenting the algorithm along with a Java implementation. Since then, the algorithm has been adopted by many companies worldwide (see below). The HOTP algorithm is a freely available open standard.", "links": ["Android (operating system)", "BlackBerry", "Burton Group", "Chrome OS", "Gartner", "Google Authenticator", "Google Chrome", "HMAC", "HOTP", "IETF", "IOS", "IPhone", "Initiative For Open Authentication", "Initiative for Open Authentication", "J2ME", "Linux", "MOTP", "Mac OS X", "Maemo", "Mask (computing)", "Most significant bit", "MultiOTP", "OCRA", "One-time password", "OpenAM", "OpenPGP card", "Open standard", "Pluggable Authentication Modules", "PrivacyIDEA", "Push technology", "S/KEY", "SHA-1", "Sailfish OS", "Security token", "Smartphone", "TiQR", "Time-based One-time Password Algorithm", "Windows Mobile", "YubiKey"], "categories": ["All accuracy disputes", "All articles with unsourced statements", "Articles with disputed statements from August 2015", "Articles with unsourced statements from August 2015", "Computer access control protocols", "Cryptographic algorithms", "Internet protocols"], "title": "HMAC-based One-time Password Algorithm"}
{"summary": "Industrial-grade primes (the term is apparently due to Henri Cohen) are integers for which primality has not been certified (i.e. rigorously proven), but they have undergone probable prime tests such as the Miller-Rabin primality test, which has a positive, but negligible, failure rate, or the Baillie-PSW primality test, which no composites are known to pass.\nIndustrial-grade primes are sometimes used instead of certified primes in algorithms such as RSA encryption, which require the user to generate large prime numbers. Certifying the primality of large numbers (over 100 digits for instance) is significantly harder than showing they are industrial-grade primes. The latter can be done almost instantly with a failure rate so low that it is highly unlikely to ever fail in practice. In other words, the number is believed to be prime with very high, but not absolute, confidence.", "links": ["101 (number)", "103 (number)", "107 (number)", "109 (number)", "113 (number)", "11 (number)", "127 (number)", "131 (number)", "137 (number)", "139 (number)", "13 (number)", "149 (number)", "151 (number)", "157 (number)", "163 (number)", "167 (number)", "173 (number)", "179 (number)", "17 (number)", "181 (number)", "191 (number)", "193 (number)", "197 (number)", "199 (number)", "19 (number)", "211 (number)", "223 (number)", "227 (number)", "229 (number)", "233 (number)", "239 (number)", "23 (number)", "241 (number)", "251 (number)", "257 (number)", "263 (number)", "269 (number)", "271 (number)", "277 (number)", "281 (number)", "283 (number)", "293 (number)", "29 (number)", "2 (number)", "307 (number)", "311 (number)", "313 (number)", "317 (number)", "31 (number)", "331 (number)", "337 (number)", "347 (number)", "349 (number)", "353 (number)", "359 (number)", "367 (number)", "373 (number)", "379 (number)", "37 (number)", "383 (number)", "389 (number)", "397 (number)", "3 (number)", "401 (number)", "409 (number)", "419 (number)", "41 (number)", "421 (number)", "431 (number)", "433 (number)", "439 (number)", "43 (number)", "443 (number)", "449 (number)", "457 (number)", "461 (number)", "463 (number)", "467 (number)", "479 (number)", "47 (number)", "487 (number)", "491 (number)", "499 (number)", "503 (number)", "509 (number)", "521 (number)", "523 (number)", "53 (number)", "541 (number)", "59 (number)", "5 (number)", "61 (number)", "67 (number)", "71 (number)", "73 (number)", "79 (number)", "7 (number)", "83 (number)", "89 (number)", "97 (number)", "Algorithms", "Almost prime", "Baillie-PSW primality test", "Balanced prime", "Bell number", "Bi-twin chain", "Carol number", "Chen prime", "Circular prime", "Complex number", "Composite number", "Cousin prime", "Cuban prime", "Cullen number", "Cunningham chain", "Dihedral prime", "Double Mersenne number", "Eisenstein prime", "Emirp", "Euclid number", "Factorial prime", "Failure rate", "Fermat number", "Fibonacci prime", "Formula for primes", "Fortunate number", "Full reptend prime", "Gaussian integer", "Gigantic prime", "Good prime", "Happy number", "Henri Cohen (number theorist)", "Higgs prime", "Highly cototient number", "Illegal prime", "Integer", "Integer sequence prime", "Interprime", "Kynea number", "Largest known prime number", "Leyland number", "List of prime numbers", "Lucas number", "Lucky number", "Megaprime", "Mersenne prime", "Miller-Rabin primality test", "Mills' constant", "Minimal prime (recreational mathematics)", "Motzkin number", "Newman\u2013Shanks\u2013Williams prime", "Number theory", "Palindromic prime", "Partition (number theory)", "Pell number", "Permutable prime", "Perrin number", "Pierpont prime", "Pillai prime", "Primality", "Primality test", "Prime Pages", "Prime gap", "Prime k-tuple", "Prime number", "Prime numbers", "Prime quadruplet", "Prime triplet", "Primes in arithmetic progression", "Primeval number", "Primorial prime", "Probable prime", "Proth number", "Pseudoprime", "Pythagorean prime", "Quartan prime", "RSA encryption", "Ramanujan prime", "Regular prime", "Repunit", "Safe prime", "Self number", "Semiprime", "Sexy prime", "Smarandache\u2013Wellin number", "Solinas prime", "Sophie Germain prime", "Stern prime", "Strobogrammatic prime", "Strong prime", "Super-prime", "Supersingular prime (for an elliptic curve)", "Supersingular prime (moonshine theory)", "Thabit number", "Titanic prime", "Truncatable prime", "Twin prime", "Unique prime", "Wagstaff prime", "Wall\u2013Sun\u2013Sun prime", "Weakly prime number", "Wieferich pair", "Wieferich prime", "Wilson prime", "Wolstenholme prime", "Woodall number"], "categories": ["All stub articles", "Cryptographic algorithms", "Number theory stubs", "Prime numbers"], "title": "Industrial-grade prime"}
{"summary": "In cryptography, the so-called product ciphers are a certain kind of ciphers, where the (de-)ciphering of data is done in \"rounds\". The general setup of each round is the same, except for some hard-coded parameters and a part of the cipher key, called a subkey. A key schedule is an algorithm that, given the key, calculates the subkeys for these rounds.", "links": ["3-Way", "3-subset meet-in-the-middle attack", "ARIA (cipher)", "Advanced Encryption Standard", "Advanced Encryption Standard process", "Akelarre (cipher)", "Anubis (cipher)", "Avalanche effect", "BATON", "BEAR and LION ciphers", "BaseKing", "BassOmatic", "Biclique attack", "Block cipher", "Block cipher mode of operation", "Block size (cryptography)", "Blowfish (cipher)", "Boomerang attack", "Brute-force attack", "CAST-128", "CAST-256", "CIKS-1", "CIPHERUNICORN-A", "CIPHERUNICORN-E", "CLEFIA", "COCONUT98", "CRYPTON", "CRYPTREC", "CS-Cipher", "Camellia (cipher)", "Cellular Message Encryption Algorithm", "Chi-square test", "Chiasmus (cipher)", "Cipher security summary", "Cobra ciphers", "Crab (cipher)", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Cryptomeria cipher", "DEAL", "DES-X", "DES supplementary material", "DFC (cipher)", "Data Encryption Standard", "Davies' attack", "Differential-linear attack", "Differential cryptanalysis", "Distinguishing attack", "E2 (cipher)", "EFF DES cracker", "FEA-M", "FEAL", "FROG", "Feistel cipher", "GDES", "GOST (block cipher)", "Grand Cru (cipher)", "Hasty Pudding cipher", "Hierocrypt", "Higher-order differential cryptanalysis", "History of cryptography", "ICE (cipher)", "IDEA NXT", "Impossible differential cryptanalysis", "Initialization vector", "Integral cryptanalysis", "Intel Cascade Cipher", "International Data Encryption Algorithm", "Interpolation attack", "Iraqi block cipher", "KASUMI", "KHAZAD", "KN-Cipher", "KeeLoq", "Kendall tau rank correlation coefficient", "Key (cryptography)", "Key size", "Key whitening", "Khufu and Khafre", "Known-key distinguishing attack", "LOKI", "LOKI97", "Ladder-DES", "Lai-Massey scheme", "Lars Knudsen", "Libelle (cipher)", "Linear cryptanalysis", "Lucifer (cipher)", "M6 (cipher)", "M8 (cipher)", "MAGENTA", "MARS (cryptography)", "MESH (cipher)", "MISTY1", "MMB", "MULTI2", "MacGuffin (cipher)", "Madryga", "Meet-in-the-middle attack", "Mercy (cipher)", "Message authentication code", "Mod n cryptanalysis", "MultiSwap", "NESSIE", "NOEKEON", "NUSH", "NewDES", "New Data Seal", "Nimbus (cipher)", "Nothing up my sleeve number", "One-way function", "Outline of cryptography", "PRESENT (cipher)", "Padding (cryptography)", "Partitioning cryptanalysis", "Permutation box", "Piling-up lemma", "Product cipher", "Public-key cryptography", "Q (cipher)", "RC2", "RC5", "RC6", "REDOC", "Rebound attack", "Red Pike (cipher)", "Related-key attack", "Rijndael key schedule", "Rotational cryptanalysis", "S-1 block cipher", "S-box", "SAFER", "SAVILLE", "SC2000", "SEED", "SHACAL", "SHARK", "SMS4", "SXAL/MBAL", "Serpent (cipher)", "Simon (cipher)", "Skipjack (cipher)", "Slide attack", "Speck (cipher)", "Spectr-H64", "Square (cipher)", "Steganography", "Stream cipher", "Substitution-permutation network", "Symmetric-key algorithm", "Threefish", "Time/memory/data tradeoff attack", "Timing attack", "Tiny Encryption Algorithm", "Treyfer", "Triple DES", "Truncated differential cryptanalysis", "Twofish", "UES (cipher)", "Weak key", "Whitening transformation", "XSL attack", "XTEA", "XXTEA", "Xenon (cipher)", "Xmx", "Zodiac (cipher)"], "categories": ["All articles needing additional references", "Articles needing additional references from July 2008", "Cryptographic algorithms"], "title": "Key schedule"}
{"summary": "Key Wrap constructions are a class of symmetric encryption algorithms designed to encapsulate (encrypt) cryptographic key material. The Key Wrap algorithms are intended for applications such as (a) protecting keys while in untrusted storage, or (b) transmitting keys over untrusted communications networks. The constructions are typically built from standard primitives such as block ciphers and cryptographic hash functions.\nKey Wrap may be considered as a form of key encapsulation algorithm, although it should not be confused with the more commonly known asymmetric (public-key) key encapsulation algorithms (e.g., PSEC-KEM). Key Wrap algorithms can be used in a similar application: to securely transport a session key by encrypting it under a long-term encryption key.", "links": ["Adaptive chosen ciphertext attack", "Advanced Encryption Standard", "Authenticated encryption", "Block cipher", "Cryptographic hash function", "Deterministic encryption", "Entropic security", "Key encapsulation", "Key management", "National Institute of Standards and Technology", "Offline private key protocol", "PSEC-KEM", "Phillip Rogaway", "SHA-1", "Symmetric encryption", "Triple DES"], "categories": ["Cryptographic algorithms"], "title": "Key Wrap"}
{"summary": "Kochanski multiplication is an algorithm that allows modular arithmetic (multiplication or operations based on it, such as exponentiation) to be performed efficiently when the modulus is large (typically several hundred bits). This has particular application in number theory and in cryptography: for example, in the RSA cryptosystem and Diffie-Hellman key exchange.\nThe most common way of implementing large-integer multiplication in hardware is to express the multiplier in binary and enumerate its bits, one bit at a time, starting with the most significant bit, perform the following operations on an accumulator:\nDouble the contents of the accumulator (if the accumulator stores numbers in binary, as is usually the case, this is a simple \"shift left\" that requires no actual computation).\nIf the current bit of the multiplier is 1, add the multiplicand into the accumulator; if it is 0, do nothing.\nFor an n-bit multiplier, this will take n clock cycles (where each cycle does either a shift or a shift-and-add).\nTo convert this into an algorithm for modular multiplication, with a modulus r, it is necessary to subtract r conditionally at each stage:\nDouble the contents of the accumulator.\nIf the result is greater than or equal to r, subtract r. (Equivalently, subtract r from the accumulator and store the result back into the accumulator if and only if it is non-negative).\nIf the current bit of the multiplier is 1, add the multiplicand into the accumulator; if it is 0, do nothing.\nIf the result of the addition is greater than or equal to r, subtract r. If no addition took place, do nothing.\nThis algorithm works. However, it is critically dependent on the speed of addition.\nAddition of long integers suffers from the problem that carries have to be propagated from right to left and the final result is not known until this process has been completed. Carry propagation can be speeded up with carry look-ahead logic, but this still makes addition very much slower than it needs to be (for 512-bit addition, addition with carry look-ahead is 32 times slower than addition without carries at all).\nNon-modular multiplication can make use of carry-save adders, which save time by storing the carries from each digit position and using them later: for example, by computing 111111111111+000000000010 as 111111111121 instead waiting for the carry to propagate through the whole number to yield the true binary value 1000000000001. That final propagation still has to be done to yield a binary result but this only needs to be done once at the very end of the multiplication.\nUnfortunately the modular multiplication method outlined above needs to know the magnitude of the accumulated value at every step, in order to decide whether to subtract r: for example, if it needs to know whether the value in the accumulator is greater than 1000000000000, the carry-save representation 111111111121 is useless and needs to be converted to its true binary value for the comparison to be made.\nIt therefore seems that one can have either the speed of carry-save or modular multiplication, but not both.", "links": ["Accumulator (computing)", "Algorithm", "Binary numeral system", "Carry-save adder", "Carry (arithmetic)", "Carry look-ahead adder", "Cryptography", "Diffie-Hellman key exchange", "Modular arithmetic", "Modular exponentiation", "Montgomery reduction", "Number theory", "RSA (algorithm)"], "categories": ["Cryptographic algorithms", "Modular arithmetic"], "title": "Kochanski multiplication"}
{"summary": "Master Password is an algorithm designed by Maarten Billemont for creating unique passwords in a reproducible manner. It differs from traditional password managers in that the passwords are not stored on disk or in the cloud, but are recreated every time by using information entered by the user; most importantly, their full name, a master password, and a unique name for the service the password is intended for.\nBy not storing the passwords anywhere, this approach tries to make it harder for attackers to steal or intercept them. It also removes the need for synchronization between devices, and backups of potential password databases.", "links": ["ASCII printable characters", "Android (operating system)", "Brute-force attack", "C (programming language)", "Command-line", "GNU General Public License", "GPLv3", "HMAC-SHA256", "IOS", "JavaScript", "Java (programming language)", "Key derivation function", "List of software categories", "Microsoft Windows", "OS X", "Operating system", "Password", "Password manager", "Rainbow tables", "Salt (cryptography)", "Scrypt", "Software developer", "Software license", "Software release life cycle", "Unix-like"], "categories": ["Cryptographic algorithms", "Free security software", "Official website not in Wikidata"], "title": "Master Password"}
{"summary": "Modular exponentiation is a type of exponentiation performed over a modulus. It is useful in computer science, especially in the field of public-key cryptography.\nThe operation of modular exponentiation calculates the remainder when an integer b (the base) raised to the eth power (the exponent), be, is divided by a positive integer m (the modulus). In symbols, given base b, exponent e, and modulus m, the modular exponentiation c is: c \u2261 be (mod m).\nFor example, given b = 5, e = 3 and m = 13, the solution c = 8 is the remainder of dividing 53 = 125 by 13.\nGiven integers b and e, and a positive integer m, a unique solution c exists with the property 0 \u2264 c < m.\nModular exponentiation can be performed with a negative exponent e by finding the modular multiplicative inverse d of b modulo m using the extended Euclidean algorithm. That is:\n where e < 0 and \nModular exponentiation similar to the one described above are considered easy to compute, even when the numbers involved are enormous. On the other hand, computing the discrete logarithm \u2013 that is, the task of finding the exponent e when given b, c, and m \u2013 is believed to be difficult. This one-way function behavior makes modular exponentiation a candidate for use in cryptographic algorithms.", "links": [".NET Framework", "AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Assertion (computing)", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Big O notation", "Binary GCD algorithm", "Binary numeral system", "Bruce Schneier", "Chakravala method", "Cipolla's algorithm", "Computer science", "Continued fraction factorization", "Cornacchia's algorithm", "Diffie-Hellman key exchange", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Exponentiation", "Exponentiation by squaring", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Fibonacci numbers", "FileMaker", "Function field sieve", "F\u00fcrer's algorithm", "GNU Multiple Precision Arithmetic Library", "General number field sieve", "Generating primes", "Go (programming language)", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Java (programming language)", "Karatsuba algorithm", "Kochanski multiplication", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular multiplicative inverse", "Montgomery reduction", "Multiplication algorithm", "Number theory", "One-way function", "PHP", "Perl", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Positive integer", "Primality test", "Proth's theorem", "Public-key cryptography", "Python (programming language)", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Quantum computing", "Quantum gate", "RSA (algorithm)", "Rational sieve", "Reversible computing", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "University of Minnesota", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Cryptographic algorithms", "Modular arithmetic", "Number theoretic algorithms"], "title": "Modular exponentiation"}
{"summary": "In modular arithmetic computation, Montgomery modular multiplication, more commonly referred to as Montgomery multiplication, is a method for performing fast modular multiplication, introduced in 1985 by the American mathematician Peter L. Montgomery.  \nGiven two integers a and b, the classical modular multiplication algorithm computes ab mod N. Montgomery multiplication works by transforming a and b into a special representation known as Montgomery form. For a modulus N, the Montgomery form of a is defined to be aR mod N for some constant R depending only on N and the underlying computer architecture. If aR mod N and bR mod N are the Montgomery forms of a and b, then their Montgomery product is abR mod N. Montgomery multiplication is a fast algorithm to compute the Montgomery product. Transforming the result out of Montgomery form yields the classical modular product ab mod N.\nBecause of the overhead involved in converting a and b into Montgomery form, computing a single product by Montgomery multiplication is slower than computing the product in the integers and performing a modular reduction by division or Barrett reduction. However, when many products are required, as in modular exponentiation, the conversion to Montgomery form becomes a negligible fraction of the time of the computation, and performing the computation by Montgomery multiplication is faster than the available alternatives. Many important cryptosystems such as RSA and Diffie\u2013Hellman key exchange are based on arithmetic operations modulo a large number, and for these cryptosystems, the increased speed afforded by Montgomery multiplication can be important in practice.", "links": ["Alfred J. Menezes", "Barrett reduction", "B\u00e9zout's identity", "Carry-save adder", "CiteSeer", "Diffie\u2013Hellman key exchange", "Euclidean division", "Exponentiation by squaring", "Extended Euclidean algorithm", "Isomorphism", "Jacobi symbol", "Little endian", "Mathematics of Computation", "Modular exponentiation", "Modular inverse", "Peter Montgomery (mathematician)", "Quotient ring", "RSA (cryptosystem)", "Scott A. Vanstone", "Side channel attack"], "categories": ["Computer arithmetic", "Cryptographic algorithms", "Modular arithmetic"], "title": "Montgomery modular multiplication"}
{"summary": "In cryptography, MOSQUITO was a stream cypher algorithm designed by Joan Daemen and Paris Kitsos. It was submitted to the eSTREAM Project of the eCRYPT network. After the initial design was broken by Joux and Muller, a tweaked version named MOUSTIQUE was proposed which made it to Phase 3 of the eSTREAM evaluation process as the only self-synchronizing cipher remaining. However, MOUSTIQUE was subsequently broken by K\u00e4sper et al., leaving the design of a secure and efficient self-synchronising stream cipher as an open research problem.", "links": ["A5/1", "A5/2", "Achterbahn", "Algorithm", "Block cipher", "Block cipher modes of operation", "Correlation attack", "Correlation immunity", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "E0 (cipher)", "ECRYPT", "ESTREAM", "F-FCSR", "FISH (cipher)", "Grain (cipher)", "HC-256", "History of cryptography", "ISAAC (cipher)", "Initialization vector", "Joan Daemen", "Linear feedback shift register", "MICKEY", "MUGI", "Message authentication code", "Mosquito", "Mosquito (disambiguation)", "NLFSR", "Outline of cryptography", "Panama (cryptography)", "Phelix", "Pike (cipher)", "Public-key cryptography", "Py (cipher)", "QUAD (cipher)", "RC4", "Rabbit (cipher)", "SEAL (cipher)", "SNOW", "SOBER", "SOBER-128", "SOSEMANUK", "Salsa20", "Scream (cipher)", "Shift register", "Shrinking generator", "Steganography", "Stream cipher", "Symmetric-key algorithm", "T-function", "Trivium (cipher)", "VEST", "WAKE (cipher)"], "categories": ["All articles to be expanded", "All stub articles", "Articles needing translation from Russian Wikipedia", "Articles to be expanded from August 2013", "Cryptographic algorithms", "Cryptography stubs"], "title": "MOSQUITO"}
{"summary": "The plaintext-recovery-under-chosen-plaintext-attack advantage (PR-CPA advantage) is defined as the probability that an algorithm with fixed computational resources can use a chosen-plaintext attack to decrypt a randomly selected message that has been encrypted with a symmetric cipher. It is regarded as a fundamental quantity in cryptography since every symmetric encryption scheme must obviously must have a very low PR-CPA advantage to be secure. Though having a low susceptibility to this sort of attack is a necessary condition for an encryption scheme's security, it is not sufficient to ensure security. This is because partial information about the plaintext can often be recovered (for example the least significant bit of the message).", "links": ["Chosen-plaintext attack", "Mihir Bellare", "Shafi Goldwasser", "Symmetric cipher"], "categories": ["Cryptographic algorithms", "Theory of cryptography"], "title": "PR-CPA advantage"}
{"summary": "A randomness extractor, often simply called an \"extractor\", is a function, which being applied to output from a weakly random entropy source, together with a short, uniformly random seed, generates a highly random output that appears independent from the source and uniformly distributed. Examples of weakly random sources include radioactive decay or thermal noise; the only restriction on possible sources is that there is no way they can be fully controlled, calculated or predicted, and that a lower bound on their entropy rate can be established. For a given source, a randomness extractor can even be considered to be a true random number generator (TRNG); but there is no single extractor that has been proven to produce truly random output from any type of weakly random source.\nSometimes the term \"bias\" is used to denote a weakly random source's departure from uniformity, and in older literature, some extractors are called unbiasing algorithms, as they take the randomness from a so-called \"biased\" source and output a distribution that appears unbiased. The weakly random source will always be longer than the extractor's output, but an efficient extractor is one that lowers this ratio of lengths as much as possible, while simultaneously keeping the seed length low. Intuitively, this means that as much randomness as possible has been \"extracted\" from the source.\nNote that an extractor has some conceptual similarities with a pseudorandom generator (PRG), but the two concepts are not identical. Both are functions that take as input a small, uniformly random seed and produce a longer output that \"looks\" uniformly random. Some pseudorandom generators are, in fact, also extractors. (When a PRG is based on the existence of hard-core predicates, one can think of the weakly random source as a set of truth tables of such predicates and prove that the output is statistically close to uniform.) However, the general PRG definition does not specify that a weakly random source must be used, and while in the case of an extractor, the output should be statistically close to uniform, in a PRG it is only required to be computationally indistinguishable from uniform, a somewhat weaker concept.\nNIST Special Publication 800-90B (draft) recommends several extractors, including the SHA hash family and states that if the amount of entropy input is twice the number of bits output from them, that output can be considered essentially fully random.", "links": ["Bernoulli sequence", "Computational complexity theory", "Computationally indistinguishable", "Concatenation", "Correlation", "Cryptographic hash", "Cryptographic hash function", "Cryptography", "Decorrelation", "Disperser", "Encryption", "Exchangeable random variables", "Fuzzy extractor", "Hard-core predicate", "Hardware random number generator", "Independent and identically distributed random variables", "Information entropy", "John von Neumann", "Key generation", "Min-entropy", "NIST", "Negligible function", "Polynomial time", "Probabilistic method", "Pseudorandom generator", "Quantum cryptography", "Radioactive decay", "Random", "Randomness merger", "Secure Hash Algorithm", "Statistically close", "Thermal noise", "Total variation distance", "Uniform distribution (discrete)"], "categories": ["Computational complexity theory", "Cryptographic algorithms", "Random number generation", "Randomness"], "title": "Randomness extractor"}
{"summary": "The RC algorithms are a set of symmetric-key encryption algorithms invented by Ron Rivest. The \"RC\" may stand for either Rivest's cipher or, more informally, Ron's code. Despite the similarity in their names, the algorithms are for the most part unrelated. There have been six RC algorithms so far:\nRC1 was never published.\nRC2 was a 64-bit block cipher developed in 1987.\nRC3 was broken before ever being used.\nRC4 is the world's most widely used stream cipher.\nRC5 is a 32/64/128-bit block cipher developed in 1994.\nRC6, a 128-bit block cipher based heavily on RC5, was an AES finalist developed in 1997.", "links": ["AES process", "Block cipher", "RC2", "RC4", "RC5", "RC6", "Ron Rivest", "Stream cipher", "Symmetric-key encryption algorithm"], "categories": ["Cryptographic algorithms"], "title": "RC algorithm"}
{"summary": "In cryptography, a public key exchange is a cryptographic algorithm which allows two parties to create and share a secret key which they use to encrypt messages between themselves. The Ring Learning with Errors Key Exchange (RLWE-KEX) is one of a new class of public key exchange algorithms that are designed to be secure against an adversary that possesses a quantum computer. This is important because all of the public key algorithms in use today are easily broken by a quantum computer and scientists are making steady progress toward creating such a computer. The RLWE-KEX is one of a set of Post Quantum cryptographic algorithms which are based on the difficulty of solving certain mathematical problems involving lattices. Unlike older lattice based cryptographic algorithms, the RLWE-KEX is provably reducible to a known hard problem in lattices.", "links": ["Cryptographic algorithm", "Cryptography", "Diffie-Hellman", "Diffie\u2013Hellman key exchange", "Digital object identifier", "Digital signature", "Discrete logarithms", "Elliptic Curve Diffie-Hellman", "Elliptic curve", "Elliptic curve Diffie\u2013Hellman", "Finite field", "Floor and ceiling functions", "Forward secrecy", "Gaussian distribution", "Homomorphic encryption", "Ideal lattice cryptography", "Infinity norm", "Integer factorization", "International Standard Book Number", "Key exchange", "Key exchange algorithm", "Lattice-based cryptography", "Learning with errors", "Mass surveillance", "Nearest integer function", "Polynomial ring", "Post-quantum cryptography", "Prime number", "Public-key cryptography", "Public key", "Public key algorithm", "Quantum Safe Cryptography", "Quantum computer", "Quantum computing", "Ring Learning with Errors", "Ring learning with errors signature", "Ring of polynomials", "Shortest vector problem", "Uniform distribution (discrete)"], "categories": ["All articles needing expert attention", "All articles that are too technical", "Articles needing expert attention from June 2015", "Cryptographic algorithms", "Wikipedia articles that are too technical from June 2015"], "title": "Ring learning with errors key exchange"}
{"summary": "In cryptography, an S-box (substitution-box) is a basic component of symmetric key algorithms which performs substitution. In block ciphers, they are typically used to obscure the relationship between the key and the ciphertext \u2014 Shannon's property of confusion.\nIn general, an S-box takes some number of input bits, m, and transforms them into some number of output bits, n, where n is not necessarily equal to m. An m\u00d7n S-box can be implemented as a lookup table with 2m words of n bits each. Fixed tables are normally used, as in the Data Encryption Standard (DES), but in some ciphers the tables are generated dynamically from the key (e.g. the Blowfish and the Twofish encryption algorithms).\nOne good example of a fixed table is the S-box from DES (S5), mapping 6-bit input into a 4-bit output:\nGiven a 6-bit input, the 4-bit output is found by selecting the row using the outer two bits (the first and last bits), and the column using the inner four bits. For example, an input \"011011\" has outer bits \"01\" and inner bits \"1101\"; the corresponding output would be \"1001\".\nThe 8 S-boxes of DES were the subject of intense study for many years out of a concern that a backdoor \u2014 a vulnerability known only to its designers \u2014 might have been planted in the cipher. The S-box design criteria were eventually published (in Coppersmith 1994) after the public rediscovery of differential cryptanalysis, showing that they had been carefully tuned to increase resistance against this specific attack. Biham and Shamir found that even small modifications to an S-box could significantly weaken DES.\nThere has been a great deal of research into the design of good S-boxes, and much more is understood about their use in block ciphers than when DES was released.\nAny S-box where each output bit is produced by a bent function of the input bits, and where any linear combination of the output bits is also a bent function of the input bits, is a perfect S-box.", "links": ["3-Way", "3-subset meet-in-the-middle attack", "ARIA (cipher)", "Advanced Encryption Standard", "Advanced Encryption Standard process", "Akelarre (cipher)", "Anubis (cipher)", "Avalanche effect", "BATON", "BEAR and LION ciphers", "Backdoor (computing)", "BaseKing", "BassOmatic", "Bent function", "Biclique attack", "Bijection", "Bit", "Block cipher", "Block cipher mode of operation", "Block size (cryptography)", "Blowfish (cipher)", "Boolean function", "Boomerang attack", "Brighton", "Bruce Schneier", "Brute-force attack", "CAST-128", "CAST-256", "CIKS-1", "CIPHERUNICORN-A", "CIPHERUNICORN-E", "CLEFIA", "COCONUT98", "CRYPTON", "CRYPTREC", "CS-Cipher", "Camellia (cipher)", "Carlisle Adams", "Cellular Message Encryption Algorithm", "Chi-square test", "Chiasmus (cipher)", "Chuck Easttom", "Cipher", "Cipher security summary", "Ciphertext", "Claude Shannon", "Cobra ciphers", "Confusion and diffusion", "Crab (cipher)", "Cryptanalysis", "Cryptographic hash function", "Cryptographic key", "Cryptographically secure pseudorandom number generator", "Cryptography", "Cryptomeria cipher", "DEAL", "DES-X", "DES supplementary material", "DFC (cipher)", "Data Encryption Standard", "Davies' attack", "Differential-linear attack", "Differential cryptanalysis", "Digital object identifier", "Distinguishing attack", "Don Coppersmith", "E2 (cipher)", "EFF DES cracker", "EUROCRYPT", "FEA-M", "FEAL", "FROG", "Feistel cipher", "GDES", "GOST (block cipher)", "Grand Cru (cipher)", "Hasty Pudding cipher", "Hierocrypt", "Higher-order differential cryptanalysis", "History of cryptography", "ICE (cipher)", "IDEA NXT", "Impossible differential cryptanalysis", "Initialization vector", "Integral cryptanalysis", "Intel Cascade Cipher", "International Data Encryption Algorithm", "International Standard Book Number", "Interpolation attack", "Iraqi block cipher", "John Wiley & Sons", "KASUMI", "KHAZAD", "KN-Cipher", "Kaisa Nyberg", "KeeLoq", "Kendall tau rank correlation coefficient", "Key schedule", "Key size", "Key whitening", "Khufu and Khafre", "Known-key distinguishing attack", "LOKI", "LOKI97", "Ladder-DES", "Lai-Massey scheme", "Libelle (cipher)", "Linear cryptanalysis", "Lookup table", "Lucifer (cipher)", "M6 (cipher)", "M8 (cipher)", "MAGENTA", "MARS (cryptography)", "MESH (cipher)", "MISTY1", "MMB", "MULTI2", "MacGuffin (cipher)", "Madryga", "Meet-in-the-middle attack", "Mercy (cipher)", "Message authentication code", "Mod n cryptanalysis", "MultiSwap", "NESSIE", "NOEKEON", "NUSH", "NewDES", "New Data Seal", "Nimbus (cipher)", "Nothing up my sleeve number", "Outline of cryptography", "PDF", "PRESENT (cipher)", "Padding (cryptography)", "Partitioning cryptanalysis", "Permutation box", "Piling-up lemma", "PostScript", "Product cipher", "Public-key cryptography", "Q (cipher)", "Queen's University", "RC2", "RC5", "RC6", "REDOC", "Rebound attack", "Red Pike (cipher)", "Related-key attack", "Rijndael S-box", "Rotational cryptanalysis", "S-1 block cipher", "SAFER", "SAVILLE", "SC2000", "SEED", "SHACAL", "SHARK", "SMS4", "SXAL/MBAL", "Selected Areas in Cryptography", "Serpent (cipher)", "Simon (cipher)", "Skipjack (cipher)", "Slide attack", "Speck (cipher)", "Spectr-H64", "Square (cipher)", "Steganography", "Stream cipher", "Substitution-permutation network", "Substitution cipher", "Symmetric-key algorithm", "Symmetric key algorithm", "Threefish", "Time/memory/data tradeoff attack", "Timing attack", "Tiny Encryption Algorithm", "Treyfer", "Triple DES", "Truncated differential cryptanalysis", "Twofish", "UES (cipher)", "Weak key", "Whitening transformation", "XSL attack", "XTEA", "XXTEA", "Xenon (cipher)", "Xmx", "Zodiac (cipher)"], "categories": ["All articles needing additional references", "All articles with unsourced statements", "Articles needing additional references from March 2009", "Articles with unsourced statements from April 2012", "CS1 maint: Explicit use of et al.", "Cryptographic algorithms", "S-box"], "title": "S-box"}
{"summary": "In cryptography, scrypt is a password-based key derivation function created by Colin Percival, originally for the Tarsnap online backup service. The algorithm was specifically designed to make it costly to perform large-scale custom hardware attacks by requiring large amounts of memory. In 2012, the scrypt algorithm was published by IETF as an Internet Draft, intended to become an informational RFC. A simplified version of scrypt is used as a proof-of-work scheme by a number of cryptocurrencies first implemented by an anonymous programmer called ArtForz in Tenebrix followed by Fairbrix and Litecoin soon.", "links": ["Application-specific integrated circuit", "Bcrypt", "Block cipher", "Blowfish (cipher)", "Crypt (C)", "Cryptanalysis", "Cryptocurrencies", "Cryptocurrency", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Custom hardware attack", "Dogecoin", "Fairbrix", "Field-programmable gate array", "GPUs", "Hash-based message authentication code", "History of cryptography", "Internet Draft", "Internet Engineering Task Force", "Key derivation function", "Litecoin", "Message authentication code", "Outline of cryptography", "PBKDF2", "Proof-of-work", "Pseudorandom", "Public-key cryptography", "RSA Laboratories", "Random-access memory", "Request for Comments", "Script (disambiguation)", "Space\u2013time tradeoff", "Steganography", "Stream cipher", "Symmetric-key algorithm", "Tarsnap", "Tenebrix", "Time-memory tradeoff"], "categories": ["Cryptographic algorithms", "Key derivation functions"], "title": "Scrypt"}
{"summary": "In cryptology, SecureLog is an algorithm used to convert digital data into trusted data that can be verified if the authencity is questioned. SecureLog is used in IT solutions that generates data to support compliance regulations like SOX.", "links": ["Algorithm", "Archive", "Asymmetric key cryptography", "Compliance (regulation)", "Cryptology", "Digital timestamping", "Sarbanes-Oxley Act"], "categories": ["All orphaned articles", "Cryptographic algorithms", "Orphaned articles from October 2013"], "title": "SecureLog"}
{"summary": "In cryptography, an SP-network, or substitution-permutation network (SPN), is a series of linked mathematical operations used in block cipher algorithms such as AES (Rijndael). Other ciphers that use SPNs are 3-Way, SAFER, SHARK, and Square.\nSuch a network takes a block of the plaintext and the key as inputs, and applies several alternating \"rounds\" or \"layers\" of substitution boxes (S-boxes) and permutation boxes (P-boxes) to produce the ciphertext block. The S-boxes and P-boxes transform (sub-)blocks of input bits into output bits. It is common for these transformations to be operations that are efficient to perform in hardware, such as exclusive or (XOR) and bitwise rotation. The key is introduced in each round, usually in the form of \"round keys\" derived from it. (In some designs, the S-boxes themselves depend on the key.)\nDecryption is done by simply reversing the process (using the inverses of the S-boxes and P-boxes and applying the round keys in reversed order).\nAn S-box substitutes a small block of bits (the input of the S-box) by another block of bits (the output of the S-box). This substitution should be one-to-one, to ensure invertibility (hence decryption). In particular, the length of the output should be the same as the length of the input (the picture on the right has S-boxes with 4 input and 4 output bits), which is different from S-boxes in general that could also change the length, as in DES (Data Encryption Standard), for example. An S-box is usually not simply a permutation of the bits. Rather, a good S-box will have the property that changing one input bit will change about half of the output bits (or an avalanche effect). It will also have the property that each output bit will depend on every input bit.\nA P-box is a permutation of all the bits: it takes the outputs of all the S-boxes of one round, permutes the bits, and feeds them into the S-boxes of the next round. A good P-box has the property that the output bits of any S-box are distributed to as many S-box inputs as possible.\nAt each round, the round key (obtained from the key with some simple operations, for instance, using S-boxes and P-boxes) is combined using some group operation, typically XOR.\nA single typical S-box or a single P-box alone does not have much cryptographic strength: an S-box could be thought of as a substitution cipher, while a P-box could be thought of as a transposition cipher. However, a well-designed SP network with several alternating rounds of S- and P-boxes already satisfies Shannon's confusion and diffusion properties:\nThe reason for diffusion is the following: If one changes one bit of the plaintext, then it is fed into an S-box, whose output will change at several bits, then all these changes are distributed by the P-box among several S-boxes, hence the outputs of all of these S-boxes are again changed at several bits, and so on. Doing several rounds, each bit changes several times back and forth, therefore, by the end, the ciphertext has changed completely, in a pseudorandom manner. In particular, for a randomly chosen input block, if one flips the i-th bit, then the probability that the j-th output bit will change is approximately a half, for any i and j, which is the Strict Avalanche Criterion. Vice versa, if one changes one bit of the ciphertext, then attempts to decrypt it, the result is a message completely different from the original plaintext -- SP ciphers are not easily malleable.\nThe reason for confusion is exactly the same as for diffusion: changing one bit of the key changes several of the round keys, and every change in every round key diffuses over all the bits, changing the ciphertext in a very complex manner.\nEven if an attacker somehow obtains one plaintext corresponding to one ciphertext -- a known-plaintext attack, or worse, a chosen plaintext or chosen-ciphertext attack -- the confusion and diffusion make it difficult for the attacker to recover the key.\nAlthough a Feistel network that uses S-boxes (such as DES) is quite similar to SP networks, there are some differences that make either this or that more applicable in certain situations. For a given amount of confusion and diffusion, an SP network has more \"inherent parallelism\" and so \u2014 given a CPU with a large number of execution units \u2014 can be computed faster than a Feistel network.  CPUs with few execution units \u2014 such as most smart cards \u2014 cannot take advantage of this inherent parallelism. Also SP ciphers require S-boxes to be invertible (to perform decryption); Feistel inner functions have no such restriction and can be constructed as one-way functions.", "links": ["3-Way", "3-subset meet-in-the-middle attack", "ARIA (cipher)", "Advanced Encryption Standard", "Advanced Encryption Standard process", "Akelarre (cipher)", "Anubis (cipher)", "Avalanche effect", "BATON", "BEAR and LION ciphers", "BaseKing", "BassOmatic", "Biclique attack", "Bijective", "Bit", "Bitwise rotation", "Block cipher", "Block cipher mode of operation", "Block size (cryptography)", "Blowfish (cipher)", "Boomerang attack", "Brute-force attack", "CAST-128", "CAST-256", "CIKS-1", "CIPHERUNICORN-A", "CIPHERUNICORN-E", "CLEFIA", "COCONUT98", "CRYPTON", "CRYPTREC", "CS-Cipher", "Camellia (cipher)", "Cellular Message Encryption Algorithm", "Chi-square test", "Chiasmus (cipher)", "Chosen-ciphertext attack", "Chosen plaintext", "Cipher security summary", "Ciphertext", "Cobra ciphers", "Confusion and diffusion", "Crab (cipher)", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Cryptomeria cipher", "DEAL", "DES-X", "DES supplementary material", "DFC (cipher)", "Data Encryption Standard", "Davies' attack", "Decryption", "Differential-linear attack", "Differential cryptanalysis", "Diffuse", "Distinguishing attack", "E2 (cipher)", "EFF DES cracker", "Exclusive disjunction", "Execution unit", "FEA-M", "FEAL", "FROG", "Feistel cipher", "Feistel network", "GDES", "GOST (block cipher)", "Grand Cru (cipher)", "Hasty Pudding cipher", "Hierocrypt", "Higher-order differential cryptanalysis", "History of cryptography", "ICE (cipher)", "IDEA NXT", "Impossible differential cryptanalysis", "Initialization vector", "Integral cryptanalysis", "Intel Cascade Cipher", "International Data Encryption Algorithm", "International Standard Book Number", "Interpolation attack", "Iraqi block cipher", "KASUMI", "KHAZAD", "KN-Cipher", "KeeLoq", "Kendall tau rank correlation coefficient", "Key (cryptography)", "Key schedule", "Key size", "Key whitening", "Khufu and Khafre", "Known-key distinguishing attack", "Known-plaintext attack", "LOKI", "LOKI97", "Ladder-DES", "Lai-Massey scheme", "Libelle (cipher)", "Linear cryptanalysis", "Lucifer (cipher)", "M6 (cipher)", "M8 (cipher)", "MAGENTA", "MARS (cryptography)", "MESH (cipher)", "MISTY1", "MMB", "MULTI2", "MacGuffin (cipher)", "Madryga", "Malleability (cryptography)", "Meet-in-the-middle attack", "Mercy (cipher)", "Message authentication code", "Mod n cryptanalysis", "MultiSwap", "NESSIE", "NOEKEON", "NUSH", "NewDES", "New Data Seal", "Nimbus (cipher)", "One-way functions", "Outline of cryptography", "PRESENT (cipher)", "Padding (cryptography)", "Partitioning cryptanalysis", "Permutation", "Permutation box", "Piling-up lemma", "Plaintext", "Product cipher", "Pseudorandom", "Public-key cryptography", "Q (cipher)", "RC2", "RC5", "RC6", "REDOC", "Rebound attack", "Red Pike (cipher)", "Related-key attack", "Rotational cryptanalysis", "S-1 block cipher", "S-box", "SAFER", "SAVILLE", "SC2000", "SEED", "SHACAL", "SHARK", "SMS4", "SXAL/MBAL", "Serpent (cipher)", "Simon (cipher)", "Skipjack (cipher)", "Slide attack", "Smart card", "Speck (cipher)", "Spectr-H64", "Square (cipher)", "Steganography", "Stream cipher", "Strict Avalanche Criterion", "Substitution box", "Substitution cipher", "Symmetric-key algorithm", "Threefish", "Time/memory/data tradeoff attack", "Timing attack", "Tiny Encryption Algorithm", "Transposition cipher", "Treyfer", "Triple DES", "Truncated differential cryptanalysis", "Twofish", "UES (cipher)", "Weak key", "Whitening transformation", "XOR", "XSL attack", "XTEA", "XXTEA", "Xenon (cipher)", "Xmx", "Zodiac (cipher)"], "categories": ["Block ciphers", "Cryptographic algorithms", "Permutations"], "title": "Substitution-permutation network"}
{"summary": "The Supersingular Isogeny Diffie\u2013Hellman Key Exchange (SIDH) is a post-quantum public key cryptographic algorithm used to establish a secret key between two parties over an otherwise insecure communications channel. It was designed to resist cryptanalytic attack by an adversary in possession of a quantum computer. Because the SIDH has key sizes, computations and forward security protection similar to that of the widely supported Elliptic Curve Diffie\u2013Hellman key exchange it is a natural candidate to replace Diffie-Hellman and Elliptic Curve Diffie-Hellman in the face of a growing quantum computer threat.", "links": ["Diffie\u2013Hellman", "Diffie\u2013Hellman key exchange", "Digital object identifier", "Elliptic Curve DSA", "Elliptic curve Diffie\u2013Hellman", "Forward secrecy", "Heartbleed", "Isogeny", "J-invariant", "Kernel (algebra)", "McEliece cryptosystem", "NTRU", "Post-quantum", "Post-quantum cryptography", "Quantum computer", "Quantum computing", "RSA (cryptosystem)", "Rational map", "Supersingular elliptic curve"], "categories": ["Cryptographic algorithms"], "title": "Supersingular Isogeny Key Exchange"}
{"summary": "Symmetric-key algorithms are algorithms for cryptography that use the same cryptographic keys for both encryption of plaintext and decryption of ciphertext. The keys may be identical or there may be a simple transformation to go between the two keys. The keys, in practice, represent a shared secret between two or more parties that can be used to maintain a private information link. This requirement that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption, in comparison to public-key encryption.", "links": ["3-Way", "3-subset meet-in-the-middle attack", "A5/1", "A5/2", "ARIA (cipher)", "Achterbahn", "Advanced Encryption Standard", "Advanced Encryption Standard process", "Akelarre (cipher)", "Algorithm", "Anubis (cipher)", "Asymmetric-key cryptography", "Avalanche effect", "BATON", "BEAR and LION ciphers", "BaseKing", "BassOmatic", "Biclique attack", "Block cipher", "Block cipher mode of operation", "Block cipher modes of operation", "Block size (cryptography)", "Blowfish (cipher)", "Boomerang attack", "Brute-force attack", "CAST-128", "CAST-256", "CAST5", "CBC-MAC", "CIKS-1", "CIPHERUNICORN-A", "CIPHERUNICORN-E", "CLEFIA", "COCONUT98", "CRYPTON", "CRYPTREC", "CS-Cipher", "Camellia (cipher)", "Cellular Message Encryption Algorithm", "Chi-square test", "Chiasmus (cipher)", "Chosen-plaintext attack", "Cipher security summary", "Ciphertext", "CiteSeer", "Cobra ciphers", "Correlation attack", "Correlation immunity", "Crab (cipher)", "Cryptanalysis", "Cryptographic hash function", "Cryptographic key", "Cryptographic primitive", "Cryptographically secure pseudorandom number generator", "Cryptography", "Cryptomeria cipher", "DEAL", "DES-X", "DES supplementary material", "DFC (cipher)", "Data Encryption Standard", "Davies' attack", "Differential-linear attack", "Differential cryptanalysis", "Distinguishing attack", "E0 (cipher)", "E2 (cipher)", "EFF DES cracker", "ESTREAM", "Entropy (information theory)", "F-FCSR", "FEA-M", "FEAL", "FISH (cipher)", "FROG", "Feistel cipher", "GDES", "GOST (block cipher)", "Grain (cipher)", "Grand Cru (cipher)", "HC-256", "Hash function", "Hasty Pudding cipher", "Hierocrypt", "Higher-order differential cryptanalysis", "History of cryptography", "ICE (cipher)", "IDEA NXT", "ISAAC (cipher)", "Impossible differential cryptanalysis", "Initialization vector", "Integral cryptanalysis", "Integrated Authority File", "Intel Cascade Cipher", "International Data Encryption Algorithm", "International Standard Book Number", "Interpolation attack", "Iraqi block cipher", "KASUMI", "KHAZAD", "KN-Cipher", "KeeLoq", "Kendall tau rank correlation coefficient", "Key schedule", "Key size", "Key whitening", "Khufu and Khafre", "Known-key distinguishing attack", "Known-plaintext attack", "LOKI", "LOKI97", "Ladder-DES", "Lai-Massey scheme", "Libelle (cipher)", "Linear cryptanalysis", "Linear feedback shift register", "Lucifer (cipher)", "M6 (cipher)", "M8 (cipher)", "MAGENTA", "MARS (cryptography)", "MESH (cipher)", "MICKEY", "MISTY1", "MMB", "MUGI", "MULTI2", "MacGuffin (cipher)", "Madryga", "Meet-in-the-middle attack", "Mercy (cipher)", "Message authentication code", "Mod n cryptanalysis", "MultiSwap", "NESSIE", "NIST", "NLFSR", "NOEKEON", "NUSH", "NewDES", "New Data Seal", "Nimbus (cipher)", "Non-repudiation", "One-way compression function", "Outline of cryptography", "PRESENT (cipher)", "Padding (cryptography)", "Panama (cryptography)", "Partitioning cryptanalysis", "Permutation box", "Phelix", "Pike (cipher)", "Piling-up lemma", "Plaintext", "Product cipher", "Public-key cryptography", "Public-key encryption", "Py (cipher)", "QUAD (cipher)", "Q (cipher)", "RC2", "RC4", "RC5", "RC6", "REDOC", "Rabbit (cipher)", "Rebound attack", "Red Pike (cipher)", "Related-key attack", "Rotational cryptanalysis", "S-1 block cipher", "S-box", "SAFER", "SAVILLE", "SC2000", "SEAL (cipher)", "SEED", "SHACAL", "SHARK", "SMS4", "SNOW", "SOBER", "SOBER-128", "SOSEMANUK", "SXAL/MBAL", "Salsa20", "Scream (cipher)", "Serpent (cipher)", "Shared secret", "Shift register", "Shrinking generator", "Simon (cipher)", "Skipjack (cipher)", "Slide attack", "Speck (cipher)", "Spectr-H64", "Square (cipher)", "Steganography", "Stream cipher", "Substitution-permutation network", "T-function", "Threefish", "Time/memory/data tradeoff attack", "Timing attack", "Tiny Encryption Algorithm", "Treyfer", "Triple DES", "Trivium (cipher)", "Truncated differential cryptanalysis", "Twofish", "UES (cipher)", "VEST", "WAKE (cipher)", "Weak key", "Whitening transformation", "XSL attack", "XTEA", "XXTEA", "Xenon (cipher)", "Xmx", "Zodiac (cipher)"], "categories": ["All articles needing additional references", "All articles with unsourced statements", "Articles needing additional references from April 2012", "Articles with unsourced statements from April 2012", "Cryptographic algorithms", "Wikipedia articles with GND identifiers"], "title": "Symmetric-key algorithm"}
{"summary": "Time-based One-time Password Algorithm (TOTP) is an algorithm that computes a one-time password from a shared secret key and the current time. It has been adopted as Internet Engineering Task Force standard RFC 6238, is the cornerstone of Initiative For Open Authentication (OATH), and is used in a number of two factor authentication systems.\nTOTP is an example of a hash-based message authentication code (HMAC). It combines a secret key with the current timestamp using a cryptographic hash function to generate a one-time password. The timestamp typically increases in 30-second intervals, so passwords generated close together in time from the same secret key will be equal.\nIn a typical two-factor authentication application, user authentication proceeds as follows: a user will enter username and password into a website or other server, generate a one-time password for the server using TOTP running locally on a smartphone or other device, and type that password into the server as well. The server will then also run TOTP to verify the entered one-time password. For this to work, the clocks of the user's device and the server need to be roughly synchronized (the server will typically accept one-time passwords generated from timestamps that differ by \u00b11 from the client's timestamp). A single secret key, to be used for all subsequent authentication sessions, must have been shared between the server and the user's device over a secure channel ahead of time. If some more steps are carried out, the user can also authenticate the server using TOTP.", "links": ["Amazon Web Services", "Android (operating system)", "Bitbucket", "BlackBerry", "Chrome OS", "Cryptographic hash function", "Dropbox (service)", "Evernote", "Facebook", "Gandi", "GitHub", "Google", "Google Authenticator", "Google Chrome", "HMAC", "HMAC-based One-time Password Algorithm", "HOTP", "Hash-based message authentication code", "Hover (domain registrar)", "IOS", "Initiative For Open Authentication", "Internet Engineering Task Force", "LastPass", "LinOTP", "Linode", "Linux", "MOTP", "Mask (computing)", "Microsoft", "Most significant bit", "MultiOTP", "OCRA", "OS X", "One-time password", "OpenAM", "Pebble (watch)", "Pluggable Authentication Modules", "PrivacyIDEA", "Red Hat", "Request for Comments", "SHA-1", "Sailfish OS", "Salesforce.com", "Shared secret", "Smartwatch", "Two-factor authentication", "Two factor authentication", "Web application", "Windows (operating system)", "Windows Phone", "WordPress", "XenForo", "YubiKey"], "categories": ["All pages needing cleanup", "Articles needing cleanup from April 2014", "Computer access control", "Cryptographic algorithms", "Internet protocols", "Wikipedia list cleanup from April 2014"], "title": "Time-based One-time Password Algorithm"}
{"summary": "In cryptography, a Type 1 product is a device or system certified by the National Security Agency (NSA) for use in cryptographically securing classified U.S. Government information.\nType 1 certification is a rigorous process that includes testing and formal analysis of (among other things) cryptographic security, functional security, tamper resistance, emissions security (EMSEC/TEMPEST), and security of the product manufacturing and distribution process.\nFor a historically oriented list of NSA encryption products (most of them Type 1), see NSA encryption systems. For algorithms that NSA has participated in the development of, see NSA cryptography.\nTypes 1 through 4 are defined in the National Information Assurance Glossary (CNSSI No. 4009) which defines Type 1, Type 2, Type 3, and Type 4 products and keys.\nA Type 1 product is defined as:\nClassified or controlled cryptographic item endorsed by the NSA for securing classified and sensitive U.S. Government information, when appropriately keyed. The term refers only to products, and not to information, key, services, or controls. Type 1 products contain approved NSA algorithms. They are available to U.S. Government users, their contractors, and federally sponsored non-U.S. Government activities subject to export restrictions in accordance with International Traffic in Arms Regulations.", "links": ["Classified information", "Cryptography", "EMSEC", "Information", "International Traffic in Arms Regulations", "NSA Suite A Cryptography", "NSA Suite B Cryptography", "NSA cryptography", "NSA encryption systems", "National Information Assurance Glossary", "National Security Agency", "TEMPEST", "Tamper resistance", "Type 2 product", "Type 3 product", "Type 4 product", "United States"], "categories": ["All stub articles", "Cryptographic algorithms", "Cryptography stubs", "National Security Agency encryption devices", "Type 1 encryption algorithms"], "title": "Type 1 product"}
{"summary": "In cryptography, Type 2 products are unclassified cryptographic equipment, assemblies, or components, endorsed by the National Security Agency (NSA), for use in telecommunications and automated information systems for the protection of national security information.\nNote: The term refers only to products, and not to information, key, services, or controls. Type 2 products may not be used for classified information, but contain classified NSA algorithms (e.g. CORDOBA) that distinguish them from products containing unclassified algorithms like DES. Type 2 products are subject to export restrictions in accordance with the International Traffic in Arms Regulations.", "links": ["CORDOBA", "Classified information", "Cryptographic key", "Cryptography", "Data Encryption Standard", "Export of cryptography", "Federal Standard 1037C", "Information system", "International Traffic in Arms Regulations", "NSA Suite A Cryptography", "NSA Suite B Cryptography", "National Information Systems Security Glossary", "National Security Agency", "National security", "Telecommunications", "Type 1 product", "Type 3 product", "Type 4 product"], "categories": ["All articles lacking sources", "All stub articles", "Articles lacking sources from February 2008", "Cryptographic algorithms", "Cryptography stubs", "National Security Agency encryption devices"], "title": "Type 2 product"}
{"summary": "In NSA terminology, a Type 3 product is a device for use with Sensitive, But Unclassified (SBU) information on non-national security systems. Approved algorithms include DES, Triple DES, and AES (although AES might also be usable in NSA-certified Type 1 products).", "links": ["Advanced Encryption Standard", "Classified information in the United States", "Data Encryption Standard", "NSA Suite A Cryptography", "NSA Suite B Cryptography", "National Security Agency", "Triple DES", "Type 1 encryption", "Type 1 product", "Type 2 product", "Type 4 product"], "categories": ["All articles lacking sources", "All stub articles", "Articles lacking sources from February 2008", "Cryptographic algorithms", "Cryptography stubs", "National Security Agency encryption devices"], "title": "Type 3 product"}
{"summary": "In NSA terminology, a Type 4 algorithm is an encryption algorithm that has been registered with NIST but is not a Federal Information Processing Standard (FIPS). Type 4 algorithms may not be used to protect classified information.\nAlternatively, some sources use \"Type 4\" specifically to refer to exportable algorithms \u2014 once limited to 40-bit keys \u2014 which can be relatively easily broken with even a modest amount of computing power.", "links": ["40 bit encryption", "Classified information in the United States", "Encryption", "Federal Information Processing Standard", "NIST", "NSA", "NSA Suite A Cryptography", "NSA Suite B Cryptography", "Type 1 encryption", "Type 2 encryption", "Type 3 encryption"], "categories": ["All articles lacking sources", "All stub articles", "Articles lacking sources from February 2008", "Cryptographic algorithms", "Cryptography stubs", "National Security Agency encryption devices"], "title": "Type 4 product"}
{"summary": "The Coppersmith method, proposed by Don Coppersmith, is a method to find small integer roots of polynomial equations. These polynomials can be univariate or bivariate. In cryptography the algorithm is mainly used in attacks on RSA when parts of the secret key are known.\nThe method uses the Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm (LLL) to find a polynomial that has the roots of the target polynomial as roots and has small coefficients.", "links": ["Antoine Joux", "Coppersmith's Attack", "Cryptography", "Digital object identifier", "Don Coppersmith", "Lattice (group)", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Newton's method", "Polynomial", "Public key cryptography", "RSA (algorithm)", "Root of a function"], "categories": ["All articles needing additional references", "All articles needing cleanup", "All articles needing expert attention", "Articles needing additional references from January 2010", "Articles needing cleanup from January 2010", "Articles needing expert attention from January 2010", "Articles needing expert attention with no reason or talk parameter", "Asymmetric-key algorithms", "Cleanup tagged articles without a reason field from January 2010", "Mathematics articles needing expert attention", "Wikipedia pages needing cleanup from January 2010"], "title": "Coppersmith method"}
{"summary": "Discrete logarithm records are the best results achieved to date in solving the discrete logarithm problem, which is the problem of finding solutions x to the equation gx = h given elements g and h of a finite cyclic group G. The difficulty of this problem is the basis for the security of several cryptographic systems, including Diffie\u2013Hellman key agreement, ElGamal encryption, the ElGamal signature scheme, the Digital Signature Algorithm, and the elliptic curve cryptography analogs of these. Common choices for G used in these algorithms include the multiplicative group of integers modulo p, the multiplicative group of a finite field, and the group of points on an elliptic curve over a finite field.", "links": ["AlphaServer GS1280", "Antoine Joux", "Certicom", "Cryptographic", "Diffie\u2013Hellman", "Digital Signature Algorithm", "Discrete logarithm", "ElGamal encryption", "ElGamal signature scheme", "Elliptic curve", "Elliptic curve cryptography", "Erich Wenger", "Field-programmable gate array", "Finite field", "Function field sieve", "Graz University of Technology", "Kintex-7", "Koblitz curve", "Number field sieve", "Paul Wolfger", "Peter Montgomery (mathematician)", "PlayStation 3", "Pollard's rho algorithm for logarithms", "Safe prime", "Strong prime", "Virtex-6"], "categories": ["All articles containing potentially dated statements", "Articles containing potentially dated statements from 2010", "Articles containing potentially dated statements from 2014", "Articles containing potentially dated statements from January 2014", "Asymmetric-key algorithms", "Computational hardness assumptions", "Logarithms", "Modular arithmetic", "World records"], "title": "Discrete logarithm records"}
{"summary": "In cryptography, the three-pass protocol for sending messages is a framework which allows one party to securely send a message to a second party without the need to exchange or distribute encryption keys. This message protocol should not be confused with various other algorithms which use 3 passes for authentication.\nIt is called the three-pass protocol because the sender and the receiver exchange three encrypted messages. The first three-pass protocol was developed by Adi Shamir circa 1980, and is described in more detail in a later section. The basic concept of the Three-Pass Protocol is that each party has a private encryption key and a private decryption key. The two parties use their keys independently, first to encrypt the message, and then to decrypt the message.\nThe protocol uses an encryption function E and a decryption function D. The encryption function uses an encryption key e to change a plaintext message m into an encrypted message, or ciphertext, E(e,m). Corresponding to each encryption key e there is a decryption key d which allows the message to be recovered using the decryption function, D(d,E(e,m))=m. Sometimes the encryption function and decryption function are the same.\nIn order for the encryption function and decryption function to be suitable for the Three-Pass Protocol they must have the property that for any message m, any encryption key e with corresponding decryption key d and any independent encryption key k,  D(d,E(k,E(e,m))) = E(k,m). In other words, it must be possible to remove the first encryption with the key e even though a second encryption with the key k has been performed. This will always be possible with a commutative encryption. A commutative encryption is an encryption that is order-independent, i.e. it satisfies E(a,E(b,m))=E(b,E(a,m)) for all encryption keys a and b and all messages m. Commutative encryptions satisfy D(d,E(k,E(e,m))) = D(d,E(e,E(k,m))) = E(k,m).\nThe Three-Pass Protocol works as follows:\nThe sender chooses a private encryption key s and a corresponding decryption key t. The sender encrypts the message m with the key s and sends the encrypted message E(s,m) to the receiver.\nThe receiver chooses a private encryption key r and a corresponding decryption key q and super-encrypts the first message E(s,m) with the key r and sends the doubly encrypted message E(r,E(s,m)) back to the sender.\nThe sender decrypts the second message with the key t. Because of the commutativity property described above D(t,E(r,E(s,m)))=E(r,m) which is the message encrypted with only the receiver's private key. The sender sends this to the receiver.\nThe receiver can now decrypt the message using the key q, namely D(q,E(r,m))=m the original message.\nNotice that all of the operations involving the sender's private keys s and t are performed by the sender, and all of the operations involving the receiver's private keys r and q are performed by the receiver, so that neither party needs to know the other party's keys.", "links": ["Adi Shamir", "Algebraic Eraser Diffie\u2013Hellman", "Authentication", "Basis vector", "Benaloh cryptosystem", "Binary numeral system", "Block cipher", "Blum\u2013Goldwasser cryptosystem", "CEILIDH", "CRYPTREC", "Cayley\u2013Purser algorithm", "Ciphertext", "Circular shift", "Cramer\u2013Shoup cryptosystem", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Damg\u00e5rd\u2013Jurik cryptosystem", "Diffie\u2013Hellman key exchange", "Digital Signature Algorithm", "Digital signature", "Discrete logarithm", "EdDSA", "Efficient Probabilistic Public-Key Encryption Scheme", "ElGamal encryption", "ElGamal signature scheme", "Elliptic Curve Digital Signature Algorithm", "Elliptic curve Diffie\u2013Hellman", "Elliptic curve cryptography", "Encrypted key exchange", "Encryption", "Encryption key", "Exponentiation", "Fermat's Little Theorem", "Finite field", "GMR (cryptography)", "Goldwasser\u2013Micali cryptosystem", "Hidden Field Equations", "History of cryptography", "IEEE P1363", "Integer Factorization", "Integrated Encryption Scheme", "James Massey", "Jim K. Omura", "Key (cryptography)", "Key size", "Lagrange's theorem (group theory)", "Lamport signature", "MQV", "Man-in-the-middle attack", "McEliece cryptosystem", "Merkle\u2013Hellman knapsack cryptosystem", "Message authentication code", "NESSIE", "NSA Suite B Cryptography", "NTRUEncrypt", "NTRUSign", "Naccache\u2013Stern cryptosystem", "Naccache\u2013Stern knapsack cryptosystem", "Non-commutative cryptography", "Normal basis", "Okamoto\u2013Uchiyama cryptosystem", "Optimal asymmetric encryption padding", "Order (group theory)", "Outline of cryptography", "Paillier cryptosystem", "Plaintext", "Post-quantum cryptography", "Prime number", "Public-key cryptography", "Public-key infrastructure", "Public key fingerprint", "RSA (algorithm)", "RSA problem", "Rabin cryptosystem", "Row vector", "SPEKE (cryptography)", "Schmidt\u2013Samoa cryptosystem", "Schnorr signature", "Secure Remote Password protocol", "Station-to-Station protocol", "Steganography", "Stream cipher", "Superencryption", "Symmetric-key algorithm", "Web of trust", "XTR"], "categories": ["Asymmetric-key algorithms", "Cryptographic protocols"], "title": "Three-pass protocol"}
{"summary": "In cryptography, the three-pass protocol for sending messages is a framework which allows one party to securely send a message to a second party without the need to exchange or distribute encryption keys. This message protocol should not be confused with various other algorithms which use 3 passes for authentication.\nIt is called the three-pass protocol because the sender and the receiver exchange three encrypted messages. The first three-pass protocol was developed by Adi Shamir circa 1980, and is described in more detail in a later section. The basic concept of the Three-Pass Protocol is that each party has a private encryption key and a private decryption key. The two parties use their keys independently, first to encrypt the message, and then to decrypt the message.\nThe protocol uses an encryption function E and a decryption function D. The encryption function uses an encryption key e to change a plaintext message m into an encrypted message, or ciphertext, E(e,m). Corresponding to each encryption key e there is a decryption key d which allows the message to be recovered using the decryption function, D(d,E(e,m))=m. Sometimes the encryption function and decryption function are the same.\nIn order for the encryption function and decryption function to be suitable for the Three-Pass Protocol they must have the property that for any message m, any encryption key e with corresponding decryption key d and any independent encryption key k,  D(d,E(k,E(e,m))) = E(k,m). In other words, it must be possible to remove the first encryption with the key e even though a second encryption with the key k has been performed. This will always be possible with a commutative encryption. A commutative encryption is an encryption that is order-independent, i.e. it satisfies E(a,E(b,m))=E(b,E(a,m)) for all encryption keys a and b and all messages m. Commutative encryptions satisfy D(d,E(k,E(e,m))) = D(d,E(e,E(k,m))) = E(k,m).\nThe Three-Pass Protocol works as follows:\nThe sender chooses a private encryption key s and a corresponding decryption key t. The sender encrypts the message m with the key s and sends the encrypted message E(s,m) to the receiver.\nThe receiver chooses a private encryption key r and a corresponding decryption key q and super-encrypts the first message E(s,m) with the key r and sends the doubly encrypted message E(r,E(s,m)) back to the sender.\nThe sender decrypts the second message with the key t. Because of the commutativity property described above D(t,E(r,E(s,m)))=E(r,m) which is the message encrypted with only the receiver's private key. The sender sends this to the receiver.\nThe receiver can now decrypt the message using the key q, namely D(q,E(r,m))=m the original message.\nNotice that all of the operations involving the sender's private keys s and t are performed by the sender, and all of the operations involving the receiver's private keys r and q are performed by the receiver, so that neither party needs to know the other party's keys.", "links": ["Adi Shamir", "Algebraic Eraser Diffie\u2013Hellman", "Authentication", "Basis vector", "Benaloh cryptosystem", "Binary numeral system", "Block cipher", "Blum\u2013Goldwasser cryptosystem", "CEILIDH", "CRYPTREC", "Cayley\u2013Purser algorithm", "Ciphertext", "Circular shift", "Cramer\u2013Shoup cryptosystem", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Damg\u00e5rd\u2013Jurik cryptosystem", "Diffie\u2013Hellman key exchange", "Digital Signature Algorithm", "Digital signature", "Discrete logarithm", "EdDSA", "Efficient Probabilistic Public-Key Encryption Scheme", "ElGamal encryption", "ElGamal signature scheme", "Elliptic Curve Digital Signature Algorithm", "Elliptic curve Diffie\u2013Hellman", "Elliptic curve cryptography", "Encrypted key exchange", "Encryption", "Encryption key", "Exponentiation", "Fermat's Little Theorem", "Finite field", "GMR (cryptography)", "Goldwasser\u2013Micali cryptosystem", "Hidden Field Equations", "History of cryptography", "IEEE P1363", "Integer Factorization", "Integrated Encryption Scheme", "James Massey", "Jim K. Omura", "Key (cryptography)", "Key size", "Lagrange's theorem (group theory)", "Lamport signature", "MQV", "Man-in-the-middle attack", "McEliece cryptosystem", "Merkle\u2013Hellman knapsack cryptosystem", "Message authentication code", "NESSIE", "NSA Suite B Cryptography", "NTRUEncrypt", "NTRUSign", "Naccache\u2013Stern cryptosystem", "Naccache\u2013Stern knapsack cryptosystem", "Non-commutative cryptography", "Normal basis", "Okamoto\u2013Uchiyama cryptosystem", "Optimal asymmetric encryption padding", "Order (group theory)", "Outline of cryptography", "Paillier cryptosystem", "Plaintext", "Post-quantum cryptography", "Prime number", "Public-key cryptography", "Public-key infrastructure", "Public key fingerprint", "RSA (algorithm)", "RSA problem", "Rabin cryptosystem", "Row vector", "SPEKE (cryptography)", "Schmidt\u2013Samoa cryptosystem", "Schnorr signature", "Secure Remote Password protocol", "Station-to-Station protocol", "Steganography", "Stream cipher", "Superencryption", "Symmetric-key algorithm", "Web of trust", "XTR"], "categories": ["Asymmetric-key algorithms", "Cryptographic protocols"], "title": "Three-pass protocol"}
{"summary": "A primality test is an algorithm for determining whether an input number is prime. Amongst other fields of mathematics, it is used for cryptography. Unlike integer factorization, primality tests do not generally give prime factors, only stating whether the input number is prime or not. Factorization is thought to be a computationally difficult problem, whereas primality testing is comparatively easy (its running time is polynomial in the size of the input). Some primality tests prove that a number is prime, while others like Miller\u2013Rabin prove that a number is composite. Therefore, the latter might be called compositeness tests instead of primality tests.", "links": ["AC0", "AKS algorithm", "AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Agrawal's conjecture", "Algorithm", "Analytic number theory", "Ancient Egyptian multiplication", "ArXiv", "Baby-step giant-step", "Baillie-PSW primality test", "Baillie\u2013PSW primality test", "Big O notation", "Binary GCD algorithm", "Carl Pomerance", "Carmichael number", "Chakravala method", "Charles E. Leiserson", "Christos Papadimitriou", "Cipolla's algorithm", "Clifford Stein", "Co-NP", "Composite number", "Computational complexity theory", "Continued fraction factorization", "Coprime", "Cornacchia's algorithm", "Counterexample", "Cryptography", "Deterministic algorithm", "Digital object identifier", "Discrete logarithm", "Divisibility", "Divisor", "Dixon's factorization method", "Donald Knuth", "Elliptic curve primality", "Elliptic curve primality proving", "Eric W. Weisstein", "Euclidean algorithm", "Euler's factorization method", "Euler pseudoprime", "Extended Euclidean algorithm", "Factorization", "Fermat's factorization method", "Fermat's little theorem", "Fermat primality test", "Fibonacci number", "Frobenius pseudoprime", "Function field sieve", "F\u00fcrer's algorithm", "Gary L. Miller (mathematician)", "General number field sieve", "Generalized Riemann hypothesis", "Generating primes", "Greatest common divisor", "Hans Riesel", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Introduction to Algorithms", "Jacobi symbol", "Jahrbuch \u00fcber die Fortschritte der Mathematik", "John L. Selfridge", "John Selfridge", "Journal of Computer and System Sciences", "Karatsuba algorithm", "L (complexity)", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Leonard Adleman", "Long multiplication", "Lucas primality test", "Lucas pseudoprime", "Lucas sequence", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Manindra Agrawal", "MathWorld", "Mathematical Reviews", "Mathematics", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Multiplication algorithm", "Multiplicative order", "NC (complexity)", "NP (complexity)", "Neeraj Kayal", "Nitin Saxena", "Number theory", "P-complete", "P (complexity)", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Polynomial time", "Primality certificate", "Prime factor", "Prime number", "Primitive root modulo n", "Primorial", "Probable prime", "Proth's theorem", "Pseudocode", "Pseudoprime", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Quantum computer", "Quasi-polynomial time", "RP (complexity)", "RSA (algorithm)", "Randomized algorithm", "Rational sieve", "Recursion", "Remainder", "Richard Crandall", "Ronald L. Rivest", "Run-time complexity", "Sample space", "Samuel S. Wagstaff, Jr.", "Samuel Wagstaff", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Sophie Germain prime", "Special number field sieve", "Springer-Verlag", "Strong pseudoprime", "The Art of Computer Programming", "Thomas H. Cormen", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Vaughan Pratt", "Wheel factorization", "Williams' p + 1 algorithm", "Wilson's theorem", "ZPP (complexity)", "Zentralblatt MATH"], "categories": ["All articles needing additional references", "All articles with specifically marked weasel-worded phrases", "Articles needing additional references from August 2013", "Articles with specifically marked weasel-worded phrases from April 2010", "Asymmetric-key algorithms", "Pages containing cite templates with deprecated parameters", "Primality tests"], "title": "Primality test"}
{"summary": "Schoof's algorithm is an efficient algorithm to count points on elliptic curves over finite fields. The algorithm has applications in elliptic curve cryptography where it is important to know the number of points to judge the difficulty of solving the discrete logarithm problem in the group of points on an elliptic curve.\nThe algorithm was published by Ren\u00e9 Schoof in 1985 and it was a theoretical breakthrough, as it was the first deterministic polynomial time algorithm for counting points on elliptic curves. Before Schoof's algorithm, approaches to counting points on elliptic curves such as the naive and baby-step giant-step algorithms were, for the most part, tedious and had an exponential running time.\nThis article explains Schoof's approach, laying emphasis on the mathematical ideas underlying the structure of the algorithm.", "links": ["A. O. L. Atkin", "AGPLv3", "AKS primality test", "Abelian group", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algebraic closure", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "C++", "Chakravala method", "Chinese Remainder Theorem", "Chinese remainder theorem", "Cipolla's algorithm", "Continued fraction factorization", "Cornacchia's algorithm", "Counting points on elliptic curves", "Discrete logarithm", "Discrete logarithm problem", "Division Polynomials", "Division polynomial", "Division polynomials", "Dixon's factorization method", "Elliptic curve", "Elliptic curve cryptography", "Elliptic curve primality", "Elliptic curve primality proving", "Elliptic curves", "Euclidean algorithm", "Euler's factorization method", "Exponentiation by squaring", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Finite fields", "Frobenius endomorphism", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generalized Riemann Hypothesis", "Generating primes", "Greatest common divisor", "Group (mathematics)", "Group morphism", "Hasse's theorem on elliptic curves", "Imaginary hyperelliptic curve", "Index calculus algorithm", "Integer factorization", "Integer square root", "Karatsuba algorithm", "Las Vegas algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Miller\u2013Rabin primality test", "Modular exponentiation", "Modular forms", "Multiplication algorithm", "Noam Elkies", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Point at infinity", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Prime number theorem", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Ren\u00e9 Schoof", "Schoof\u2013Elkies\u2013Atkin algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Torsion subgroup", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Asymmetric-key algorithms", "Elliptic curve cryptography", "Elliptic curves", "Finite fields", "Group theory", "Number theory"], "title": "Schoof's algorithm"}
{"summary": "The Schoof\u2013Elkies\u2013Atkin algorithm (SEA) is an algorithm used for finding the order of or calculating the number of points on an elliptic curve over a finite field. Its primary application is in elliptic curve cryptography. The algorithm is an extension of Schoof's algorithm by Noam Elkies and A. O. L. Atkin to significantly improve its efficiency (under heuristic assumptions).", "links": ["A. O. L. Atkin", "Algorithm", "Classical modular curve", "Division polynomials", "Elliptic curve", "Elliptic curve cryptography", "Finite field", "Isogeny", "J-invariant", "Las Vegas algorithm", "Mathworld", "Noam Elkies", "Order (group theory)", "PARI/GP", "Schoof's algorithm"], "categories": ["Asymmetric-key algorithms", "Elliptic curve cryptography", "Finite fields", "Group theory", "Number theory"], "title": "Schoof\u2013Elkies\u2013Atkin algorithm"}
{"summary": "In cryptography, the three-pass protocol for sending messages is a framework which allows one party to securely send a message to a second party without the need to exchange or distribute encryption keys. This message protocol should not be confused with various other algorithms which use 3 passes for authentication.\nIt is called the three-pass protocol because the sender and the receiver exchange three encrypted messages. The first three-pass protocol was developed by Adi Shamir circa 1980, and is described in more detail in a later section. The basic concept of the Three-Pass Protocol is that each party has a private encryption key and a private decryption key. The two parties use their keys independently, first to encrypt the message, and then to decrypt the message.\nThe protocol uses an encryption function E and a decryption function D. The encryption function uses an encryption key e to change a plaintext message m into an encrypted message, or ciphertext, E(e,m). Corresponding to each encryption key e there is a decryption key d which allows the message to be recovered using the decryption function, D(d,E(e,m))=m. Sometimes the encryption function and decryption function are the same.\nIn order for the encryption function and decryption function to be suitable for the Three-Pass Protocol they must have the property that for any message m, any encryption key e with corresponding decryption key d and any independent encryption key k,  D(d,E(k,E(e,m))) = E(k,m). In other words, it must be possible to remove the first encryption with the key e even though a second encryption with the key k has been performed. This will always be possible with a commutative encryption. A commutative encryption is an encryption that is order-independent, i.e. it satisfies E(a,E(b,m))=E(b,E(a,m)) for all encryption keys a and b and all messages m. Commutative encryptions satisfy D(d,E(k,E(e,m))) = D(d,E(e,E(k,m))) = E(k,m).\nThe Three-Pass Protocol works as follows:\nThe sender chooses a private encryption key s and a corresponding decryption key t. The sender encrypts the message m with the key s and sends the encrypted message E(s,m) to the receiver.\nThe receiver chooses a private encryption key r and a corresponding decryption key q and super-encrypts the first message E(s,m) with the key r and sends the doubly encrypted message E(r,E(s,m)) back to the sender.\nThe sender decrypts the second message with the key t. Because of the commutativity property described above D(t,E(r,E(s,m)))=E(r,m) which is the message encrypted with only the receiver's private key. The sender sends this to the receiver.\nThe receiver can now decrypt the message using the key q, namely D(q,E(r,m))=m the original message.\nNotice that all of the operations involving the sender's private keys s and t are performed by the sender, and all of the operations involving the receiver's private keys r and q are performed by the receiver, so that neither party needs to know the other party's keys.", "links": ["Adi Shamir", "Algebraic Eraser Diffie\u2013Hellman", "Authentication", "Basis vector", "Benaloh cryptosystem", "Binary numeral system", "Block cipher", "Blum\u2013Goldwasser cryptosystem", "CEILIDH", "CRYPTREC", "Cayley\u2013Purser algorithm", "Ciphertext", "Circular shift", "Cramer\u2013Shoup cryptosystem", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Damg\u00e5rd\u2013Jurik cryptosystem", "Diffie\u2013Hellman key exchange", "Digital Signature Algorithm", "Digital signature", "Discrete logarithm", "EdDSA", "Efficient Probabilistic Public-Key Encryption Scheme", "ElGamal encryption", "ElGamal signature scheme", "Elliptic Curve Digital Signature Algorithm", "Elliptic curve Diffie\u2013Hellman", "Elliptic curve cryptography", "Encrypted key exchange", "Encryption", "Encryption key", "Exponentiation", "Fermat's Little Theorem", "Finite field", "GMR (cryptography)", "Goldwasser\u2013Micali cryptosystem", "Hidden Field Equations", "History of cryptography", "IEEE P1363", "Integer Factorization", "Integrated Encryption Scheme", "James Massey", "Jim K. Omura", "Key (cryptography)", "Key size", "Lagrange's theorem (group theory)", "Lamport signature", "MQV", "Man-in-the-middle attack", "McEliece cryptosystem", "Merkle\u2013Hellman knapsack cryptosystem", "Message authentication code", "NESSIE", "NSA Suite B Cryptography", "NTRUEncrypt", "NTRUSign", "Naccache\u2013Stern cryptosystem", "Naccache\u2013Stern knapsack cryptosystem", "Non-commutative cryptography", "Normal basis", "Okamoto\u2013Uchiyama cryptosystem", "Optimal asymmetric encryption padding", "Order (group theory)", "Outline of cryptography", "Paillier cryptosystem", "Plaintext", "Post-quantum cryptography", "Prime number", "Public-key cryptography", "Public-key infrastructure", "Public key fingerprint", "RSA (algorithm)", "RSA problem", "Rabin cryptosystem", "Row vector", "SPEKE (cryptography)", "Schmidt\u2013Samoa cryptosystem", "Schnorr signature", "Secure Remote Password protocol", "Station-to-Station protocol", "Steganography", "Stream cipher", "Superencryption", "Symmetric-key algorithm", "Web of trust", "XTR"], "categories": ["Asymmetric-key algorithms", "Cryptographic protocols"], "title": "Three-pass protocol"}
{"summary": "In cryptography, the three-pass protocol for sending messages is a framework which allows one party to securely send a message to a second party without the need to exchange or distribute encryption keys. This message protocol should not be confused with various other algorithms which use 3 passes for authentication.\nIt is called the three-pass protocol because the sender and the receiver exchange three encrypted messages. The first three-pass protocol was developed by Adi Shamir circa 1980, and is described in more detail in a later section. The basic concept of the Three-Pass Protocol is that each party has a private encryption key and a private decryption key. The two parties use their keys independently, first to encrypt the message, and then to decrypt the message.\nThe protocol uses an encryption function E and a decryption function D. The encryption function uses an encryption key e to change a plaintext message m into an encrypted message, or ciphertext, E(e,m). Corresponding to each encryption key e there is a decryption key d which allows the message to be recovered using the decryption function, D(d,E(e,m))=m. Sometimes the encryption function and decryption function are the same.\nIn order for the encryption function and decryption function to be suitable for the Three-Pass Protocol they must have the property that for any message m, any encryption key e with corresponding decryption key d and any independent encryption key k,  D(d,E(k,E(e,m))) = E(k,m). In other words, it must be possible to remove the first encryption with the key e even though a second encryption with the key k has been performed. This will always be possible with a commutative encryption. A commutative encryption is an encryption that is order-independent, i.e. it satisfies E(a,E(b,m))=E(b,E(a,m)) for all encryption keys a and b and all messages m. Commutative encryptions satisfy D(d,E(k,E(e,m))) = D(d,E(e,E(k,m))) = E(k,m).\nThe Three-Pass Protocol works as follows:\nThe sender chooses a private encryption key s and a corresponding decryption key t. The sender encrypts the message m with the key s and sends the encrypted message E(s,m) to the receiver.\nThe receiver chooses a private encryption key r and a corresponding decryption key q and super-encrypts the first message E(s,m) with the key r and sends the doubly encrypted message E(r,E(s,m)) back to the sender.\nThe sender decrypts the second message with the key t. Because of the commutativity property described above D(t,E(r,E(s,m)))=E(r,m) which is the message encrypted with only the receiver's private key. The sender sends this to the receiver.\nThe receiver can now decrypt the message using the key q, namely D(q,E(r,m))=m the original message.\nNotice that all of the operations involving the sender's private keys s and t are performed by the sender, and all of the operations involving the receiver's private keys r and q are performed by the receiver, so that neither party needs to know the other party's keys.", "links": ["Adi Shamir", "Algebraic Eraser Diffie\u2013Hellman", "Authentication", "Basis vector", "Benaloh cryptosystem", "Binary numeral system", "Block cipher", "Blum\u2013Goldwasser cryptosystem", "CEILIDH", "CRYPTREC", "Cayley\u2013Purser algorithm", "Ciphertext", "Circular shift", "Cramer\u2013Shoup cryptosystem", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Damg\u00e5rd\u2013Jurik cryptosystem", "Diffie\u2013Hellman key exchange", "Digital Signature Algorithm", "Digital signature", "Discrete logarithm", "EdDSA", "Efficient Probabilistic Public-Key Encryption Scheme", "ElGamal encryption", "ElGamal signature scheme", "Elliptic Curve Digital Signature Algorithm", "Elliptic curve Diffie\u2013Hellman", "Elliptic curve cryptography", "Encrypted key exchange", "Encryption", "Encryption key", "Exponentiation", "Fermat's Little Theorem", "Finite field", "GMR (cryptography)", "Goldwasser\u2013Micali cryptosystem", "Hidden Field Equations", "History of cryptography", "IEEE P1363", "Integer Factorization", "Integrated Encryption Scheme", "James Massey", "Jim K. Omura", "Key (cryptography)", "Key size", "Lagrange's theorem (group theory)", "Lamport signature", "MQV", "Man-in-the-middle attack", "McEliece cryptosystem", "Merkle\u2013Hellman knapsack cryptosystem", "Message authentication code", "NESSIE", "NSA Suite B Cryptography", "NTRUEncrypt", "NTRUSign", "Naccache\u2013Stern cryptosystem", "Naccache\u2013Stern knapsack cryptosystem", "Non-commutative cryptography", "Normal basis", "Okamoto\u2013Uchiyama cryptosystem", "Optimal asymmetric encryption padding", "Order (group theory)", "Outline of cryptography", "Paillier cryptosystem", "Plaintext", "Post-quantum cryptography", "Prime number", "Public-key cryptography", "Public-key infrastructure", "Public key fingerprint", "RSA (algorithm)", "RSA problem", "Rabin cryptosystem", "Row vector", "SPEKE (cryptography)", "Schmidt\u2013Samoa cryptosystem", "Schnorr signature", "Secure Remote Password protocol", "Station-to-Station protocol", "Steganography", "Stream cipher", "Superencryption", "Symmetric-key algorithm", "Web of trust", "XTR"], "categories": ["Asymmetric-key algorithms", "Cryptographic protocols"], "title": "Three-pass protocol"}
{"summary": "In cryptography, XTR is an algorithm for public-key encryption. XTR stands for \u2018ECSTR\u2019, which is an abbreviation for Efficient and Compact Subgroup Trace Representation. It is a method to represent elements of a subgroup of a multiplicative group of a finite field. To do so, it uses the trace over  to represent elements of a subgroup of .\nFrom a security point of view, XTR relies on the difficulty of solving Discrete Logarithm related problems in the full multiplicative group of a finite field. Unlike many cryptographic protocols that are based on the generator of the full multiplicative group of a finite field, XTR uses the generator  of a relatively small subgroup of some prime order  of a subgroup of . With the right choice of , computing Discrete Logarithms in the group, generated by , is, in general, as hard as it is in  and thus cryptographic applications of XTR use  arithmetics while achieving full  security leading to substantial savings both in communication and computational overhead without compromising security. Some other advantages of XTR are its fast key generation, small key sizes and speed.", "links": ["Algebraic Eraser Diffie\u2013Hellman", "Algorithm", "Alice and Bob", "Asymmetric key", "Benaloh cryptosystem", "Block cipher", "Blum\u2013Goldwasser cryptosystem", "CEILIDH", "CRYPTREC", "Cayley\u2013Purser algorithm", "Characteristic (algebra)", "CiteSeer", "Conjugate element (field theory)", "Cramer\u2013Shoup cryptosystem", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Cyclic group", "Cyclotomic polynomial", "Damg\u00e5rd\u2013Jurik cryptosystem", "Decisional Diffie\u2013Hellman assumption", "Diffie-Hellman", "Diffie-Hellman key agreement", "Diffie-Hellman key exchange", "Diffie-Hellman problem", "Diffie\u2013Hellman key exchange", "Digital Signature Algorithm", "Digital signature", "Discrete Logarithm", "Discrete logarithm", "Discrete logarithm problem", "EdDSA", "Efficient Probabilistic Public-Key Encryption Scheme", "ElGamal encryption", "ElGamal signature scheme", "Elliptic Curve Digital Signature Algorithm", "Elliptic curve Diffie\u2013Hellman", "Elliptic curve cryptography", "Elliptic curves", "Encrypted key exchange", "Field norm", "Field trace", "Finite field", "GMR (cryptography)", "Goldwasser\u2013Micali cryptosystem", "Group (mathematics)", "Group generator", "Hidden Field Equations", "History of cryptography", "Hybrid cryptosystem", "IEEE P1363", "Integer", "Integer Factorization", "Integrated Encryption Scheme", "Irreducibility (mathematics)", "Key size", "Lamport signature", "MQV", "McEliece cryptosystem", "Merkle\u2013Hellman knapsack cryptosystem", "Message authentication code", "Minimal polynomial (field theory)", "NESSIE", "NSA Suite B Cryptography", "NTRUEncrypt", "NTRUSign", "Naccache\u2013Stern cryptosystem", "Naccache\u2013Stern knapsack cryptosystem", "Non-commutative cryptography", "Normal basis", "Number Field Sieve", "Okamoto\u2013Uchiyama cryptosystem", "Optimal asymmetric encryption padding", "Outline of cryptography", "Overhead (computing)", "Paillier cryptosystem", "Pohlig-Hellman algorithm", "Pollard's rho algorithm", "Post-quantum cryptography", "Prime factorization", "Public-key cryptography", "Public-key infrastructure", "Public key", "Public key fingerprint", "Quadratic reciprocity", "RSA (algorithm)", "RSA problem", "Rabin cryptosystem", "Root of a function", "SPEKE (cryptography)", "Schmidt\u2013Samoa cryptosystem", "Schnorr signature", "Secret key", "Secure Remote Password protocol", "Shared secret", "Station-to-Station protocol", "Steganography", "Stream cipher", "Subgroup", "Symmetric-key algorithm", "Symmetric key", "Three-pass protocol", "Web of trust", "XTR"], "categories": ["Asymmetric-key algorithms", "Finite fields"], "title": "XTR"}
{"summary": "crypt is the library function which is used to compute a password hash that can be used to store user account passwords while keeping them relatively secure (a passwd file). The output of the function is not simply the hash\u2014 it is a text string which also encodes the salt (usually the first two characters are the salt itself and the rest is the hashed result), and identifies the hash algorithm used (defaulting to the \"traditional\" one explained below). This output string is what is meant for putting in a password record which may be stored in a plain text file.\nMore formally, crypt provides cryptographic key derivation functions for password validation and storage on Unix systems.", "links": ["Authenticated encryption", "Avalanche effect", "BLAKE (hash function)", "Base64", "Bcrypt", "Berkeley Software Design", "Birthday attack", "Block cipher", "Blowfish (cipher)", "Brute force attack", "CBC-MAC", "CCM mode", "CMAC", "CRYPTREC", "CWC mode", "Code", "Collision (computer science)", "Collision attack", "Crypt (Unix)", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Data Authentication Algorithm", "Data Encryption Standard", "David Mazi\u00e8res", "Dictionary attack", "EAX mode", "Elliptic curve only hash", "Fast Syndrome Based Hash", "GNU C Library", "GOST (hash function)", "Galois/Counter Mode", "Gr\u00f8stl", "HAS-160", "HAVAL", "HMAC", "Hash function security summary", "History of cryptography", "IAPM (mode)", "JH (hash function)", "Key derivation function", "Key stretching", "Known plaintext", "Kupyna", "LM hash", "Length extension attack", "Linux", "M-209", "MD2 (cryptography)", "MD4", "MD5", "MD6", "MDC-2", "Manual page (Unix)", "Merkle\u2013Damg\u00e5rd construction", "Message authentication", "Message authentication code", "Moore's Law", "N-Hash", "NESSIE", "NIST hash function competition", "NTLM", "National Institute of Standards and Technology", "Niels Provos", "OCB mode", "One-key MAC", "Outline of cryptography", "PBKDF2", "PHP", "PMAC (cryptography)", "Passwd (file)", "Perl", "Pike (programming language)", "Poly1305", "Poul-Henning Kamp", "Preimage attack", "Public-key cryptography", "Python (programming language)", "RIPEMD", "RadioGat\u00fan", "Rainbow table", "Red Hat", "Ruby (programming language)", "SHA-1", "SHA-2", "SHA-3", "SWIFFT", "Salt (cryptography)", "Scrypt", "Seventh Edition Unix", "Shadow password", "Side-channel attack", "SipHash", "Skein (hash function)", "Snefru", "Steganography", "Stream cipher", "Streebog", "Sun Microsystems", "Symmetric-key algorithm", "Tiger (cryptography)", "UMAC", "USENIX", "Ulrich Drepper", "VMAC", "Very smooth hash", "Whirlpool (cryptography)"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from April 2010", "Articles with unsourced statements from July 2011", "Broken cryptography algorithms", "Computer access control protocols", "Cryptographic hash functions", "Key derivation functions", "Password authentication"], "title": "Crypt (C)"}
{"summary": "Dual Elliptic Curve Deterministic Random Bit Generator (Dual_EC_DRBG) is an algorithm from the branch of cryptography known as elliptic curve cryptography that implements a cryptographically secure pseudorandom number generator (CSPRNG) capable of generating a random bit stream. The algorithm is based on the mathematics of the elliptic curve discrete logarithm problem (ECDLP). Despite public criticism, it was for some time one of the four (now three) CSPRNGs standardized in NIST SP 800-90A as originally published circa March 2007.\nWeaknesses in the cryptographic security of the algorithm were known and publicly criticised well before the algorithm became part of a formal standard endorsed by the ANSI, ISO, and formerly by the National Institute of Standards and Technology (NIST). One of the weaknesses publicly identified was the potential of the algorithm to harbour a backdoor advantageous to the algorithm's designers\u2014the United States government's National Security Agency (NSA)\u2014and no-one else. In 2013, the New York Times reported that documents in their possession but never released to the public \"appear to confirm\" that the backdoor was real, and had been deliberately inserted by the NSA as part of the NSA's Bullrun decryption program. In December 2013, a Reuters news article alleged that in 2004, before NIST standardized Dual_EC_DRBG, NSA paid RSA Security $10 million in a secret deal to use Dual_EC_DRBG as the default in the RSA BSAFE cryptography library, which resulted in RSA Security becoming the most important distributor of the insecure algorithm. RSA responded that they \"categorically deny\" that they had ever knowingly colluded with the NSA to adopt an algorithm that was known to be flawed, saying \"we have never kept [our] relationship [with the NSA] a secret\".\nSometime before its first known publication in 2004, a possible backdoor was discovered with the Dual_EC_DRBG's design, with the design of Dual_EC_DRBG having the unusual property that it was theoretically impossible for anyone but Dual_EC_DRBG's designers (NSA) to confirm the backdoor's existence. Bruce Schneier concluded shortly after standardization that the \"rather obvious\" backdoor (along with other deficiencies) would mean that nobody would use Dual_EC_DRBG. The backdoor would allow NSA to decrypt for example SSL/TLS encryption which used Dual_EC_DRBG as a CSPRNG.\nMembers of the ANSI standard group, to which Dual_EC_DRBG was first submitted, were aware of the exact mechanism of the potential backdoor and how to disable it, but did not take sufficient steps to unconditionally disable the backdoor or to widely publicize it. The general cryptographic community was initially not aware of the potential backdoor, until Dan Shumow and Niels Ferguson's publication, or of Certicom's Daniel R. L. Brown and Scott Vanstone's 2005 patent application describing the backdoor mechanism.\nIn September 2013, The New York Times reported that internal NSA memos leaked by Edward Snowden indicated that the NSA had worked during the standardization process to eventually become the sole editor of the Dual_EC_DRBG standard, and concluded that the Dual_EC_DRBG standard did indeed contain a backdoor for the NSA. As response, NIST stated that \"NIST would not deliberately weaken a cryptographic standard.\" According to the New York Times story, the NSA spends $250 million per year to insert backdoors in software and hardware as part of the Bullrun program. A Presidential advisory committee subsequently set up to examine NSA's conduct recommended among other things that the US government \"fully support and not undermine efforts to create encryption standards\".\nIn April 21, 2014, NIST withdrew Dual_EC_DRBG from its draft guidance on random number generators recommending \"current users of Dual_EC_DRBG transition to one of the three remaining approved algorithms as quickly as possible.\"", "links": ["ANSI X9.82", "ANSI X9.82, Part 3", "ANSI X9.82 DRBG", "Adam L. Young", "Advantage (cryptography)", "American National Standards Institute", "Ars Technica", "Backdoor (computing)", "BlackBerry", "Bruce Schneier", "Bullrun (decryption program)", "CRYPTO (conference)", "Certicom", "Computational hardness assumption", "Crypto AG", "Cryptographic nonce", "Cryptographically secure pseudorandom number generator", "Cryptovirology", "Cygnacom", "Dan Shumow", "Daniel J. Bernstein", "Decisional Diffie\u2013Hellman assumption", "Discrete logarithm problem", "Edward Snowden", "Elliptic curve", "Elliptic curve cryptography", "FIPS 140-2", "ISO/IEC 18031", "ISO 18031", "International Organization for Standardization", "Jeffrey Carr", "John Kelsey (cryptanalyst)", "Kleptogram", "Kleptography", "Matt Blaze", "Matthew Green (cryptographer)", "Microsoft", "Moti Yung", "NIST", "NIST SP 800-90A", "National Institute of Standards and Technology", "National Security Agency", "Niels Ferguson", "Nothing up my sleeve number", "Passive aggressive", "Provable security", "RSA BSAFE", "RSA Conference", "RSA Security", "Random number generator attack", "Ruben Niederhagen", "Tanja Lange", "The New York Times", "Transport Layer Security", "Windows Registry", "Windows Vista", "Wired (magazine)", "Wired News"], "categories": ["Articles with underscores in the title", "Broken cryptography algorithms", "Conspiracy theories", "Cryptographically secure pseudorandom number generators", "Kleptography", "National Institute of Standards and Technology", "National Security Agency", "Pseudorandom number generators"], "title": "Dual EC DRBG"}
{"summary": "MS-CHAP is the Microsoft version of the Challenge-Handshake Authentication Protocol, CHAP. The protocol exists in two versions, MS-CHAPv1 (defined in RFC 2433) and MS-CHAPv2 (defined in RFC 2759). MS-CHAPv2 was introduced with Windows NT 4.0 SP4 and was added to Windows 98 in the \"Windows 98 Dial-Up Networking Security Upgrade Release\" and Windows 95 in the \"Dial Up Networking 1.3 Performance & Security Update for MS Windows 95\" upgrade. Windows Vista dropped support for MS-CHAPv1.\nMS-CHAP is used as one authentication option in Microsoft's implementation of the PPTP protocol for virtual private networks. It is also used as an authentication option with RADIUS servers which are used for WiFi security using the WPA-Enterprise protocol. It is further used as the main authentication option of the Protected Extensible Authentication Protocol (PEAP).\nCompared with CHAP, MS-CHAP:\nis enabled by negotiating CHAP Algorithm 0x80 (0x81 for MS-CHAPv2) in LCP option 3, Authentication Protocol\nprovides an authenticator-controlled password change mechanism\nprovides an authenticator-controlled authentication retry mechanism\ndefines failure codes returned in the Failure packet message field\nMS-CHAPv2 provides mutual authentication between peers by piggybacking a peer challenge on the Response packet and an authenticator response on the Success packet.", "links": ["ACF2", "AKA (security)", "Authentication", "Authentication protocol", "BSD Authentication", "Bruce Schneier", "CAVE-based authentication", "CRAM-MD5", "Central Authentication Service", "Challenge-Handshake Authentication Protocol", "Common Data Security Architecture", "Diameter (protocol)", "EAuthentication", "EFF DES cracker", "Extensible Authentication Protocol", "Generic Security Services Application Program Interface", "Host Identity Protocol", "Java Authentication and Authorization Service", "Kerberos (protocol)", "LAN Manager", "MS-CHAPv2", "Microsoft", "Moxie Marlinspike", "NT LAN Manager", "Novell Modular Authentication Service", "OAuth", "OpenID", "OpenID Connect", "PPTP", "Password-authenticated key agreement", "Password Authentication Protocol", "Pluggable authentication module", "Protected Extensible Authentication Protocol", "RADIUS", "RFID-Authentication Protocols", "Resource Access Control Facility", "Secure Remote Password protocol", "Security Support Provider Interface", "Simple Authentication and Security Layer", "TACACS", "TACACS+", "Virtual private network", "Wi-Fi Protected Access", "WiFi", "Windows 95", "Windows 98", "Windows NT 4.0", "Windows Vista", "Woo\u2013Lam", "XUDA"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from July 2013", "Broken cryptography algorithms", "Computer access control protocols", "Internet protocols", "Microsoft Windows security technology"], "title": "MS-CHAP"}
{"summary": "Wired Equivalent Privacy (WEP) is a security algorithm for IEEE 802.11 wireless networks. Introduced as part of the original 802.11 standard ratified in 1997, its intention was to provide data confidentiality comparable to that of a traditional wired network. WEP, recognizable by the key of 10 or 26 hexadecimal digits, was at one time widely in use and was often the first security choice presented to users by router configuration tools.\nIn 2003 the Wi-Fi Alliance announced that WEP had been superseded by Wi-Fi Protected Access (WPA). In 2004, with the ratification of the full 802.11i standard (i.e. WPA2), the IEEE declared that both WEP-40 and WEP-104 have been deprecated.", "links": ["3Com", "40-bit encryption", "ASCII", "Address Resolution Protocol", "Adi Shamir", "Agere Systems", "Aircrack-ng", "Algorithm", "Brute force attack", "Business Wire", "CRC-32", "Cleartext", "Computer network", "Concatenated", "Confidentiality", "Data integrity", "David A. Wagner", "Deauth", "Export of cryptography", "Extensible Authentication Protocol", "Federal Bureau of Investigation", "Firmware", "Fluhrer, Mantin and Shamir attack", "Hexadecimal", "IEEE 802.11", "IPSec", "Ian Goldberg", "Initialization vector", "Itsik Mantin", "Key (cryptography)", "Local area network", "Lucent Technologies", "Mark Handley (computer scientist)", "Nikita Borisov", "PCI DSS", "Payment Card Industry", "Proxim Wireless", "RC4", "RSA Security", "Related key attack", "Replay attack", "Scott Fluhrer", "Secure Shell", "Shared key", "Stream cipher", "Stream cipher attack", "Temporal Key Integrity Protocol", "Tj maxx", "Tunneling protocols", "WPA2", "Wi-Fi Alliance", "Wi-Fi Protected Access", "Wireless access point", "Wireless ad hoc network", "Wireless cracking"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from December 2011", "Broken cryptography algorithms", "Computer network security", "Cryptographic protocols", "IEEE 802.11", "Wireless networking"], "title": "Wired Equivalent Privacy"}
{"summary": "The Berlekamp\u2013Massey algorithm is an algorithm that will find the shortest linear feedback shift register (LFSR) for a given binary output sequence. The algorithm will also find the minimal polynomial of a linearly recurrent sequence in an arbitrary field. The field requirement means that the Berlekamp\u2013Massey algorithm requires all non-zero elements to have a multiplicative inverse. Reeds and Sloane offer an extension to handle a ring.\nElwyn Berlekamp invented an algorithm for decoding Bose\u2013Chaudhuri\u2013Hocquenghem (BCH) codes. James Massey recognized its application to linear feedback shift registers and simplified the algorithm. Massey termed the algorithm the LFSR Synthesis Algorithm (Berlekamp Iterative Algorithm), but it is now known as the Berlekamp\u2013Massey algorithm.", "links": ["Algorithm", "Assignment (computer science)", "BCH code", "Berlekamp's algorithm", "Berlekamp\u2013Welch algorithm", "Bit", "CiteSeer", "Digital object identifier", "Elwyn Berlekamp", "Encyclopedia of Mathematics", "Eric W. Weisstein", "Exclusive or", "Field (mathematics)", "Finite field", "International Standard Book Number", "James Massey", "Linear feedback shift register", "MathWorld", "Minimal polynomial (field theory)", "N. J. A. Sloane", "NLFSR", "PlanetMath", "Recurrence relation", "Reeds\u2013Sloane algorithm", "Reed\u2013Solomon error correction", "Ring (mathematics)", "Springer Science+Business Media"], "categories": ["All articles with dead external links", "Articles with German-language external links", "Articles with dead external links from June 2015", "Cryptanalytic algorithms", "Error detection and correction"], "title": "Berlekamp\u2013Massey algorithm"}
{"summary": "The Reeds\u2013Sloane algorithm, named after J. A. Reeds and N. J. A. Sloane, is an extension of the Berlekamp\u2013Massey algorithm, an algorithm for finding the shortest linear feedback shift register (LFSR) for a given output sequence, for use on sequences that take their values from the integers mod n.", "links": ["Berlekamp\u2013Massey algorithm", "Digital object identifier", "Integers mod n", "Linear feedback shift register", "MathWorld", "N. J. A. Sloane", "Sequence"], "categories": ["All stub articles", "Cryptanalytic algorithms", "Cryptography stubs"], "title": "Reeds\u2013Sloane algorithm"}
{"summary": "A rainbow table is a precomputed table for reversing cryptographic hash functions, usually for cracking password hashes. Tables are usually used in recovering a plaintext password up to a certain length consisting of a limited set of characters. It is a practical example of a space/time trade-off, using less computer processing time and more storage than a brute-force attack which calculates a hash on every attempt, but more processing time and less storage than a simple lookup table with one entry per hash. Use of a key derivation function that employs a salt makes this attack infeasible.\nRainbow tables are an application of an earlier, simpler algorithm by Martin Hellman.", "links": ["A5/1", "Authenticated encryption", "Authentication", "Avalanche effect", "BLAKE (hash function)", "BSD", "Bcrypt", "Big-O notation", "Birthday attack", "Block cipher", "Bruce Schneier", "Brute-force attack", "Brute force attack", "CBC-MAC", "CCM mode", "CMAC", "CRYPTREC", "CWC mode", "Collision (computer science)", "Collision attack", "Concatenation", "Crypt (C)", "Crypt (Unix)", "Cryptanalysis", "Cryptographic hash", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "DMOZ", "Data Authentication Algorithm", "David A. Wagner", "David Mazi\u00e8res", "Dictionary attack", "Digital object identifier", "EAX mode", "Elliptic curve only hash", "Fast Syndrome Based Hash", "GOST (hash function)", "Galois/Counter Mode", "Gr\u00f8stl", "HAS-160", "HAVAL", "HMAC", "Hash chain", "Hash function security summary", "History of cryptography", "IAPM (mode)", "IEEE Transactions on Information Theory", "International Standard Book Number", "JH (hash function)", "John Kelsey (cryptanalyst)", "Key derivation function", "Key stretching", "Kupyna", "LM hash", "Lecture Notes in Computer Science", "Length extension attack", "Linux", "Lookup table", "MD2 (cryptography)", "MD4", "MD5", "MD6", "MDC-2", "Martin Hellman", "Merkle\u2013Damg\u00e5rd construction", "Message authentication", "Message authentication code", "Microsoft", "Microsoft Windows", "N-Hash", "NESSIE", "NIST hash function competition", "NTLM", "Niels Provos", "OCB mode", "One-key MAC", "Ophcrack", "Outline of cryptography", "PBKDF2", "PMAC (cryptography)", "Password", "Password cracking", "Plaintext", "Pollard's kangaroo algorithm", "Poly1305", "Precomputed", "Preimage attack", "Public-key cryptography", "RIPEMD", "RadioGat\u00fan", "RainbowCrack", "SHA-1", "SHA-2", "SHA-3", "SHA1", "SWIFFT", "Salt (cryptography)", "Santa Barbara, California", "Scrypt", "Side-channel attack", "SipHash", "Skein (hash function)", "Snefru", "Solaris (operating system)", "Space-time tradeoff", "Steganography", "Stream cipher", "Streebog", "Symmetric-key algorithm", "Tiger (cryptography)", "UMAC", "USENIX", "Udi Manber", "Unix", "VMAC", "Very smooth hash", "Whirlpool (cryptography)"], "categories": ["All articles lacking in-text citations", "All articles with unsourced statements", "Articles lacking in-text citations from March 2009", "Articles with DMOZ links", "Articles with unsourced statements from January 2011", "Articles with unsourced statements from January 2014", "Articles with unsourced statements from July 2013", "Cryptographic attacks", "Cryptographic hash functions", "Search algorithms"], "title": "Rainbow table"}
{"summary": "In mathematics and computing universal hashing (in a randomized algorithm or data structure) refers to selecting a hash function at random from a family of hash functions with a certain mathematical property (see definition below). This guarantees a low number of collisions in expectation, even if the data is chosen by an adversary. Many universal families are known (for hashing integers, vectors, strings), and their evaluation is often very efficient. Universal hashing has numerous uses in computer science, for example in implementations of hash tables, randomized algorithms, and cryptography.", "links": ["2-choice hashing", "ArXiv", "Bit masking", "Brian Kernighan", "C (programming language)", "Computing", "Cryptography", "Cuckoo hashing", "Daniel J. Bernstein", "Dennis Ritchie", "Digital object identifier", "Donald Knuth", "Double hashing", "Dynamic perfect hashing", "Expected value", "Geometric distribution", "Hash function", "Hash table", "Image (mathematics)", "International Standard Book Number", "K-independent hashing", "Linear congruential generator", "Low-discrepancy sequence", "Mark N. Wegman", "Mathematics", "Mersenne prime", "Message authentication code", "Mihai P\u0103tra\u015fcu", "Mikkel Thorup", "Min-wise independence", "Pairwise independent", "Perfect hashing", "Poly1305-AES", "Randomized algorithm", "Ring (mathematics)", "Rolling hashing", "Statistical distance", "Tabulation hashing", "The Art of Computer Programming", "UMAC", "Universal one-way hash function"], "categories": ["Computational complexity theory", "Cryptographic hash functions", "Hashing", "Search algorithms"], "title": "Universal hashing"}
{"summary": "A cryptosystem is information-theoretically secure if its security derives purely from information theory. That is, it cannot be broken even when the adversary has unlimited computing power. The adversary simply does not have enough information to break the encryption, so these cryptosystems are considered cryptanalytically unbreakable.\nAn encryption protocol that has information-theoretic security does not depend for its effectiveness on unproven assumptions about computational hardness, and such an algorithm is not vulnerable to future developments in computer power such as quantum computing. An example of an information-theoretically secure cryptosystem is the one-time pad. The concept of information-theoretically secure communication was introduced in 1949 by American mathematician Claude Shannon, the inventor of information theory, who used it to prove that the one-time pad system was secure. Information-theoretically secure cryptosystems have been used for the most sensitive governmental communications, such as diplomatic cables and high-level military communications, because of the great efforts enemy governments expend toward breaking them.\nAn interesting special case is perfect security: an encryption algorithm is perfectly secure if a ciphertext produced using it provides no information about the plaintext without knowledge of the key. If E is a perfectly secure encryption function, for any fixed message m there must exist for each ciphertext c at least one key k such that . It has been proved that any cipher with the perfect secrecy property must use keys with effectively the same requirements as one-time pad keys.\nIt is common for a cryptosystem to leak some information but nevertheless maintain its security properties even against an adversary that has unlimited computational resources. Such a cryptosystem would have information theoretic but not perfect security. The exact definition of security would depend on the cryptosystem in question.\nThere are a variety of cryptographic tasks for which information-theoretic security is a meaningful and useful requirement. A few of these are:\nSecret sharing schemes such as Shamir's are information-theoretically secure (and also perfectly secure) in that less than the requisite number of shares of the secret provide no information about the secret.\nMore generally, secure multiparty computation protocols often, but not always, have information-theoretic security.\nPrivate information retrieval with multiple databases can be achieved with information-theoretic privacy for the user's query.\nReductions between cryptographic primitives or tasks can often be achieved information-theoretically. Such reductions are important from a theoretical perspective, because they establish that primitive  can be realized if primitive  can be realized.\nSymmetric encryption can be constructed under an information-theoretic notion of security called entropic security, which assumes that the adversary knows almost nothing about the message being sent. The goal here is to hide all functions of the plaintext rather than all information about it.\nQuantum cryptography is largely part of information-theoretic cryptography.\nConventional secrecy entails encrypting messages. Beyond this, some scenarios require covert communication, a stronger type of secrecy which also hides the fact that communication is happening at all.", "links": ["Aaron D. Wyner", "Adversary (cryptography)", "Ciphertext", "Claude Shannon", "Colluding", "Cryptanalysis", "Cryptosystem", "Digital object identifier", "Diplomatic cable", "Entropic security", "Imre Csisz\u00e1r", "Information theory", "Key (cryptography)", "Leftover hash lemma", "MIMO", "Multiplicative noise", "One-time pad", "Plaintext", "Private information retrieval", "Quantum computer", "Quantum cryptography", "RSA (algorithm)", "Reconfigurable antenna", "Reduction (complexity)", "Secrecy", "Secret sharing", "Secure multiparty computation", "Semantic security", "Shamir's Secret Sharing", "Shlomo Shamai", "Symmetric encryption", "Wireless"], "categories": ["Information-theoretically secure algorithms", "Theory of cryptography"], "title": "Information-theoretic security"}
{"summary": "Shamir's Secret Sharing is an algorithm in cryptography created by Adi Shamir. It is a form of secret sharing, where a secret is divided into parts, giving each participant its own unique part, where some of the parts or all of them are needed in order to reconstruct the secret.\nCounting on all participants to combine the secret might be impractical, and therefore sometimes the threshold scheme is used where any  of the parts are sufficient to reconstruct the original secret.", "links": ["Adi Shamir", "Algorithm", "Alice and Bob", "Chung Laung Liu", "Cryptography", "Cubic function", "Curve fitting", "Degree of a polynomial", "Digital object identifier", "Donald Knuth", "Finite field", "Finite field arithmetic", "Homomorphic secret sharing", "Information theoretic security", "Lagrange polynomial", "Line (geometry)", "Natural numbers", "Parabola", "Partial Password", "Point (geometry)", "Polynomial", "Safe", "Secret sharing", "The Art of Computer Programming", "Two-man rule"], "categories": ["All articles needing expert attention", "All articles that are too technical", "Articles needing expert attention from March 2014", "Information-theoretically secure algorithms", "Secret sharing", "Wikipedia articles that are too technical from March 2014"], "title": "Shamir's Secret Sharing"}
{"summary": "In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.\nWhen the numbers are very large, no efficient, non-quantum integer factorization algorithm is known; an effort by several researchers concluded in 2009, factoring a 232-digit number (RSA-768), utilizing hundreds of machines over a span of two years. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.\nNot all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.\nMany cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem\u2014for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure.\n^ Kleinjung; et al. (2010-02-18). \"Factorization of a 768-bit RSA modulus\" (PDF). International Association for Cryptologic Research. Retrieved 2010-08-09.", "links": ["AKS primality test", "Abundant number", "Achilles number", "Adleman\u2013Pomerance\u2013Rumely primality test", "Advanced Micro Devices", "Algebraic-group factorisation algorithms", "Algebraic number theory", "Algorithm", "Aliquot sequence", "Almost perfect number", "Amicable numbers", "Ancient Egyptian multiplication", "Arithmetic number", "BQP", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Betrothed numbers", "Big O notation", "Binary GCD algorithm", "Binary search", "Bit", "Canonical representation of a positive integer", "Carl Pomerance", "Chakravala method", "Cipolla's algorithm", "Co-NP", "Co-NP-complete", "Colossally abundant number", "Complexity class", "Composite number", "Computational complexity theory", "Computer science", "Congruence of squares", "Continued fraction factorization", "Cornacchia's algorithm", "Cryptography", "David Bressoud", "Decision problem", "Deficient number", "Digital object identifier", "Discrete logarithm", "Discriminant of a quadratic form", "Divisor", "Divisor function", "Dixon's algorithm", "Dixon's factorization method", "Donald Knuth", "Elliptic curve", "Elliptic curve method", "Elliptic curve primality", "Elliptic curve primality proving", "Empty product", "Equidigital number", "Erd\u0151s\u2013Nicolas number", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Extravagant number", "FNP (complexity)", "FP (complexity)", "Factorization", "Fast Fourier transform", "Fermat's factorization method", "Fermat primality test", "Friendly number", "Frugal number", "Function field sieve", "Function problem", "Fundamental theorem of arithmetic", "F\u00fcrer's algorithm", "General number field sieve", "Generalized Riemann hypothesis", "Generating primes", "Generating set of a group", "Greatest common divisor", "Group (mathematics)", "Harmonic divisor number", "Hemiperfect number", "Highly abundant number", "Highly composite number", "Hyperperfect number", "Ideal class group", "Index calculus algorithm", "Integer factorization records", "Integer square root", "International Association for Cryptologic Research", "International Standard Book Number", "Karatsuba algorithm", "Kronecker symbol", "L-notation", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "List of unsolved problems in computer science", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Manindra Agrawal", "Mathematical Reviews", "Mathematics", "Maurice Kraitchik", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Multiplicative partition", "Multiply perfect number", "NP-complete", "NP-intermediate", "NP (complexity)", "Nature (journal)", "Number theory", "Opteron", "P (complexity)", "Partition (number theory)", "Perfect number", "Perfect power", "Peter Shor", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Polynomial time", "Powerful number", "Practical number", "Primality test", "Prime decomposition", "Prime decomposition (3-manifold)", "Prime factor", "Prime number", "Primitive abundant number", "Probabilistic algorithm", "Pronic number", "Proth's theorem", "Public-key", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic form", "Quadratic residue", "Quadratic sieve", "Quantum computer", "Quasiperfect number", "RSA-768", "RSA (algorithm)", "RSA number", "RSA problem", "Randomized algorithm", "Rational sieve", "Regular number", "Richard Crandall", "Rough number", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Semiperfect number", "Semiprime", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Smooth number", "Sociable number", "Solovay\u2013Strassen primality test", "Special number field sieve", "Sphenic number", "Square-free integer", "Stan Wagon", "Sublime number", "Superabundant number", "Superior highly composite number", "Superperfect number", "Sylow theorems", "The Art of Computer Programming", "Time complexity", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "UP (complexity)", "Unitary divisor", "Unitary perfect number", "Untouchable number", "Unusual number", "Weird number", "Wheel factorization", "Williams' p + 1 algorithm", "YouTube"], "categories": ["Computational hardness assumptions", "Integer factorization algorithms", "Unsolved problems in computer science"], "title": "Integer factorization"}
{"summary": "Algebraic-group factorisation algorithms are algorithms for factoring an integer N by working in an algebraic group defined modulo N whose group structure is the direct sum of the 'reduced groups' obtained by performing the equations defining the group arithmetic modulo the unknown prime factors p1, p2, ... By the Chinese remainder theorem, arithmetic modulo N corresponds to arithmetic in all the reduced groups simultaneously.\nThe aim is to find an element which is not the identity of the group modulo N, but is the identity modulo one of the factors, so a method for recognising such one-sided identities is required. In general, one finds them by performing operations that move elements around and leave the identities in the reduced groups unchanged. Once the algorithm finds a one-sided identity all future terms will also be one-sided identities, so checking periodically suffices.\nComputation proceeds by picking an arbitrary element x of the group modulo N and computing a large and smooth multiple Ax of it; if the order of at least one but not all of the reduced groups is a divisor of A, this yields a factorisation. It need not be a prime factorisation, as the element might be an identity in more than one of the reduced groups.\nGenerally, A is taken as a product of the primes below some limit K, and Ax is computed by successive multiplication of x by these primes; after each multiplication, or every few multiplications, the check is made for a one-sided identity.", "links": ["Algebraic group", "Binary exponentiation", "Chinese remainder theorem", "Elliptic curve", "Elliptic curve method", "Greatest common divisor", "Hasse's theorem on elliptic curves", "Integer factorization", "Inverse function", "Modular arithmetic", "Multiplicative group", "Pollard's p-1 algorithm", "Quadratic residue", "Smooth number", "Williams' p + 1 algorithm"], "categories": ["All articles lacking sources", "Articles lacking sources from January 2015", "Integer factorization algorithms"], "title": "Algebraic-group factorisation algorithm"}
{"summary": "In number theory, a congruence of squares is a congruence commonly used in integer factorization algorithms.", "links": ["Congruence relation", "Continued fraction factorization", "Dixon's factorization method", "Equation", "Euclidean algorithm", "Factor base", "Fermat's factorization method", "General number field sieve", "Greatest common divisor", "Integer", "Integer factorization", "Number theory", "Quadratic sieve"], "categories": ["All articles lacking sources", "Articles lacking sources from December 2009", "Integer factorization algorithms", "Modular arithmetic"], "title": "Congruence of squares"}
{"summary": "In number theory, the continued fraction factorization method (CFRAC) is an integer factorization algorithm. It is a general-purpose algorithm, meaning that it is suitable for factoring any integer n, not depending on special form or properties. It was described by D. H. Lehmer and R. E. Powers in 1931, and developed as a computer algorithm by Michael A. Morrison and John Brillhart in 1975.\nThe continued fraction method is based on Dixon's factorization method. It uses convergents in the regular continued fraction expansion of\n.\nSince this is a quadratic irrational, the continued fraction must be periodic (unless n is square, in which case the factorization is obvious).\nIt has a time complexity of , in the O and L notations.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Big O notation", "Binary GCD algorithm", "Carl Pomerance", "Chakravala method", "Cipolla's algorithm", "Continued fraction", "Convergent (continued fraction)", "Cornacchia's algorithm", "Derrick Henry Lehmer", "Digital object identifier", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer square root", "JSTOR", "John Brillhart", "Karatsuba algorithm", "L-notation", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Periodic continued fraction", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic irrational", "Quadratic residue", "Quadratic sieve", "R. E. Powers", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["All stub articles", "Integer factorization algorithms", "Number theory stubs"], "title": "Continued fraction factorization"}
{"summary": "In number theory, Dixon's factorization method (also Dixon's random squares method or Dixon's algorithm) is a general-purpose integer factorization algorithm; it is the prototypical factor base method. Unlike for other factor base methods, its run-time bound comes with a rigorous proof that does not rely on conjectures about the smoothness properties of the values taken by polynomial.\nThe algorithm was designed by John D. Dixon, a mathematician at Carleton University, and was published in 1981.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Big-O notation", "Binary GCD algorithm", "Block Lanczos algorithm for nullspace of a matrix over a finite field", "Carleton University", "Chakravala method", "Cipolla's algorithm", "Congruence of squares", "Continued fraction factorization", "Cornacchia's algorithm", "Dickman\u2013de Bruijn function", "Digital object identifier", "Discrete logarithm", "Elliptic curve primality", "Elliptic curve primality proving", "Euclid's algorithm", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Factor base", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "Gaussian elimination", "General number field sieve", "Generating primes", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer square root", "JSTOR", "John D. Dixon", "Karatsuba algorithm", "L-notation", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Linear algebra", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Mathematics of Computation", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "Pseudo-random", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Smooth number", "Solovay\u2013Strassen primality test", "Special number field sieve", "Square number", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Integer factorization algorithms"], "title": "Dixon's factorization method"}
{"summary": "Euler's factorization method is a technique for factoring a number by writing it as a sum of two squares in two different ways. For example the number  can be written as  or as  and Euler's method gives the factorization .\nThe idea that two distinct representations of an odd positive integer may lead to a factorization was apparently first proposed by Marin Mersenne. However, it was not put to use extensively until Euler one hundred years later. His most celebrated use of the method that now bears his name was to factor the number , which apparently was previously thought to be prime even though it is not a pseudoprime by any major primality test.\nEuler's factorization method is more effective than Fermat's for integers whose factors are not close together and potentially much more efficient than trial division if one can find representations of numbers as sums of two squares reasonably easily. Euler's development ultimately permitted much more efficient factoring of numbers and, by the 1910s, the development of large factor tables going up to about ten million. The methods used to find representations of numbers as sums of two squares are essentially the same as with finding differences of squares in Fermat's factorization method.\nThe great disadvantage of Euler's factorization method is that it cannot be applied to factoring an integer with any prime factor of the form 4k + 3 occurring to an odd power in its prime factorization, as such a number can never be the sum of two squares. Even odd composite numbers of the form 4k + 1 are often the product of two primes of the form 4k + 3 (e.g. 3053 = 43 \u00d7 71) and again cannot be factored by Euler's method.\nThis restricted applicability has made Euler's factorization method disfavoured for computer factoring algorithms, since any user attempting to factor a random integer is unlikely to know whether Euler's method can actually be applied to the integer in question. It is only relatively recently that there have been attempts to develop Euler's method into computer algorithms for use on specialised numbers where it is known Euler's method can be applied.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Brahmagupta\u2013Fibonacci identity", "Chakravala method", "Cipolla's algorithm", "Composite number", "Computer", "Continued fraction factorization", "Cornacchia's algorithm", "Digital object identifier", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Marin Mersenne", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "Pseudoprime", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from July 2013", "Integer factorization algorithms"], "title": "Euler's factorization method"}
{"summary": "In computational number theory, a factor base is a small set of prime numbers commonly used as a mathematical tool in algorithms involving extensive sieving for potential factors of a given integer.", "links": ["Block Lanczos algorithm for nullspace of a matrix over a finite field", "Computational number theory", "Dixon's factorization method", "Gaussian elimination", "General number field sieve", "Index calculus algorithm", "International Standard Book Number", "Matrix (mathematics)", "Number theory", "Prime number", "Quadratic sieve", "Set theory", "Sieve theory", "System of linear equations", "Vector space"], "categories": ["All stub articles", "Integer factorization algorithms", "Number theory stubs"], "title": "Factor base"}
{"summary": "In number theory, the general number field sieve (GNFS) is the most efficient classical algorithm known for factoring integers larger than 100 digits. Heuristically, its complexity for factoring an integer n (consisting of  bits) is of the form\n\n(in L-notation), where ln is the natural logarithm. It is a generalization of the special number field sieve: while the latter can only factor numbers of a certain special form, the general number field sieve can factor any number apart from prime powers (which are trivial to factor by taking roots). When the term number field sieve (NFS) is used without qualification, it refers to the general number field sieve.\nThe principle of the number field sieve (both special and general) can be understood as an improvement to the simpler rational sieve or quadratic sieve. When using such algorithms to factor a large number n, it is necessary to search for smooth numbers (i.e. numbers with small prime factors) of order n1/2. The size of these values is exponential in the size of n (see below). The general number field sieve, on the other hand, manages to search for smooth numbers that are subexponential in the size of n. Since these numbers are smaller, they are more likely to be smooth than the numbers inspected in previous algorithms. This is the key to the efficiency of the number field sieve. In order to achieve this speed-up, the number field sieve has to perform computations and factorizations in number fields. This results in many rather complicated aspects of the algorithm, as compared to the simpler rational sieve.\nNote that log2 n is the number of bits in the binary representation of n, that is the size of the input to the algorithm, so any element of the order nc for a constant c is exponential in log n. The running time of the number field sieve is super-polynomial but sub-exponential in the size of the input.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algebraic number", "Algebraic number field", "Algorithm", "Algorithmic efficiency", "Ancient Egyptian multiplication", "Arjen Lenstra", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Block Lanczos algorithm for nullspace of a matrix over a finite field", "Block Wiedemann algorithm", "Carl Pomerance", "Centrum Wiskunde & Informatica", "Chakravala method", "Cipolla's algorithm", "Computational complexity theory", "Continued fraction factorization", "Cornacchia's algorithm", "Cunningham project", "Degree of a polynomial", "Digital object identifier", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Field norm", "Function field sieve", "F\u00fcrer's algorithm", "GPL", "Gaussian elimination", "Generating primes", "Greatest common divisor", "Hendrik Lenstra", "Heuristic", "Homomorphism", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Internet", "Irreducible polynomial", "Jason Papadopoulos", "Karatsuba algorithm", "L-notation", "Lattice sieving", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Multiplication algorithm", "Natural logarithm", "Number field", "Number theory", "Paul Leyland", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Polynomial", "Primality test", "Prime power", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Radix", "Rational number", "Rational sieve", "Ring of integers", "Root of a function", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Smooth number", "Solovay\u2013Strassen primality test", "Special number field sieve", "Thorsten Kleinjung", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "United Kingdom", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Integer factorization algorithms"], "title": "General number field sieve"}
{"summary": "Lattice sieving is a technique for finding smooth values of a bivariate polynomial  over a large region. It is almost exclusively used in conjunction with the number field sieve. The original idea of the lattice sieve came from John Pollard.\nThe algorithm implicitly involves the ideal structure of the number field of the polynomial; it takes advantage of the theorem that any prime ideal above some rational prime p can be written as . One then picks many prime numbers q of an appropriate size, usually just above the factor base limit, and proceeds by\n\nFor each q, list the prime ideals above q by factorising the polynomial f(a,b) over For each of these prime ideals, which are called 'special 's, construct a reduced basis  for the lattice L generated by ; set a two-dimensional array called the sieve region to zero.\nFor each prime ideal  in the factor base, construct a reduced basis  for the sublattice of L generated byFor each element of that sublattice lying within a sufficiently large sieve region, add  to that entry.\n\nRead out all the entries in the sieve region with a large enough value\n\nFor the number field sieve application, it is necessary for two polynomials both to have smooth values; this is handled by running the inner loop over both polynomials, whilst the special-q can be taken from either side.", "links": ["Factor base", "Ideal (ring theory)", "John Pollard (mathematician)", "Lattice reduction", "Number field", "Number field sieve", "Prime ideal", "Sieve region", "Smooth number"], "categories": ["Integer factorization algorithms"], "title": "Lattice sieving"}
{"summary": "The Lenstra elliptic curve factorization or the elliptic curve factorization method (ECM) is a fast, sub-exponential running time algorithm for integer factorization which employs elliptic curves. For general purpose factoring, ECM is the third-fastest known factoring method. The second fastest is the multiple polynomial quadratic sieve and the fastest is the general number field sieve. The Lenstra elliptic curve factorization is named after Hendrik Lenstra.\nPractically speaking, ECM is considered a special purpose factoring algorithm as it is most suitable for finding small factors. Currently, it is still the best algorithm for divisors not greatly exceeding 20 to 25 digits (64 to 83 bits or so), as its running time is dominated by the size of the smallest factor p rather than by the size of the number n to be factored. Frequently, ECM is used to remove small factors from a very large integer with many factors; if the remaining integer is still composite, then it has only large factors and is factored using general purpose techniques. The largest factor found using ECM so far has 83 digits and was discovered on 7 September 2013 by R. Propper. Increasing the number of curves tested improves the chances of finding a factor, but they are not linear with the increase in the number of digits.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Annals of Mathematics", "Arjen Lenstra", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Big O notation", "Binary GCD algorithm", "Binary digit", "Canfield\u2013Erd\u0151s\u2013Pomerance theorem", "Carl Pomerance", "Chakravala method", "Cipolla's algorithm", "Continued fraction factorization", "Cornacchia's algorithm", "Decimal", "Digital object identifier", "Discrete logarithm", "Divisor", "Dixon's factorization method", "Edwards curve", "Elliptic curve", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Exponential running time", "Extended Euclidean algorithm", "Factorial", "Fermat's factorization method", "Fermat's little theorem", "Fermat primality test", "Finite field", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "General purpose computer", "Generating primes", "Greatest common divisor", "Group (mathematics)", "Hasse's theorem on elliptic curves", "Hendrik Lenstra", "Heuristic", "Hyperelliptic curve", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Karatsuba algorithm", "L-notation", "Lagrange's theorem (group theory)", "Lehmer's GCD algorithm", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Linear", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Mathematical Reviews", "Mathematics of Computation", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Modular inverse", "Montgomery curve", "Multiplication algorithm", "Multiplicative group", "Notices of the American Mathematical Society", "Number theory", "OCLC", "PDF", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Point (geometry)", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Projective space", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Relatively prime", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Smooth number", "Solovay\u2013Strassen primality test", "Special number field sieve", "Strong prime", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Torsion group", "Trial division", "UBASIC", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["All articles containing potentially dated statements", "Articles containing potentially dated statements from 2006", "Finite fields", "Integer factorization algorithms"], "title": "Lenstra elliptic curve factorization"}
{"summary": "Pollard's p \u2212 1 algorithm is a number theoretic integer factorization algorithm, invented by John Pollard in 1974. It is a special-purpose algorithm, meaning that it is only suitable for integers with specific types of factors; it is the simplest example of an algebraic-group factorisation algorithm.\nThe factors it finds are ones for which the number preceding the factor, p \u2212 1, is powersmooth; the essential observation is that, by working in the multiplicative group modulo a composite number N, we are also working in the multiplicative groups modulo all of N's factors.\nThe existence of this algorithm leads to the concept of safe primes, being primes for which p \u2212 1 is two times a Sophie Germain prime q and thus minimally smooth. These primes are sometimes construed as \"safe for cryptographic purposes\", but they might be unsafe \u2014 in current recommendations for cryptographic strong primes (e.g. ANSI X9.31), it is necessary but not sufficient that p \u2212 1 has at least one large prime factor. Most sufficiently large primes are strong; if a prime used for cryptographic purposes turns out to be non-strong, it is much more likely to be through malice than through an accident of random number generation. This terminology is considered obsolete by the cryptography industry. [1]", "links": ["AKS primality test", "ANSI X9.31", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algebraic-group factorisation algorithm", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Chakravala method", "Cipolla's algorithm", "Continued fraction factorization", "Cornacchia's algorithm", "Digital object identifier", "Discrete logarithm", "Dixon's factorization method", "Dixon's theorem", "Elliptic curve method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat's little theorem", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Great Internet Mersenne Prime Search", "Greatest common divisor", "Index calculus algorithm", "Integer", "Integer factorization", "Integer square root", "John Pollard (mathematician)", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "MPrime", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Multiplication algorithm", "Natural logarithm", "Necessary but not sufficient", "Number theory", "Obsolete", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Prime95", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Random number generation", "Rational sieve", "Safe prime", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Smooth number", "Solovay\u2013Strassen primality test", "Sophie Germain prime", "Special number field sieve", "Strong prime", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Integer factorization algorithms"], "title": "Pollard's p \u2212 1 algorithm"}
{"summary": "Pollard's rho algorithm is a special-purpose integer factorization algorithm. It was invented by John Pollard in 1975. It is particularly effective for a composite number having a small prime factor.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Birthday paradox", "Birthday problem", "Chakravala method", "Charles E. Leiserson", "Cipolla's algorithm", "Clifford Stein", "Composite number", "Continued fraction factorization", "Cornacchia's algorithm", "Cycle detection", "Digital object identifier", "Directed graph", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Eric W. Weisstein", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat number", "Fermat primality test", "Floyd's cycle-finding algorithm", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Introduction to Algorithms", "John Pollard (mathematician)", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "MathWorld", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "Pseudo-random sequence", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Richard Brent (scientist)", "Ronald L. Rivest", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Thomas H. Cormen", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "UNIVAC", "UNIVAC 1110", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Integer factorization algorithms"], "title": "Pollard's rho algorithm"}
{"summary": "The quadratic sieve algorithm (QS) is an integer factorization algorithm and, in practice, the second fastest method known (after the general number field sieve). It is still the fastest for integers under 100 decimal digits or so, and is considerably simpler than the number field sieve. It is a general-purpose factorization algorithm, meaning that its running time depends solely on the size of the integer to be factored, and not on special structure or properties. It was invented by Carl Pomerance in 1981 as an improvement to Schroeppel's linear sieve.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Aurifeuillian factorization", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Bellcore", "Binary GCD algorithm", "Bitset", "Bitwise operation", "Block Wiedemann algorithm", "Carl Pomerance", "Central processing unit", "Chakravala method", "Cipolla's algorithm", "Congruence of squares", "Continued fraction factorization", "Cornacchia's algorithm", "Discrete logarithm", "Division (mathematics)", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "Fundamental theorem of arithmetic", "F\u00fcrer's algorithm", "Gaussian elimination", "General number field sieve", "Generating primes", "Gigabyte", "Greatest common divisor", "Index calculus algorithm", "Integer", "Integer factorization", "Integer square root", "International Standard Book Number", "Karatsuba algorithm", "L-notation", "Left null space", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Linear algebra", "Linear dependency", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "MIPS-year", "Magma computer algebra system", "MasPar", "Matrix (mathematics)", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Multi-precision", "Multiplication algorithm", "Number theory", "PARI/GP", "Parallel algorithm", "Parallel computing", "Parity (mathematics)", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Pollard rho", "Polynomial", "Primality test", "Prime number", "Processors", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "RSA-129", "RSA-130", "RSA number", "Rational sieve", "Real number", "Richard Crandall", "SQUFOF", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Semiprime", "Shanks' square forms factorization", "Shanks\u2013Tonelli algorithm", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Sieve theory", "Single-precision", "Smooth number", "Software for Algebra and Geometry Experimentation", "Solovay\u2013Strassen primality test", "Special number field sieve", "Square number", "Telcordia Technologies", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Vector (mathematics)", "Wheel factorization", "Williams' p + 1 algorithm", "Zero vector"], "categories": ["Integer factorization algorithms"], "title": "Quadratic sieve"}
{"summary": "The quantum algorithm for linear systems of equations, designed by Aram Harrow, Avinatan Hassidim, and Seth Lloyd, is a quantum algorithm for solving linear systems formulated in 2009. The algorithm estimates the result of a scalar measurement on the solution vector to a given linear system of equations.\nThe algorithm is one of the main fundamental algorithms expected to provide an exponential speedup over their classical counterparts, along with Shor's factoring algorithm, Grover's search algorithm and quantum simulation. Provided the linear system is a sparse and has a low condition number , and that the user is interested in the result of a scalar measurement on the solution vector, instead of the values of the solution vector itself, then the algorithm has a runtime of , where  is the number of variables in the linear system.. This offers an exponential speedup over the fastest classical algorithm, which runs in  (or  for positive semidefinite matrices).\nAn implementation of the quantum algorithm for linear systems of equations was first demonstrated in 2013 by Cai et al., Barz et al.and Pan et al. in parallel. The demonstrations consisted of simple linear equations on specially designed quantum devices.\nDue to the prevalence of linear systems in virtually all areas of science and engineering, the quantum algorithm for linear systems of equations has the potential for widespread applicability.", "links": ["Adiabatic quantum computation", "Algorithmic cooling", "Amplitude amplification", "BQP", "Big data", "Cavity quantum electrodynamics", "Charge qubit", "Circuit quantum electrodynamics", "Classical capacity", "Cluster state", "Condition number", "Conjugate Gradient method", "Conjugate gradient method", "Deutsch\u2013Jozsa algorithm", "EQP (complexity)", "Entanglement-assisted classical capacity", "Entanglement-assisted stabilizer formalism", "Entanglement distillation", "Flux qubit", "Gaussian elimination", "Grover's algorithm", "Hermitian matrix", "Kane quantum computer", "LOCC", "Least-squares fit", "Linear optical quantum computing", "Loss\u2013DiVincenzo quantum computer", "Machine learning", "Nitrogen-vacancy center", "Nuclear magnetic resonance quantum computer", "One-way quantum computer", "Optical lattice", "PSPACE", "Phase estimation", "Phase qubit", "PostBQP", "QMA", "Quantum Fourier transform", "Quantum Turing machine", "Quantum algorithm", "Quantum annealing", "Quantum capacity", "Quantum channel", "Quantum circuit", "Quantum complexity theory", "Quantum computer", "Quantum convolutional code", "Quantum cryptography", "Quantum decoherence", "Quantum energy teleportation", "Quantum error correction", "Quantum gate", "Quantum information", "Quantum information science", "Quantum key distribution", "Quantum machine learning", "Quantum network", "Quantum optics", "Quantum phase estimation", "Quantum phase estimation algorithm", "Quantum programming", "Quantum teleportation", "Qubit", "Seth Lloyd", "Shor's Algorithm", "Shor's algorithm", "Simon's problem", "Sparse matrix", "Spin (physics)", "Stabilizer code", "Superconducting quantum computing", "Superdense coding", "System of linear equations", "Timeline of quantum computing", "Topological quantum computer", "Trapped ion quantum computer", "Ultracold atom", "Unitary operator", "Universal quantum simulator"], "categories": ["Articles containing proofs", "Integer factorization algorithms", "Pages with duplicate reference names", "Pages with reference errors", "Quantum algorithms", "Quantum information science"], "title": "Quantum algorithm for linear systems of equations"}
{"summary": "In mathematics, the rational sieve is a general algorithm for factoring integers into prime factors. It is essentially a special case of the general number field sieve, and while it is far less efficient than the general algorithm, it is conceptually far simpler. So while it is rather useless as a practical factoring algorithm, it is a helpful first step for those trying to understand how the general number field sieve works.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Algorithmic efficiency", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Chakravala method", "Cipolla's algorithm", "Composite number", "Congruence of squares", "Continued fraction factorization", "Coprime", "Cornacchia's algorithm", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Factor base", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer square root", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Linear algebra", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Mathematics", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Modular multiplicative inverse", "Multiplication algorithm", "Newton's method", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Smooth number", "Solovay\u2013Strassen primality test", "Special number field sieve", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Integer factorization algorithms"], "title": "Rational sieve"}
{"summary": "The RSA Factoring Challenge was a challenge put forward by RSA Laboratories on March 18, 1991 to encourage research into computational number theory and the practical difficulty of factoring large integers and cracking RSA keys used in cryptography. They published a list of semiprimes (numbers with exactly two prime factors) known as the RSA numbers, with a cash prize for the successful factorization of some of them. The smallest of them, a 100 decimal digit number called RSA-100 was factored by April 1, 1991, but many of the bigger numbers have still not been factored and are expected to remain unfactored for quite some time, however advances in quantum computers make this prediction uncertain due to Shor's algorithm.\nThe RSA challenges ended in 2007. RSA Laboratories stated: \"Now that the industry has a considerably more advanced understanding of the cryptanalytic strength of common symmetric-key and public-key algorithms, these challenges are no longer active.\"\nThe factoring challenge was intended to track the cutting edge in integer factorization. A primary application is for choosing the key length of the RSA public-key encryption scheme. Progress in this challenge should give an insight into which key sizes are still safe and for how long. As RSA Laboratories is a provider of RSA-based products, the challenge was used by them as an incentive for the academic community to attack the core of their solutions \u2014 in order to prove its strength.\nThe RSA numbers were generated on a computer with no network connection of any kind. The computer's hard drive was subsequently destroyed so that no record would exist, anywhere, of the solution to the factoring challenge.\nThe first RSA numbers generated, RSA-100 to RSA-500 and RSA-617, were labeled according to their number of decimal digits; the other RSA numbers (beginning with RSA-576) were generated later and labelled according to their number of binary digits.", "links": ["Arjen K. Lenstra", "Base 10", "Base 2", "Binary numeral system", "Computational number theory", "Cryptography", "Decimal", "Factorization", "Herman te Riele", "Integer", "Integer factorization records", "Jens Franke", "Kazumaro Aoki", "Key length", "Key size", "M.S. Manasse", "Moscow State University", "Paul Zimmermann", "Prime factor", "Prime number", "Public-key encryption", "Public key algorithm", "Quantum computer", "RSA-100", "RSA-1024", "RSA-110", "RSA-120", "RSA-129", "RSA-130", "RSA-140", "RSA-150", "RSA-1536", "RSA-155", "RSA-160", "RSA-170", "RSA-180", "RSA-190", "RSA-200", "RSA-2048", "RSA-210", "RSA-220", "RSA-230", "RSA-232", "RSA-240", "RSA-250", "RSA-260", "RSA-270", "RSA-280", "RSA-290", "RSA-300", "RSA-309", "RSA-310", "RSA-320", "RSA-330", "RSA-340", "RSA-350", "RSA-360", "RSA-370", "RSA-380", "RSA-390", "RSA-400", "RSA-410", "RSA-420", "RSA-430", "RSA-440", "RSA-450", "RSA-460", "RSA-470", "RSA-480", "RSA-490", "RSA-500", "RSA-576", "RSA-617", "RSA-640", "RSA-704", "RSA-768", "RSA-896", "RSA (algorithm)", "RSA Laboratories", "RSA Secret-Key Challenge", "RSA numbers", "Scientific American", "Semiprime", "Shor's algorithm", "Symmetric key algorithm", "T. Denny", "The Magic Words are Squeamish Ossifrage", "Thorsten Kleinjung", "USD", "University of Bonn"], "categories": ["1991 establishments in the United States", "2007 disestablishments", "Cryptography contests", "Integer factorization algorithms", "RSA Factoring Challenge"], "title": "RSA Factoring Challenge"}
{"summary": "In mathematics, the RSA numbers are a set of large semiprimes (numbers with exactly two prime factors) that are part of the RSA Factoring Challenge. The challenge was to find the prime factors but it was declared inactive in 2007. It was created by RSA Laboratories in March 1991 to encourage research into computational number theory and the practical difficulty of factoring large integers.\nRSA Laboratories published a number of semiprimes with 100 to 617 decimal digits. Cash prizes of varying size were offered for factorization of some of them. The smallest RSA number was factored in a few days. Most of the numbers have still not been factored and many of them are expected to remain unfactored for many years to come. As of September 2013, 18 of the 54 listed numbers have been factored: the 17 smallest from RSA-100 to RSA-704, plus RSA-768.\nThe RSA challenge officially ended in 2007 but people are still attempting to find the factorizations. According to RSA Laboratories, \"Now that the industry has a considerably more advanced understanding of the cryptanalytic strength of common symmetric-key and public-key algorithms, these challenges are no longer active.\" Some of the smaller prizes had been awarded at the time. The remaining prizes were retracted.\nThe first RSA numbers generated, from RSA-100 to RSA-500, were labeled according to their number of decimal digits. Later, beginning with RSA-576, binary digits are counted instead. An exception to this is RSA-617, which was created before the change in the numbering scheme. The numbers are listed in increasing order below.", "links": ["Advanced Micro Devices", "Alec Muffett", "Arjen K. Lenstra", "Arjen Lenstra", "Athlon 64", "Binary numeral system", "Brandon Dixon", "Brian Murphy (mathematician)", "Bruce Dodson", "Burt Kaliski", "Central processing unit", "Centrum Wiskunde & Informatica", "Chris Putnam", "Computational number theory", "Computational power", "Damian Weber", "Decimal", "Derek Atkins", "Digital object identifier", "Eric W. Weisstein", "Fachhochschule Braunschweig/Wolfenb\u00fcttel", "Federal Office for Information Security", "Fran\u00e7ois Morain", "Free Software Foundation", "GHz", "General number field sieve", "Gerard Guillerm", "Germany", "Herman te Riele", "Integer", "Integer factorization", "Integer factorization records", "Internet", "Internet Archive", "Jeff Gilchrist", "Jens Franke", "Jim Cowie", "Joel Marchand", "Joerg Zayer", "Karen Aardal", "MIPS-year", "Marije Elkenbracht-Huizing", "Martin Gardner", "MasPar", "MathWorld", "Mathematics", "Michael Graff", "Moscow State University", "Multiple Polynomial Quadratic Sieve", "Number Field Sieve", "Opteron", "Paul Leyland", "Paul Zimmermann", "Peter Montgomery (mathematician)", "Polynomial", "Prime factor", "Quadratic sieve", "RSA Factoring Challenge", "RSA Laboratories", "RSA Secret-Key Challenge", "RSA Security", "Sci.crypt", "Scientific American", "Semiprimes", "Stefania Cavallar", "Steven Levy", "The Magic Words are Squeamish Ossifrage", "Thorsten Kleinjung", "United States dollar", "University of Bonn", "Walter Lioen", "Wired News", "Wojtek Furmanski"], "categories": ["All articles containing potentially dated statements", "Articles containing potentially dated statements from September 2013", "Integer factorization algorithms", "Large integers", "RSA Factoring Challenge"], "title": "RSA numbers"}
{"summary": "Shanks's square forms factorization is a method for integer factorization devised by Daniel Shanks as an improvement on Fermat's factorization method.\nThe success of Fermat's method depends on finding integers  and  such that , where  is the integer to be factored. An improvement (noticed by Kraitchik) is to look for integers  and  such that . Finding a suitable pair  does not guarantee a factorization of , but it implies that  is a factor of , and there is a good chance that the prime divisors of  are distributed between these two factors, so that calculation of the greatest common divisor of  and  will give a non-trivial factor of .\nA practical algorithm for finding pairs  which satisfy  was developed by Shanks, who named it Square Forms Factorization or SQUFOF. The algorithm can be expressed in terms of continued fractions, or in terms of quadratic forms. Although there are now much more efficient factorization methods available, SQUFOF has the advantage that it is small enough to be implemented on a programmable calculator.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Chakravala method", "Cipolla's algorithm", "Continued fraction factorization", "Cornacchia's algorithm", "Daniel Shanks", "David Bressoud", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Hans Riesel", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Maurice Kraitchik", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Prime divisor", "Prime number", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Square number", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["All articles lacking in-text citations", "All articles needing expert attention", "Articles lacking in-text citations from March 2015", "Articles needing expert attention from November 2008", "Articles needing expert attention with no reason or talk parameter", "Integer factorization algorithms", "Mathematics articles needing expert attention"], "title": "Shanks' square forms factorization"}
{"summary": "Shor's algorithm, named after mathematician Peter Shor, is a quantum algorithm (an algorithm that runs on a quantum computer) for integer factorization formulated in 1994. Informally it solves the following problem: given an integer N, find its prime factors.\nOn a quantum computer, to factor an integer N, Shor's algorithm runs in polynomial time (the time taken is polynomial in log N, which is the size of the input). Specifically it takes quantum gates of order O((log N)2(log log N)(log log log N)) using fast multiplication, demonstrating that the integer factorization problem can be efficiently solved on a quantum computer and is thus in the complexity class BQP. This is substantially faster than the most efficient known classical factoring algorithm, the general number field sieve, which works in sub-exponential time \u2014 about O(e1.9 (log N)1/3 (log log N)2/3). The efficiency of Shor's algorithm is due to the efficiency of the quantum Fourier transform, and modular exponentiation by repeated squarings.\nIf a quantum computer with a sufficient number of qubits could operate without succumbing to noise and other quantum decoherence phenomena, Shor's algorithm could be used to break public-key cryptography schemes such as the widely used RSA scheme. RSA is based on the assumption that factoring large numbers is computationally intractable. So far as is known, this assumption is valid for classical (non-quantum) computers; no classical algorithm is known that can factor in polynomial time. However, Shor's algorithm shows that factoring is efficient on an ideal quantum computer, so it may be feasible to defeat RSA by constructing a large quantum computer. It was also a powerful motivator for the design and construction of quantum computers and for the study of new quantum computer algorithms. It has also facilitated research on new cryptosystems that are secure from quantum computers, collectively called post-quantum cryptography.\nIn 2001, Shor's algorithm was demonstrated by a group at IBM, who factored 15 into 3 \u00d7 5, using an NMR implementation of a quantum computer with 7 qubits. After IBM's implementation, two independent groups, one at the University of Science and Technology of China, and the other one at the University of Queensland, have implemented Shor's algorithm using photonic qubits, emphasizing that multi-qubit entanglement was observed when running the Shor's algorithm circuits. In 2012, the factorization of 15 was repeated. Also in 2012, the factorization of 21 was achieved, setting the record for the largest number factored with a quantum computer. In April 2012, the factorization of 143 was achieved, although this used adiabatic quantum computation rather than Shor's algorithm. It was discovered in November 2014 that this adiabatic quantum computation in 2012 had in fact also factored larger numbers, the largest being 56153, which is currently the record for the largest integer factored on a quantum device.", "links": ["AKS primality test", "Adiabatic quantum computation", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Algorithmic cooling", "Ancient Egyptian multiplication", "ArXiv", "BQP", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Bibcode", "Big O notation", "Binary GCD algorithm", "B\u00e9zout's identity", "Cavity quantum electrodynamics", "Chakravala method", "Charge qubit", "Chinese remainder theorem", "Cipolla's algorithm", "Circuit quantum electrodynamics", "Classical capacity", "Cluster state", "Complexity class", "Composite number", "Continued fraction", "Continued fraction factorization", "Coprime", "Cornacchia's algorithm", "Deutsch\u2013Jozsa algorithm", "Digital object identifier", "Discrete logarithm", "Divides", "Dixon's factorization method", "EQP (complexity)", "Elliptic curve primality", "Elliptic curve primality proving", "Entanglement-assisted classical capacity", "Entanglement-assisted stabilizer formalism", "Entanglement distillation", "Euclidean algorithm", "Euler's factorization method", "Euler's totient function", "Exponentiating by squaring", "Exponentiation by squaring", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Flux qubit", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Group (mathematics)", "Group homomorphism", "Grover's algorithm", "Hadamard transform", "Hidden subgroup problem", "Imaginary unit", "Index calculus algorithm", "Integer factorization", "Integer square root", "Interference (wave propagation)", "Irreducible fraction", "Isaac Chuang", "Kane quantum computer", "Karatsuba algorithm", "LOCC", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Linear optical quantum computing", "Long multiplication", "Loss\u2013DiVincenzo quantum computer", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Measurement in quantum mechanics", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Modulo operation", "Multiplication algorithm", "Multiplicative group of integers modulo n", "Nature (journal)", "Nicholas Rush", "Nitrogen-vacancy center", "No cloning theorem", "Noise", "Nontrivial", "Nuclear magnetic resonance (NMR) quantum computing", "Nuclear magnetic resonance quantum computer", "Number theory", "One-way quantum computer", "Optical lattice", "Order (group theory)", "Periodic function", "Peter Shor", "Phase qubit", "Physical Review Letters", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Polynomial", "Polynomial time", "Positive real axis", "Post-quantum cryptography", "PostBQP", "Primality test", "Primality testing", "Prime factor", "Proth's theorem", "Pseudo-polynomial time", "PubMed Identifier", "Public-key cryptography", "P\u00e9pin's test", "QMA", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Quantum Fourier transform", "Quantum Turing machine", "Quantum algorithm", "Quantum annealing", "Quantum capacity", "Quantum channel", "Quantum circuit", "Quantum cloning", "Quantum complexity theory", "Quantum computer", "Quantum convolutional code", "Quantum cryptography", "Quantum decoherence", "Quantum energy teleportation", "Quantum error correction", "Quantum gate", "Quantum information", "Quantum information science", "Quantum key distribution", "Quantum network", "Quantum optics", "Quantum phase estimation algorithm", "Quantum programming", "Quantum superposition", "Quantum teleportation", "Qubit", "Qubits", "RSA (algorithm)", "Rational sieve", "Reversible computing", "Root of unity", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Scott Aaronson", "Shanks' square forms factorization", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Simon's problem", "Solovay\u2013Strassen primality test", "Special number field sieve", "Spin (physics)", "Stabilizer code", "Stargate Universe", "Sub-exponential time", "Superconducting quantum computing", "Superdense coding", "The Bat Jar Conjecture", "The Big Bang Theory", "Timeline of quantum computing", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Topological quantum computer", "Trapped ion quantum computer", "Trial division", "Ultracold atom", "Universal quantum simulator", "University of California, Berkeley", "University of Queensland", "University of Science and Technology of China", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["All articles lacking in-text citations", "All articles needing expert attention", "All articles that are too technical", "Articles containing proofs", "Articles lacking in-text citations from September 2010", "Articles needing expert attention from February 2014", "Integer factorization algorithms", "Post-quantum cryptography", "Quantum algorithms", "Quantum information science", "Wikipedia articles that are too technical from February 2014"], "title": "Shor's algorithm"}
{"summary": "In number theory, a branch of mathematics, the special number field sieve (SNFS) is a special-purpose integer factorization algorithm. The general number field sieve (GNFS) was derived from it.\nThe special number field sieve is efficient for integers of the form re \u00b1 s, where r and s are small (for instance Mersenne numbers).\nHeuristically, its complexity for factoring an integer  is of the form:\n\nin O and L-notations.\nThe SNFS has been used extensively by NFSNet (a volunteer distributed computing effort), NFS@Home and others to factorise numbers of the Cunningham project; for some time the records for integer factorisation have been numbers factored by SNFS.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algebraic number field", "Algorithm", "Algorithmic efficiency", "Ancient Egyptian multiplication", "Arjen Lenstra", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Big O notation", "Binary GCD algorithm", "Carl Pomerance", "Chakravala method", "Cipolla's algorithm", "Computational complexity theory", "Continued fraction factorization", "Cornacchia's algorithm", "Cunningham project", "Digital object identifier", "Discrete logarithm", "Distributed computing", "Dixon's factorization method", "Elliptic curve method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Fibonacci number", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Hendrik Lenstra", "Heuristic", "Index calculus algorithm", "Integer", "Integer factorization", "Integer factorization records", "Integer square root", "International Standard Book Number", "Irreducible polynomial", "Karatsuba algorithm", "L-notation", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Linear algebra", "Long multiplication", "Lucas number", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "MIT", "Mathematics", "Mersenne number", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Relatively prime", "Ring (mathematics)", "Ring homomorphism", "Root of a function", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Smooth number", "Solovay\u2013Strassen primality test", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Unique factorization domain", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Integer factorization algorithms"], "title": "Special number field sieve"}
{"summary": "Trial division is the most laborious but easiest to understand of the integer factorization algorithms. The essential idea behind trial division tests to see if an integer n, the integer to be factored, can be divided by each number in turn that is less than n. For example, for the integer n = 12, the only numbers that divide it are 1,2,3,4,6,12. Selecting only the largest powers of primes in this list gives that 12 = 3 \u00d7 4.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Carl Pomerance", "Chakravala method", "Cipolla's algorithm", "Composite number", "Continued fraction factorization", "Cornacchia's algorithm", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Grid computing", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Judges of the International Criminal Court", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Primality testing", "Prime-counting function", "Prime number", "Proth's theorem", "Public key cryptography", "Python (programming language)", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "RSA-768", "Rational sieve", "Richard Crandall", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Springer-Verlag", "Square number", "Supercomputer", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Undergraduate Texts in Mathematics", "Wheel factorization", "Williams' p + 1 algorithm", "Worst case", "Zentralblatt MATH"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from March 2014", "Articles with example Python code", "Division (mathematics)", "Integer factorization algorithms"], "title": "Trial division"}
{"summary": "In computational number theory, Williams' p + 1 algorithm is an integer factorization algorithm, one of the family of algebraic-group factorisation algorithms. It was invented by Hugh C. Williams in 1982.\nIt works well if the number N to be factored contains one or more prime factors p such that\np + 1\nis smooth, i.e. p + 1 contains only small factors. It uses Lucas sequences to perform exponentiation in a quadratic field.\nIt is analogous to Pollard's p \u2212 1 algorithm.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algebraic-group factorisation algorithms", "Algorithm", "American Mathematical Society", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Chakravala method", "Cipolla's algorithm", "Computational number theory", "Continued fraction factorization", "Cornacchia's algorithm", "Cyclotomic polynomial", "Digital object identifier", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Hugh C. Williams", "Index calculus algorithm", "Integer factorization", "Integer square root", "JSTOR", "Jacobi symbol", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas sequence", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Mathematical Reviews", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p - 1 algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic field", "Quadratic non-residue", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Smooth number", "Solovay\u2013Strassen primality test", "Special number field sieve", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization"], "categories": ["Integer factorization algorithms"], "title": "Williams' p + 1 algorithm"}
{"summary": "In computational number theory, the Adleman\u2013Pomerance\u2013Rumely primality test is an algorithm for determining whether a number is prime. Unlike other, more efficient algorithms for this purpose, it avoids the use of random numbers, so it is a deterministic primality test. It is named after its discoverers, Leonard Adleman, Carl Pomerance, and Robert Rumely. The test involves arithmetic in cyclotomic fields.\nIt was later improved by Henri Cohen and Hendrik Willem Lenstra, commonly referred to as APR-CL. It can test primality of an integer n in time:", "links": ["AKS primality test", "Algorithm", "Ancient Egyptian multiplication", "Annals of Mathematics", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Carl Pomerance", "Chakravala method", "Cipolla's algorithm", "Computational number theory", "Continued fraction factorization", "Cornacchia's algorithm", "Cyclotomic field", "Data structure", "Deterministic algorithm", "Digital object identifier", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Hans Riesel", "Hendrik Lenstra", "Hendrik Willem Lenstra", "Henri Cohen (number theorist)", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Leonard Adleman", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Mathematics of Computation", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Prime number", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Robert Rumely", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "UBASIC", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Number theory stubs", "Primality tests"], "title": "Adleman\u2013Pomerance\u2013Rumely primality test"}
{"summary": "BATON is a Type 1 block cipher in use since at least 1995 by the United States government to secure classified information.\nWhile the BATON algorithm itself is secret (as is the case with all algorithms in the NSA's Suite A), the public PKCS#11 standard includes some general information about how it is used. It has a 320-bit key and uses a 128-bit block in most modes, and also supports a 96-bit electronic codebook mode. 160 bits of the key are checksum material. It supports a \"shuffle\" mode of operation, like the NSA cipher JUNIPER. It may use up to 192 bits as an initialization vector, regardless of the block size.\nIn response to a Senate question about encrypted video links, NSA said that BATON could be used for encryption at speeds higher than those possible with Skipjack.", "links": ["\"shuffle\" mode", "3-Way", "3-subset meet-in-the-middle attack", "802.11b", "APCO Project 25", "ARIA (cipher)", "Advanced Encryption Standard", "Advanced Encryption Standard process", "Advanced INFOSEC Machine", "Akelarre (cipher)", "Anubis (cipher)", "Avalanche effect", "BEAR and LION ciphers", "BaseKing", "BassOmatic", "Baton (disambiguation)", "Biclique attack", "Block cipher", "Block cipher mode of operation", "Block size (cryptography)", "Blowfish (cipher)", "Boomerang attack", "Brute-force attack", "CAST-128", "CAST-256", "CDSA/CSSM", "CIKS-1", "CIPHERUNICORN-A", "CIPHERUNICORN-E", "CLEFIA", "COCONUT98", "CRYPTON", "CRYPTREC", "CS-Cipher", "CYPRIS (microchip)", "Camellia (cipher)", "Cellular Message Encryption Algorithm", "Checksum", "Chi-square test", "Chiasmus (cipher)", "Cipher security summary", "Classified information in the United States", "Cobra ciphers", "Crab (cipher)", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Cryptomeria cipher", "DEAL", "DES-X", "DES supplementary material", "DFC (cipher)", "Data Encryption Standard", "Datacryptor 2000", "Davies' attack", "Differential-linear attack", "Differential cryptanalysis", "Distinguishing attack", "E2 (cipher)", "EFF DES cracker", "Electronic codebook", "FEA-M", "FEAL", "FNBDT", "FROG", "Feistel cipher", "GDES", "GOST (block cipher)", "Grand Cru (cipher)", "HAIPE-IS", "Hasty Pudding cipher", "Hierocrypt", "Higher-order differential cryptanalysis", "History of cryptography", "ICE (cipher)", "IDEA NXT", "IPsec", "Impossible differential cryptanalysis", "Initialization vector", "Integral cryptanalysis", "Intel Cascade Cipher", "International Data Encryption Algorithm", "Interpolation attack", "Iraqi block cipher", "JUNIPER", "KASUMI", "KHAZAD", "KN-Cipher", "KOV-14", "KeeLoq", "Kendall tau rank correlation coefficient", "Key schedule", "Key size", "Key whitening", "Khufu and Khafre", "Known-key distinguishing attack", "LOKI", "LOKI97", "Ladder-DES", "Lai-Massey scheme", "Libelle (cipher)", "Linear cryptanalysis", "Lucifer (cipher)", "M6 (cipher)", "M8 (cipher)", "MAGENTA", "MARS (cryptography)", "MESH (cipher)", "MISTY1", "MMB", "MULTI2", "MYK-85", "MacGuffin (cipher)", "Madryga", "Meet-in-the-middle attack", "Mercy (cipher)", "Message authentication code", "Mod n cryptanalysis", "MultiSwap", "NESSIE", "NOEKEON", "NSA", "NSA Suite A Cryptography", "NUSH", "National Security Agency", "NewDES", "New Data Seal", "Nimbus (cipher)", "Outline of cryptography", "PC Card", "PKCS11", "PRESENT (cipher)", "Padding (cryptography)", "Partitioning cryptanalysis", "Permutation box", "Piling-up lemma", "Product cipher", "Public-key cryptography", "Q (cipher)", "RC2", "RC5", "RC6", "REDOC", "Rebound attack", "Red Pike (cipher)", "Related-key attack", "Rotational cryptanalysis", "S-1 block cipher", "S-box", "SAFER", "SAVILLE", "SC2000", "SEED", "SHACAL", "SHARK", "SMS4", "SXAL/MBAL", "SafeXcel-3340", "SecNet-11", "Secure Terminal Equipment", "Serpent (cipher)", "Sierra (microchip)", "Simon (cipher)", "Skipjack (cipher)", "Slide attack", "Speck (cipher)", "Spectr-H64", "Square (cipher)", "Steganography", "Stream cipher", "Substitution-permutation network", "Symmetric-key algorithm", "Thales Group", "Threefish", "Time/memory/data tradeoff attack", "Timing attack", "Tiny Encryption Algorithm", "Treyfer", "Triple DES", "Truncated differential cryptanalysis", "Twofish", "Type 1 encryption", "UES (cipher)", "United States government", "Weak key", "Whitening transformation", "XSL attack", "XTEA", "XXTEA", "Xenon (cipher)", "Xmx", "Zodiac (cipher)"], "categories": ["All stub articles", "Block ciphers", "Cryptography stubs", "Type 1 encryption algorithms"], "title": "BATON"}
{"summary": "The vast majority of the National Security Agency's work on encryption is classified, but from time to time NSA participates in standards processes or otherwise publishes information about its cryptographic algorithms. The NSA has categorized encryption items into four product types, and algorithms into two suites. The following is a brief and incomplete summary of public knowledge about NSA algorithms and protocols.", "links": ["ACCORDIAN", "AIM (2004)", "AIM (2004) for IFF Mode 5", "APCO Project 25", "Advanced Encryption Standard", "Advanced INFOSEC Machine", "Asymmetric-key algorithm", "BATON", "BAYLESS", "BYTEMAN", "Block cipher", "CARDHOLDER", "CARDIGAN", "CARIBOU", "CDSA/CSSM", "CORDOBA", "CRAYON", "CXS-2000", "CXS-810", "CYPRIS (microchip)", "Classified information", "Cooperative key generation", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Data Encryption Standard", "Digital Signature Algorithm", "Digital signature", "EKMS", "Enhanced FIREFLY", "FASTHASH", "FIREFLY", "Firmware", "Fortezza", "Fortezza Plus", "GOODSPEED", "HAVE QUICK", "History of cryptography", "Identification Friend or Foe", "Indictor", "JACKNIFE", "JUNIPER", "KEESEE", "KG-194", "KG-227", "KG-228", "KG-81", "KG-94", "KG-95", "KI-17", "KOV-14", "Key Exchange Algorithm", "MAYFLY", "MCU-100", "MCU-600", "MEDLEY", "MISSI", "MYK-15A", "MYK-16", "MYK-17", "MYK-85", "Mark XII IFF", "Message authentication code", "NSA Suite A Cryptography", "NSA Suite B Cryptography", "NSA encryption systems", "National Information Assurance Glossary", "National Security Agency", "Outline of cryptography", "PADSTONE", "PEGASUS", "PHALANX", "PKCS", "PKCS11", "Palladium Secure Modem", "Public-key cryptography", "Public key cryptography", "Quantum computing", "Radar", "SAVILLE", "SafeXcel-3340", "SecNet-11", "SecNet54", "SecNet 54", "Secure Hash Algorithm", "Sierra (microchip)", "Sierra II (microchip)", "Simon (cipher)", "Skipjack (cipher)", "Speck (cipher)", "Standardization", "Steganography", "Stream cipher", "Symmetric-key algorithm", "Type 1 encryption", "Type 2 encryption", "Type 3 encryption", "Type 4 encryption", "U-AYJ", "U-BLW", "U-BLX", "U-TXZ", "VALLOR", "VINSON", "ViaSat KG-25x", "WALBURN", "WEASEL", "Windster"], "categories": ["All articles needing additional references", "Articles needing additional references from February 2008", "National Security Agency", "National Security Agency cryptography", "National Security Agency encryption devices", "Type 1 encryption algorithms", "Type 2 encryption algorithms", "Type 3 encryption algorithms"], "title": "NSA cryptography"}
{"summary": "SAVILLE is a classified NSA Type 1 encryption algorithm. It is used broadly, often for voice encryption, and implemented in a large number of encryption devices.\nLittle is known publicly about the algorithm itself due to its classified nature and inclusion in the NSA's Suite A. Some documentation related to the KYK-13 fill device and statements made by military officials suggest that SAVILLE has a 128-bit key. On the AIM microchip, it runs at 4% of the clock rate (compare DES at 76% and BATON at 129%). The Cypris chip mentions 2 modes; specifications for Windster and Indictor specify that they provide Saville I.\nSome devices and protocols that implement SAVILLE:\nThe VINSON family (voice encryption)\nAPCO Project 25 (single-channel land mobile radios) (Saville has algorithm ID 04)\nVersatile encryption chips: AIM, Cypris, Sierra I/II, Windster, Indictor, Presidio, Railman", "links": ["3-Way", "3-subset meet-in-the-middle attack", "APCO Project 25", "ARIA (cipher)", "Advanced Encryption Standard", "Advanced Encryption Standard process", "Advanced INFOSEC Machine", "Akelarre (cipher)", "Anubis (cipher)", "Avalanche effect", "BATON", "BEAR and LION ciphers", "BaseKing", "BassOmatic", "Biclique attack", "Block cipher", "Block cipher mode of operation", "Block size (cryptography)", "Blowfish (cipher)", "Boomerang attack", "Brute-force attack", "CAST-128", "CAST-256", "CIKS-1", "CIPHERUNICORN-A", "CIPHERUNICORN-E", "CLEFIA", "COCONUT98", "CRYPTON", "CRYPTREC", "CS-Cipher", "Camellia (cipher)", "Cellular Message Encryption Algorithm", "Chi-square test", "Chiasmus (cipher)", "Cipher security summary", "Cobra ciphers", "Crab (cipher)", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Cryptomeria cipher", "Cypris (microchip)", "DEAL", "DES-X", "DES supplementary material", "DFC (cipher)", "Data Encryption Standard", "Davies' attack", "Differential-linear attack", "Differential cryptanalysis", "Distinguishing attack", "E2 (cipher)", "EFF DES cracker", "FEA-M", "FEAL", "FROG", "Feistel cipher", "Fill device", "GDES", "GOST (block cipher)", "Grand Cru (cipher)", "Hasty Pudding cipher", "Hierocrypt", "Higher-order differential cryptanalysis", "History of cryptography", "ICE (cipher)", "IDEA NXT", "Impossible differential cryptanalysis", "Indictor (microchip)", "Initialization vector", "Integral cryptanalysis", "Intel Cascade Cipher", "International Data Encryption Algorithm", "Interpolation attack", "Iraqi block cipher", "KASUMI", "KHAZAD", "KN-Cipher", "KYK-13", "KeeLoq", "Kendall tau rank correlation coefficient", "Key schedule", "Key size", "Key whitening", "Khufu and Khafre", "Known-key distinguishing attack", "LOKI", "LOKI97", "Ladder-DES", "Lai-Massey scheme", "Libelle (cipher)", "Linear cryptanalysis", "Lucifer (cipher)", "M6 (cipher)", "M8 (cipher)", "MAGENTA", "MARS (cryptography)", "MESH (cipher)", "MISTY1", "MMB", "MULTI2", "MacGuffin (cipher)", "Madryga", "Meet-in-the-middle attack", "Mercy (cipher)", "Message authentication code", "Microprocessor", "Mod n cryptanalysis", "MultiSwap", "NESSIE", "NOEKEON", "NSA Suite A Cryptography", "NUSH", "National Security Agency", "NewDES", "New Data Seal", "Nimbus (cipher)", "Outline of cryptography", "PRESENT (cipher)", "Padding (cryptography)", "Partitioning cryptanalysis", "Permutation box", "Piling-up lemma", "Presidio (microchip)", "Product cipher", "Public-key cryptography", "Q (cipher)", "RC2", "RC5", "RC6", "REDOC", "Railman (microchip)", "Rebound attack", "Red Pike (cipher)", "Related-key attack", "Rotational cryptanalysis", "S-1 block cipher", "S-box", "SAFER", "SC2000", "SEED", "SHACAL", "SHARK", "SMS4", "SXAL/MBAL", "Saville (disambiguation)", "Serpent (cipher)", "Sierra (microchip)", "Simon (cipher)", "Skipjack (cipher)", "Slide attack", "Speck (cipher)", "Spectr-H64", "Square (cipher)", "Steganography", "Stream cipher", "Substitution-permutation network", "Symmetric-key algorithm", "Threefish", "Time/memory/data tradeoff attack", "Timing attack", "Tiny Encryption Algorithm", "Treyfer", "Triple DES", "Truncated differential cryptanalysis", "Twofish", "Type 1 encryption", "UES (cipher)", "VINSON", "Weak key", "Whitening transformation", "Windster (microchip)", "XSL attack", "XTEA", "XXTEA", "Xenon (cipher)", "Xmx", "Zodiac (cipher)"], "categories": ["All articles lacking sources", "All stub articles", "Articles lacking sources from February 2008", "Block ciphers", "Cryptography stubs", "Type 1 encryption algorithms"], "title": "SAVILLE"}
{"summary": "In cryptography, Skipjack is a block cipher\u2014an algorithm for encryption\u2014developed by the U.S. National Security Agency (NSA). Initially classified, it was originally intended for use in the controversial Clipper chip. Subsequently, the algorithm was declassified and now provides a unique insight into the cipher designs of a government intelligence agency.", "links": ["3-Way", "3-subset meet-in-the-middle attack", "ARIA (cipher)", "Abstract algebra", "Adi Shamir", "Advanced Encryption Standard", "Advanced Encryption Standard process", "Akelarre (cipher)", "Alex Biryukov", "Algorithm", "Anubis (cipher)", "Avalanche effect", "BATON", "BEAR and LION ciphers", "Back-story", "Backdoor (computing)", "BaseKing", "BassOmatic", "Biclique attack", "Block cipher", "Block cipher mode of operation", "Block size (cryptography)", "Blowfish (cipher)", "Boomerang attack", "Brute-force attack", "CAST-128", "CAST-256", "CIKS-1", "CIPHERUNICORN-A", "CIPHERUNICORN-E", "CLEFIA", "COCONUT98", "CRYPTON", "CRYPTREC", "CS-Cipher", "Camellia (cipher)", "Cellular Message Encryption Algorithm", "Chi-square test", "Chiasmus (cipher)", "Cipher", "Cipher security summary", "Classified information", "Clipper chip", "Cobra ciphers", "Combinatorics", "Crab (cipher)", "Cryptanalysis", "Cryptographic hash function", "Cryptographically secure pseudorandom number generator", "Cryptography", "Cryptomeria cipher", "DEAL", "DES-X", "DES supplementary material", "DFC (cipher)", "Dan Brown", "Data Encryption Standard", "David A. Wagner", "Davies' attack", "Differential-linear attack", "Differential cryptanalysis", "Digital Fortress", "Digital object identifier", "Distinguishing attack", "Dual EC DRBG", "Dystopia (video game)", "E2 (cipher)", "EFF DES cracker", "Electronics Letters", "Eli Biham", "Encryption", "FEA-M", "FEAL", "FROG", "Feistel cipher", "GDES", "GOST (block cipher)", "Grand Cru (cipher)", "Half-Life 2", "Hasty Pudding cipher", "Hierocrypt", "Higher-order differential cryptanalysis", "History of cryptography", "ICE (cipher)", "IDEA NXT", "Impossible differential cryptanalysis", "Initialization vector", "Integral cryptanalysis", "Intel Cascade Cipher", "International Data Encryption Algorithm", "Interpolation attack", "Iraqi block cipher", "KASUMI", "KHAZAD", "KN-Cipher", "KeeLoq", "Kendall tau rank correlation coefficient", "Key (cryptography)", "Key escrow", "Key schedule", "Key size", "Key whitening", "Khufu and Khafre", "Known-key distinguishing attack", "LOKI", "LOKI97", "Ladder-DES", "Lai-Massey scheme", "Lars Knudsen", "Law Enforcement Access Field", "Libelle (cipher)", "Linear cryptanalysis", "Lucifer (cipher)", "M6 (cipher)", "M8 (cipher)", "MAGENTA", "MARS (cryptography)", "MESH (cipher)", "MISTY1", "MMB", "MULTI2", "MacGuffin (cipher)", "Madryga", "Meet-in-the-middle attack", "Mercy (cipher)", "Message authentication code", "Mod n cryptanalysis", "MultiSwap", "NESSIE", "NOEKEON", "NUSH", "National Security Agency", "NewDES", "New Data Seal", "Nimbus (cipher)", "Outline of cryptography", "PRESENT (cipher)", "Padding (cryptography)", "Partitioning cryptanalysis", "Permutation box", "Piling-up lemma", "Portable Document Format", "Product cipher", "Public-key cryptography", "Public-key encryption", "Q (cipher)", "RC2", "RC5", "RC6", "REDOC", "Rebound attack", "Red Pike (cipher)", "Related-key attack", "Rotational cryptanalysis", "S-1 block cipher", "S-box", "SAFER", "SAVILLE", "SC2000", "SEED", "SHACAL", "SHARK", "SMS4", "SXAL/MBAL", "Serpent (cipher)", "Simon (cipher)", "Slide attack", "Speck (cipher)", "Spectr-H64", "Square (cipher)", "Steganography", "Stream cipher", "Substitution-permutation network", "Symmetric-key algorithm", "Tamper resistance", "Threefish", "Time/memory/data tradeoff attack", "Timing attack", "Tiny Encryption Algorithm", "Treyfer", "Triple DES", "Truncated differential cryptanalysis", "Twofish", "Type 1 product", "UES (cipher)", "United States", "Wayback Machine", "Weak key", "Whitening transformation", "XSL attack", "XTEA", "XXTEA", "Xenon (cipher)", "Xmx", "Zodiac (cipher)"], "categories": ["All articles lacking in-text citations", "All articles with specifically marked weasel-worded phrases", "Articles lacking in-text citations from March 2009", "Articles with specifically marked weasel-worded phrases from January 2014", "Block ciphers", "National Security Agency cryptography", "Pages using duplicate arguments in template calls", "Type 2 encryption algorithms"], "title": "Skipjack (cipher)"}
{"summary": "In statistics and data mining, affinity propagation (AP) is a clustering algorithm based on the concept of \"message passing\" between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not require the number of clusters to be determined or estimated before running the algorithm. Like k-medoids, AP finds \"exemplars\", members of the input set that are representative of clusters.", "links": ["Cluster analysis", "Computer science", "Data mining", "Digital object identifier", "ELKI", "If and only if", "Java (programming language)", "Julia (programming language)", "K-means clustering", "K-medoids", "Log-probability", "Markov clustering", "Principal components analysis", "Protein interaction graph", "PubMed Identifier", "Python (programming language)", "R (programming language)", "Science (journal)", "Scikit-learn", "Statistics", "Text mining"], "categories": ["All stub articles", "Computer science stubs", "Data clustering algorithms"], "title": "Affinity propagation"}
{"summary": "The basic sequential algorithmic scheme (BSAS) is a very basic clustering algorithm that is easy to understand. In the basic form vectors are presented only once and the number of clusters is not known at priori. What is needed is the dissimilarity measured as the distance d (x, C) between a vector point x and a cluster C, threshold of dissimilarity \u0398 and the number of maximum clusters allowed q. The idea is to assign every newly presented vector to an existing cluster or create a new cluster for this sample, depending on the distance to the already defined clusters. As pseudocode, the algorithm looks like the following:\n\n 1. m = 1; Cm = {x1}; // Init first cluster = first sample     \n 2. for every sample x from 2 to N      \n   a. find cluster Ck such that min d(x, Ck)     \n   b. if d(x, Ck) > \u0398 AND (m < q)      \n     i. m = m + 1; Cm = {x} // Create a new cluster      \n   c. else           i. Ck = Ck + {x} // Add sample to the nearest cluster      \n     ii. Update representative if needed      \n 3. end algorithm\n\nAs can be seen the algorithm is simple but still quite efficient. Different choices for the distance function lead to different results and unfortunately the order in which the samples are presented can also have a great effect to the final result. What\u2019s also very important is a correct value for \u0398. This value has a direct effect on the number of formed clusters. If \u0398 is too small unnecessary clusters are created and if too large a value is chosen less than required number of clusters are formed.\nOne detail is that if q is not defined the algorithm \u2018decides\u2019 the number of clusters on its own. This might be wanted under some circumstances but when dealing with limited resources a limited q is usually chosen. Also, BSAS can be used with a similarity function simply by replacing the min function with max.\nThere exists a modification to BSAS called modified BSAS (MBSAS), which runs twice through the samples. It overcomes the drawback that a final cluster for a single sample is decided before all the clusters have been created. The first phase of the algorithm creates the clusters (just like 2b in BSAS) and assigns only a single sample to each cluster. Then the second phase runs through the remaining samples and classifies them to the created clusters (step 2c in BSAS).", "links": ["Algorithm", "Clustering algorithm", "Data structure"], "categories": ["Algorithms and data structures stubs", "All articles covered by WikiProject Wikify", "All articles needing additional references", "All articles needing cleanup", "All articles with too few wikilinks", "All stub articles", "Articles covered by WikiProject Wikify from March 2014", "Articles needing additional references from May 2014", "Articles needing cleanup from February 2014", "Articles with too few wikilinks from March 2014", "Cleanup tagged articles with a reason field from February 2014", "Computer science stubs", "Data clustering algorithms", "Wikipedia pages needing cleanup from February 2014"], "title": "Basic sequential algorithmic scheme"}
{"summary": "BIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets. An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database.\nIts inventors claim BIRCH to be the \"first clustering algorithm proposed in the database area to handle 'noise' (data points that are not part of the underlying pattern) effectively\", beating DBSCAN by two months. The algorithm received the SIGMOD 10 year test of time award in 2006.", "links": ["Anomaly detection", "Artificial neural network", "Association rule learning", "Autoencoder", "Bayesian network", "Bias-variance dilemma", "Birch", "Birch (disambiguation)", "Boosting (machine learning)", "Bootstrap aggregating", "Branching factor", "Canonical correlation analysis", "Cluster analysis", "Computational learning theory", "Conditional random field", "Convolutional neural network", "DBSCAN", "Data clustering", "Data mining", "Data point", "Data set", "Decision tree learning", "Deep learning", "Digital object identifier", "Dimensionality reduction", "Empirical risk minimization", "Ensemble learning", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Feature vector", "Grammar induction", "Graphical model", "Hidden Markov model", "Hierarchical clustering", "Independent component analysis", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Leaf node", "Learning to rank", "Linear discriminant analysis", "Linear regression", "Local outlier factor", "Logistic regression", "Machine learning", "Mean-shift", "Multilayer perceptron", "Naive Bayes classifier", "Non-negative matrix factorization", "OPTICS algorithm", "Online machine learning", "Perceptron", "Performance tuning", "Primary storage", "Principal component analysis", "Probably approximately correct learning", "Random forest", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Self-organizing map", "Semi-supervised learning", "Statistical classification", "Statistical learning theory", "Structured prediction", "Supervised learning", "Support vector machine", "T-distributed stochastic neighbor embedding", "Time constraint", "Tree (data structure)", "Tree data structure", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory"], "categories": ["Data clustering algorithms", "Wikipedia articles needing clarification from December 2014"], "title": "BIRCH"}
{"summary": "The canopy clustering algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and Lyle Ungar in 2000. It is often used as preprocessing step for the K-means algorithm or the Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, where using another algorithm directly may be impractical due to the size of the data set.\nThe algorithm proceeds as follows, using two thresholds  (the loose distance) and  (the tight distance), where  .\nBegin with the set of data points to be clustered.\nRemove a point from the set, beginning a new 'canopy'.\nFor each point left in the set, assign it to the new canopy if the distance less than the loose distance .\nIf the distance of the point is additionally less than the tight distance , remove it from the original set.\nRepeat from step 2 until there are no more data points in the set to cluster.\nThese relatively cheaply clustered canopies can be sub-clustered using a more expensive but accurate algorithm.\nAn important note is that individual data points may be part of several canopies. As an additional speed-up, an approximate and fast distance metric can be used for 3, where a more accurate and slow distance metric can be used for step 4.\nSince the algorithm uses distance functions and requires the specification of distance thresholds, its applicability for high-dimensional data is limited by the curse of dimensionality. Only when a cheap and approximative \u2013 low-dimensional \u2013 distance function is available, the produced canopies will preserve the clusters produced by K-means.\n\n", "links": ["Algorithm", "Andrew McCallum", "Computer cluster", "Curse of dimensionality", "Data clustering", "Data set", "Data structure", "Digital object identifier", "Hierarchical clustering", "K-means algorithm"], "categories": ["Algorithms and data structures stubs", "All articles with dead external links", "All stub articles", "Articles with dead external links from September 2015", "Computer science stubs", "Data clustering algorithms", "Statistical algorithms"], "title": "Canopy clustering algorithm"}
{"summary": "In data mining, cluster-weighted modeling (CWM) is an algorithm-based approach to non-linear prediction of outputs (dependent variables) from inputs (independent variables) based on density estimation using a set of models (clusters) that are each notionally appropriate in a sub-region of the input space. The overall approach works in jointly input-output space and an initial version was proposed by Neil Gershenfeld.", "links": ["Bayesian analysis", "Conditional expected value", "Conditional probability distribution", "Conditional variance", "Data mining", "Data transformation", "Density estimation", "Dependent and independent variables", "Digital object identifier", "Gaussian function", "Joint probability distribution", "Marginal distribution", "Mixture model", "Neil Gershenfeld", "Normal distribution", "Regression analysis"], "categories": ["Data clustering algorithms", "Estimation of densities", "Multivariate statistics"], "title": "Cluster-weighted modeling"}
{"summary": "COBWEB is an incremental system for hierarchical conceptual clustering. COBWEB was invented by Professor Douglas H. Fisher, currently at Vanderbilt University.\nCOBWEB incrementally organizes observations into a classification tree. Each node in a classification tree represents a class (concept) and is labeled by a probabilistic concept that summarizes the attribute-value distributions of objects classified under the node. This classification tree can be used to predict missing attributes or the class of a new object.\nThere are four basic operations COBWEB employs in building the classification tree. Which operation is selected depends on the category utility of the classification achieved by applying it. The operations are:\nMerging Two Nodes\nMerging two nodes means replacing them by a node whose children is the union of the original nodes' sets of children and which summarizes the attribute-value distributions of all objects classified under them.\nSplitting a node\nA node is split by replacing it with its children.\nInserting a new node\nA node is created corresponding to the object being inserted into the tree.\nPassing an object down the hierarchy\nEffectively calling the COBWEB algorithm on the object and the subtree rooted in the node.", "links": ["Category utility", "Classification tree", "Conceptual clustering", "Digital object identifier", "Douglas H. Fisher (Computer Science)", "International Standard Book Number"], "categories": ["Artificial intelligence", "Data clustering algorithms"], "title": "Cobweb (clustering)"}
{"summary": "In computer science, constrained clustering is a class of semi-supervised learning algorithms. Typically, constrained clustering incorporates either a set of must-link constraints, cannot-link constraints, or both, with a Data clustering algorithm. Both a must-link and a cannot-link constraint define a relationship between two data instances. A must-link constraint is used to specify that the two instances in the must-link relation should be associated with the same cluster. A cannot-link constraint is used to specify that the two instances in the cannot-link relation should not be associated with the same cluster. These sets of constraints acts as a guide for which a constrained clustering algorithm will attempt to find clusters in a data set which satisfy the specified must-link and cannot-link constraints. Some constrained clustering algorithms will abort if no such clustering exists which satisfies the specified constraints. Others will try to minimize the amount of constraint violation should it be impossible to find a clustering which satisfies the constraints. Constraints could also be used to guide the selection of a clustering model among several possible solutions. \nExamples of constrained clustering algorithms include:\nCOP K-means \nPCKmeans\nCMWK-Means", "links": ["Computer science", "Data clustering", "Digital object identifier", "International Standard Book Number", "Semi-supervised learning"], "categories": ["All stub articles", "Cluster analysis", "Computer science stubs", "Data clustering algorithms"], "title": "Constrained clustering"}
{"summary": "CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases that is more robust to outliers and identifies clusters having non-spherical shapes and size variances.\n\n", "links": ["Analysis of algorithms", "BIRCH (data clustering)", "Computational complexity theory", "Data clustering", "Data point", "Database", "Digital object identifier", "Hierarchical clustering", "International Standard Book Number", "K-means clustering", "Kd-tree", "Middle ground", "Outlier", "Primary storage", "Random sample", "Robust statistics", "Sample space", "Sampling (statistics)", "Sum of squared error", "Trade-off"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from May 2015", "Data clustering algorithms"], "title": "CURE data clustering algorithm"}
{"summary": "In computer science, data stream clustering is defined as the clustering of data that arrive continuously such as telephone records, multimedia data, financial transactions etc. Data stream clustering is usually studied as a streaming algorithm and the objective is, given a sequence of points, to construct a good clustering of the stream, using a small amount of memory and time.", "links": ["Approximation algorithm", "BIRCH (data clustering)", "C2ICM(incremental clustering)", "CURE data clustering algorithm", "Category utility", "CiteSeer", "Cluster analysis", "Cobweb (clustering)", "Computer science", "Decision tree learning", "Digital object identifier", "Divide-and-conquer algorithm", "K-means clustering", "K-medoids", "Local search (optimization)", "Streaming algorithm"], "categories": ["Data clustering algorithms"], "title": "Data stream clustering"}
{"summary": "Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander and Xiaowei Xu in 1996. It is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.\nIn 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, KDD.", "links": ["AAAI Press", "Anomaly detection", "Apache Commons", "Artificial neural network", "Association for Computing Machinery", "Association rule learning", "Autoencoder", "BIRCH", "Ball tree", "Bayesian network", "Bias-variance dilemma", "Boosting (machine learning)", "Bootstrap aggregating", "Canonical correlation analysis", "CiteSeer", "Cluster analysis", "Clustering high-dimensional data", "Computational learning theory", "Conditional random field", "Convolutional neural network", "Curse of dimensionality", "Data clustering", "Data mining", "Decision tree learning", "Deep learning", "Digital object identifier", "Dimensionality reduction", "ELKI", "Empirical risk minimization", "Ensemble learning", "Euclidean distance", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Fixed-radius near neighbors", "Grammar induction", "Graphical model", "Hans-Peter Kriegel", "Hidden Markov model", "Hierarchical clustering", "Independent component analysis", "Inlining", "International Standard Book Number", "K-distance graph", "K-means algorithm", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Kd-tree", "Learning to rank", "Linear discriminant analysis", "Linear regression", "Local outlier factor", "Logistic regression", "Machine learning", "Mean-shift", "Metric (mathematics)", "Minkowski distance", "Multilayer perceptron", "Naive Bayes classifier", "Non-negative matrix factorization", "OPTICS algorithm", "Online machine learning", "Perceptron", "PreDeCon", "Principal component analysis", "Probably approximately correct learning", "Pseudocode", "R* tree", "R (programming language)", "Random forest", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "SUBCLU", "Scikit-learn", "Self-organizing map", "Semi-supervised learning", "Spatial index", "Springer-Verlag", "Statistical classification", "Statistical learning theory", "Structured prediction", "Supervised learning", "Support vector machine", "T-distributed stochastic neighbor embedding", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory", "Weka (machine learning)"], "categories": ["Data clustering algorithms"], "title": "DBSCAN"}
{"summary": "In statistics, an expectation\u2013maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.", "links": ["A. W. F. Edwards", "Affine connection", "Allen Craig", "Anders Martin-L\u00f6f", "Annals of Statistics", "Anomaly detection", "Arthur P. Dempster", "Artificial neural network", "Association rule learning", "Autoencoder", "BIRCH", "Baum-Welch algorithm", "Bayes theorem", "Bayesian inference", "Bayesian network", "Bias-variance dilemma", "Bibcode", "Bimodal distribution", "Binomial distribution", "Biometrika", "Boosting (machine learning)", "Bootstrap aggregating", "C.F. Jeff Wu", "Canonical correlation analysis", "Christopher Bishop", "CiteSeer", "Closed-form expression", "Cluster analysis", "Communications in Statistics", "Computational learning theory", "Computer vision", "Conditional probability distribution", "Conditional random field", "Conjugate gradient", "Continuous random variable", "Convolutional neural network", "D. Basu", "D. R. Cox", "DBSCAN", "Data clustering", "Data mining", "David J.C. MacKay", "Decision tree learning", "Deep learning", "Density estimation", "Derivative", "Digital object identifier", "Dimensionality reduction", "Discrete random variable", "Donald Rubin", "ELKI", "Empirical risk minimization", "Ensemble learning", "Entropy (information theory)", "Expectation-maximization algorithm", "Expected value", "Exponential family", "Factor analysis", "Feature engineering", "Feature learning", "Frank Dellaert", "Gauss\u2013Newton method", "Geoffrey Hinton", "George A. Barnard", "Geyser", "Gibbs' inequality", "Gradient descent", "Grammar induction", "Graphical model", "Graphical models", "Hidden Markov model", "Hierarchical clustering", "Independent component analysis", "Indicator function", "Information geometry", "Inside-outside algorithm", "International Standard Book Number", "Item response theory", "Iterative method", "JSTOR", "Journal of the Royal Statistical Society, Series B", "Journal of the Royal Statistical Society, Series C", "K-means algorithm", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Kalman filter", "Kullback\u2013Leibler divergence", "Latent variable", "Latent variables", "Learning to rank", "Likelihood function", "Linear discriminant analysis", "Linear regression", "Local maximum", "Local outlier factor", "Logistic regression", "MM algorithm", "Machine learning", "Marginal likelihood", "Markov blanket", "Mathematical Reviews", "Mathematical singularity", "Maximum a posteriori", "Maximum likelihood", "Maximum likelihood estimate", "Maximum likelihood estimation", "Maximum likelihood estimator", "Mean-shift", "Medical imaging", "Message passing (disambiguation)", "Metaheuristic", "Michael I. Jordan", "Misnomer", "Missing values", "Mixture model", "Multilayer perceptron", "Multivariate normal distribution", "Naive Bayes classifier", "Nan Laird", "Natural language processing", "Newton\u2013Raphson", "Non-negative matrix factorization", "Normal distribution", "OPTICS algorithm", "Old Faithful", "Online machine learning", "Ordered subset expectation maximization", "Parameter", "Parameters", "Per Martin-L\u00f6f", "Perceptron", "Positron emission tomography", "Posterior probabilities", "Principal component analysis", "Probabilistic context-free grammar", "Probability density function", "Probability distribution", "Probably approximately correct learning", "Psychometrics", "Random-restart hill climbing", "Random forest", "Rasch model", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Robert Tibshirani", "SOCR", "Saddle point", "Self-organizing map", "Semi-supervised learning", "Simulated annealing", "Single photon emission computed tomography", "Statistical classification", "Statistical learning theory", "Statistical model", "Statistics", "Structured prediction", "Sufficient statistic", "Supervised learning", "Support vector machine", "T-distributed stochastic neighbor embedding", "Total absorption spectroscopy", "Trevor Hastie", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory", "Variational Bayes", "Viterbi algorithm", "Weighted average", "Yasuo Matsuyama"], "categories": ["Data clustering algorithms", "Estimation theory", "Machine learning algorithms", "Missing data", "Optimization algorithms and methods", "Statistical algorithms"], "title": "Expectation\u2013maximization algorithm"}
{"summary": "Fuzzy clustering by Local Approximation of MEmberships (FLAME) is a data clustering algorithm that defines clusters in the dense parts of a dataset and performs cluster assignment solely based on the neighborhood relationships among objects. The key feature of this algorithm is that the neighborhood relationships among neighboring objects in the feature space are used to constrain the memberships of neighboring objects in the fuzzy membership space.", "links": ["Data clustering", "Fuzzy clustering"], "categories": ["Data clustering algorithms", "Wikipedia articles with possible conflicts of interest from August 2010"], "title": "FLAME clustering"}
{"summary": "Data clustering is the process of dividing data elements into classes or clusters so that items in the same class are as similar as possible, and items in different classes are as dissimilar as possible. Depending on the nature of the data and the purpose for which clustering is being used, different measures of similarity may be used to place items into classes, where the similarity measure controls how the clusters are formed. Some examples of measures that can be used as in clustering include distance, connectivity, and intensity.\nIn hard clustering, data is divided into distinct clusters, where each data element belongs to exactly one cluster. In fuzzy clustering (also referred to as soft clustering), data elements can belong to more than one cluster, and associated with each element is a set of membership levels. These indicate the strength of the association between that data element and a particular cluster. Fuzzy clustering is a process of assigning these membership levels, and then using them to assign data elements to one or more clusters.\nOne of the most widely used fuzzy clustering algorithms is the Fuzzy C-Means (FCM) Algorithm (Bezdek 1981). The FCM algorithm attempts to partition a finite collection of  elements  into a collection of c fuzzy clusters with respect to some given criterion. Given a finite set of data, the algorithm returns a list of  cluster centres  and a partition matrix , where each element  tells the degree to which element  belongs to cluster . Like the K-means clustering, the FCM aims to minimize an objective function:\n\nwhere:\n\nThis differs from the k-means objective function by the addition of the membership values  and the fuzzifier , with . The fuzzifier  determines the level of cluster fuzziness. A large  results in smaller memberships  and hence, fuzzier clusters. In the limit , the memberships  converge to 0 or 1, which implies a crisp partitioning. In the absence of experimentation or domain knowledge,  is commonly set to 2.", "links": ["Cluster Analysis", "Data clustering", "Determining the number of clusters in a data set", "Digital object identifier", "Expectation-maximization algorithm", "FLAME Clustering", "Fuzzy logic", "Hard clustering", "International Standard Book Number", "K-means clustering", "PubMed Identifier", "Soft K-means"], "categories": ["All articles lacking in-text citations", "All articles needing cleanup", "Articles lacking in-text citations from August 2009", "Articles needing cleanup from October 2011", "Articles with inconsistent citation formats", "Cleanup tagged articles with a reason field from October 2011", "Data clustering algorithms", "Wikipedia pages needing cleanup from October 2011"], "title": "Fuzzy clustering"}
{"summary": "The information bottleneck method is a technique in information theory introduced by Naftali Tishby et al. for finding the best tradeoff between accuracy and complexity (compression) when summarizing (e.g. clustering) a random variable X, given a joint probability distribution between X and an observed relevant variable Y. Other applications include distributional clustering, and dimension reduction. In a well defined sense it generalized the classical notion of minimal sufficient statistics from parametric statistics to arbitrary distributions, not necessarily of exponential form. It does so by relaxing the sufficiency condition to capture some fraction of the mutual information with the relevant variable Y.\nThe compressed variable is  and the algorithm minimizes the following quantity\n\nwhere  are the mutual information between  and  respectively, and  is a Lagrange multiplier.", "links": ["Accuracy", "Bernard Silverman", "Bernie Silverman", "Canonical correlation", "Data clustering", "Data compression", "Density estimation", "Digital object identifier", "Dimension reduction", "Distance matrix", "Information theory", "International Standard Book Number", "International Standard Serial Number", "Joint probability distribution", "Kullback\u2013Leibler distance", "Lagrange multiplier", "Mutual information", "Naftali Tishby", "Random variable", "Rate\u2013distortion theory", "Sufficient statistics"], "categories": ["Data clustering algorithms", "Multivariate statistics"], "title": "Information bottleneck method"}
{"summary": "In data mining and machine learning,  -flats algorithm   is an iterative method which aims to partition  observations into  clusters where each cluster is close to a -flat, where  is a given integer.\nIt is a generalization of the -means algorithm. In -means algorithm, clusters are formed in the way that each cluster is close to one point, which is a -flat.  -flats algorithm gives better clustering result than -means algorithm for some data set.", "links": ["Classification in machine learning", "Data mining", "Flat (geometry)", "Gaussian clouds", "Hyperplane", "K-means clustering", "Machine learning", "Partition of a set"], "categories": ["All articles needing expert attention", "All articles that are too technical", "Articles needing expert attention from December 2011", "Data clustering algorithms", "Wikipedia articles that are too technical from December 2011"], "title": "K q-flats"}
{"summary": "k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.\nThe problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.\nThe algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means because of the k in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.", "links": ["Anomaly detection", "Apache Mahout", "Apache Spark", "ArXiv", "Artificial neural network", "Association for Computational Linguistics", "Association rule learning", "Astronomy", "Autoencoder", "BIRCH", "Bayesian network", "Bell Labs", "Bias-variance dilemma", "Bilateral filter", "Boosting (machine learning)", "Bootstrap aggregating", "Canonical correlation analysis", "Centroid", "Centroidal Voronoi tessellation", "Centroids", "CiteSeer", "Cluster analysis", "Color quantization", "Computational learning theory", "Computer vision", "Conditional random field", "Convolutional neural network", "CrimeStat", "DBSCAN", "Data Mining in Agriculture", "Data mining", "David MacKay (scientist)", "David Mount", "Decision tree learning", "Deep learning", "Determining the number of clusters in a data set", "Dictionary learning", "Digital object identifier", "Dimensionality reduction", "Discrete and Computational Geometry", "ELKI", "EM clustering", "Empirical risk minimization", "Ensemble learning", "Environment for DeveLoping KDD-Applications Supported by Index-Structures", "Euclidean distance", "Expectation-maximization algorithm", "Expectation\u2013maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Fuzzy clustering", "GNU Octave", "Gaussian distribution", "Geostatistics", "Grammar induction", "Graphical model", "Head/tail Breaks", "Heuristic algorithm", "Hidden Markov model", "Hierarchical clustering", "Hugo Steinhaus", "IChrome Ltd.", "IEEE Transactions on Information Theory", "Independent component analysis", "Integer lattice", "International Standard Book Number", "Iris (plant)", "Iris flower data set", "JSTOR", "Jenks natural breaks optimization", "Journal of the Royal Statistical Society, Series C", "Julia language", "K-SVD", "K-means++", "K-means clustering", "K-medians clustering", "K-medoids", "K-nearest neighbor", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "K q-flats", "Kd-tree", "Learning to rank", "Least-squares estimation", "Lecture Notes in Computer Science", "Linde\u2013Buzo\u2013Gray algorithm", "Linear classifier", "Linear discriminant analysis", "Linear regression", "Lloyd's algorithm", "Local optimum", "Local outlier factor", "Logistic regression", "MATLAB", "MLPACK (C++ library)", "Machine Learning (journal)", "Machine learning", "MapReduce", "Market segmentation", "Mathematica", "Mathematical Reviews", "Mean", "Mean-shift", "Mean shift", "Medoids", "Metric (mathematics)", "Mixture model", "Multilayer perceptron", "NP-hard", "Naive Bayes classifier", "Named entity recognition", "Nathan Netanyahu", "Natural language processing", "Nearest centroid classifier", "Non-negative matrix factorization", "OPTICS algorithm", "Online machine learning", "OpenCV", "Partition of a set", "Perceptron", "Principal component analysis", "Probably approximately correct learning", "Proceedings of the Royal Society A", "Prototype", "Pulse-code modulation", "R (programming language)", "Radial basis function", "Radial basis function network", "Random forest", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "SAS System", "Sampling (statistics)", "SciPy", "Scikit-learn", "Self-organizing map", "Semi-supervised learning", "Silhouette (clustering)", "Smoothed analysis", "Spherical k-means", "Stata", "Statistical classification", "Statistical learning theory", "Structured prediction", "Supervised learning", "Support vector machine", "Symposium on Computational Geometry", "T-distributed stochastic neighbor embedding", "Taxicab geometry", "Torch (machine learning)", "Triangle inequality", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory", "Variance", "Vector quantization", "Voronoi cell", "Voronoi diagram", "Weka (machine learning)", "Whitening transformation", "X-means clustering", "Zentralblatt MATH"], "categories": ["All articles with dead external links", "All articles with unsourced statements", "Articles with dead external links from January 2013", "Articles with unsourced statements from March 2014", "CS1 French-language sources (fr)", "Data clustering algorithms", "Statistical algorithms"], "title": "K-means clustering"}
{"summary": "In data mining, k-means++ is an algorithm for choosing the initial values (or \"seeds\") for the k-means clustering algorithm. It was proposed in 2007 by David Arthur and Sergei Vassilvitskii, as an approximation algorithm for the NP-hard k-means problem\u2014a way of avoiding the sometimes poor clusterings found by the standard k-means algorithm. It is similar to the first of three seeding methods proposed, in independent work, in 2006 by Rafail Ostrovsky, Yuval Rabani, Leonard Schulman and Chaitanya Swamy. (The distribution of the first seed is different.)", "links": ["Data mining", "Digital object identifier", "ELKI", "GNU R", "GraphLab", "K-means clustering", "Leonard Schulman", "Lloyd's algorithm", "NP-hard", "OpenCV", "Scikit-learn", "Vanilla (computing)", "Weka (machine learning)"], "categories": ["All articles with dead external links", "Articles with dead external links from May 2013", "Data clustering algorithms", "Statistical algorithms"], "title": "K-means++"}
{"summary": "In statistics and data mining, k-medians clustering is a cluster analysis algorithm. It is a variation of k-means clustering where instead of calculating the mean for each cluster to determine its centroid, one instead calculates the median. This has the effect of minimizing error over all clusters with respect to the 1-norm distance metric, as opposed to the square of the 2-norm distance metric (which k-means does.)\nThis relates directly to the k-median problem which is the problem of finding k centers such that the clusters formed by them are the most compact. Formally, given a set of data points x, the k centers ci are to be chosen so as to minimize the sum of the distances from each x to the nearest ci.\nThe criterion function formulated in this way is sometimes a better criterion than that used in the k-means clustering algorithm, in which the sum of the squared distances is used. The sum of distances is widely used in applications such as facility location.\nThe proposed algorithm uses Lloyd-style iteration which alternates between an expectation (E) and maximization (M) step, making this an Expectation\u2013maximization algorithm. In the E step, all objects are assigned to their nearest median. In the M step, the medians are recomputed by using the median in each single dimension.", "links": ["Algorithm", "Anil K. Jain (computer scientist, born 1948)", "Cluster analysis", "Data mining", "Data structure", "ELKI", "Euclidean distance", "Expectation\u2013maximization algorithm", "FORTRAN", "Facility location", "GNU R", "K-means", "K-means clustering", "K-medoids", "Manhattan distance", "Mean", "Median", "Medoid", "Norm (mathematics)", "Silhouette (clustering)", "Stata", "Statistics"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Data clustering algorithms", "Operations research", "Statistical algorithms", "Statistics stubs"], "title": "K-medians clustering"}
{"summary": "The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers (medoids or exemplars) and works with an arbitrary matrix of distances between datapoints instead of . This method was proposed in 1987 for the work with  norm and other distances.\nk-medoid is a classical partitioning technique of clustering that clusters the data set of n objects into k clusters known a priori. A useful tool for determining k is the silhouette.\nIt is more robust to noise and outliers as compared to k-means because it minimizes a sum of pairwise dissimilarities instead of a sum of squared Euclidean distances.\nA medoid can be defined as the object of a cluster whose average dissimilarity to all the objects in the cluster is minimal. i.e. it is a most centrally located point in the cluster.", "links": ["Algorithm", "Data clustering", "ELKI", "Julia language", "K-means", "Lloyd's algorithm", "MATLAB", "Manhattan distance", "Medoid", "Medoids", "Minkowski distance", "R (programming language)", "RapidMiner", "Silhouette (clustering)"], "categories": ["All articles needing style editing", "All articles with unsourced statements", "Articles with unsourced statements from May 2015", "Data clustering algorithms", "Statistical algorithms", "Wikipedia articles needing style editing from September 2015"], "title": "K-medoids"}
{"summary": "In applied mathematics, K-SVD is a dictionary learning algorithm for creating a dictionary for sparse representations, via a singular value decomposition approach. K-SVD is a generalization of the k-means clustering method, and it works by iteratively alternating between sparse coding the input data based on the current dictionary, and updating the atoms in the dictionary to better fit the data. K-SVD can be found widely in use in applications such as image processing, audio processing, biology, and document analysis.", "links": ["Anomaly detection", "Applied mathematics", "Artificial neural network", "Association rule learning", "Autoencoder", "BIRCH", "Bayesian network", "Bias-variance dilemma", "Boosting (machine learning)", "Bootstrap aggregating", "Canonical correlation analysis", "Cluster analysis", "Computational learning theory", "Conditional random field", "Convolutional neural network", "DBSCAN", "Data mining", "Decision tree learning", "Deep learning", "Dictionary learning", "Digital object identifier", "Dimensionality reduction", "Empirical risk minimization", "Ensemble learning", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Grammar induction", "Graphical model", "Hidden Markov model", "Hierarchical clustering", "Independent component analysis", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "L1 norm", "L2 norm", "Learning to rank", "Linear discriminant analysis", "Linear regression", "Local outlier factor", "Logistic regression", "Lp norm", "Lp space", "Machine learning", "Matching pursuit", "Matrix norm", "Mean-shift", "Multilayer perceptron", "Naive Bayes classifier", "Nearest neighbor search", "Non-negative matrix factorization", "OPTICS algorithm", "Online machine learning", "Perceptron", "Principal component analysis", "Probably approximately correct learning", "Random forest", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Self-organizing map", "Semi-supervised learning", "Singular value decomposition", "Sparse approximation", "Sparse representation", "Sparse vector", "Statistical classification", "Statistical learning theory", "Structured prediction", "Supervised learning", "Support vector machine", "T-distributed stochastic neighbor embedding", "Uniform norm", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory"], "categories": ["All articles lacking reliable references", "All articles with close paraphrasing", "All pages needing cleanup", "Articles lacking reliable references from May 2014", "Articles needing cleanup from May 2014", "Articles with close paraphrasing from May 2014", "Data clustering algorithms", "Linear algebra", "Norms (mathematics)"], "title": "K-SVD"}
{"summary": "The Linde\u2013Buzo\u2013Gray algorithm (introduced by Yoseph Linde, Andr\u00e9s Buzo and Robert M. Gray in 1980) is a vector quantization algorithm to derive a good codebook.\nIt is similar to the k-means method in data clustering.", "links": ["Algorithm", "Codebook", "Data clustering", "Data structure", "Digital object identifier", "IEEE Transactions on Communications", "K-means", "Lloyd's algorithm", "Robert M. Gray", "Vector quantization"], "categories": ["Algorithms and data structures stubs", "All articles lacking reliable references", "All stub articles", "Articles lacking reliable references from June 2012", "Artificial neural networks", "Computer science stubs", "Data clustering algorithms", "Machine learning algorithms"], "title": "Linde\u2013Buzo\u2013Gray algorithm"}
{"summary": "Mean shift is a non-parametric feature-space analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. Application domains include cluster analysis in computer vision and image processing.\n^ Cheng, Yizong (August 1995). \"Mean Shift, Mode Seeking, and Clustering\". IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE) 17 (8): 790\u2013799. doi:10.1109/34.400568. \n^ Comaniciu, Dorin; Peter Meer (May 2002). \"Mean Shift: A Robust Approach Toward Feature Space Analysis\". IEEE Transactions on Pattern Analysis and Machine Intelligence (IEEE) 24 (5): 603\u2013619. doi:10.1109/34.1000236.", "links": ["Anomaly detection", "Apache Mahout", "Artificial neural network", "Association rule learning", "Autoencoder", "BIRCH", "Bayesian network", "Bias-variance dilemma", "Boosting (machine learning)", "Bootstrap aggregating", "Canonical correlation analysis", "Cluster analysis", "Computational learning theory", "Computer vision", "Conditional random field", "Convolutional neural network", "DBSCAN", "Data mining", "Decision tree learning", "Deep learning", "Density function", "Digital object identifier", "Dimensionality reduction", "Empirical risk minimization", "Ensemble learning", "Ensemble tracking", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Feature space", "Grammar induction", "Graphical model", "Hidden Markov model", "Hierarchical clustering", "ImageJ", "Image processing", "Independent component analysis", "International Standard Book Number", "K-means", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Kernel (statistics)", "Kernel density estimation", "Learning to rank", "Linear discriminant analysis", "Linear regression", "Local outlier factor", "Logistic regression", "Machine learning", "Mean-shift", "Mode (statistics)", "Multilayer perceptron", "Naive Bayes classifier", "Non-negative matrix factorization", "Non-parametric", "OPTICS algorithm", "Online machine learning", "OpenCV", "Orfeo toolbox", "Perceptron", "Principal component analysis", "Probably approximately correct learning", "Radial basis function kernel", "Random forest", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Scikit-learn", "Self-organizing map", "Semi-supervised learning", "Statistical classification", "Statistical learning theory", "Structured prediction", "Supervised learning", "Support vector machine", "T-distributed stochastic neighbor embedding", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory", "YouTube"], "categories": ["Computer vision", "Data clustering algorithms", "Pages using citations with accessdate and no URL"], "title": "Mean shift"}
{"summary": "In the theory of cluster analysis, the nearest-neighbor chain algorithm is a method that can be used to perform several types of agglomerative hierarchical clustering, using an amount of memory that is linear in the number of points to be clustered and an amount of time linear in the number of distinct distances between pairs of points. The main idea of the algorithm is to find pairs of clusters to merge by following paths in the nearest neighbor graph of the clusters until the paths terminate in pairs of mutual nearest neighbors. The algorithm was developed and implemented in 1982 by J. P. Benz\u00e9cri and J. Juan, based on earlier methods that constructed hierarchical clusterings using mutual nearest neighbor pairs without taking advantage of nearest neighbor chains.", "links": ["ArXiv", "Binary tree", "Cardinality", "Centroid", "Closest pair", "Cluster analysis", "Complete-linkage clustering", "Data structure", "David Eppstein", "Digital object identifier", "Disjoint set", "Euclidean space", "Greedy algorithm", "Herbert Edelsbrunner", "Hierarchical clustering", "International Standard Book Number", "JSTOR", "K-means clustering", "Mathematical Reviews", "Maximal element", "Memoization", "Minimum spanning tree", "Nearest neighbor graph", "Outlier", "Prim's algorithm", "Quadtree", "Shlomo Moran", "Single-linkage clustering", "Stack (data structure)", "W. T. Williams", "Ward's method"], "categories": ["Data clustering algorithms"], "title": "Nearest-neighbor chain algorithm"}
{"summary": "Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and J\u00f6rg Sander. Its basic idea is similar to DBSCAN, but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. In order to do so, the points of the database are (linearly) ordered such that points which are spatially closest become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that needs to be accepted for a cluster in order to have both points belong to the same cluster. This is represented as a dendrogram.", "links": ["AAAI Press", "ACM Press", "Anomaly detection", "Artificial neural network", "Association rule learning", "Autoencoder", "BIRCH", "Bayesian network", "Bias-variance dilemma", "Boosting (machine learning)", "Bootstrap aggregating", "Canonical correlation analysis", "Cluster analysis", "Computational learning theory", "Conditional random field", "Convolutional neural network", "Correlation clustering", "DBSCAN", "Data mining", "Decision tree learning", "Deep learning", "Dendrogram", "Digital object identifier", "Dimensionality reduction", "ELKI", "Empirical risk minimization", "Ensemble learning", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Fixed-radius near neighbors", "Grammar induction", "Graphical model", "Hans-Peter Kriegel", "Heap (data structure)", "Hidden Markov model", "Hierarchical clustering", "Independent component analysis", "International Standard Book Number", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Learning to rank", "Linear discriminant analysis", "Linear regression", "Local outlier factor", "Logistic regression", "Machine learning", "Mean-shift", "Multilayer perceptron", "Naive Bayes classifier", "Non-negative matrix factorization", "Online machine learning", "Perceptron", "Principal component analysis", "Priority queue", "Probably approximately correct learning", "Random forest", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Self-organizing map", "Semi-supervised learning", "Single-linkage clustering", "Spanning tree", "Spatial index", "Springer-Verlag", "Statistical classification", "Statistical learning theory", "Structured prediction", "Subspace clustering", "Supervised learning", "Support vector machine", "T-distributed stochastic neighbor embedding", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory", "Weka (machine learning)"], "categories": ["Articles with specifically marked weasel-worded phrases from October 2014", "Data clustering algorithms"], "title": "OPTICS algorithm"}
{"summary": "SUBCLU is an algorithm for clustering high-dimensional data by Karin Kailing, Hans-Peter Kriegel and Peer Kr\u00f6ger. It is a subspace clustering algorithm that builds on the density-based clustering algorithm DBSCAN. SUBCLU can find clusters in axis-parallel subspaces, and uses a bottom-up, greedy strategy to remain efficient.", "links": ["Apriori algorithm", "Axis-parallel", "Cluster analysis", "Clustering high-dimensional data", "DBSCAN", "Environment for DeveLoping KDD-Applications Supported by Index-Structures", "Greedy algorithm", "Hans-Peter Kriegel", "Monotonicity", "Subspace clustering", "Top-down and bottom-up design"], "categories": ["All articles needing additional references", "All articles needing expert attention", "Articles needing additional references from February 2010", "Articles needing expert attention from February 2010", "Articles needing expert attention with no reason or talk parameter", "Data clustering algorithms", "Statistics articles needing expert attention"], "title": "SUBCLU"}
{"summary": "The \u03b1-algorithm is an algorithm used in process mining, aimed at reconstructing causality from a set of sequences of events. It was first put forward by van der Aalst, Weijters and M\u0103ru\u015fter. Several extensions or modifications of it have since been presented, which will be listed below.\nIt constructs P/T nets with special properties (workflow nets) from event logs (as might be collected by an ERP system). Each transition in the net corresponds to an observed task.", "links": ["Alphabet (computer science)", "Enterprise resource planning", "Petri net", "Petri nets", "Process mining", "Sequence of events", "Sound SWF net", "String (computer science)", "Wil van der Aalst", "Workflow net"], "categories": ["All articles to be expanded", "Articles to be expanded from May 2010", "Data mining algorithms", "Process mining"], "title": "Alpha algorithm"}
{"summary": "Apriori is an algorithm for frequent item set mining and association rule learning over transactional databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.", "links": ["Association rule learning", "Association rules", "Breadth-first search", "Database", "Databases", "Downward closure lemma", "ELKI", "Hash tree (persistent data structure)", "MIT license", "Market basket analysis", "Max-Miner", "Orange (software)", "R (programming language)", "Stock-keeping unit", "Winepi"], "categories": ["Articles with example pseudocode", "Data mining algorithms"], "title": "Apriori algorithm"}
{"summary": "FSA-Red Algorithm is an algorithm for data reduction which is suitable to build strong association rule using data mining method such as Apriori algorithm.", "links": ["Algorithm", "Apriori algorithm", "Association rule learning", "Data mining", "Feri Sulianta"], "categories": ["Data analysis", "Data mining algorithms", "Formal sciences"], "title": "FSA-Red Algorithm"}
{"summary": "GSP Algorithm (Generalized Sequential Pattern algorithm) is an algorithm used for sequence mining. The algorithms for solving sequence mining problems are mostly based on the a priori (level-wise) algorithm. One way to use the level-wise paradigm is to first discover all the frequent items in a level-wise fashion. It simply means counting the occurrences of all singleton elements in the database. Then, the transactions are filtered by removing the non-frequent items. At the end of this step, each transaction consists of only the frequent elements it originally contained. This modified database becomes an input to the GSP algorithm. This process requires one pass over the whole database.\nGSP Algorithm makes multiple database passes. In the first pass, all single items (1-sequences) are counted. From the frequent items, a set of candidate 2-sequences are formed, and another pass is made to identify their frequency. The frequent 2-sequences are used to generate the candidate 3-sequences, and this process is repeated until no more frequent sequences are found. There are two main steps in the algorithm.\nCandidate Generation. Given the set of frequent (k-1)-frequent sequences F(k-1), the candidates for the next pass are generated by joining F(k-1) with itself. A pruning phase eliminates any sequence, at least one of whose subsequences is not frequent.\nSupport Counting. Normally, a hash tree\u2013based search is employed for efficient support counting. Finally non-maximal frequent sequences are removed.", "links": ["Algorithm", "Apriori algorithm", "Database", "Google Books", "Hash tree (persistent data structure)", "International Standard Book Number", "Sequence mining", "Transaction (database)"], "categories": ["All articles lacking sources", "Articles lacking sources from May 2007", "Articles with example pseudocode", "Data mining algorithms", "Pages with citations lacking titles"], "title": "GSP Algorithm"}
{"summary": "The Teiresias algorithm is a combinatorial algorithm for the discovery of rigid patterns (motifs) in biological sequences. It is named after the Greek prophet Teiresias and was created in 1997 by Isidore Rigoutsos and Aris Floratos.\nThe problem of finding sequence similarities in the primary structure of related proteins or genes is one of the problems arising in the analysis of biological sequences. It can be shown that pattern discovery in its general form is NP-hard. The Teiresias algorithm, is based on the observation that if a pattern spans many positions and appears exactly k times in the input then all fragments (sub patterns) of the pattern have to appear at least k times in the input. The algorithm is able to produce all patterns that have a user-defined number of copies in the given input, and manages to be very efficient by avoiding the enumeration of the entire space. Finally, the algorithm reports motifs that are maximal in both length and composition.\nA new implementation of the Teiresias algorithm was recently made available by the Computational Medicine Center at Thomas Jefferson University. Teiresias is also accessible through an interactive web-based user interface by the same center. See external links for both.", "links": ["Aris Floratos", "Isidore Rigoutsos", "NP-hard", "Regular expressions", "Teiresias"], "categories": ["Data mining algorithms", "Official website not in Wikidata", "Pattern matching"], "title": "Teiresias algorithm"}
{"summary": "In data mining, the WINEPI algorithm is an influential algorithm for episode mining, which helps discover the knowledge hidden in an event sequence.\nWINEPI derives part of its name from the fact that it uses a sliding window to go through the event sequence.\nThe outcome of the algorithm are episode rules describe temporal relationships between events and form an extension of association rules.", "links": ["Algorithm", "Association rules", "Data mining", "Data structure", "Digital object identifier", "Sliding window"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Data mining algorithms"], "title": "WINEPI"}
{"summary": "In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into \"spam\" or \"non-spam\" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.).\nIn the terminology of machine learning, classification is considered an instance of supervised learning, i.e. learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance.\nOften, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type), ordinal (e.g. \"large\", \"medium\" or \"small\"), integer-valued (e.g. the number of occurrences of a part word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.\nAn algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term \"classifier\" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.\nTerminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. There is also some argument over whether classification methods that do not involve a statistical model can be considered \"statistical\". Other fields may use different terminology: e.g. in community ecology, the term \"classification\" normally refers to cluster analysis, i.e. a type of unsupervised learning, rather than the supervised learning described in this article.", "links": ["Accelerated failure time model", "Accuracy", "Actuarial science", "Adaptive kernel density estimation", "Akaike information criterion", "Algorithm", "Analysis of covariance", "Analysis of variance", "Anderson\u2013Darling test", "Anomaly detection", "Arithmetic mean", "Artificial intelligence", "Artificial neural network", "Artificial neural networks", "Association rule learning", "Asymptotic theory (statistics)", "Autocorrelation", "Autoencoder", "Autoregressive conditional heteroskedasticity", "Autoregressive\u2013moving-average model", "BIRCH", "Bar chart", "Bayes estimator", "Bayes factor", "Bayesian inference", "Bayesian linear regression", "Bayesian network", "Bayesian probability", "Bias-variance dilemma", "Bias of an estimator", "Binary classification", "Binary data", "Binomial regression", "Bioinformatics", "Biological classification", "Biometric", "Biometrika", "Biostatistics", "Biplot", "Blocking (statistics)", "Blood pressure", "Blood type", "Boosting (machine learning)", "Boosting (meta-algorithm)", "Bootstrap aggregating", "Bootstrapping (statistics)", "Box plot", "Box\u2013Jenkins", "Breusch\u2013Godfrey test", "C. R. Rao", "Canonical correlation analysis", "Cartography", "Categorical data", "Categorical variable", "Census", "Chemometrics", "Chi-square test", "Class membership probabilities", "Classification rule", "Clinical study design", "Clinical trial", "Cluster analysis", "Cluster sampling", "Coefficient of determination", "Coefficient of variation", "Cohen's kappa", "Cointegration", "Community ecology", "Completeness (statistics)", "Compound term processing", "Computational learning theory", "Computer vision", "Conditional random field", "Confidence interval", "Confounding", "Contingency table", "Continuous probability distribution", "Control chart", "Convolutional neural network", "Copula (probability theory)", "Correlation and dependence", "Correlogram", "Count data", "Credible interval", "Credit scoring", "Crime statistics", "Cross-correlation", "DBSCAN", "Data collection", "Data mining", "Data warehouse", "Decision tree learning", "Decomposition of time series", "Deep learning", "Degrees of freedom (statistics)", "Demographic statistics", "Density estimation", "Dependent variable", "Descriptive statistics", "Design of experiments", "Dickey\u2013Fuller test", "Digital object identifier", "Dimensionality reduction", "Discrete choice", "Distance", "Document classification", "Dot product", "Drug development", "Drug discovery", "Durbin\u2013Watson statistic", "Econometrics", "Effect size", "Efficiency (statistics)", "Email", "Empirical distribution function", "Empirical risk minimization", "Engineering statistics", "Ensemble learning", "Environmental statistics", "Epidemiology", "Errors and residuals in statistics", "Estimator", "Expectation-maximization algorithm", "Experiment", "Explanatory variable", "Explanatory variables", "Exponential family", "Exponential smoothing", "F-test", "Factor analysis", "Factorial experiment", "Failure rate", "Fan chart (statistics)", "Feature (pattern recognition)", "Feature engineering", "Feature learning", "Feature space", "Feature vector", "First-hitting-time model", "Fisher's linear discriminant", "Forest plot", "Fourier analysis", "Frequency distribution", "Frequency domain", "Frequentist inference", "Function (mathematics)", "Fuzzy logic", "G-test", "General linear model", "Generalized linear model", "Geographic information system", "Geometric mean", "Geostatistics", "Goodness of fit", "Grammar induction", "Granger causality", "Graphical model", "Grouped data", "Handwriting recognition", "Harmonic mean", "Heteroscedasticity", "Hidden Markov model", "Hierarchical clustering", "Histogram", "Homoscedasticity", "Independent component analysis", "Independent variable", "Index of dispersion", "Information retrieval", "Integer", "International Standard Book Number", "Interquartile range", "Isotonic regression", "Jarque\u2013Bera test", "Johansen test", "K-means clustering", "K-nearest neighbor algorithm", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Kaplan\u2013Meier estimator", "Kendall tau rank correlation coefficient", "Kolmogorov\u2013Smirnov test", "Kriging", "Kruskal\u2013Wallis one-way analysis of variance", "Kurtosis", "L-moment", "Learning to rank", "Learning vector quantization", "Least squares support vector machine", "Likelihood-ratio test", "Linear", "Linear classifier", "Linear combination", "Linear discriminant analysis", "Linear function", "Linear predictor function", "Linear regression", "List of fields of application of statistics", "List of statistics articles", "Ljung\u2013Box test", "Local outlier factor", "Location parameter", "Log-rank test", "Logistic regression", "Machine learning", "Mahalanobis distance", "Mann\u2013Whitney U test", "Markov chain Monte Carlo", "Maximum a posteriori estimation", "Maximum likelihood", "McNemar's test", "Mean", "Mean-shift", "Median", "Median-unbiased estimator", "Medical imaging", "Medical statistics", "Method of moments (statistics)", "Methods engineering", "Metric (mathematics)", "Minimum-variance unbiased estimator", "Minimum distance estimation", "Mixed model", "Mode (statistics)", "Moment (mathematics)", "Multiclass classification", "Multilayer perceptron", "Multinomial logistic regression", "Multivariate adaptive regression splines", "Multivariate analysis of variance", "Multivariate normal distribution", "Multivariate statistics", "Naive Bayes classifier", "National accounts", "Natural experiment", "Nelson\u2013Aalen estimator", "No free lunch in search and optimization", "Non-negative matrix factorization", "Nonlinear", "Nonlinear regression", "Nonparametric regression", "OPTICS algorithm", "Observation", "Observational study", "Official statistics", "Online machine learning", "Opinion poll", "Optical character recognition", "Optimal design", "Order statistic", "Ordinal data", "Ordinary least squares", "Outline of statistics", "Parametric statistics", "Parse tree", "Parsing", "Part of speech", "Part of speech tagging", "Partial autocorrelation function", "Partial correlation", "Partition of sums of squares", "Pattern recognition", "Pearson product-moment correlation coefficient", "Percentile", "Perceptron", "Permutation test", "Pie chart", "Poisson regression", "Posterior probability", "Power (statistics)", "Precision and recall", "Principal component analysis", "Prior probability", "Probabilistic classification", "Probabilistic design", "Probability", "Probably approximately correct learning", "Probit regression", "Proportional hazards model", "Psychometrics", "Quadratic classifier", "Quality control", "Quantitative structure-activity relationship", "Quasi-experiment", "Questionnaire", "Q\u2013Q plot", "R. A. Fisher", "Radar chart", "Random assignment", "Random forest", "Randomization test", "Randomized controlled trial", "Randomized experiment", "Range (statistics)", "Rank correlation", "Rank statistics", "Real number", "Receiver operating characteristic", "Recommender system", "Record value", "Recurrent neural network", "Regression analysis", "Regression model validation", "Reinforcement learning", "Relevance vector machine", "Reliability engineering", "Replication (statistics)", "Restricted Boltzmann machine", "Robust regression", "Robust statistics", "Run chart", "Sample size determination", "Sampling (statistics)", "Sampling distribution", "Scan statistic", "Scatter plot", "Scientific control", "Score test", "Search engines", "Seasonal adjustment", "Self-organizing map", "Semi-supervised learning", "Semiparametric regression", "Sequence labeling", "Shape of the distribution", "Shapiro\u2013Wilk test", "Similarity function", "Simple linear regression", "Simultaneous equations model", "Skewness", "Social statistics", "Spam filtering", "Spatial analysis", "Spearman's rank correlation coefficient", "Spectral density estimation", "Speech recognition", "Standard deviation", "Standard error", "Stationary process", "Statistical dispersion", "Statistical graphics", "Statistical hypothesis testing", "Statistical inference", "Statistical learning theory", "Statistical model", "Statistical natural language processing", "Statistical population", "Statistical power", "Statistical process control", "Statistical theory", "Statistically independent", "Statistics", "Stem-and-leaf display", "Stratified sampling", "Structural break", "Structured prediction", "Student's t-test", "Sufficient statistic", "Supervised learning", "Support vector machine", "Survey methodology", "Survival analysis", "Survival function", "Syntactic structure", "System identification", "T-distributed stochastic neighbor embedding", "T. W. Anderson", "Time domain", "Time series", "Toxicogenomics", "Training set", "Trend estimation", "U-statistic", "Uncertainty coefficient", "University of Leicester", "Unsupervised learning", "Utility", "Vapnik\u2013Chervonenkis theory", "Variable kernel density estimation", "Variance", "Vector autoregression", "Vector space", "Video tracking", "Wald test", "Wavelet", "Wilcoxon signed-rank test", "Z-test"], "categories": ["All articles lacking in-text citations", "All articles with unsourced statements", "All pages needing cleanup", "Articles lacking in-text citations from January 2010", "Articles needing cleanup from May 2012", "Articles with sections that need to be turned into prose from May 2012", "Articles with unsourced statements from August 2014", "Classification algorithms", "Machine learning", "Statistical classification"], "title": "Statistical classification"}
{"summary": "AdaBoost, short for \"Adaptive Boosting\", is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire who won the G\u00f6del Prize in 2003 for their work. It can be used in conjunction with many other types of learning algorithms to improve their performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers. In some problems, however, it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing (i.e., their error rate is smaller than 0.5 for binary classification), the final model can be proven to converge to a strong learner\nWhile every learning algorithm will tend to suit some problem types better than others, and will typically have many different parameters and configurations to be adjusted before achieving optimal performance on a dataset, AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier. When used with decision tree learning, information gathered at each stage of the AdaBoost algorithm about the relative 'hardness' of each training sample is fed into the tree growing algorithm such that later trees tend to focus on harder to classify examples.", "links": ["Backfitting algorithm", "Boosting (meta-algorithm)", "Bootstrap aggregating", "BrownBoost", "CiteSeer", "CoBoosting", "Continuously differentiable function", "Convex function", "Convex set", "Curse of dimensionality", "Decision tree learning", "Digital object identifier", "Early stopping", "Gradient boosting", "Gradient descent", "G\u00f6del Prize", "Haar-like features", "Huber loss function", "International Standard Book Number", "LPBoost", "Least squares", "Linear programming", "Logistic regression", "Logit", "LogitBoost", "Machine learning", "Meta-algorithm", "Monotonic function", "Neural network", "Newton's method", "Newton-Raphson", "Numerical instability", "Outlier", "Overfitting", "Overfitting (machine learning)", "Regression analysis", "Robert Schapire", "Smoothing spline", "Support vector machine", "Viola\u2013Jones object detection framework", "Yoav Freund"], "categories": ["Classification algorithms", "Ensemble learning"], "title": "AdaBoost"}
{"summary": "ALOPEX (an acronym from \"ALgorithms Of Pattern EXtraction\") is a correlation based machine learning algorithm first proposed by Tzanakou and Harth in 1974.", "links": ["Artificial intelligence", "Backpropagation", "Evangelia Micheli-Tzanakou", "Machine learning"], "categories": ["All stub articles", "Artificial intelligence stubs", "Artificial neural networks", "Classification algorithms"], "title": "ALOPEX"}
{"summary": "Boosting is a machine learning ensemble meta-algorithm for reducing bias primarily and also variance in supervised learning, and a family of machine learning algorithms which convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): Can a set of weak learners create a single strong learner? A weak learner is defined to be a classifier which is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.\nRobert Schapire's affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.\nWhen first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. \"Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm [\u2026] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner].\" Algorithms that achieve hypothesis boosting quickly became simply known as \"boosting\". Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.", "links": ["AdaBoost", "Alternating decision tree", "AnyBoost", "Boost by majority", "Boosting methods for object categorization", "Bootstrap aggregating", "BrownBoost", "Cascading classifiers", "CiteSeer", "CoBoosting", "Convex function", "Cross-validation (statistics)", "Digital object identifier", "Ensemble learning", "Function space", "GentleBoost", "Gradient boosting", "Gradient descent", "G\u00f6del Prize", "International Standard Book Number", "LPBoost", "Leo Breiman", "Leslie Valiant", "Logistic regression", "LogitBoost", "Machine learning", "MadaBoost", "Margin classifier", "Meta-algorithm", "Michael Kearns", "Michael Kearns (computer scientist)", "Neural network", "Orange (software)", "Principle of maximum entropy", "Random forest", "RankBoost", "Robert Schapire", "Scikit-learn", "Statistics", "Supervised learning", "Support vector machine", "TotalBoost", "Weka (machine learning)", "Yoav Freund", "Zhou Zhihua"], "categories": ["All articles to be expanded", "All articles to be merged", "Articles to be expanded from December 2009", "Articles to be merged from December 2012", "Classification algorithms", "Ensemble learning"], "title": "Boosting (machine learning)"}
{"summary": "Boruta is an algorithm in the field of machine-learning, and more specifically, a feature-selection algorithm. The aim of the algorithm as presented in the original paper describing it is to find all relevant features (compare with minimal-optimal features set). The Boruta algorithm is not a stand-alone algorithm, but is implemented as a wrapper algorithm around the random-forest classification algorithm. In its essence, Boruta works in an iterative manner, and in each iteration the aim is to remove features which according to a statistical test, are less relevant than what is defined by the authors as a random probe. One of the fundamental components of Boruta is the use of shadow attributes. Shadow attributes are pseudo-features that are added to the information system, and produced by taking existing features from the original data-set and shuffling the values of those features between the original samples (data points). After generating the shadow attributes the procedure proceeds with building random-forest trees and comparing the Z-scores obtained by original features to Z-scores obtained by the shadow attributes. This comparison is the foundation for Boruta to decide whether a feature is important or not.\n\n High level pseudo-code:\n\n1.  Copy all variables (features)\n2.  Shuffle values in each feature\n3.  Run random-forest on the extended system (shuffled features), gather Z scores\n4.  Find maximum MSZA (max Z-score among shadow attributes)\n5.  Run random-forest on original features\n6.  Assign each original feature a hit if feature Z-score > MSZA\n7.  If Z-score <= MSZA, perform two-side equality test against MSZA\n8.  If Z-score < MSZA significantly, drop feature as unimportant\n9.  If Z-score > MSZA significantly, keep feature as important\n10. Repeat from step 5 until all importance is determined for all features or max RF runs have been reached", "links": ["Algorithm", "Feature (machine learning)", "Feature selection", "Machine learning", "Random forest", "Statistical classification", "Statistical significance", "Statistical test", "Wrapper (data mining)", "Z-scores"], "categories": ["All articles needing additional references", "All articles with topics of unclear notability", "Articles needing additional references from March 2015", "Articles needing additional references from September 2014", "Articles with topics of unclear notability from March 2015", "Classification algorithms", "Decision trees", "Ensemble learning"], "title": "Boruta (algorithm)"}
{"summary": "BrownBoost is a boosting algorithm that may be robust to noisy datasets. BrownBoost is an adaptive version of the boost by majority algorithm. As is true for all boosting algorithms, BrownBoost is used in conjunction with other machine learning methods. BrownBoost was introduced by Yoav Freund in 2001.", "links": ["AdaBoost", "Alternating decision tree", "AnyBoost", "Boost by majority", "Boosting (machine learning)", "Boosting (meta-algorithm)", "Generalization error", "JBoost", "LogitBoost", "Machine learning", "Newton's method", "Yoav Freund"], "categories": ["Classification algorithms", "Ensemble learning"], "title": "BrownBoost"}
{"summary": "C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier.\nIt became quite popular after ranking #1 in the Top 10 Algorithms in Data Mining pre-eminent paper published by Springer LNCS in 2008.", "links": ["Boosting (meta-algorithm)", "Data mining", "Decision tree learning", "Entropy (information theory)", "ID3 algorithm", "Information gain", "Java (programming language)", "Lecture Notes in Computer Science", "Open source", "Pseudocode", "Ross Quinlan", "Springer Science+Business Media", "Statistical classification", "Weka (machine learning)", "Winnow (algorithm)"], "categories": ["All NPOV disputes", "All articles lacking in-text citations", "Articles lacking in-text citations from July 2008", "Classification algorithms", "Decision trees", "NPOV disputes from August 2011"], "title": "C4.5 algorithm"}
{"summary": "Sukhotin's algorithm (introduced by Boris V. Sukhotin) is a statistical classification algorithm for classifying characters in a text as vowels or consonants. It may also be of use in some of substitution ciphers and has been considered in deciphering the Voynich manuscript, though one problem is to agree on the set of symbols the manuscript is written in.", "links": ["Algorithm", "Computational linguistics", "Consonant", "Cryptanalysis", "Statistical classification", "Substitution cipher", "Vowel", "Voynich manuscript"], "categories": ["Classification algorithms", "Natural language processing"], "title": "Sukhotin's algorithm"}
{"summary": "Co-training is a machine learning algorithm used when there are only small amounts of labeled data and large amounts of unlabeled data. One of its uses is in text mining for search engines. It was introduced by Avrim Blum and Tom Mitchell in 1998.", "links": ["Avrim Blum", "CiteSeer", "Computer science", "Conditionally independent", "Digital object identifier", "Functional genomics", "Hyperlinks", "ICML", "International Conference on Machine Learning", "International Standard Book Number", "International Standard Serial Number", "Machine learning", "PubMed Central", "PubMed Identifier", "Search engines", "Semi-supervised learning", "Tag (metadata)", "Training set"], "categories": ["Classification algorithms"], "title": "Co-training"}
{"summary": "CoBoost is a semi-supervised training algorithm proposed by Collins and Singer in 1999. The original application for the algorithm was the task of Named Entity Classification using very weak learners. It can be used for performing semi-supervised learning in cases in which there exist redundancy in features.\nIt may be seen as a combination of co-training and boosting. Each example is available in two views (subsections of the feature set), and boosting is applied iteratively in alternation with each view using predicted labels produced in the alternate view on the previous iteration. CoBoosting is not a valid boosting algorithm in the PAC learning sense.\n\n", "links": ["AdaBoost", "Boosting (meta-algorithm)", "Co-training", "Named-entity recognition"], "categories": ["Classification algorithms"], "title": "CoBoosting"}
{"summary": "Compositional pattern-producing networks (CPPNs), are a variation of artificial neural networks (ANNs) which differ in their set of activation functions and how they are applied.\nWhile ANNs often contain only sigmoid functions and sometimes Gaussian functions, CPPNs can include both types of functions and many others. The choice of functions for the canonical set can be biased toward specific types of patterns and regularities. For example, periodic functions such as sine produce segmented patterns with repetitions, while symmetric functions such as Gaussian produce symmetric patterns. Linear functions can be employed to produce linear or fractal-like patterns. Thus, the architect of a CPPN-based genetic art system can bias the types of patterns it generates by deciding the set of canonical functions to include.\nFurthermore, unlike typical ANNs, CPPNs are applied across the entire space of possible inputs so that they can represent a complete image. Since they are compositions of functions, CPPNs in effect encode images at infinite resolution and can be sampled for a particular display at whatever resolution is optimal.\nCPPNs can be evolved through neuroevolution techniques such as NeuroEvolution of Augmenting Topologies (called CPPN-NEAT).\nCPPNs have been shown to be a very powerful encoding when evolving the following:\nNeural Networks, via the HyperNEAT algorithm\n2D images, on \"PicBreeder.org\"\n3D objects, on \"EndlessForms.com\"\nRobot Morphologies Rigid Robots Soft Robots", "links": ["Artificial neural network", "Evolutionary art", "Fractal", "Gaussian function", "HyperNEAT", "Interactive evolutionary computation", "NeuroEvolution of Augmenting Topologies", "Neuroevolution", "Sigmoid function", "Sine"], "categories": ["Artificial neural networks", "Classification algorithms"], "title": "Compositional pattern-producing network"}
{"summary": "In a statistical-classification problem with two classes, a decision boundary or decision surface is a hypersurface that partitions the underlying vector space into two sets, one for each class. The classifier will classify all the points on one side of the decision boundary as belonging to one class and all those on the other side as belonging to the other class.\nA decision boundary is the region of a problem space in which the output label of a classifier is ambiguous.\nIf the decision surface is a hyperplane, then the classification problem is linear, and the classes are linearly separable.\nDecision boundaries are not always clear cut. That is, the transition from one class in the feature space to another is not discontinuous, but gradual. This effect is common in fuzzy logic based classification algorithms, where membership in one class or another is ambiguous.", "links": ["Artificial intelligence", "Artificial neural network", "Backpropagation", "Hyperplane", "Hypersurface", "Kernel trick", "Linearly separable", "Maximum-margin hyperplane", "Pattern recognition", "Perceptron", "Statistical classification", "Support vector machine", "Vector space"], "categories": ["All articles needing additional references", "All stub articles", "Articles needing additional references from September 2014", "Artificial intelligence stubs", "Classification algorithms", "Pattern recognition", "Statistical classification"], "title": "Decision boundary"}
{"summary": "The generalization error of a machine learning model is a function that measures how well a learning machine generalizes to unseen data. It is measured as the distance between the error on the training set and the test set and is averaged over the entire set of possible training data that can be generated after each iteration of the learning process. It has this name because this function indicates the capacity of a machine that learns with the specified algorithm to infer a rule (or generalize) that is used by the teacher machine to generate data based only on a few examples.\nThe theoretical model assumes a probability distribution of the examples, and a function giving the exact target. The model can also include noise in the example (in the input and/or target output). The generalization error is usually defined as the expected value of the square of the difference between the learned function and the exact target (mean-square error). In practical cases, the distribution and target are unknown; statistical estimates are used.\nThe performance of a machine learning algorithm is measured by plots of the generalization error values through the learning process and are called learning curve.\nThe generalization error of a perceptron is the probability of the student perceptron to classify an example differently from the teacher and is given by the overlap of the student and teacher synaptic vectors and is a function of their scalar product.", "links": ["Algorithm", "Bias-variance dilemma", "Function (mathematics)", "Generalization (logic)", "Hasty generalization", "Learning curve", "Machine learning", "Perceptron", "Problem of induction", "Scalar product", "Synaptic vectors"], "categories": ["All stub articles", "Classification algorithms", "Robotics stubs"], "title": "Generalization error"}
{"summary": "Group method of data handling (GMDH) is a family of inductive algorithms for computer-based mathematical modeling of multi-parametric datasets that features fully automatic structural and parametric optimization of models.\nGMDH is used in such fields as data mining, knowledge discovery, prediction, complex systems modeling, optimization and pattern recognition.\nGMDH algorithms are characterized by inductive procedure that performs sorting-out of gradually complicated polynomial models and selecting the best solution by means of the so-called external criterion.\nA GMDH model with multiple inputs and one output is a subset of components of the base function (1):\n\nwhere f are elementary functions dependent on different sets of inputs, a are coefficients and m is the number of the base function components.\nIn order to find the best solution GMDH algorithms consider various component subsets of the base function (1) called partial models. Coefficients of these models are estimated by the least squares method. GMDH algorithms gradually increase the number of partial model components and find a model structure with optimal complexity indicated by the minimum value of an external criterion. This process is called self-organization of models.\nThe most popular base function used in GMDH is the gradually complicated Kolmogorov-Gabor polynomial (2):\n\nThe resulting models are also known as polynomial neural networks. J\u00fcrgen Schmidhuber cites GDMH as one of the earliest deep learning methods, remarking that it was used to train eight-layer neural nets as early as 1971.", "links": ["Alexey Grigorevich Ivakhnenko", "ArXiv", "Artificial Neural Network", "Complex systems", "Data mining", "Deep learning", "Forecasting", "J\u00fcrgen Schmidhuber", "Kiev", "Knowledge discovery", "Least squares", "Optimization (mathematics)", "Pattern recognition", "Shannon's Theorem", "Ukrainian Soviet Socialist Republic", "Weka (machine learning)"], "categories": ["All articles with failed verification", "Articles with failed verification from July 2015", "Artificial neural networks", "Classification algorithms", "Computational statistics", "Regression variable selection"], "title": "Group method of data handling"}
{"summary": "In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domains.", "links": ["Algorithm", "Backtracking", "C4.5 algorithm", "Classification and regression tree", "Decision tree learning", "Entropy (information theory)", "Information gain in decision trees", "Machine learning", "Natural language processing", "Overfitting", "Ross Quinlan"], "categories": ["Articles with example pseudocode", "Classification algorithms", "Decision trees"], "title": "ID3 algorithm"}
{"summary": "Info Fuzzy Networks(IFN) is a greedy machine learning algorithm for supervised learning. The data structure produced by the learning algorithm is also called Info Fuzzy Network. IFN construction is quite similar to decision trees' construction. However, IFN constructs a directed graph and not a tree. IFN also uses the conditional mutual information metric in order to choose features during the construction stage while decision trees usually use other metrics like entropy or gini.", "links": ["Abraham Kandel", "Accuracy", "Algorithm", "Anomaly detection", "Association rules", "C4.5", "Conditional mutual information", "Data structure", "Decision tree", "Decision tree learning", "Directed edge", "Directed graph", "Discretization of continuous features", "Entropy (information theory)", "Feature selection", "Gini coefficient", "Greedy algorithm", "Likelihood ratio test", "Machine learning", "Majority rule", "Mark Last", "Statistical classification", "Statistical significance", "Supervised learning", "Tree (data structure)"], "categories": ["All articles with topics of unclear notability", "All orphaned articles", "Articles with topics of unclear notability from May 2010", "Classification algorithms", "Orphaned articles from May 2010"], "title": "Information Fuzzy Networks"}
{"summary": "In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\nIn k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n\nIn k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\n\nk-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.\nBoth for classification and regression, it can be useful to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.\nThe neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\nA shortcoming of the k-NN algorithm is that it is sensitive to the local structure of the data. The algorithm has nothing to do with and is not to be confused with k-means, another popular machine learning technique.", "links": ["Analytica Chimica Acta", "Annals of Statistics", "Anomaly detection", "Artificial neural network", "Association rule learning", "Autoencoder", "BIRCH", "Bayes error rate", "Bayesian network", "Belur V. Dasarathy", "Bias-variance dilemma", "Boosting (machine learning)", "Bootstrap aggregating", "Canonical correlation", "Canonical correlation analysis", "Closest pair of points problem", "Cluster analysis", "Computational learning theory", "Computer vision", "Conditional random field", "Confusion matrix", "Consistency (statistics)", "Continuous variable", "Convolutional neural network", "Curse of Dimensionality", "DBSCAN", "Data mining", "Decision boundary", "Decision tree learning", "Deep learning", "Digital object identifier", "Dimension reduction", "Dimensionality reduction", "Embedding", "Empirical risk minimization", "Ensemble learning", "Euclidean distance", "Evolutionary algorithm", "Expectation-maximization algorithm", "Facial recognition system", "Factor analysis", "Feature (machine learning)", "Feature engineering", "Feature extraction", "Feature learning", "Feature scaling", "Feature selection", "Feature space", "Feature vector", "Forest Ecology and Management", "GPU", "Grammar induction", "Graphical model", "Haar wavelet", "Hamming distance", "Heuristic (computer science)", "Hidden Markov model", "Hierarchical clustering", "Hyperparameter optimization", "Independent component analysis", "Instance-based learning", "Integer", "International Standard Book Number", "K-means", "K-means clustering", "K-nearest neighbors classification", "Kernel (statistics)", "Large Margin Nearest Neighbor", "Large margin nearest neighbor", "Lazy learning", "Learning to rank", "Likelihood-ratio test", "Linear discriminant analysis", "Linear regression", "Local outlier factor", "Locality Sensitive Hashing", "Logistic regression", "MIT Press", "Machine learning", "Mahalanobis distance", "Mean-shift", "Metric (mathematics)", "Multilayer perceptron", "Mutual information", "Naive Bayes classifier", "Nearest centroid classifier", "Nearest neighbor search", "Neighbourhood components analysis", "Non-negative matrix factorization", "Non-parametric statistics", "OPTICS algorithm", "Online machine learning", "OpenCV", "Pattern recognition", "Perceptron", "Peter E. Hart", "Principal Component Analysis", "Principal component analysis", "Probably approximately correct learning", "Pseudo-metric", "PubMed Identifier", "RMSE", "Random forest", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Self-organizing map", "Semi-supervised learning", "Statistical classification", "Statistical learning theory", "Structured prediction", "Supervised learning", "Support vector machine", "T-distributed stochastic neighbor embedding", "Thomas M. Cover", "Time series", "Unsupervised learning", "VLDB", "Vapnik\u2013Chervonenkis theory", "Variable kernel density estimation"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from December 2008", "Articles with unsourced statements from March 2013", "Classification algorithms", "Machine learning algorithms", "Search algorithms"], "title": "K-nearest neighbors algorithm"}
{"summary": "In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over pairs of data points in raw representation.\nKernel methods owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the \"kernel trick\". Kernel functions have been introduced for sequence data, graphs, text, images, as well as vectors.\nAlgorithms capable of operating with kernels include the kernel perceptron, support vector machines (SVM), Gaussian processes, principal components analysis (PCA), canonical correlation analysis, ridge regression, spectral clustering, linear adaptive filters and many others. Any linear model can be turned into a non-linear model by applying the kernel trick to the model: replacing its features (predictors) by a kernel function.\nMost kernel algorithms are based on convex optimization or eigenproblems and are statistically well-founded. Typically, their statistical properties are analyzed using statistical learning theory (for example, using Rademacher complexity).", "links": ["3D reconstruction", "Adaptive filter", "Anomaly detection", "Artificial neural network", "Association rule learning", "Autoencoder", "BIRCH", "Bayesian network", "Bias-variance dilemma", "Binary classifier", "Bioinformatics", "Boosting (machine learning)", "Bootstrap aggregating", "Canonical correlation analysis", "Chemoinformatics", "CiteSeer", "Cluster analysis", "Computational learning theory", "Conditional random field", "Convex optimization", "Convolutional neural network", "Correlation", "Counting measure", "Covariance function", "Covariance matrix", "DBSCAN", "Data mining", "Decision boundary", "Decision tree learning", "Deep learning", "Digital object identifier", "Dimensionality reduction", "Eigenvalue, eigenvector and eigenspace", "Empirical risk minimization", "Ensemble learning", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Feature vector", "Fisher kernel", "Gaussian process", "Gaussian processes", "Geostatistics", "Gram matrix", "Grammar induction", "Graph kernel", "Graphical model", "Handwriting recognition", "Hidden Markov model", "Hierarchical clustering", "Independent component analysis", "Information extraction", "Inner product", "Inner product space", "Instance-based learning", "Integral", "International Standard Book Number", "Inverse distance weighting", "John Shawe-Taylor", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Kernel function", "Kernel methods for vector output", "Kernel perceptron", "Kernel regression", "Kernel smoothing", "Kriging", "Learning algorithms", "Learning to rank", "Linear discriminant analysis", "Linear model", "Linear regression", "Local outlier factor", "Logistic regression", "Machine learning", "Mean-shift", "Measure (mathematics)", "Mehryar Mohri", "Mercer's condition", "Mercer's theorem", "Multilayer perceptron", "Naive Bayes classifier", "Nello Cristianini", "Non-negative matrix factorization", "OPTICS algorithm", "Online machine learning", "Pattern analysis", "Perceptron", "Polynomial kernel", "Positive-definite kernel", "Positive-definite matrix", "Positive definite kernel", "Principal component analysis", "Principal components", "Principal components analysis", "Probably approximately correct learning", "RBF kernel", "Rademacher complexity", "Random forest", "Ranking", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Ridge regression", "Self-organizing map", "Semi-supervised learning", "Sign function", "Similarity function", "Spectral clustering", "Statistical classification", "Statistical learning theory", "String kernel", "Structured prediction", "Supervised learning", "Support vector machine", "Support vector machines", "T-distributed stochastic neighbor embedding", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from January 2011", "Classification algorithms", "Geostatistics", "Kernel methods for machine learning"], "title": "Kernel method"}
{"summary": "Large margin nearest neighbor (LMNN) classification is a statistical machine learning algorithm. It learns a Pseudometric designed for k-nearest neighbor classification. The algorithm is based on semidefinite programming, a sub-class of convex optimization.\nThe goal of supervised learning (more specifically classification) is to learn a decision rule that can categorize data instances into pre-defined classes. The k-nearest neighbor rule assumes a training data set of labeled instances (i.e. the classes are known). It classifies a new data instance with the class obtained from the majority vote of the k closest (labeled) training instances. Closeness is measured with a pre-defined metric. Large Margin Nearest Neighbors is an algorithm that learns this global (pseudo-)metric in a supervised fashion to improve the classification accuracy of the k-nearest neighbor rule.", "links": ["Algorithm", "Classification (machine learning)", "Cluster analysis", "Convex optimization", "Cross-validation (statistics)", "Data mining", "Dimension reduction", "Ellipsoid", "Journal of Machine Learning Research", "K-nearest neighbor", "Learning Vector Quantization", "Leave-one-out", "Linear discriminant analysis", "Machine learning", "Mahalanobis metric", "Matlab", "Metric (mathematics)", "Nearest neighbor search", "Neighbourhood components analysis", "Open source", "Pattern recognition", "Polynomial transformations", "Positive semi-definite", "Predictive analytics", "Proceedings of International Conference on Machine Learning", "Pseudometric", "Pseudometric space", "Semidefinite programming", "Similarity learning", "Slack variable", "Supervised learning", "Working set"], "categories": ["Classification algorithms", "Machine learning"], "title": "Large margin nearest neighbor"}
{"summary": "In computer science, learning vector quantization (LVQ), is a prototype-based supervised classification algorithm. LVQ is the supervised counterpart of vector quantization systems.", "links": ["Algorithm", "Artificial neural network", "Computer science", "Euclidean distance", "Feature space", "Hebbian learning", "K-Nearest Neighbor algorithm", "Neural gas", "Prototype", "Self-organizing map", "Statistical classification", "Supervised learning", "Teuvo Kohonen", "Vector quantization", "Vector space", "Winner-take-all (computing)"], "categories": ["Artificial neural networks", "Classification algorithms"], "title": "Learning vector quantization"}
{"summary": "In machine learning and computational learning theory, LogitBoost is a boosting algorithm formulated by Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The original paper casts the AdaBoost algorithm into a statistical framework. Specifically, if one considers AdaBoost as a generalized additive model and then applies the cost functional of logistic regression, one can derive the LogitBoost algorithm.", "links": ["AdaBoost", "Artificial intelligence", "Boosting (meta-algorithm)", "Computational learning theory", "Convex optimization", "Generalized additive model", "Gradient boosting", "Jerome H. Friedman", "Logistic model tree", "Logistic regression", "Machine learning", "Robert Tibshirani", "Trevor Hastie"], "categories": ["All stub articles", "Artificial intelligence stubs", "Classification algorithms", "Ensemble learning", "Machine learning algorithms"], "title": "LogitBoost"}
{"summary": "In machine learning, a margin classifier is a classifier which is able to give an associated distance from the decision boundary for each example. For instance, if a linear classifier (e.g. perceptron or linear discriminant analysis) is used, the distance (typically euclidean distance, though others may be used) of an example from the separating hyperplane is the margin of that example.\nThe notion of margin is important in several machine learning classification algorithms, as it can be used to bound the generalization error of the classifier. These bounds are frequently shown using the VC dimension. Of particular prominence is the generalization error bound on boosting algorithms and support vector machines.", "links": ["AdaBoost", "AnyBoost", "Boosting (machine learning)", "Boosting (meta-algorithm)", "BrownBoost", "Error bound", "Euclidean distance", "Generalization error", "Linear classifier", "Linear discriminant analysis", "LogitBoost", "Machine learning", "Maximum-margin hyperplane", "Perceptron", "Statistical classification", "Support vector machine", "VC dimension", "Voted-perceptron"], "categories": ["Classification algorithms", "Statistical classification"], "title": "Margin classifier"}
{"summary": "Margin-infused relaxed algorithm (MIRA) is a machine learning algorithm, an online algorithm for multiclass classification problems. It is designed to learn a set of parameters (vector or matrix) by processing all the given training examples one-by-one and updating the parameters according to each training example, so that the current training example is classified correctly with a margin against incorrect classifications at least as large as their loss. The change of the parameters is kept as small as possible.\nA two-class version called binary MIRA simplifies the algorithm by not requiring the solution of a quadratic programming problem (see below). When used in a one-vs.-all configuration, binary MIRA can be extended to a multiclass learner that approximates full MIRA, but may be faster to train.\nThe flow of the algorithm looks as follows:\n\nThe update step is then formalized as a quadratic programming problem: Find , so that , i.e. the score of the current correct training  must be greater than the score of any other possible  by at least the loss (number of errors) of that  in comparison to .", "links": ["Apache Mahout", "Association for Computational Linguistics", "Hadoop", "Journal of Machine Learning Research", "Machine learning", "Margin (machine learning)", "Multiclass classification", "One-vs.-all", "Online algorithm", "Quadratic programming"], "categories": ["Classification algorithms"], "title": "Margin Infused Relaxed Algorithm"}
{"summary": "In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple target labels must be assigned to each instance. Multi-label classification should not be confused with multiclass classification, which is the problem of categorizing instances into one of more than two classes. Formally, multi-label learning can be phrased as the problem of finding a model that maps inputs x to binary vectors y, rather than scalar outputs as in the ordinary classification problem.\nThere are two main methods for tackling the multi-label classification problem: problem transformation methods and algorithm adaptation methods. Problem transformation methods transform the multi-label problem into a set of binary classification problems, which can then be handled using single-class classifiers. Algorithm adaptation methods adapt the algorithms to directly perform multi-label classification. In other words, rather than trying to convert the problem to a simpler problem, they try to address the problem in its full form.", "links": ["AdaBoost", "Binary classification", "Boosting (machine learning)", "Classifier chains", "Decision trees", "Digital object identifier", "ECML PKDD", "Ensemble learning", "F1 score", "HIV", "Hamming distance", "Harmonic mean", "K-nearest neighbors", "Kernel methods for vector output", "Loss function", "Machine learning", "Multiclass classification", "Multiple-instance learning", "Neural networks", "Precision and recall", "PubMed Identifier", "RAKEL", "Rachel (given name)", "Scikit-learn", "Statistical classification", "Stratified sampling", "Weka (machine learning)"], "categories": ["Classification algorithms"], "title": "Multi-label classification"}
{"summary": "Not to be confused with multi-label classification.\nIn machine learning, multiclass or multinomial classification is the problem of classifying instances into one of the more than two classes (classifying instances into one of the two classes is called binary classification).\nWhile some classification algorithms naturally permit the use of more than two classes, others are by nature binary algorithms; these can, however, be turned into multinomial classifiers by a variety of strategies.\nMulticlass classification should not be confused with multi-label classification, where multiple labels are to be predicted for each instance.", "links": ["Artificial intelligence", "Binary classification", "Heuristic", "Machine learning", "Multi-label classification", "One-class classification", "Perceptron", "Statistical classification"], "categories": ["All stub articles", "Artificial intelligence stubs", "Classification algorithms", "Statistical classification"], "title": "Multiclass classification"}
{"summary": "Multifactor dimensionality reduction (MDR) is a data mining approach for detecting and characterizing combinations of attributes or independent variables that interact to influence a dependent or class variable. MDR was designed specifically to identify interactions among discrete variables that influence a binary outcome and is considered a nonparametric alternative to traditional statistical methods such as logistic regression.\nThe basis of the MDR method is a constructive induction algorithm that converts two or more variables or attributes to a single attribute. This process of constructing a new attribute changes the representation space of the data. The end goal is to create or discover a representation that facilitates the detection of nonlinear or nonadditive interactions among the attributes such that prediction of the class variable is improved over that of the original representation of the data.", "links": ["Atrial fibrillation", "Attribute (computing)", "Autism", "Binary numeral system", "Bladder cancer", "Breast cancer", "Cardiovascular disease", "Constructive induction", "Cross-validation (statistics)", "Data Mining", "Data mining", "Decision tree learning", "Dimensionality reduction", "Discrete random variable", "Economics", "Engineering", "Epistasis", "Exclusive OR", "Hypertension", "Independent variable", "Interaction", "Logical operator", "Logistic regression", "Machine Learning", "Machine learning", "Meteorology", "Model validation", "Multilinear Subspace Learning", "Naive Bayes classifier", "Neural networks", "Nonlinear", "Nonparametric", "Open-source", "Overfitting", "P-value", "Prostate cancer", "Resampling (statistics)", "Schizophrenia", "Type II diabetes"], "categories": ["All articles lacking in-text citations", "All articles with unsourced statements", "Articles lacking in-text citations from November 2010", "Articles with unsourced statements from December 2010", "Classification algorithms", "Data mining", "Dimension reduction", "Use dmy dates from December 2010"], "title": "Multifactor dimensionality reduction"}
{"summary": "Multispectral remote sensing is the collection and analysis of reflected, emitted, or back-scattered energy from an object or an area of interest in multiple bands of regions of the electromagnetic spectrum (Jensen, 2005). Subcategories of multispectral remote sensing include hyperspectral, in which hundreds of bands are collected and analyzed, and ultraspectral remote sensing where many hundreds of bands are used (Logicon, 1997). The main purpose of multispectral imaging is the potential to classify the image using multispectral classification. This is a much faster method of image analysis than is possible by human interpretation.\nThe Iterative Self-Organizing Data Analysis Technique (ISODATA) algorithm used for Multispectral pattern recognition was developed by Geoffrey H. Ball and David J. Hall, working in the Stanford Research Institute in Menlo Park, CA. They published their findings in a technical report entitled: ISODATA, a novel method of data analysis and pattern classification (Stanford Research Institute, 1965). ISODATA is defined in the abstract as: 'a novel method of data analysis and pattern classification, is described in verbal and pictorial terms, in terms of a two-dimensional example, and by giving the mathematical calculations that the method uses. The technique clusters many-variable data around points in the data's original high- dimensional space and by doing so provides a useful description of the data.' (1965, pp v.)ISODATA was developed to facilitate the modelling and tracking of weather patterns.", "links": ["AVHRR", "AVIRIS", "Advanced Spaceborne Thermal Emission and Reflection Radiometer", "Arithmetic mean", "Bar graph", "CASI 3", "Correlation matrix", "EROS (satellite)", "Earth Observing-1", "Electromagnetic spectrum", "GOES", "GeoEye", "IKONOS", "Indian Remote Sensing satellite", "International Space Station", "Logicon", "MISR", "MODIS", "Multispectral Scanner", "Pixel", "QuickBird", "Remote sensing", "Rule-of-thumb", "SPOT (satellites)", "SeaWiFS", "Space Shuttle", "Standard deviation", "Thematic Mapper", "Variance-covariance matrix"], "categories": ["Classification algorithms", "Imaging"], "title": "Multispectral pattern recognition"}
{"summary": "In machine learning, a nearest centroid classifier or nearest prototype classifier is a classification model that assigns to observations the label of the class of training samples whose mean (centroid) is closest to the observation.\nWhen applied to text classification using tf*idf vectors to represent documents, the nearest centroid classifier is known as the Rocchio classifier because of its similarity to the Rocchio algorithm for relevance feedback.\nAn extended version of the nearest centroid classifier has found applications in the medical domain, specifically classification of tumors.", "links": ["Centroid", "Cluster hypothesis", "Digital object identifier", "K-means clustering", "K-nearest neighbor algorithm", "Linear discriminant analysis", "Machine learning", "Mean", "Relevance feedback", "Robert Tibshirani", "Rocchio algorithm", "Statistical classification", "Text classification", "Tf*idf", "Trevor Hastie", "Tumor"], "categories": ["Classification algorithms"], "title": "Nearest centroid classifier"}
{"summary": "In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers: functions that can decide whether an input (represented by a vector of numbers) belongs to one class or another. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time.\nThe perceptron algorithm dates back to the late 1950s; its first implementation, in custom hardware, was one of the first artificial neural networks to be produced.", "links": ["AI winter", "Anomaly detection", "Artificial intelligence", "Artificial neural network", "Artificial neuron", "Association rule learning", "Autoencoder", "BIRCH", "Backpropagation", "Bayesian network", "Bernard Widrow", "Bias-variance dilemma", "Binary Space Partition", "Binary classification", "Binary function", "Boosting (machine learning)", "Bootstrap aggregating", "Cadmium sulfide", "Canonical correlation analysis", "Cluster analysis", "Computational learning theory", "Conditional random field", "Convolutional neural network", "Cornell Aeronautical Laboratory", "DBSCAN", "Data mining", "Decision boundary", "Decision tree learning", "Deep learning", "Delta rule", "Digital object identifier", "Dimensionality reduction", "Dot product", "Empirical risk minimization", "Ensemble learning", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Feature vector", "Feedforward neural network", "Frank Rosenblatt", "Gaussian distribution", "Grammar induction", "Graphical model", "Heaviside step function", "Hidden Markov model", "Hierarchical clustering", "History of artificial intelligence", "Hyperbolic function", "Hyperplane", "Hyperplane separation theorem", "IBM 704", "Independent component analysis", "International Standard Book Number", "JSTOR", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Kernel perceptron", "Kernel trick", "Learning to rank", "Linear classifier", "Linear discriminant analysis", "Linear predictor function", "Linear regression", "Linearly separable", "Local outlier factor", "Logistic regression", "Machine Learning (journal)", "Machine learning", "Marvin Minsky", "Mean-shift", "Mehryar Mohri", "Michael Collins (computational linguist)", "Multiclass classification", "Multilayer perceptron", "Naive Bayes classifier", "National Diet Library", "Natural language processing", "Neural network", "Non-negative matrix factorization", "OPTICS algorithm", "Office of Naval Research", "Offline learning", "Online algorithm", "Online machine learning", "Overfitting", "Part-of-speech tagging", "Perceptron", "Perceptrons", "Perceptrons (book)", "Photocell", "Potentiometer", "Principal component analysis", "Probably approximately correct learning", "Python (programming language)", "Random forest", "Random matrix", "Ra\u00fal Rojas", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Robert Schapire", "Robustness (computer science)", "Self-organizing map", "Semi-supervised learning", "Seymour Papert", "Sheffer stroke", "Sigma-pi unit", "Statistical classification", "Statistical learning theory", "Stephen Grossberg", "Stochastic gradient descent", "Structured prediction", "Supervised classification", "Supervised learning", "Support vector machine", "Syntactic parsing", "T-distributed stochastic neighbor embedding", "The New York Times", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory", "Vector space", "Winnow (algorithm)", "XOR", "Yoav Freund"], "categories": ["All accuracy disputes", "Articles with disputed statements from August 2015", "Articles with disputed statements from June 2014", "Articles with example Python code", "Artificial neural networks", "Classification algorithms"], "title": "Perceptron"}
{"summary": "Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forests correct for decision trees' habit of overfitting to their training set.\nThe algorithm for inducing a random forest was developed by Leo Breiman and Adele Cutler, and \"Random Forests\" is their trademark. The method combines Breiman's \"bagging\" idea and the random selection of features, introduced independently by Ho and Amit and Geman in order to construct a collection of decision trees with controlled variance.\nThe selection of a random subset of features is an example of the random subspace method, which, in Ho's formulation, is a way to implement classification proposed by Eugene Kleinberg.", "links": ["Annals of Statistics", "Anomaly detection", "Artificial neural network", "Association rule learning", "Autoencoder", "BIRCH", "Bayesian network", "Bias-variance dilemma", "Bias\u2013variance dilemma", "Bias\u2013variance tradeoff", "Boosting (machine learning)", "Bootstrap aggregating", "Bootstrapping (statistics)", "Canonical correlation analysis", "Classification and regression tree", "Cluster analysis", "Computational learning theory", "Conditional random field", "Convolutional neural network", "Correlation", "DBSCAN", "Data mining", "Decision tree", "Decision tree learning", "Deep learning", "Digital object identifier", "Dimensionality reduction", "Donald Geman", "Empirical risk minimization", "Ensemble learning", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Generalization error", "Gini impurity", "Gradient boosting", "Grammar induction", "Graphical model", "Hidden Markov model", "Hierarchical clustering", "Independent component analysis", "Information gain", "International Standard Book Number", "Jerome H. Friedman", "K-means clustering", "K-nearest neighbor algorithm", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Kernel random forest", "Learning to rank", "Lecture Notes in Computer Science", "Leo Breiman", "Linear discriminant analysis", "Linear regression", "Linear subspace", "Local outlier factor", "Logistic regression", "Machine Learning (journal)", "Machine learning", "Mathematical Reviews", "Mean-shift", "Mode (statistics)", "Multilayer perceptron", "Multinomial logistic regression", "Naive Bayes classifier", "Neural Computation", "Non-negative matrix factorization", "Non-parametric statistics", "OPTICS algorithm", "Online machine learning", "Overfitting", "Partial permutation", "Perceptron", "Principal component analysis", "Probably approximately correct learning", "PubMed Identifier", "R (programming language)", "R programming language", "Random subspace method", "Random tree (disambiguation)", "Randomized algorithm", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Robert Tibshirani", "Self-organizing map", "Semi-supervised learning", "Statistical classification", "Statistical learning theory", "Structured prediction", "Supervised learning", "Support vector machine", "T-distributed stochastic neighbor embedding", "Trademark", "Trevor Hastie", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory"], "categories": ["All articles to be merged", "All articles with unsourced statements", "Articles to be merged from May 2015", "Articles with unsourced statements from June 2015", "CS1 errors: external links", "Classification algorithms", "Decision trees", "Ensemble learning", "Pages using duplicate arguments in template calls"], "title": "Random forest"}
{"summary": "Random subspace method (or attribute bagging) is an ensemble classifier that consists of several classifiers each operating in a subspace of the original feature space, and outputs the class based on the outputs of these individual classifiers. Random subspace method has been used for linear classifiers, support vector machines, nearest neighbours and other types of classifiers. This method is also applicable to one-class classifiers.\nThe algorithm is an attractive choice for classification problems where the number of features is much larger than the number of training objects, such as fMRI data or gene expression data.", "links": ["Algorithm", "Digital object identifier", "Ensemble learning", "One-class classification", "Posterior probabilities", "Statistical classification", "Support vector machine"], "categories": ["CS1 errors: chapter ignored", "Classification algorithms", "Ensemble learning", "Pages containing cite templates with deprecated parameters", "Pages with citations having bare URLs", "Pages with citations lacking titles"], "title": "Random subspace method"}
{"summary": "In mathematics, a relevance vector machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification. The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.\nIt is actually equivalent to a Gaussian process model with covariance function:\n\nwhere  is the kernel function (usually Gaussian),'s as the variances of the prior on the weight vector  ,and  are the input vectors of the training set.\nCompared to that of support vector machines (SVM), the Bayesian formulation of the RVM avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, which are guaranteed to find a global optimum (of the convex problem).\nThe relevance vector machine is patented in the United States by Microsoft.", "links": ["Bayesian inference", "Covariance function", "Expectation maximization", "Gaussian process", "Journal of Machine Learning Research", "Kernel function", "Kernel trick", "Machine learning", "Mathematics", "Microsoft", "Parsimony", "Platt scaling", "Probabilistic classification", "Regression analysis", "Sequential minimal optimization", "Software patents under United States patent law", "Support vector machine", "Training set"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from February 2010", "Classification algorithms", "Kernel methods for machine learning", "Nonparametric Bayesian statistics"], "title": "Relevance vector machine"}
{"summary": "Rules Extraction System (RULES) is one family of inductive learning that include several covering algorithms. This family is used to build a predictive model based on given observation. It works based on the concept of separate-and-conquer to directly induce rules from a given training set and build its knowledge repository.\nAlgorithms under RULES family are usually available in data mining tools, such as KEEL and WEKA, known for knowledge extraction and decision making.", "links": ["C4.5 algorithm", "Decision tree learning", "Machine learning", "Weka (machine learning)"], "categories": ["All orphaned articles", "Classification algorithms", "Knowledge engineering", "Orphaned articles from February 2015"], "title": "Rules Extraction System Family"}
{"summary": "In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis. Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.", "links": ["Alexey Chervonenkis", "Algorithm", "Anomaly detection", "ArXiv", "Artificial neural network", "Association for Computing Machinery", "Association rule learning", "Autoencoder", "BIRCH", "Bayesian network", "Bayesian optimization", "Bernhard E. Boser", "Bias-variance dilemma", "Bibcode", "Binary classification", "Binary classifier", "Boosting (machine learning)", "Bootstrap aggregating", "Canonical correlation analysis", "Class membership probabilities", "Cluster analysis", "Computational learning theory", "Conditional random field", "Convolutional neural network", "Coordinate descent", "Corinna Cortes", "Cross-validation (statistics)", "DBSCAN", "Data mining", "Decision tree learning", "Deep learning", "Digital object identifier", "Dimensionality reduction", "Directed acyclic graph", "Dot product", "Dual problem", "Empirical risk minimization", "Ensemble learning", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Feature space", "Fisher kernel", "Gaussian", "Generalization error", "Grammar induction", "Graphical model", "Grid search", "Hidden Markov model", "Hierarchical clustering", "High-dimensional space", "Hilbert space", "Homogeneous polynomial", "Hyperbolic function", "Hyperplane", "In situ adaptive tabulation", "Independent component analysis", "Integrated Authority File", "Interior point method", "International Standard Book Number", "Isabelle M. Guyon", "John Shawe-Taylor", "Journal of Machine Learning Research", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Karush\u2013Kuhn\u2013Tucker conditions", "Kernel (integral operator)", "Kernel machines", "Kernel trick", "LIBSVM", "Lagrange multipliers", "Learning to rank", "Least squares support vector machine", "Lecture Notes in Computer Science", "Linear classifier", "Linear discriminant analysis", "Linear regression", "Linear separability", "Linearly separable", "Local outlier factor", "Logistic regression", "MATLAB", "Machine Learning (journal)", "Machine learning", "Margin classifier", "Maximum-margin hyperplane", "Mean-shift", "Minimum Message Length", "Minimum message length", "Multiclass problem", "Multilayer perceptron", "Multivariate adaptive regression splines", "Naive Bayes classifier", "Nello Cristianini", "Newton's method", "Non-negative matrix factorization", "Nonlinear", "Normal (geometry)", "OPTICS algorithm", "Online machine learning", "Optimization (mathematics)", "Paris Kanellakis Award", "Perceptron", "Platt scaling", "Polynomial kernel", "Positive-definite kernel", "Predictive analytics", "Principal component analysis", "Probabilistic classification", "Probably approximately correct learning", "Quadratic programming", "Radial basis function", "Radial basis function kernel", "Random forest", "Rate of convergence", "Real number", "Recurrent neural network", "Regression analysis", "Regularization (mathematics)", "Regularization perspectives on support vector machines", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Saddle point", "Scikit-learn", "Secure Virtual Machine", "Self-organizing map", "Semi-supervised learning", "Sequential Minimal Optimization", "Sequential minimal optimization", "Shogun (toolbox)", "Space mapping", "Statistical classification", "Statistical learning theory", "Stochastic gradient descent", "Structured SVM", "Structured prediction", "Supervised learning", "T-distributed stochastic neighbor embedding", "Tikhonov regularization", "Transduction (machine learning)", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory", "Vladimir N. Vapnik", "Weka (machine learning)", "Winnow (algorithm)"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from February 2015", "Articles with unsourced statements from June 2013", "Classification algorithms", "Statistical classification", "Support vector machines", "Wikipedia articles with GND identifiers"], "title": "Support vector machine"}
{"summary": "Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features. This allows for representing pattern structures, taking into account more complex interrelationships between attributes than is possible in the case of flat, numerical feature vectors of fixed dimensionality, that are used in statistical classification.\nSyntactic pattern recognition can be used instead of statistical pattern recognition if there is clear structure in the patterns. One way to present such structure is by means of a strings of symbols from a formal language. In this case the differences in the structures of the classes are encoded as different grammars.\nAn example of this would be diagnosis of the heart with ECG measurements. ECG waveforms can be approximated with diagonal and vertical line segments. If normal and unhealthy waveforms can be described as formal grammars, measured ECG signal can be classified as healthy or unhealthy by first describing it in term of the basic line segments and then trying to parse the descriptions according to the grammars. Another example is tessellation of tiling patterns.\nA second way to represent relations are graphs, where nodes are connected if corresponding subpatterns are related. An item can be labeled as belonging to a class if its graph representation is isomorphic with prototype graphs of the class.\nTypically, patterns are constructed from simpler sub patterns in a hierarchical fashion. This helps in dividing the recognition task into easier subtask of first identifying sub patterns and only then the actual patterns.\nStructural methods provide descriptions of items, which may be useful in their own right. For example, syntactic pattern recognition can be used to find out what objects are present in an image. Furthermore, structural methods are strong in finding a correspondence mapping between two images of an object. Under natural conditions, corresponding features will be in different positions and/or may be occluded in the two images, due to camera-attitude and perspective, as in face recognition. A graph-matching algorithm will yield the optimal correspondence.", "links": ["Cardinality", "Electrocardiogram", "Face recognition", "Feature vector", "Formal grammar", "Formal language", "Grammar induction", "Graph (mathematics)", "Heart", "Hopcroft\u2013Karp algorithm", "International Standard Book Number", "Isomorphic", "Nominal data", "Pattern recognition", "Statistical classification", "String (computer science)", "String matching", "Structural information theory", "Tessellation", "Waveform"], "categories": ["Classification algorithms"], "title": "Syntactic pattern recognition"}
{"summary": "There are many types of artificial neural networks (ANN).\nArtificial neural networks are computational models inspired by biological neural networks, and are used to approximate functions that are generally unknown. Particularly, they are inspired by the behaviour of neurons and the electrical signals they convey between input (such as from the eyes or nerve endings in the hand), processing, and output from the brain (such as reacting to light, touch, or heat). The way neurons semantically communicate is an area of ongoing research. Most artificial neural networks bear only some resemblance to their more complex biological counterparts, but are very effective at their intended tasks (e.g. classification or segmentation).\nSome ANNs are adaptive systems and are used for example to model populations and environments, which constantly change.\nNeural networks can be hardware- (neurons are represented by physical components) or software-based (computer models), and can use a variety of topologies and learning algorithms.", "links": ["ADALINE", "Activation function", "Adaptive resonance theory", "Adaptive system", "Algorithm", "Artificial life", "Artificial neural network", "Autoassociative memory", "Autoencoder", "Back-propagation", "Backpropagation", "Backpropagation through time", "Bayesian networks", "Bayesian statistics", "Bernard Widrow", "Biological neural network", "Biologically inspired computing", "Bionics", "Blue Gene", "Blue brain", "Boltzmann machine", "Bootstrap Aggregating", "Carnegie Mellon University", "Committee machine", "Complex number", "Compositional pattern-producing network", "Computational model", "Connectionist expert system", "Content-addressable memory", "Decision tree learning", "Differentiable", "Digital object identifier", "Dileep George", "Echo state network", "Evolutionary computation", "Expert system", "Feedforward neural network", "Fitness function", "Function (mathematics)", "Fuzzy logic", "GPGPU", "Gaussian function", "Genetic algorithm", "Geoff Hinton", "Gradient descent", "Grid computing", "HP Labs", "Hebbian learning", "Hidden Markov model", "Hierarchical temporal memory", "Holographic associative memory", "Hopfield network", "IBM", "In Situ Adaptive Tabulation", "Inference system", "Instantaneously trained neural networks", "Iteratively re-weighted least squares", "Jeff Elman", "Jeff Hawkins", "John Hopfield", "J\u00fcrgen Schmidhuber", "Kernel function", "Learning Vector Quantization", "Linear discriminant analysis", "Liquid state machines", "Logistic regression", "Long short term memory", "Machine learning", "McCulloch-Pitts neuron", "Memory-prediction framework", "Memristor", "Michael I. Jordan", "Modular neural network", "Modular neural networks", "Monte Carlo sampling", "Multilayer perceptron", "Nearest neighbor (pattern recognition)", "Neocortex", "Network topology", "Neural Gas", "Neural network software", "Neural networks", "NeuroEvolution of Augmented Topologies", "Neuroevolution", "Neuron", "Ni1000", "Numenta", "On Intelligence", "Online machine learning", "Optical neural network", "Overfitting", "Particle swarm optimization", "Paul Werbos", "Perceptron", "Physical neural network", "Population model", "Predictive analytics", "Principal components analysis", "Product of Experts", "PubMed Identifier", "Pulse computer", "Radial basis function network", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Reward function", "Ridge regression", "Ronald J. Williams", "Scott Fahlman", "Self-organizing map", "Sepp Hochreiter", "Sigmoid function", "Simulated annealing", "Spiking neural network", "Statistical sampling", "Stochastic neural network", "Supercomputer", "Supervised learning", "Support vector machine", "Systolic array", "Terry Sejnowski", "Teuvo Kohonen", "Time delay neural network", "Time domain", "Tony Robinson", "Universal approximation theorem", "Unsupervised learning", "Utility function", "Vanishing gradient problem", "Wireless sensor network"], "categories": ["All articles with unsourced statements", "All copied and pasted articles and sections", "Articles with unsourced statements from October 2008", "Artificial neural networks", "Classification algorithms", "Computational neuroscience", "Computational statistics", "Copied and pasted articles and sections with url provided from November 2014"], "title": "Types of artificial neural networks"}
{"summary": "The winnow algorithm is a technique from machine learning for learning a linear classifier from labeled examples. It is very similar to the perceptron algorithm. However, the perceptron algorithm uses an additive weight-update scheme, while Winnow uses a multiplicative scheme that allows it to perform much better when many dimensions are irrelevant (hence its name). It is a simple algorithm that scales well to high-dimensional data. During training, Winnow is shown a sequence of positive and negative examples. From these it learns a decision hyperplane that can then be used to label novel examples as positive or negative. The algorithm can also be used in the online learning setting, where the learning and the classification phase are not clearly separated.", "links": ["Boolean-valued", "Features (pattern recognition)", "Hyperplane", "Linear classifier", "Machine learning", "Multi-label classification", "Online machine learning", "Perceptron", "Upper and lower bounds"], "categories": ["Classification algorithms"], "title": "Winnow (algorithm)"}
{"summary": "Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.", "links": ["Annals of Statistics", "Anomaly detection", "Artificial neural network", "Artificial neural networks", "Association rule learning", "Autoencoder", "BIRCH", "Bayes classifier", "Bayesian network", "Bias-variance dilemma", "Boosting (machine learning)", "Boosting (meta-algorithm)", "Bootstrap (statistics)", "Bootstrapping (statistics)", "Canonical correlation analysis", "CiteSeer", "Classic data sets", "Classification and regression tree", "Cluster analysis", "Computational learning theory", "Conditional random field", "Convolutional neural network", "Cross-validation (statistics)", "DBSCAN", "Data mining", "Decision tree learning", "Deep learning", "Digital object identifier", "Dimensionality reduction", "E (mathematical constant)", "Empirical risk minimization", "Ensemble learning", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Grammar induction", "Graphical model", "Hidden Markov model", "Hierarchical clustering", "Independent component analysis", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Learning to rank", "Leo Breiman", "Linear discriminant analysis", "Linear regression", "Local outlier factor", "Local regression", "Logistic regression", "Machine Learning (journal)", "Machine learning", "Mean-shift", "Meta-algorithm", "Multilayer perceptron", "Naive Bayes classifier", "Non-negative matrix factorization", "OPTICS algorithm", "Online machine learning", "Overfitting", "Ozone", "Perceptron", "Peter Rousseeuw", "Prime (symbol)", "Principal component analysis", "Probability distribution", "Probably approximately correct learning", "R (programming language)", "Random forest", "Random subspace method", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Sampling (statistics)", "Self-organizing map", "Semi-supervised learning", "Statistical classification", "Statistical learning theory", "Structured prediction", "Supervised learning", "Support vector machine", "T-distributed stochastic neighbor embedding", "Training set", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory", "Variance", "Weighted nearest neighbour classifier"], "categories": ["Computational statistics", "Ensemble learning", "Machine learning algorithms"], "title": "Bootstrap aggregating"}
{"summary": "In computer science, Algorithms for Recovery and Isolation Exploiting Semantics, or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; it is used by IBM DB2, Microsoft SQL Server and many other database systems.\nThree main principles lie behind ARIES\nWrite ahead logging: Any change to an object is first recorded in the log, and the log must be written to stable storage before changes to the object are written to disk.\nRepeating history during Redo: On restart after a crash, ARIES retraces the actions of a database before the crash and brings the system back to the exact state that it was in before the crash. Then it undoes the transactions still active at crash time.\nLogging changes during Undo: Changes made to the database while undoing transactions are logged to ensure such an action isn't repeated in the event of repeated restarts.", "links": ["Algorithm", "Computer science", "Database log", "Database system", "IBM DB2", "Logging changes during Undo", "Microsoft SQL Server", "No-force", "Repeating history during Redo", "Write ahead logging"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from March 2013", "Database algorithms"], "title": "Algorithms for Recovery and Isolation Exploiting Semantics"}
{"summary": "The Chase is a simple fixed-point algorithm testing and enforcing implication of data dependencies in database systems. It plays important roles in database theory as well as in practice. It is used, directly or indirectly, on an everyday basis by people who design databases, and it is used in commercial systems to reason about the consistency and correctness of a data design. New applications of the chase in meta-data management and data exchange are still being discovered.\nThe Chase has its origins in two seminal papers, one by Alfred V. Aho, Catriel Beeri, and Jeffrey D. Ullman and the other by David Maier, Alberto O. Mendelzon, and Yehoshua Sagiv.\nIn its simplest application the chase is used for testing whether the projection of a relation schema constrained by some functional dependencies onto a given decomposition can be recovered by rejoining the projections. Let t be a tuple in  where R is a relation and F is a set of functional dependencies (FD). If tuples in R are represented as t1, ..., tk, the join of the projections of each ti should agree with t on  where i = 1, 2, ..., k. If ti is not on , the value is unknown.\nThe chase can be done by drawing a tableau (which is the same formalism used in tableau query). Suppose R has attributes A, B, ... and components of t are a, b, .... For ti use the same letter as t in the components that are in Si but subscript the letter with i if the component is not in i. Then, ti will agree with t if it is in Si and will have a unique value otherwise.\nThe chase process is confluent.", "links": ["Alberto O. Mendelzon", "Alfred Aho", "Alfred V. Aho", "Attribute (computing)", "Catriel Beeri", "Confluence (rewriting system)", "Database", "Database theory", "David Maier", "Fixed-point iteration", "Functional dependency", "International Standard Book Number", "Jeffrey D. Ullman", "Jeffrey Ullman", "Jennifer Widom", "Join dependency", "Projection (relational algebra)", "Relation (database)", "Relation schema", "Richard B. Hull", "Serge Abiteboul", "Tableau query", "Victor Vianu", "Yehoshua Sagiv"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from November 2012", "Database algorithms", "Database theory"], "title": "Chase (algorithm)"}
{"summary": "In computer science, write-ahead logging (WAL) is a family of techniques for providing atomicity and durability (two of the ACID properties) in database systems.\nIn a system using WAL, all modifications are written to a log before they are applied. Usually both redo and undo information is stored in the log.\nThe purpose of this can be illustrated by an example. Imagine a program that is in the middle of performing some operation when the machine it is running on loses power. Upon restart, that program might well need to know whether the operation it was performing succeeded, half-succeeded, or failed. If a write-ahead log is used, the program can check this log and compare what it was supposed to be doing when it unexpectedly lost power to what was actually done. On the basis of this comparison, the program could decide to undo what it had started, complete what it had started, or keep things as they are.\nWAL allows updates of a database to be done in-place. Another way to implement atomic updates is with shadow paging, which is not in-place. The main advantage of doing updates in-place is that it reduces the need to modify indexes and block lists.\nARIES is a popular algorithm in the WAL family.\nFile systems typically use a variant of WAL for at least file system metadata called journaling.", "links": ["ACID", "Algorithms for Recovery and Isolation Exploiting Semantics", "Atomic (computer science)", "Computer science", "Database", "Database log", "Database system", "Durability (database systems)", "File system", "In-place algorithm", "Journaling file system", "Metadata", "Shadow paging"], "categories": ["All stub articles", "Database algorithms", "Database stubs"], "title": "Write-ahead logging"}
{"summary": "A block-nested loop (BNL) is an algorithm used to join two relations in a relational database.\nThis algorithm is a variation on the simple nested loop join used to join two relations  and  (the \"outer\" and \"inner\" join operands, respectively). Suppose . In a traditional nested loop join,  will be scanned once for every tuple of . If there are many qualifying  tuples, and particularly if there is no applicable index for the join key on , this operation will be very expensive.\nThe block nested loop join algorithm improves on the simple nested loop join by only scanning  once for every group of  tuples. For example, one variant of the block nested loop join reads an entire page of  tuples into memory and loads them into a hash table. It then scans , and probes the hash table to find  tuples that match any of the tuples in the current page of . This reduces the number of scans of  that are necessary.\nA more aggressive variant of this algorithm loads as many pages of  as can be fit in the available memory, loading all such tuples into a hash table, and then repeatedly scans . This further reduces the number of scans of  that are necessary. In fact, this algorithm is essentially a special-case of the classic hash join algorithm.\nThe block nested loop runs in  I/Os where  is the number of available pages of internal memory and  and  is size of  and  respectively in pages. Note that block nested loop runs in  I/Os if  fits in the available internal memory.", "links": ["Algorithm", "Hash join", "Hash table", "Join (SQL)", "Nested loop join", "Page (computing)", "Relational database"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from August 2015", "Join algorithms"], "title": "Block nested loop"}
{"summary": "The hash join is an example of a join algorithm and is used in the implementation of a relational database management system.\nThe task of a join algorithm is to find, for each distinct value of the join attribute, the set of tuples in each relation which have that value.\nHash joins require an equijoin predicate (a predicate comparing values from one table with values from the other table using the equals operator '=').", "links": ["Block nested loop", "Database management system", "Digital object identifier", "Equijoin", "Hash function", "Hash table", "Jim Gray (computer scientist)", "Join (SQL)", "Relational database", "Symmetric Hash Join", "Syntactic predicate", "Tuple"], "categories": ["Hashing", "Join algorithms"], "title": "Hash join"}
{"summary": "A nested loop join is a naive algorithm that joins two sets by using two nested loops. Join operations are important to database management.", "links": ["Algorithm", "Block nested loop", "Computer science", "Database", "Loop (computing)", "Memory (computing)"], "categories": ["All articles lacking sources", "All articles needing expert attention", "All stub articles", "Articles lacking sources from January 2010", "Articles needing expert attention from March 2011", "Articles needing expert attention with no reason or talk parameter", "Computer science stubs", "Join algorithms", "Mathematics articles needing expert attention"], "title": "Nested loop join"}
{"summary": "The sort-merge join (also known as merge join) is a join algorithm and is used in the implementation of a relational database management system.\nThe basic problem of a join algorithm is to find, for each distinct value of the join attribute, the set of tuples in each relation which display that value. The key idea of the Sort-merge algorithm is to first sort the relations by the join attribute, so that interleaved linear scans will encounter these sets at the same time.\nIn practice, the most expensive part of performing a sort-merge join is arranging for both inputs to the algorithm to be presented in sorted order. This can be achieved via an explicit sort operation (often an external sort), or by taking advantage of a pre-existing ordering in one or both of the join relations. The latter condition can occur because an input to the join might be produced by an index scan of a tree-based index, another merge join, or some other plan operator that happens to produce output sorted on an appropriate key.\nLet's say that we have two relations  and  and .  fits in  pages memory and  fits in  pages memory. So, in the worst case Sort-Merge Join will run in  I/Os. In the case that  and  are not ordered the worst case time cost will contain additional terms of sorting time: , which equals  (as linearithmic terms outweigh the linear terms, see Big O notation \u2013 Orders of common functions).", "links": ["Big O notation", "Database management system", "External sort", "Join (SQL)", "Join algorithm", "Linearithmic time", "Relational database", "Tuple"], "categories": ["All articles lacking sources", "Articles lacking sources from December 2009", "Join algorithms"], "title": "Sort-merge join"}
{"summary": "A fast Fourier transform (FFT) algorithm computes the discrete Fourier transform (DFT) of a sequence, or its inverse. Fourier analysis converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa. A FFT rapidly computes such transformations by factorizing the DFT matrix into a product of sparse (mostly zero) factors. As a result, it manages to reduce the complexity of computing the DFT from , which arises if one simply applies the definition of DFT, to , where  is the data size.\nFast Fourier transforms are widely used for many applications in engineering, science, and mathematics. The basic ideas were popularized in 1965, but some algorithms had been derived as early as 1805. In 1994 Gilbert Strang described the FFT as \"the most important numerical algorithm of our lifetime\" and it was included in Top 10 Algorithms of 20th Century by the IEEE journal Computing in Science & Engineering.", "links": ["2 Pallas", "3 Juno", "Algorithm", "Approximation error", "Arithmetic complexity of the discrete Fourier transform", "Asymptotically optimal", "Big O notation", "Bluestein's FFT algorithm", "Bruun's FFT algorithm", "Butterfly diagram", "Cache-oblivious", "Cache (computing)", "Carl Friedrich Gauss", "Central processing unit", "Charles E. Leiserson", "Chebyshev approximation", "Chinese Remainder Theorem", "Chirp-z algorithm", "Circulant matrix", "Clifford Stein", "Complex number", "Composite number", "Computational complexity theory", "Connexions", "Convolution", "Convolution theorem", "Cooley\u2013Tukey FFT algorithm", "Coordinate vector", "Coprime", "Cornelius Lanczos", "Cyclotomic polynomial", "DFT matrix", "Digital object identifier", "Digital signal processing", "Dirichlet series", "Discrete Fourier transform", "Discrete Hartley transform", "Discrete cosine transform", "Discrete sine transform", "Distributed memory", "Divide and conquer algorithm", "Even and odd functions", "FFT", "FFTPACK", "FFTW", "FFT (disambiguation)", "Factorization", "Fast Folding Algorithm", "Fast Walsh\u2013Hadamard transform", "Fast multipole method", "Finite field", "Fixed-point FFT algorithms", "Fixed-point arithmetic", "Floating-point", "Floating-point unit", "Fourier analysis", "Fourier transform on finite groups", "Frank Yates", "Frequency domain", "G.C. Danielson", "Generalized distributive law", "Generating set of a group", "Generating trigonometric tables", "Gilbert Strang", "Goertzel algorithm", "Graph (mathematics)", "Group (mathematics)", "Group representation", "Group theory", "Hadamard transform", "Hartley transform", "International Standard Book Number", "International Standard Serial Number", "Introduction to Algorithms", "J. W. Cooley", "J. W. Tukey", "JSTOR", "James Cooley", "John Tukey", "Joseph Fourier", "LIGO", "List of unsolved problems in computer science", "Lock-in amplifier", "Math Kernel Library", "Matrix (mathematics)", "Matrix decomposition", "Multidimensional transform", "Multiplication algorithm", "Non-uniform discrete Fourier transform", "Number-theoretic transform", "Number theory", "Numerical analysis", "Numerical stability", "Nyquist\u2013Shannon sampling theorem", "Odlyzko\u2013Sch\u00f6nhage algorithm", "Orders of magnitude", "Orthogonality", "Out-of-core", "Overlap add", "Overlap save", "Pairwise summation", "Parallel computing", "Partial differential equation", "Piotr Indyk", "Pipeline (computing)", "Polynomial", "Power of two", "Prime-factor FFT algorithm", "Prime number", "Primitive root of unity", "Proof by exhaustion", "Proportionality (mathematics)", "Quantum Fourier transform", "Quick Fourier transform algorithm", "Rader's FFT algorithm", "Recurrence relation", "Recursion", "Richard Garwin", "Ronald L. Rivest", "Root mean square", "Roots of unity", "Round-off error", "SIAM Journal on Scientific Computing", "Satisfiability Modulo Theories", "Sequence", "Shmuel Winograd", "Sinusoidal functions", "Sparse matrix", "Spectral estimation", "Spectral music", "Spectrum analyzer", "Spherical harmonics", "Split-radix FFT", "Split-radix FFT algorithm", "Thomas H. Cormen", "Thomas J. Watson Research Center", "Time series", "Toeplitz matrix", "Transpose", "Trigonometric function", "Twiddle factor", "Upper bound", "Vector-radix FFT algorithm", "Wavelet", "Wilkinson Microwave Anisotropy Probe", "Winograd FFT algorithm"], "categories": ["All articles needing additional references", "Articles needing additional references from June 2015", "Articles needing additional references from October 2015", "Articles with inconsistent citation formats", "Digital signal processing", "Discrete transforms", "FFT algorithms", "Unsolved problems in computer science"], "title": "Fast Fourier transform"}
{"summary": "In computational mathematics, the Hadamard ordered fast Walsh\u2013Hadamard transform (FWHTh) is an efficient algorithm to compute the Walsh\u2013Hadamard transform (WHT). A naive implementation of the WHT would have a computational complexity of O(). The FWHTh requires only  additions or subtractions.\nThe FWHTh is a divide and conquer algorithm that recursively breaks down a WHT of size  into two smaller WHTs of size . This implementation follows the recursive definition of the  Hadamard matrix :\n\nThe  normalization factors for each stage may be grouped together or even omitted.\nThe sequency ordered, also known as Walsh ordered, fast Walsh\u2013Hadamard transform, FWHTw, is obtained by computing the FWHTh as above, and then rearranging the outputs.", "links": ["Algorithm", "Big O notation", "Computational complexity theory", "Data structure", "Digital object identifier", "Divide and conquer algorithm", "Fast Fourier transform", "Hadamard matrix", "Recursion", "Signal processing", "Walsh matrix", "Walsh\u2013Hadamard transform"], "categories": ["Algorithms and data structures stubs", "All articles lacking in-text citations", "All stub articles", "Articles lacking in-text citations from September 2015", "Computer science stubs", "Digital signal processing", "Signal processing stubs"], "title": "Fast Walsh\u2013Hadamard transform"}
{"summary": "The Goertzel algorithm is a Digital Signal Processing (DSP) technique that provides a means for efficient evaluation of individual terms of the Discrete Fourier Transform (DFT), thus making it useful in certain practical applications, such as recognition of DTMF tones produced by the buttons pushed on a telephone keypad. The algorithm was first described by Gerald Goertzel in 1958.\nLike the DFT, the Goertzel algorithm analyses one selectable frequency component from a discrete signal. Unlike direct DFT calculations, the Goertzel algorithm applies a single real-valued coefficient at each iteration, using real-valued arithmetic for real-valued input sequences. For covering a full spectrum, the Goertzel algorithm has a higher order of complexity than Fast Fourier Transform (FFT) algorithms; but for computing a small number of selected frequency components, it is more numerically efficient. The simple structure of the Goertzel algorithm makes it well suited to small processors and embedded applications, though not limited to these.\nThe Goertzel algorithm can also be used \"in reverse\" as a sinusoid synthesis function, which requires only 1 multiplication and 1 subtraction per generated sample.", "links": ["Aliasing", "Array data type", "Big O notation", "Bluestein's FFT algorithm", "Computational complexity theory", "Digital Signal Processing", "Digital filter", "Digital object identifier", "Discrete Fourier Transform", "Discrete Fourier transform", "Discrete signal", "Dual-tone multi-frequency signaling", "Fast Fourier Transform", "Fast Fourier transform", "Finite impulse response", "Frequency-shift keying", "Gerald Goertzel", "Infinite impulse response", "International Standard Serial Number", "Marginal stability", "Numerical stability", "Nyquist\u2013Shannon sampling theorem", "Object-oriented programming", "Phase-shift keying", "Pole (complex analysis)", "Pseudocode", "Z transform"], "categories": ["All articles needing additional references", "All articles needing cleanup", "All articles with specifically marked weasel-worded phrases", "All articles with unsourced statements", "Articles needing additional references from February 2014", "Articles needing cleanup from February 2014", "Articles with specifically marked weasel-worded phrases from February 2014", "Articles with unsourced statements from February 2014", "Cleanup tagged articles with a reason field from February 2014", "Digital signal processing", "FFT algorithms", "Pages using citations with accessdate and no URL", "Pages using web citations with no URL", "Wikipedia pages needing cleanup from February 2014"], "title": "Goertzel algorithm"}
{"summary": "Least mean squares (LMS) algorithms are a class of adaptive filter used to mimic a desired filter by finding the filter coefficients that relate to producing the least mean squares of the error signal (difference between the desired and the actual signal). It is a stochastic gradient descent method in that the filter is only adapted based on the error at the current time. It was invented in 1960 by Stanford University professor Bernard Widrow and his first Ph.D. student, Ted Hoff.", "links": ["Adaptive filter", "Autocorrelation", "Bernard Widrow", "Conjugate transpose", "Eigenvalue spread", "Estimator", "Expected value", "Gradient", "Hermitian transpose", "Kernel adaptive filter", "Least squares", "Loss function", "Multidelay block frequency domain adaptive filter", "Partial derivative", "Recursive least squares", "Similarities between Wiener and LMS", "Stanford University", "Steepest descent", "Stochastic gradient descent", "Ted Hoff", "White noise", "Wiener filter", "Zero forcing equalizer"], "categories": ["Digital signal processing", "Filter theory", "Stochastic algorithms"], "title": "Least mean squares filter"}
{"summary": "The Ramer\u2013Douglas\u2013Peucker algorithm (RDP) is an algorithm for reducing the number of points in a curve that is approximated by a series of points. The initial form of the algorithm was independently suggested in 1972 by Urs Ramer and 1973 by David Douglas and Thomas Peucker and several others in the following decade. This algorithm is also known under the names Douglas\u2013Peucker algorithm, iterative end-point fit algorithm and split-and-merge algorithm.", "links": ["Algorithm", "Cartographic generalization", "Digital object identifier", "Hausdorff distance", "Lang simplification algorithm", "Laser rangefinder", "Master theorem", "Opheim simplification algorithm", "Polygonal chain", "Recursion", "Reumann\u2013Witkam algorithm", "Vector graphics", "Visvalingam\u2013Whyatt algorithm", "Zhao Saalfeld algorithm"], "categories": ["Articles with example pseudocode", "Computer graphics algorithms", "Digital signal processing", "Geometric algorithms"], "title": "Ramer\u2013Douglas\u2013Peucker algorithm"}
{"summary": "The cyclotomic fast Fourier transform is a type of fast Fourier transform algorithm over finite fields. This algorithm first decomposes a DFT into several circular convolutions, and then derives the DFT results from the circular convolution results. When applied to a DFT over , this algorithm has a very low multiplicative complexity. In practice, since there usually exist efficient algorithms for circular convolutions with specific lengths, this algorithm is very efficient.", "links": ["BCH codes", "Characteristic (algebra)", "Circulant matrix", "Complex number", "Convolution", "Digital object identifier", "Discrete Fourier transform", "Error-correcting code", "Fast Fourier transform", "Finite field", "Linearized polynomial", "Normal basis", "Primitive root", "Reed\u2013Solomon error correction"], "categories": ["Discrete transforms", "FFT algorithms"], "title": "Cyclotomic fast Fourier transform"}
{"summary": "Bruun's algorithm is a fast Fourier transform (FFT) algorithm based on an unusual recursive polynomial-factorization approach, proposed for powers of two by G. Bruun in 1978 and generalized to arbitrary even composite sizes by H. Murakami in 1996. Because its operations involve only real coefficients until the last computation stage, it was initially proposed as a way to efficiently compute the discrete Fourier transform (DFT) of real data. Bruun's algorithm has not seen widespread use, however, as approaches based on the ordinary Cooley\u2013Tukey FFT algorithm have been successfully adapted to real data with at least as much efficiency. Furthermore, there is evidence that Bruun's algorithm may be intrinsically less accurate than Cooley\u2013Tukey in the face of finite numerical precision (Storn, 1993).\nNevertheless, Bruun's algorithm illustrates an alternative algorithmic framework that can express both itself and the Cooley\u2013Tukey algorithm, and thus provides an interesting perspective on FFTs that permits mixtures of the two algorithms and other generalizations.", "links": ["Chinese Remainder Theorem", "Cooley\u2013Tukey FFT algorithm", "Degree of a polynomial", "Discrete Fourier transform", "Discrete cosine transform", "Fast Fourier transform", "ICASSP", "IEEE", "Polynomial", "Polynomial remainder theorem", "Power of two", "Relatively prime polynomials", "Root of unity", "Sic"], "categories": ["FFT algorithms"], "title": "Bruun's FFT algorithm"}
{"summary": "This article is about butterfly diagrams in FFT algorithms; for the sunspot diagrams of the same name, see Solar cycle.\n\nIn the context of fast Fourier transform algorithms, a butterfly is a portion of the computation that combines the results of smaller discrete Fourier transforms (DFTs) into a larger DFT, or vice versa (breaking a larger DFT up into subtransforms). The name \"butterfly\" comes from the shape of the data-flow diagram in the radix-2 case, as described below. The earliest occurrence in print of the term is thought to be in a 1969 MIT technical report. The same structure can also be found in the Viterbi algorithm, used for finding the most likely sequence of hidden states.\nMost commonly, the term \"butterfly\" appears in the context of the Cooley\u2013Tukey FFT algorithm, which recursively breaks down a DFT of composite size n = rm into r smaller transforms of size m where r is the \"radix\" of the transform. These smaller DFTs are then combined via size-r butterflies, which themselves are DFTs of size r (performed m times on corresponding outputs of the sub-transforms) pre-multiplied by roots of unity (known as twiddle factors). (This is the \"decimation in time\" case; one can also perform the steps in reverse, known as \"decimation in frequency\", where the butterflies come first and are post-multiplied by twiddle factors. See also the Cooley\u2013Tukey FFT article.)", "links": ["Butterfly", "Composite number", "Cooley\u2013Tukey FFT", "Cooley\u2013Tukey FFT algorithm", "Discrete Fourier transform", "Fast Fourier transform", "International Standard Book Number", "MIT Lincoln Laboratory", "Massachusetts Institute of Technology", "Mathematical diagram", "Morpho (butterfly)", "Recursion", "Root of unity", "Signal-flow graph", "Solar cycle", "Twiddle factor", "Viterbi algorithm", "Zassenhaus lemma"], "categories": ["Diagrams", "FFT algorithms"], "title": "Butterfly diagram"}
{"summary": "The Cooley\u2013Tukey algorithm, named after J.W. Cooley and John Tukey, is the most common fast Fourier transform (FFT) algorithm. It re-expresses the discrete Fourier transform (DFT) of an arbitrary composite size N = N1N2 in terms of smaller DFTs of sizes N1 and N2, recursively, to reduce the computation time to O(N log N) for highly composite N (smooth numbers). Because of the algorithm's importance, specific variants and implementation styles have become known by their own names, as described below.\nBecause the Cooley-Tukey algorithm breaks the DFT into smaller DFTs, it can be combined arbitrarily with any other algorithm for the DFT. For example, Rader's or Bluestein's algorithm can be used to handle large prime factors that cannot be decomposed by Cooley\u2013Tukey, or the prime-factor algorithm can be exploited for greater efficiency in separating out relatively prime factors.\nThe algorithm, along with its recursive application, was invented by Carl Friedrich Gauss. Cooley and Tukey independently rediscovered and popularized it 160 years later.", "links": ["2 Pallas", "3 Juno", "Adding machine", "Amortize", "Array data structure", "Asteroid", "Binary numeral system", "Bit-reversal permutation", "Bluestein's FFT algorithm", "Breadth-first", "Breadth-first search", "Butterfly (FFT algorithm)", "Butterfly diagram", "C. Sidney Burrus", "CPU cache", "CPU pipeline", "Cache-oblivious algorithm", "Cache (computing)", "Carl David Tolm\u00e9 Runge", "Carl Friedrich Gauss", "Chinese Remainder Theorem", "Column-major order", "Composite number", "Computer", "Cornelius Lanczos", "Dataflow diagram", "Depth-first", "Depth-first search", "Digital object identifier", "Discrete Fourier transform", "Divide and conquer algorithm", "E (mathematical constant)", "Fast Fourier transform", "Floating point", "G. C. Danielson", "GNU General Public License", "Helium-3", "IBM 7094", "In-place algorithm", "International Business Machines", "International Standard Book Number", "JSTOR", "James Cooley", "John Tukey", "Lemma (mathematics)", "Linearithmic", "Mathematics of Computation", "Memory locality", "New Latin", "Nuclear testing", "Out-of-core", "Permutation", "Power of two", "Prime-factor FFT algorithm", "Princeton University", "Processor register", "Pseudocode", "Rader's FFT algorithm", "Recursion", "Relatively prime", "Richard Garwin", "Roots of unity", "Row-major order", "SIMD", "Smooth number", "Soviet Union", "Split-radix FFT algorithm", "Stride of an array", "Transpose", "Twiddle factor"], "categories": ["Articles with example pseudocode", "FFT algorithms"], "title": "Cooley\u2013Tukey FFT algorithm"}
{"summary": "The Fastest Fourier Transform in the West (FFTW) is a software library for computing discrete Fourier transforms (DFTs) developed by Matteo Frigo and Steven G. Johnson at the Massachusetts Institute of Technology.\nFFTW is known as the fastest free software implementation of the Fast Fourier transform (FFT) algorithm (upheld by regular benchmarks). It can compute transforms of real and complex-valued arrays of arbitrary size and dimension in O(n log n) time.\nIt does this by supporting a variety of algorithms and choosing the one (a particular decomposition of the transform into smaller transforms) it estimates or measures to be preferable in the particular circumstances. It works best on arrays of sizes with small prime factors, with powers of two being optimal and large primes being worst case (but still O(n log n)). To decompose transforms of composite sizes into smaller transforms, it chooses among several variants of the Cooley\u2013Tukey FFT algorithm (corresponding to different factorizations and/or different memory-access patterns), while for prime sizes it uses either Rader's or Bluestein's FFT algorithm. Once the transform has been broken up into subtransforms of sufficiently small sizes, FFTW uses hard-coded unrolled FFTs for these small sizes that were produced (at compile time, not at run time) by code generation; these routines use a variety of algorithms including Cooley\u2013Tukey variants, Rader's algorithm, and prime-factor FFT algorithms.\nFor a sufficiently large number of repeated transforms it is advantageous to measure the performance of some or all of the supported algorithms on the given array size and platform. These measurements, which the authors refer to as \"wisdom\", can be stored in a file or string for later use.\nFFTW has a \"guru interface\" that intends \"to expose as much as possible of the flexibility in the underlying FFTW architecture\". This allows, among other things, multi-dimensional transforms and multiple transforms in a single call (e.g., where the data is interleaved in memory).\nFFTW has limited support for out-of-order transforms (using the MPI version). The data reordering incurs an overhead, which for in-place transforms of arbitrary size and dimension is non-trivial to avoid. It is undocumented for which transforms this overhead is significant.\nFFTW is licensed under the GNU General Public License. It is also licensed commercially by MIT and is used in the commercial MATLAB matrix package for calculating FFTs. FFTW is written in the C language, but Fortran and Ada interfaces exist, as well as interfaces for a few other languages. The Julia base library includes an interface to FFTW by default.  While the library itself is C, the code is actually generated from a program called 'genfft', which is written in OCaml.\nIn 1999, FFTW won the J. H. Wilkinson Prize for Numerical Software.", "links": ["Ada (programming language)", "Algorithm", "Automatic programming", "Benchmark (computing)", "Big O notation", "Bluestein's FFT algorithm", "C (programming language)", "Compile time", "Complex number", "Composite number", "Cooley\u2013Tukey FFT algorithm", "Digital object identifier", "Discrete Fourier transform", "FFTPACK", "Fast Fourier transform", "Fortran", "Free software", "GNU General Public License", "GPL", "Hard-coded", "Heuristic (computer science)", "International Conference on Acoustics, Speech and Signal Processing", "J. H. Wilkinson Prize for Numerical Software", "Julia (programming language)", "Library (computer science)", "Linearithmic function", "List of software categories", "Loop unwinding", "MATLAB", "MIT", "Massachusetts Institute of Technology", "Message Passing Interface", "Numerical software", "OCaml", "Platform (computing)", "Power of two", "Prime-factor FFT algorithm", "Prime factor", "Prime number", "Rader's FFT algorithm", "Run time (program lifecycle phase)", "Software developer", "Software license", "Software release life cycle"], "categories": ["FFT algorithms", "Free mathematics software", "Numerical libraries", "OCaml software"], "title": "FFTW"}
{"summary": "The prime-factor algorithm (PFA), also called the Good\u2013Thomas algorithm (1958/1963), is a fast Fourier transform (FFT) algorithm that re-expresses the discrete Fourier transform (DFT) of a size N = N1N2 as a two-dimensional N1\u00d7N2 DFT, but only for the case where N1 and N2 are relatively prime. These smaller transforms of size N1 and N2 can then be evaluated by applying PFA recursively or by using some other FFT algorithm.\nPFA should not be confused with the mixed-radix generalization of the popular Cooley\u2013Tukey algorithm, which also subdivides a DFT of size N = N1N2 into smaller transforms of size N1 and N2. The latter algorithm can use any factors (not necessarily relatively prime), but it has the disadvantage that it also requires extra multiplications by roots of unity called twiddle factors, in addition to the smaller transforms. On the other hand, PFA has the disadvantages that it only works for relatively prime factors (e.g. it is useless for power-of-two sizes) and that it requires a more complicated re-indexing of the data based on the Chinese remainder theorem (CRT). Note, however, that PFA can be combined with mixed-radix Cooley\u2013Tukey, with the former factorizing N into relatively prime components and the latter handling repeated factors.\nPFA is also closely related to the nested Winograd FFT algorithm, where the latter performs the decomposed N1 by N2 transform via more sophisticated two-dimensional convolution techniques. Some older papers therefore also call Winograd's algorithm a PFA FFT.\n(Although the PFA is distinct from the Cooley\u2013Tukey algorithm, Good's 1958 work on the PFA was cited as inspiration by Cooley and Tukey in their famous 1965 paper, and there was initially some confusion about whether the two algorithms were different. In fact, it was the only prior FFT work cited by them, as they were not then aware of the earlier research by Gauss and others.)", "links": ["Bijection", "Bluestein's FFT algorithm", "Chinese remainder theorem", "Cooley\u2013Tukey FFT algorithm", "Digital object identifier", "Discrete Fourier transform", "Fast Fourier transform", "In-place algorithm", "JSTOR", "Modular arithmetic", "Modular multiplicative inverse", "Power of two", "Rader's FFT algorithm", "Recursion", "Relatively prime", "Twiddle factor", "Winograd FFT algorithm"], "categories": ["FFT algorithms"], "title": "Prime-factor FFT algorithm"}
{"summary": "Rader's algorithm (1968) is a fast Fourier transform (FFT) algorithm that computes the discrete Fourier transform (DFT) of prime sizes by re-expressing the DFT as a cyclic convolution (the other algorithm for FFTs of prime sizes, Bluestein's algorithm, also works by rewriting the DFT as a convolution).\nSince Rader's algorithm only depends upon the periodicity of the DFT kernel, it is directly applicable to any other transform (of prime order) with a similar property, such as a number-theoretic transform or the discrete Hartley transform.\nThe algorithm can be modified to gain a factor of two savings for the case of DFTs of real data, using a slightly modified re-indexing/permutation to obtain two half-size cyclic convolutions of real data (Chu & Burrus, 1982); an alternative adaptation for DFTs of real data, using the discrete Hartley transform, was described by Johnson & Frigo (2007).\nWinograd extended Rader's algorithm to include prime-power DFT sizes  (Winograd 1976; Winograd 1978), and today Rader's algorithm is sometimes described as a special case of Winograd's FFT algorithm, also called the multiplicative Fourier transform algorithm (Tolimieri et al., 1997), which applies to an even larger class of sizes. However, for composite sizes such as prime powers, the Cooley\u2013Tukey FFT algorithm is much simpler and more practical to implement, so Rader's algorithm is typically only used for large-prime base cases of Cooley\u2013Tukey's recursive decomposition of the DFT (Frigo and Johnson, 2005).", "links": ["Base case", "Big O notation", "Bijection", "Bluestein's FFT algorithm", "Composite number", "Convolution", "Convolution theorem", "Cooley\u2013Tukey FFT algorithm", "Cunningham chain", "Discrete Fourier transform", "Discrete Hartley transform", "Fast Fourier transform", "Generating set of a group", "Group (mathematics)", "Modular arithmetic", "Modular multiplicative inverse", "Number-theoretic transform", "Number theory", "Power of two", "Prime number", "Primitive root modulo n", "Recursion (computer science)", "Sophie Germain prime", "Winograd's FFT algorithm"], "categories": ["FFT algorithms"], "title": "Rader's FFT algorithm"}
{"summary": "The split-radix FFT is a fast Fourier transform (FFT) algorithm for computing the discrete Fourier transform (DFT), and was first described in an initially little-appreciated paper by R. Yavne (1968) and subsequently rediscovered simultaneously by various authors in 1984. (The name \"split radix\" was coined by two of these reinventors, P. Duhamel and H. Hollmann.) In particular, split radix is a variant of the Cooley-Tukey FFT algorithm that uses a blend of radices 2 and 4: it recursively expresses a DFT of length N in terms of one smaller DFT of length N/2 and two smaller DFTs of length N/4.\nThe split-radix FFT, along with its variations, long had the distinction of achieving the lowest published arithmetic operation count (total exact number of required real additions and multiplications) to compute a DFT of power-of-two sizes N. The arithmetic count of the original split-radix algorithm was improved upon in 2004 (with the initial gains made in unpublished work by J. Van Buskirk via hand optimization for N=64 [1] [2]), but it turns out that one can still achieve the new lowest count by a modification of split radix (Johnson and Frigo, 2007). Although the number of arithmetic operations is not the sole factor (or even necessarily the dominant factor) in determining the time required to compute a DFT on a computer, the question of the minimum possible count is of longstanding theoretical interest. (No tight lower bound on the operation count has currently been proven.)\nThe split-radix algorithm can only be applied when N is a multiple of 4, but since it breaks a DFT into smaller DFTs it can be combined with any other FFT algorithm as desired.", "links": ["Butterfly diagram", "Computer", "Cooley-Tukey FFT algorithm", "Discrete Fourier transform", "Even and odd numbers", "Fast Fourier transform", "Henk D. L. Hollmann", "Modulo operation", "Pierre Duhamel", "Power of two", "R. Yavne", "Real number", "Recursion", "Root of unity", "Twiddle factor"], "categories": ["FFT algorithms"], "title": "Split-radix FFT algorithm"}
{"summary": "A twiddle factor, in fast Fourier transform (FFT) algorithms, is any of the trigonometric constant coefficients that are multiplied by the data in the course of the algorithm. This term was apparently coined by Gentleman & Sande in 1966, and has since become widespread in thousands of papers of the FFT literature.\nMore specifically, \"twiddle factors\" originally referred to the root-of-unity complex multiplicative constants in the butterfly operations of the Cooley-Tukey FFT algorithm, used to recursively combine smaller discrete Fourier transforms. This remains the term's most common meaning, but it may also be used for any data-independent multiplicative constant in an FFT.\nThe Prime-factor FFT algorithm is one unusual case in which an FFT can be performed without twiddle factors, albeit only for restricted factorizations of the transform size.", "links": ["Butterfly diagram", "Complex number", "Cooley-Tukey FFT algorithm", "Digital object identifier", "Discrete Fourier transform", "Fast Fourier transform", "Prime-factor FFT algorithm", "Recursion", "Root of unity", "Trigonometric function"], "categories": ["FFT algorithms"], "title": "Twiddle factor"}
{"summary": "False Radiosity is a 3D computer graphics technique used to create texture mapping for objects that emulates patch interaction algorithms in radiosity rendering. Though practiced in some form since the late 90s, this term was coined only around 2002 by architect Andrew Hartness, then head of 3D and real-time design at Ateliers Jean Nouvel.\nDuring the period of nascent commercial enthusiasm for radiosity-enhanced imagery, but prior to the democratization of powerful computational hardware, architects and graphic artists experimented with time-saving 3D rendering techniques. By darkening areas of texture maps corresponding to corners, joints and recesses, and applying maps via self-illumination or diffuse mapping in a 3D program, a radiosity-like effect of patch interaction could be created with a standard scan-line renderer. Successful emulation of radiosity required a theoretical understanding and graphic application of patch view factors, path tracing and global illumination algorithms. Texture maps were usually produced with image editing software, such as Adobe Photoshop. The advantage of this method is decreased rendering time and easily modifiable overall lighting strategies.\nAnother common approach similar to false radiosity is the manual placement of standard omni-type lights with limited attenuation in places in the 3D scene where the artist would expect radiosity reflections to occur. This method uses many lights and can require an advanced light-grouping system, depending on what assigned materials/objects are illuminated, how many surfaces require false radiosity treatment, and to what extent it is anticipated that lighting strategies be set up for frequent changes.", "links": ["3D computer graphics", "Ambient occlusion", "Architect", "Global illumination", "Graphics software", "Jean Nouvel", "Path tracing", "Radiosity (3D computer graphics)", "Rendering (computer graphics)", "Scan-line renderer", "View factor"], "categories": ["3D computer graphics", "All stub articles", "Computer graphics algorithms", "Graphics software stubs", "Image processing", "Rendering systems"], "title": "False radiosity"}
{"summary": "The firefly algorithm (FA) is a metaheuristic algorithm, inspired by the flashing behaviour of fireflies. The primary purpose for a firefly's flash is to act as a signal system to attract other fireflies. Xin-She Yang formulated this firefly algorithm by assuming:\nAll fireflies are unisexual, so that any individual firefly will be attracted to all other fireflies;\nAttractiveness is proportional to their brightness, and for any two fireflies, the less bright one will be attracted by (and thus move towards) the brighter one; however, the intensity (apparent brightness) decrease as their mutual distance increases;\nIf there are no fireflies brighter than a given firefly, it will move randomly.\nThe brightness should be associated with the objective function.\nFirefly algorithm is a nature-inspired metaheuristic optimization algorithm.", "links": ["Active matter", "Agent-based model", "Agent-based model in biology", "Algorithm", "Allee effect", "Altitudinal migration", "Animal migration", "Animal migration tracking", "Animal navigation", "Ant colony optimization algorithms", "Ant robotics", "Approximation algorithm", "ArXiv", "Artificial Ants", "Augmented Lagrangian method", "Bait ball", "Barrier function", "Bat algorithm", "Bees algorithm", "Bellman\u2013Ford algorithm", "Bird migration", "Boids", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Cell migration", "Clustering of self-propelled particles", "Coded wire tag", "Collective animal behavior", "Collective intelligence", "Collective motion", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Crowd simulation", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Decentralised system", "Diel vertical migration", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Eusociality", "Evolutionary algorithm", "Evolutionary multi-modal optimization", "Exchange algorithm", "Feeding frenzy", "Firefly", "Fish migration", "Flock (birds)", "Flocking (behavior)", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Glowworm swarm optimization", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Group size measures", "Herd", "Herd behavior", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Homing (biology)", "Hybrid algorithm", "Insect migration", "Integer programming", "Intelligent Small World Autonomous Robots for Micro-manipulation", "International Standard Book Number", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Lepidoptera migration", "Lessepsian migration", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Linde\u2013Buzo\u2013Gray algorithm", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Microbial intelligence", "Microbotics", "Minimum spanning tree", "Mixed-species foraging flock", "Mobbing (animal behavior)", "Monarch butterfly migration", "Mutualism (biology)", "Natal homing", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization (mathematics)", "Optimization algorithm", "Pack (canine)", "Pack hunter", "Particle Swarm Optimization", "Particle swarm optimization", "Patterns of self-organization in ants", "Penalty method", "Philopatry", "Powell's method", "Predator satiation", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Quorum sensing", "Reverse migration (birds)", "Revised simplex algorithm", "Salmon run", "Sardine run", "Sea turtle migration", "Self-propelled particles", "Sequential quadratic programming", "Shoaling and schooling", "Simplex algorithm", "Simulated annealing", "Sort sol (bird flock)", "Spatial organization", "Stigmergy", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Swarm (simulation)", "Swarm behaviour", "Swarm intelligence", "Swarm robotics", "Swarming (honey bee)", "Swarming (military)", "Swarming motility", "Symbrion", "Symmetric rank-one", "Symmetry breaking of escaping ants", "Tabu search", "Task allocation and partitioning of social insects", "Travelling salesman problem", "Truncated Newton method", "Trust region", "Vicsek model", "Wolfe conditions", "Xin-she Yang"], "categories": ["Evolutionary algorithms", "Image processing", "Metaheuristic", "Optimization algorithms and methods", "Swarm Intelligence", "Wikipedia articles with possible conflicts of interest from May 2014"], "title": "Firefly algorithm"}
{"summary": "Floyd\u2013Steinberg dithering is an image dithering algorithm first published in 1976 by Robert W. Floyd and Louis Steinberg. It is commonly used by image manipulation software, for example when an image is converted into GIF format that is restricted to a maximum of 256 colors.\nThe algorithm achieves dithering using error diffusion, meaning it pushes (adds) the residual quantization error of a pixel onto its neighboring pixels, to be dealt with later. It spreads the debt out according to the distribution (shown as a map of the neighboring pixels):\n\nThe pixel indicated with a star (*) indicates the pixel currently being scanned, and the blank pixels are the previously-scanned pixels. The algorithm scans the image from left to right, top to bottom, quantizing pixel values one by one. Each time the quantization error is transferred to the neighboring pixels, while not affecting the pixels that already have been quantized. Hence, if a number of pixels have been rounded downwards, it becomes more likely that the next pixel is rounded upwards, such that on average, the quantization error is close to zero.\nThe diffusion coefficients have the property that if the original pixel values are exactly halfway in between the nearest available colors, the dithered result is a checkerboard pattern. For example 50% grey data could be dithered as a black-and-white checkerboard pattern. For optimal dithering, the counting of quantization errors should be in sufficient accuracy to prevent rounding errors from affecting the result.\nIn some implementations, the horizontal direction of scan alternates between lines; this is called \"serpentine scanning\" or boustrophedon transform dithering.\nIn pseudocode:\n\nfor each y from top to bottom\n   for each x from left to right\n      oldpixel  := pixel[x][y]\n      newpixel  := find_closest_palette_color(oldpixel)\n      pixel[x][y]  := newpixel\n      quant_error  := oldpixel - newpixel\n      pixel[x+1][y  ] := pixel[x+1][y  ] + quant_error * 7/16\n      pixel[x-1][y+1] := pixel[x-1][y+1] + quant_error * 3/16\n      pixel[x  ][y+1] := pixel[x  ][y+1] + quant_error * 5/16\n      pixel[x+1][y+1] := pixel[x+1][y+1] + quant_error * 1/16\n\nWhen converting 16 bit greyscale to 8 bit, find_closest_palette_color() may perform just a simple rounding, for example:\n\nfind_closest_palette_color(oldpixel) = floor(oldpixel / 256)", "links": ["Boustrophedon transform", "David (Michelangelo)", "Dithering", "Error diffusion", "GIF", "List of monochrome and RGB palettes", "Louis Steinberg", "Pixel", "Pseudocode", "Quantization error", "Robert W. Floyd"], "categories": ["Articles with example code", "Articles with example pseudocode", "Computer graphics algorithms", "Image processing"], "title": "Floyd\u2013Steinberg dithering"}
{"summary": "Level set methods (LSM) are a conceptual framework for using level sets as a tool for numerical analysis of surfaces and shapes. The advantage of the level set model is that one can perform numerical computations involving curves and surfaces on a fixed Cartesian grid without having to parameterize these objects (this is called the Eulerian approach). Also, the level set method makes it very easy to follow shapes that change topology, for example when a shape splits in two, develops holes, or the reverse of these operations. All these make the level set method a great tool for modeling time-varying objects, like inflation of an airbag, or a drop of oil floating in water.\n\nThe figure on the right illustrates several important ideas about the level set method. In the upper-left corner we see a shape; that is, a bounded region with a well-behaved boundary. Below it, the red surface is the graph of a level set function  determining this shape, and the flat blue region represents the xy-plane. The boundary of the shape is then the zero level set of , while the shape itself is the set of points in the plane for which  is positive (interior of the shape) or zero (at the boundary).\nIn the top row we see the shape changing its topology by splitting in two. It would be quite hard to describe this transformation numerically by parameterizing the boundary of the shape and following its evolution. One would need an algorithm able to detect the moment the shape splits in two, and then construct parameterizations for the two newly obtained curves. On the other hand, if we look at the bottom row, we see that the level set function merely translated downward. This is an example of when it can be much easier to work with a shape through its level set function than with the shape directly, where using the shape directly would need to consider and handle all the possible deformations the shape might undergo.\nThus, in two dimensions, the level set method amounts to representing a closed curve  (such as the shape boundary in our example) using an auxiliary function , called the level set function.  is represented as the zero level set of  by\n\nand the level set method manipulates  implicitly, through the function . This function  is assumed to take positive values inside the region delimited by the curve  and negative values outside.", "links": ["AUSM", "Abstract additive Schwarz method", "Additive Schwarz method", "Airbag", "Alternating direction implicit method", "Analytic element method", "BDDC", "Balancing domain decomposition method", "Bibcode", "Boundary element method", "Cambridge University Press", "Cartesian grid", "Closed curve", "Collocation method", "Computational fluid dynamics", "Computational geometry", "Computer graphics", "Crank\u2013Nicolson method", "Curve", "Digital object identifier", "Discontinuous Galerkin method", "Domain decomposition methods", "Eikonal equation", "Essentially non-oscillatory", "Euclidean norm", "Extended finite element method", "FETI", "FETI-DP", "FTCS scheme", "Fictitious domain method", "Finite-difference time-domain method", "Finite difference", "Finite difference method", "Finite element method", "Finite volume method", "Godunov's scheme", "Hamilton\u2013Jacobi equation", "High-resolution scheme", "Hp-FEM", "Hyperbolic partial differential equation", "Image processing", "Image segmentation", "Immersed boundary method", "International Standard Book Number", "Isogeometric analysis", "James Sethian", "Joel H. Ferziger", "Lax\u2013Friedrichs method", "Lax\u2013Wendroff method", "Level set", "Level set (data structures)", "Level set data structures", "MUSCL scheme", "MacCormack method", "Material point method", "Meshfree methods", "Method of characteristics", "Method of lines", "Mortar methods", "Multigrid method", "Neumann\u2013Dirichlet method", "Neumann\u2013Neumann methods", "Numerical analysis", "Numerical partial differential equations", "Optimization (mathematics)", "Parabolic partial differential equation", "Parametric surface", "Partial differential equation", "Particle-in-cell", "Poincar\u00e9\u2013Steklov operator", "Pseudo-spectral method", "Riemann solver", "Ronald Fedkiw", "Schur complement method", "Schwarz alternating method", "Shape", "Smoothed-particle hydrodynamics", "Spectral element method", "Spectral method", "Springer-Verlag", "Stanley Osher", "Stochastic Eulerian Lagrangian method", "Surface", "Surfaces", "Topology", "Upwind scheme", "Upwinding", "Volume of fluid method"], "categories": ["Articles containing video clips", "Computational fluid dynamics", "Computer graphics algorithms", "Image processing", "Mathematical optimization", "Numerical analysis"], "title": "Level set method"}
{"summary": "The Shepp\u2013Logan phantom is a standard test image created by Larry Shepp and Benjamin F. Logan for their 1974 paper The Fourier Reconstruction of a Head Section. It serves as the model of a human head in the development and testing of image reconstruction algorithms.", "links": ["\"Hello, World!\" program", "3D computer graphics", "Algorithm", "Benjamin F. Logan", "Calgary corpus", "Canterbury corpus", "Computer programming", "Cornell box", "Data compression", "Data structure", "EICAR test file", "GTUBE", "Harvard sentences", "Image reconstruction", "Imaging phantom", "Indian-head test pattern", "International Standard Book Number", "Larry Shepp", "Lenna", "Lorem ipsum", "Pangram", "Philips PM5544", "Quine (computing)", "Reference implementation", "SMPTE color bars", "Standard test image", "Stanford bunny", "Stanford dragon", "Television", "Test Card F", "Testcard", "The quick brown fox jumps over the lazy dog", "Tom's Diner", "Trabb Pardo\u2013Knuth algorithm", "Typography", "Utah teapot", "Wired (website)"], "categories": ["1974 works", "Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Image processing", "Test items"], "title": "Shepp\u2013Logan phantom"}
{"summary": "The forward\u2013backward algorithm is an inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables given a sequence of observations/emissions , i.e. it computes, for all hidden state variables , the distribution . This inference task is usually called smoothing. The algorithm makes use of the principle of dynamic programming to compute efficiently the values that are required to obtain the posterior marginal distributions in two passes. The first pass goes forward in time while the second goes backward in time; hence the name forward\u2013backward algorithm.\nThe term forward\u2013backward algorithm is also used to refer to any algorithm belonging to the general class of algorithms that operate on sequence models in a forward\u2013backward manner. In this sense, the descriptions in the remainder of this article refer but to one specific instance of this class.", "links": ["Algorithm", "BCJR algorithm", "Baum-Welch algorithm", "Bayes' rule", "Belief propagation", "Conditional independence", "Dynamic programming", "Hidden Markov model", "Hidden Markov models", "IEEE", "Inference", "International Standard Book Number", "Island algorithm", "Lawrence Rabiner", "Marginal probability", "Posterior probability", "Python programming language", "Time complexity", "Viterbi algorithm"], "categories": ["Dynamic programming", "Error detection and correction", "Machine learning algorithms", "Markov models"], "title": "Forward\u2013backward algorithm"}
{"summary": "A family of hash functions is said to be -independent or -universal if selecting a hash function at random from the family guarantees that the hash codes of any designated  keys are independent random variables (see precise mathematical definitions below). Such families allow good average case performance in randomized algorithms or data structures, even if the input data is chosen by an adversary. The trade-offs between the degree of independence and the efficiency of evaluating the hash function are well studied, and many -independent families have been proposed.", "links": ["Chernoff bound", "Digital object identifier", "Hash function", "Image (mathematics)", "Independence (probability theory)", "International Standard Book Number", "Mark N. Wegman", "Statistical distance", "Tabulation hashing", "Universal hashing"], "categories": ["Error detection and correction", "Hash functions", "Pages containing cite templates with deprecated parameters", "Search algorithms"], "title": "K-independent hashing"}
{"summary": "In artificial intelligence, an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators. Artificial evolution (AE) describes a process involving individual evolutionary algorithms; EAs are individual components that participate in an AE.\nEvolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape; this generality is shown by successes in fields as diverse as engineering, art, biology, economics, marketing, genetics, operations research, robotics, social sciences, physics, politics and chemistry.\nTechniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. The computer simulations Tierra and Avida attempt to model macroevolutionary dynamics.\nIn most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.\nA possible limitation  of many evolutionary algorithms is their lack of a clear genotype-phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism. Such indirect (aka generative or developmental) encodings also enable evolution to exploit the regularity in the environment. Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype-phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes.", "links": ["Academic journal", "Adaptive dimensional search", "Algorithm", "Algorithms", "Ant colony optimization", "Art", "Artificial Bee Colony Algorithm", "Artificial Life (journal)", "Artificial development", "Artificial intelligence", "Average information", "Avida", "Bees algorithm", "Biological evolution", "Biology", "Breed", "CMA-ES", "Candidate solution", "Cellular evolutionary algorithm", "Chemistry", "Combinatorial optimization", "Convergence (evolutionary computing)", "Crossover (genetic algorithm)", "Cuckoo", "Cuckoo search", "Developmental biology", "Differential evolution", "Digital organism", "Economics", "Embryogenesis", "Encoding", "Engineering", "Entropy in thermodynamics and information theory", "Estimation of Distribution Algorithm", "Estimation of distribution algorithm", "Evolution", "Evolution strategy", "Evolutionary Computation (journal)", "Evolutionary computation", "Evolutionary data mining", "Evolutionary multimodal optimization", "Evolutionary programming", "Evolutionary robotics", "Evolvability", "Firefly algorithm", "Fitness approximation", "Fitness function", "Fitness landscape", "Gaussian adaptation", "Gene expression programming", "Generation", "Genetic algorithm", "Genetic operators", "Genetic programming", "Genetic recombination", "Genetics", "Genotype-phenotype distinction", "Graph theory", "Harmony search", "Human-based evolutionary computation", "Individual", "Ingo Rechenberg", "Interactive evolutionary computation", "International Standard Book Number", "Keane's function", "Learning classifier system", "List of digital organism simulators", "Loss function", "L\u00e9vy flight", "Machine learning", "Macroevolution", "Map of Evolutionary Algorithms", "Marketing", "Mean fitness", "Memetic algorithm", "Metaheuristic", "Microevolution", "Mutation", "Mutation (genetic algorithm)", "Natural evolution strategy", "Natural selection", "Neuroevolution", "No free lunch in search and optimization", "Numerical optimization", "Offspring", "Operations research", "Optimization", "Optimization (mathematics)", "Particle swarm optimization", "Phenotype", "Physics", "Politics", "Population", "Program synthesis", "Reinforcement learning", "Reproduce", "Reproduction", "Rosenbrock function", "S-expression", "Social sciences", "Subset", "Swarm intelligence", "Test functions for optimization", "Tierra (computer simulation)"], "categories": ["All articles with specifically marked weasel-worded phrases", "All articles with unsourced statements", "Articles that may contain original research from May 2013", "Articles with specifically marked weasel-worded phrases from May 2013", "Articles with unsourced statements from May 2015", "Articles with unsourced statements from September 2008", "Cybernetics", "Evolution", "Evolutionary algorithms", "Optimization algorithms and methods"], "title": "Evolutionary algorithm"}
{"summary": "In computer science and operations research, the artificial bee colony algorithm (ABC) is an optimization algorithm based on the intelligent foraging behaviour of honey bee swarm, proposed by Karaboga in 2005.\n\n", "links": ["Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bees algorithm", "Bellman\u2013Ford algorithm", "Biogeography-based optimization", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Computer science", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Dervis Karaboga", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Evolutionary computation", "Evolutionary multi-modal optimization", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Intelligent Water Drops", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Operations research", "Optimization algorithm", "Particle Swarm Optimization", "Pathological Brain Detection (PBD)", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Swarm intelligence", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["All articles needing expert attention", "Articles needing expert attention", "Articles needing expert attention with no reason or talk parameter", "Articles needing unspecified expert attention", "Bees", "Collective intelligence", "Evolutionary algorithms", "Optimization algorithms and methods"], "title": "Artificial bee colony algorithm"}
{"summary": "In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent systems inspired by the principles and processes of the vertebrate immune system. The algorithms typically exploit the immune system's characteristics of learning and memory to solve a problem.", "links": ["Adaptive immune system", "Affinity maturation", "Anomaly detection", "Antigens", "Apoptosis", "Artificial Immune Systems: A New Computational Intelligence Approach", "Artificial Intelligence", "Artificial intelligence", "Artificial neural networks", "Biologically-inspired computing", "Biologically inspired computing", "Cell division", "Central tolerance", "Clonal Selection Algorithm", "Clonal selection", "Computational immunology", "Computational intelligence", "DNA computing", "Danger theory", "Darwinism", "Degeneracy (biology)", "Dendritic Cell Algorithms", "Dendritic cells", "Digital object identifier", "Evolutionary computation", "Genetic algorithm", "Hill climbing", "Idiotypic network", "Immune Network Algorithms", "Immune system", "Immunocomputing", "Immunological Computation", "Innate immune system", "International Standard Book Number", "Learning", "Lymphocyte", "Machine Learning", "Mammal", "Memory", "Natural computation", "Negative Selection Algorithm", "Negative selection (immunology)", "Niels Kaj Jerne", "Optimization (mathematics)", "PDF", "Pattern recognition", "PubMed Identifier", "Reactive search optimization", "Somatic hypermutation", "Springer Science+Business Media", "Stephanie Forrest", "Swarm intelligence", "T cells", "Theoretical biology", "Thymus"], "categories": ["Artificial immune systems", "Evolutionary algorithms", "Pages using citations with accessdate and no URL", "Pages with citations lacking titles"], "title": "Artificial immune system"}
{"summary": "Bat-inspired algorithm is a metaheuristic optimization algorithm developed by Xin-She Yang in 2010. This bat algorithm is based on the echolocation behaviour of microbats with varying pulse rates of emission and loudness.", "links": ["Active matter", "Agent-based model", "Agent-based model in biology", "Allee effect", "Altitudinal migration", "Animal echolocation", "Animal migration", "Animal migration tracking", "Animal navigation", "Ant colony optimization algorithms", "Ant robotics", "Artificial Ants", "Bait ball", "Bees algorithm", "Bird migration", "Boids", "Cell migration", "Clustering of self-propelled particles", "Coded wire tag", "Collective animal behavior", "Collective intelligence", "Collective motion", "Crowd simulation", "Decentralised system", "Diel vertical migration", "Eusociality", "Feeding frenzy", "Firefly algorithm", "Fish migration", "Flock (birds)", "Flocking (behavior)", "Glowworm swarm optimization", "Group size measures", "Herd", "Herd behavior", "Homing (biology)", "Insect migration", "Intelligent Small World Autonomous Robots for Micro-manipulation", "Lecture Notes in Computer Science", "Lepidoptera migration", "Lessepsian migration", "Metaheuristic", "Microbats", "Microbial intelligence", "Microbotics", "Mixed-species foraging flock", "Mobbing (animal behavior)", "Monarch butterfly migration", "Mutualism (biology)", "Natal homing", "Optimization", "Pack (canine)", "Pack hunter", "Particle swarm optimization", "Patterns of self-organization in ants", "Philopatry", "Predator satiation", "Quorum sensing", "Random walk", "Reverse migration (birds)", "Salmon run", "Sardine run", "Sea turtle migration", "Self-propelled particles", "Shoaling and schooling", "Sort sol (bird flock)", "Spatial organization", "Stigmergy", "Swarm (simulation)", "Swarm behaviour", "Swarm intelligence", "Swarm robotics", "Swarming (honey bee)", "Swarming (military)", "Swarming motility", "Symbrion", "Symmetry breaking of escaping ants", "Task allocation and partitioning of social insects", "Vicsek model"], "categories": ["Evolutionary algorithms", "Heuristic algorithms"], "title": "Bat algorithm"}
{"summary": "Biogeography-based optimization (BBO) is an evolutionary algorithm (EA) that optimizes a function by stochastically and iteratively improving candidate solutions with regard to a given measure of quality, or fitness function. BBO belongs to the class of metaheuristics since it includes many variations, and since it does not make any assumptions about the problem and can therefore be applied to a wide class of problems.\nBBO is typically used to optimize multidimensional real-valued functions, but it does not use the gradient of the function, which means that it does not require the function to be differentiable as required by classic optimization methods such as gradient descent and quasi-newton methods. BBO can therefore be used on discontinuous functions.\nBBO optimizes a problem by maintaining a population of candidate solutions, and creating new candidate solutions by combining existing ones according to a simple formula. In this way the objective function is treated as a black box that merely provides a measure of quality given a candidate solution, and the function's gradient is not needed.\nLike many EAs, BBO was motivated by a natural process; in particular, BBO was motivated by biogeography, which is the study of the distribution of biological species through time and space. BBO was originally introduced by Dan Simon in 2008.", "links": ["Animal migration", "Artificial bee colony algorithm", "Biogeography", "Candidate solution", "Case-based reasoning", "Combinatorial optimization", "Constraint satisfaction", "Continuous function", "Convex programming", "Differentiable", "Differential evolution", "Digital object identifier", "Emigration", "Evolution strategy", "Evolutionary algorithm", "Extinction", "Fitness function", "Fitness proportionate selection", "Flotsam", "Function (mathematics)", "Genetic algorithm", "Gradient", "Gradient descent", "Harmony search", "Infinite-dimensional optimization", "Integer programming", "Iterative method", "Loss function", "Memetic algorithm", "Metaheuristic", "Multiobjective optimization", "Mutation (genetic algorithm)", "Nonlinear programming", "Optimization", "Optimization (mathematics)", "Particle swarm optimization", "Quadratic programming", "Quasi-newton methods", "R (programming language)", "Robust optimization", "Rosenbrock function", "Simplex algorithm", "Speciation", "Species", "Species diversity", "Stochastic", "Stochastic programming"], "categories": ["Evolutionary algorithms", "Stochastic optimization"], "title": "Biogeography-based optimization"}
{"summary": "A Cellular Evolutionary Algorithm (cEA) is a kind of evolutionary algorithm (EA) in which individuals cannot mate arbitrarily, but every one interacts with its closer neighbors on which a basic EA is applied (selection, variation, replacement).\n\nThe cellular model simulates Natural evolution from the point of view of the individual, which encodes a tentative (optimization, learning, search) problem solution. The essential idea of this model is to provide the EA population with a special structure defined as a connected graph, in which each vertex is an individual who communicates with his nearest neighbors. Particularly, individuals are conceptually set in a toroidal mesh, and are only allowed to recombine with close individuals. This leads us to a kind of locality known as isolation by distance. The set of potential mates of an individual is called its neighborhood. It is known that, in this kind of algorithm, similar individuals tend to cluster creating niches, and these groups operate as if they were separate sub-populations (islands). Anyway, there is no clear borderline between adjacent groups, and close niches could be easily colonized by competitive niches and maybe merge solution contents during the process. Simultaneously, farther niches can be affected more slowly.", "links": ["Academic journal", "Algorithms", "Artificial development", "Artificial intelligence", "CMA-ES", "Cellular automata", "Cellular automaton", "Convergence (evolutionary computing)", "Developmental biology", "Differential evolution", "Digital organism", "Dual-phase evolution", "Enrique Alba", "Evolution strategy", "Evolutionary Computation (journal)", "Evolutionary algorithm", "Evolutionary computation", "Evolutionary data mining", "Evolutionary multimodal optimization", "Evolutionary programming", "Evolutionary robotics", "FPGAs", "Fitness approximation", "Fitness function", "Fitness landscape", "GPUs", "Gene expression programming", "Genetic algorithm", "Genetic operators", "Genetic programming", "Human-based evolutionary computation", "Interactive evolutionary computation", "Learning classifier system", "List of digital organism simulators", "Machine learning", "Metaheuristic", "Natural evolution strategy", "Neuroevolution", "No free lunch in search and optimization", "Parallel metaheuristic", "Parallel metaheuristics", "Program synthesis"], "categories": ["Evolutionary algorithms"], "title": "Cellular evolutionary algorithm"}
{"summary": "CMA-ES stands for Covariance Matrix Adaptation Evolution Strategy. Evolution strategies (ES) are stochastic, derivative-free methods for numerical optimization of non-linear or non-convex continuous optimization problems. They belong to the class of evolutionary algorithms and evolutionary computation. An evolutionary algorithm is broadly based on the principle of biological evolution, namely the repeated interplay of variation (via recombination and mutation) and selection: in each generation (iteration) new individuals (candidate solutions, denoted as ) are generated by variation, usually in a stochastic way, of the current parental individuals. Then, some individuals are selected to become the parents in the next generation based on their fitness or objective function value . Like this, over the generation sequence, individuals with better and better -values are generated.\nIn an evolution strategy, new candidate solutions are sampled according to a multivariate normal distribution in the . Recombination amounts to selecting a new mean value for the distribution. Mutation amounts to adding a random vector, a perturbation with zero mean. Pairwise dependencies between the variables in the distribution are represented by a covariance matrix. The covariance matrix adaptation (CMA) is a method to update the covariance matrix of this distribution. This is particularly useful, if the function  is ill-conditioned.\nAdaptation of the covariance matrix amounts to learning a second order model of the underlying objective function similar to the approximation of the inverse Hessian matrix in the Quasi-Newton method in classical optimization. In contrast to most classical methods, fewer assumptions on the nature of the underlying objective function are made. Only the ranking between candidate solutions is exploited for learning the sample distribution and neither derivatives nor even the function values themselves are required by the method.", "links": ["Academic journal", "Algorithms", "Artificial development", "Artificial intelligence", "BFGS method", "Bijective", "Biological evolution", "Cellular evolutionary algorithm", "Cholesky decomposition", "Combinatorial optimization", "Condition number", "Conjugate gradient", "Constraint satisfaction", "Continuous optimization", "Convergence (evolutionary computing)", "Convex function", "Convex programming", "Covariance matrix", "Cross-Entropy Method", "Cumulative distribution function", "Derivative-free optimization", "Developmental biology", "Differential evolution", "Digital object identifier", "Digital organism", "Estimation of Distribution Algorithms", "Estimation of covariance matrices", "Evolution strategies", "Evolution strategy", "Evolutionary Computation (journal)", "Evolutionary algorithm", "Evolutionary algorithms", "Evolutionary computation", "Evolutionary data mining", "Evolutionary multimodal optimization", "Evolutionary programming", "Evolutionary robotics", "Expectation maximization", "Expected value", "Fisher information", "Fisher information metric", "Fitness approximation", "Fitness function", "Fitness landscape", "Gaussian adaptation", "Gene expression programming", "Genetic algorithm", "Genetic operators", "Genetic programming", "Global optimization", "Hessian matrix", "Human-based evolutionary computation", "If and only if", "Ill-conditioned", "Indicator function", "Infinite-dimensional optimization", "Information geometry", "Integer programming", "Interactive evolutionary computation", "Invariant (mathematics)", "Invertible matrix", "Kriging", "Learning classifier system", "Likelihood", "Likelihood function", "Linear map", "List of digital organism simulators", "MCS algorithm", "Machine learning", "Maximum-likelihood", "Maximum entropy probability distribution", "Metaheuristic", "Multiobjective optimization", "Multivariate normal distribution", "NEWUOA", "Natural Evolution Strategies", "Natural evolution strategy", "Nelder-Mead method", "Nelder\u2013Mead method", "Neuroevolution", "No free lunch in search and optimization", "Nonlinear programming", "Numerical optimization", "Objective function", "Optimization (mathematics)", "Orthogonal matrix", "Positive-definite matrix", "Premature convergence", "Principal components analysis", "Program synthesis", "Pseudocode", "PubMed Identifier", "Quadratic programming", "Quasi-Newton method", "Rate of convergence", "Relative entropy", "Robust optimization", "Scale-invariance", "Score (statistics)", "Square root of a matrix", "Stochastic", "Stochastic optimization", "Stochastic programming", "Up to", "Variable-metric"], "categories": ["Evolutionary algorithms", "Optimization algorithms and methods", "Pages with citations lacking titles", "Pages with duplicate reference names", "Pages with reference errors", "Stochastic optimization"], "title": "CMA-ES"}
{"summary": "Design Automation usually refers to electronic design automation, or Design Automation which is a Product configurator. Extending Computer-Aided Design (CAD), automated design and Computer-Automated Design (CAutoD)  are more concerned with a broader range of applications, such as automotive engineering, civil engineering, composite material design, control engineering, dynamic system identification, financial systems, industrial equipment, mechatronic systems, steel construction, structural optimisation, and the invention of novel systems.\nThe concept of CAutoD perhaps first appeared in 1963, in the IBM Journal of Research and Development [1], where a computer program was written (1) to search for logic circuits having certain constraints on hardware design and (2) to evaluate these logics in terms of their discriminating ability over samples of the character set they are expected to recognize. More recently, traditional CAD simulation is seen to be transformed to CAutoD by biologically-inspired machine learning or search techniques such as evolutionary computation, including swarm intelligence algorithms[3].", "links": ["A posteriori", "Automotive engineering", "Civil engineering", "Composite material", "Computer-Aided Design", "Control engineering", "Crossover (genetic algorithm)", "Design Automation", "Design Automation Conference", "Digital prototyping", "Electronic design automation", "Evolutionary algorithm", "Evolutionary computation", "Exhaustive search", "Exponential algorithm", "Financial", "Fitness function", "Genetic algorithm", "Loss function", "Machine learning", "Mechatronic", "Mutation", "Natural selection", "Optimization (mathematics)", "Product configurator", "Search algorithm", "Search problem", "Simulation", "Steel construction", "Survival of the fittest", "Swarm intelligence", "System identification", "Virtual engineering"], "categories": ["Computer-aided design", "Design", "Evolutionary algorithms", "Evolutionary computation"], "title": "Computer-automated design"}
{"summary": "The constructive cooperative coevolutionary algorithm (also called C3) is an global optimisation algorithm in artificial intelligence based on the multi-start architecture of the greedy randomized adaptive search procedure (GRASP). It incorporates the existing cooperative coevolutionary algorithm (CC). The considered problem is decomposed into subproblems. These subproblems are optimised separately while exchanging information in order to solve the complete problem. An optimisation algorithm, usually but not necessarily an evolutionary algorithm, is embedded in C3 for optimising those subproblems. The nature of the embedded optimisation algorithm determines whether C3's behaviour is deterministic or stochastic.\nThe C3 optimisation algorithm was originally designed for simulation-based optimisation but it can be used for global optimisation problems in general. Its strength over other optimisation algorithms, specifically cooperative coevolution, is that it is better able to handle non-separable optimisation problems.", "links": ["Artificial intelligence", "Cooperative coevolution", "Deterministic", "Differential evolution", "Evolutionary algorithm", "Genetic algorithms", "Global optimisation", "Global optimization", "Greedy randomized adaptive search procedure", "Hyper-heuristics", "Metaheuristic", "Particle swarm optimization", "Stochastic", "Stochastic optimization", "Stochastic search", "Swarm intelligence"], "categories": ["Evolutionary algorithms", "Evolutionary computation", "Mathematical optimization", "Optimization algorithms and methods"], "title": "Constructive cooperative coevolution"}
{"summary": "Cuckoo search (CS) is an optimization algorithm developed by Xin-she Yang and Suash Deb in 2009. It was inspired by the obligate brood parasitism of some cuckoo species by laying their eggs in the nests of other host birds (of other species). Some host birds can engage direct conflict with the intruding cuckoos. For example, if a host bird discovers the eggs are not their own, it will either throw these alien eggs away or simply abandon its nest and build a new nest elsewhere. Some cuckoo species such as the New World brood-parasitic Tapera have evolved in such a way that female parasitic cuckoos are often very specialized in the mimicry in colors and pattern of the eggs of a few chosen host species \nCuckoo search idealized such breeding behavior, and thus can be applied for various optimization problems. It seems that it can outperform other metaheuristic algorithms in applications.\nCuckoo search (CS) uses the following representations:\nEach egg in a nest represents a solution, and a cuckoo egg represents a new solution. The aim is to use the new and potentially better solutions (cuckoos) to replace a not-so-good solution in the nests. In the simplest form, each nest has one egg. The algorithm can be extended to more complicated cases in which each nest has multiple eggs representing a set of solutions.\nCS is based on three idealized rules:\nEach cuckoo lays one egg at a time, and dumps its egg in a randomly chosen nest;\nThe best nests with high quality of eggs will carry over to the next generation;\nThe number of available hosts nests is fixed, and the egg laid by a cuckoo is discovered by the host bird with a probability . Discovering operate on some set of worst nests, and discovered solutions dumped from farther calculations.\nIn addition, Yang and Deb discovered that the random-walk style search is better performed by L\u00e9vy flights rather than simple random walk.\nThe pseudo-code can be summarized as:\n\nObjective function: \nGenerate an initial population of  host nests; \nWhile (t<MaxGeneration) or (stop criterion)\n   Get a cuckoo randomly (say, i) and replace its solution by performing L\u00e9vy flights;\n   Evaluate its quality/fitness \n         [For maximization,  ];\n   Choose a nest among n (say, j) randomly;\n   if (),\n          Replace j by the new solution;\n   end if\n   A fraction () of the worse nests are abandoned and new ones are built;\n   Keep the best solutions/nests;\n   Rank the solutions/nests and find the current best;\n   Pass the current best solutions to the next generation;\nend while\n\nAn important advantage of this algorithm is its simplicity. In fact, comparing with other population- or agent-based metaheuristic algorithms such as particle swarm optimization and harmony search, there is essentially only a single parameter  in CS (apart from the population size ). Therefore, it is very easy to implement.", "links": ["Active matter", "Agent-based model", "Agent-based model in biology", "Algorithm", "Allee effect", "Altitudinal migration", "Animal migration", "Animal migration tracking", "Animal navigation", "Ant colony optimization algorithms", "Ant robotics", "Approximation algorithm", "ArXiv", "Artificial Ants", "Artificial bee colony algorithm", "Augmented Lagrangian method", "Bait ball", "Barrier function", "Bat algorithm", "Bees algorithm", "Bellman\u2013Ford algorithm", "Bird migration", "Boids", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Cell migration", "Clustering of self-propelled particles", "Coded wire tag", "Collective animal behavior", "Collective intelligence", "Collective motion", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Crowd simulation", "Cuckoo", "Cuckoo hashing", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Decentralised system", "Diel vertical migration", "Differential evolution", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Eusociality", "Evolutionary algorithm", "Exchange algorithm", "Feeding frenzy", "Firefly algorithm", "Fish migration", "Flock (birds)", "Flocking (behavior)", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Glowworm swarm optimization", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Group size measures", "Harmony search", "Herd", "Herd behavior", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Homing (biology)", "Insect migration", "Integer programming", "Intelligent Small World Autonomous Robots for Micro-manipulation", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Lepidoptera migration", "Lessepsian migration", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "L\u00e9vy flight", "Mathematical optimization", "Matroid", "Metaheuristic", "Microbial intelligence", "Microbotics", "Minimum spanning tree", "Mixed-species foraging flock", "Mobbing (animal behavior)", "Monarch butterfly migration", "Mutualism (biology)", "Natal homing", "Nelder\u2013Mead method", "New World", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Obligate parasite", "Optimization (mathematics)", "Optimization algorithm", "Pack (canine)", "Pack hunter", "Particle swarm optimization", "Patterns of self-organization in ants", "Penalty method", "Philopatry", "Powell's method", "Predator satiation", "Pseudo-code", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Quorum sensing", "Random walk", "Reverse migration (birds)", "Revised simplex algorithm", "Salmon run", "Sardine run", "Sea turtle migration", "Self-propelled particles", "Sequential quadratic programming", "Shoaling and schooling", "Simplex algorithm", "Simulated annealing", "Sort sol (bird flock)", "Spatial organization", "Stigmergy", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Swarm (simulation)", "Swarm behaviour", "Swarm intelligence", "Swarm robotics", "Swarming (honey bee)", "Swarming (military)", "Swarming motility", "Symbrion", "Symmetric rank-one", "Symmetry breaking of escaping ants", "Tabu search", "Tapera", "Task allocation and partitioning of social insects", "Travelling salesman problem", "Truncated Newton method", "Trust region", "Vicsek model", "Wolfe conditions", "Xin-she Yang"], "categories": ["All articles needing additional references", "Articles needing additional references from June 2015", "Evolutionary algorithms", "Optimization algorithms and methods", "Pages using citations with accessdate and no URL"], "title": "Cuckoo search"}
{"summary": "DarwinTunes is a research project into the use of natural selection to create music led by Bob MacCallum and Armand Leroi, scientists at Imperial College London. The project asks volunteers on the Internet to listen to automatically generated sound loops and rate them based on aesthetic preference. After the volunteers rate the loops on a five-point scale, software permits the highest rated loops to 'reproduce sexually' and populate the next generation of musical loops.\nIn a paper published in the Proceedings of the National Academies of Science, the DarwinTunes developers describe how their first experimental population derived from two randomly generated founding loops, allowed 100 generations of loops to evolve without any selection pressure before asking members of the public to rate the loops. The paper found that for the first 500 to 600 generations, aesthetic quality of the loops dramatically improved before reaching a stable equilibrium. They tested this using ratings by listeners and also by using sampling techniques used by music information retrieval technology\u2014namely the Chordino and Rhythm Patterns algorithms, which measure the presence of chords used in Western music and the presence of rhythm respectively.", "links": ["Digital object identifier", "Evolutionary music", "Imperial College London", "Music", "Music information retrieval", "Music theory", "Natural selection", "Proceedings of the National Academies of Science"], "categories": ["All stub articles", "Artificial life models", "Computer music software", "Evolutionary algorithms", "Music software stubs", "Music theory stubs"], "title": "DarwinTunes"}
{"summary": "Eagle strategy is a search strategy for solving nonlinear optimization problems, and this strategy was developed by Xin-she Yang and Suash Deb, based on the foraging behaviour of eagle species such as golden eagles.\nIn optimization, a common strategy is to search for the optimal solution starting from a set of initial guess solutions (either random and educated guess). In the case when the cost functions are multimodal with multiple local best solutions, the final solutions may heavily depend on the initial starting solutions. In order to minimize such dependence on initial random solutions, most modern algorithms, especially metaheuristic algorithms, are able to escape local optima by using some sophisticated random techniques. However, most of these algorithms are one-stage type; that is, once initialization is done, the search process continues until an algorithm stops. Running an algorithm many times from different initial solutions may occasionally improve the overall performance on average.\nEagle strategy improves this by using an iterative, interacting two-stage strategy to enhance the search efficiency by escaping the local optima and use initial solutions in different regions. It uses a slow search stage and a fast stage to simulate an eagle searching for prey tends to search on a large area and then quickly switches to a rapid chasing phase once a prey is in sight. In optimization, it uses a coarse search stage on a larger area in a search space in combination with an intensive faster search algorithm in the neighbourhood of promising solutions. Two stages interchanges and proceed iteratively.\nAs there are two stages in the strategy, each stage can employ different algorithms. For example, differential evolution can be used within eagle strategy. Studies show that such a combination is better than any of its components.\nIn the simplest case, when the first stage does not use any algorithm (just initialization), it essentially degenerates into a hill-climbing with random restart. However, this strategy could be potentially much more powerful if a good combination of different algorithms is used.", "links": ["Algorithm", "Differential evolution", "Eagle", "Golden eagle", "Hill-climbing", "Metaheuristic", "Optimization", "Random", "Xin-she Yang"], "categories": ["Artificial intelligence", "Evolutionary algorithms", "Optimization algorithms and methods"], "title": "Eagle strategy"}
{"summary": "there are several algorithms for locating landmarks in images such as satellite maps, medical images etc.\nnowadays evolutionary algorithms such as particle swarm optimization are so useful to perform this task. evolutionary algorithms generally have two phase, training and test.\nin the training phase, we try to learn the algorithm to locate landmark correctly. this phase performs in some iterations and finally in the last iteration we hope to obtain a system that can locate the landmark, correctly. in the particle swarm optimization there are some particles that search for the landmark. each particle uses a specific formula in each iteration to optimizes the landmark detecting.\nThe fundamental particle swarm optimization algorithm used in training phase generally as follows:\nRandomly initialise 100 individuals in the search space in the range [-1,1]\nLOOP UNTIL 100 iterations performed OR detection error of gbest is 0%\nFOR each particle p\nDetection errors at x = 0\nFOR each image i in training set\nFOR each pixel coordinate c in i\nEvaluate x of p on visual features at c\nIF evaluation is highest so far for i THEN\nDetected position in i = c\nIF distance between detected position and marked-up position > 2mm THEN\nDetection errors at x = Detection errors at x + 1\nFitness of p at x = 1- ( Detection errors at x /Total no. of images in training set)\nIF new _tness of p at x > previous _tness of p at pbest THEN\npbest _tness of p = new _tness of p at x\npbest position of p = x of p\nIF new _tness of p at x > previous gbest _tness THEN\ngbest _tness = new _tness of p at x\ngbest position of p = x of p\nFOR each particle p\nCalculate v of p\nIF magnitude of v > v max THEN\nMagnitude of v = v max\nMove x of p to next position using v\nIF x of p outside [-1,1] range THEN\nx of p = -1 or 1 as appropriate\nREPEAT\nOutput gbest of last iteration as trained detector d", "links": ["Algorithm", "Detection error", "Evolutionary algorithm", "Formula", "Iteration", "Landmark", "Landmark detecting", "Mathematical optimization", "Medical image", "Particle", "Particle swarm optimization", "Satellite map"], "categories": ["All articles needing cleanup", "All articles needing expert attention", "All orphaned articles", "Articles needing cleanup from December 2010", "Articles needing expert attention from December 2010", "Articles needing expert attention with no reason or talk parameter", "Articles needing unspecified expert attention", "Cleanup tagged articles without a reason field from December 2010", "Evolutionary algorithms", "Orphaned articles from November 2010", "Wikipedia pages needing cleanup from December 2010"], "title": "Evolutionary Algorithm for Landmark Detection"}
{"summary": "Evolutionary music is the audio counterpart to Evolutionary art, whereby algorithmic music is created using an evolutionary algorithm. The process begins with a population of individuals which by some means or other produce audio (e.g. a piece, melody, or loop), which is either initialized randomly or based on human-generated music. Then through the repeated application of computational steps analogous to biological selection, recombination and mutation the aim is for the produced audio to become more musical. Evolutionary sound synthesis is a related technique for generating sounds or synthesizer instruments. Evolutionary music is typically generated using an interactive evolutionary algorithm where the fitness function is the user or audience, as it is difficult to capture the aesthetic qualities of music computationally. However, research into automated measures of musical quality is also active. Evolutionary computation techniques have also been applied to harmonization and accompaniment tasks. The most commonly used evolutionary computation techniques are genetic algorithms and genetic programming.", "links": ["Accompaniment", "Al Biles Virtual Quintet", "Algorithmic composition", "Algorithmic music", "DarwinTunes", "Eurovision Song Contest", "Evolutionary algorithm", "Evolutionary art", "Evolutionary musicology", "Evolutionary sound synthesis", "Fitness function", "Generative music", "Genetic algorithm", "Genetic programming", "Genetic recombination", "Harmonization", "Interactive evolutionary computation", "MIDI", "Microsoft Windows", "Mp3", "Mutation", "Natural selection", "PDF", "Population", "Pun", "Rodney Waschka II", "Synthesizer", "XML"], "categories": ["Electronic music", "Evolutionary algorithms"], "title": "Evolutionary music"}
{"summary": "Evolutionary programming is one of the four major evolutionary algorithm paradigms. It is similar to genetic programming, but the structure of the program to be optimized is fixed, while its numerical parameters are allowed to evolve.\nIt was first used by Lawrence J. Fogel in the US in 1960 in order to use simulated evolution as a learning process aiming to generate artificial intelligence. Fogel used finite state machines as predictors and evolved them. Currently evolutionary programming is a wide evolutionary computing dialect with no fixed structure or (representation), in contrast with some of the other dialects. It is becoming harder to distinguish from evolutionary strategies.\nIts main variation operator is mutation; members of the population are viewed as part of a specific species rather than members of the same species therefore each parent generates an offspring, using a (\u03bc + \u03bc) survivor selection.", "links": ["Academic journal", "Algorithms", "Artificial development", "Artificial intelligence", "CMA-ES", "Cellular evolutionary algorithm", "Computer science", "Convergence (evolutionary computing)", "Developmental biology", "Differential evolution", "Digital organism", "Evolution", "Evolution strategy", "Evolutionary Computation (journal)", "Evolutionary algorithm", "Evolutionary computation", "Evolutionary computing", "Evolutionary data mining", "Evolutionary multimodal optimization", "Evolutionary robotics", "Finite state machine", "Fitness approximation", "Fitness function", "Fitness landscape", "Gene expression programming", "Genetic algorithm", "Genetic operator", "Genetic operators", "Genetic programming", "Genetic representation", "Human-based evolutionary computation", "Interactive evolutionary computation", "Lawrence J. Fogel", "Learning classifier system", "List of digital organism simulators", "Machine learning", "Mutation (genetic algorithm)", "Natural evolution strategy", "Neuroevolution", "No free lunch in search and optimization", "Program synthesis", "Selection (genetic algorithm)"], "categories": ["All stub articles", "Computer science stubs", "Evolutionary algorithms", "Optimization algorithms and methods"], "title": "Evolutionary programming"}
{"summary": "In radio communications, an evolved antenna is an antenna designed fully or substantially by an automatic computer design program that uses an evolutionary algorithm that mimics Darwinian evolution. This sophisticated procedure has been used in recent years to design a few antennas for mission-critical applications involving stringent, conflicting, or unusual design requirements, such as unusual radiation patterns, for which none of the many existing antenna types are adequate.\nThe computer program starts with simple antenna shapes, then adds or modifies elements in a semirandom manner to create a number of new candidate antenna shapes. These are then evaluated to determine how well they fulfill the design requirements, and a numerical score is computed for each. Then, in a step similar to natural selection, a portion of the candidate antennas with the worst scores are discarded, leaving a small population of the highest-scoring designs. Using these antennas, the computer repeats the procedure, generating a population of even higher-scoring designs. After a number of iterations, the population of antennas is evaluated and the highest-scoring design is chosen. The resulting antenna often outperforms the best manual designs, because it has a complicated asymmetric shape that could not have been found with traditional manual design methods.\nThe first evolved antenna designs appeared in the mid-1990s from the work of Michielssen, Altshuler, Linden, Haupt, and Rahmat-Samii. Most practitioners use the genetic algorithm technique or some variant thereof to evolve antenna designs.\nAn example of an evolved antenna is an X-band antenna evolved for a 2006 NASA mission called Space Technology 5 (ST5). The mission consists of three satellites that will take measurements in Earth's magnetosphere. Each satellite has two communication antennas to talk to ground stations. The antenna has an unusual structure and was evolved to meet a challenging set of mission requirements, notably the combination of wide beamwidth for a circularly polarized wave and wide impedance bandwidth. For comparison, a traditional approach to meet the mission requirements might involve a helical antenna design, or specifically, a quadrifilar helix. The ST5 mission successfully launched on March 22, 2006, and so this evolved antenna represents the world's first artificially-evolved object to fly in space.", "links": ["ALLISS", "AWX antenna", "Adcock antenna", "Antenna (radio)", "Batwing antenna", "Beamwidth", "Beverage antenna", "Biconical antenna", "Cage aerial", "Cantenna", "Cassegrain antenna", "Charles Samuel Franklin", "Choke ring antenna", "Coaxial antenna", "Collinear antenna array", "Conformal antenna", "Corner reflector antenna", "Crossed field antenna", "Curtain array", "Darwinism", "Dielectric resonator antenna", "Dipole antenna", "Directional antenna", "Discone antenna", "Doublet antenna", "Earth", "Evolution", "Evolutionary computing", "Folded inverted conformal antenna", "Folded unipole antenna", "Fractal antenna", "G5RV antenna", "Genetic algorithm", "Gizmotchy", "Ground dipole", "Halo antenna", "Helical antenna", "Horn antenna", "Inverted-F antenna", "Inverted vee antenna", "Isotropic radiator", "Isotropy", "J-pole antenna", "Log-periodic antenna", "Loop antenna", "Magnetosphere", "Mast radiator", "Microstrip antenna", "Monopole antenna", "NASA", "Natural selection", "Near vertical incidence skywave", "Offset dish antenna", "Omnidirectional antenna", "Parabolic antenna", "Patch antenna", "Phased array", "Planar array", "Plasma antenna", "Polarization (waves)", "Quad antenna", "Radiation pattern", "Radio communications", "Random wire antenna", "Reconfigurable antenna", "Rectenna", "Reference antenna", "Reflective array antenna", "Regenerative loop antenna", "Rhombic antenna", "Rubber ducky antenna", "Satellite", "Sector antenna", "Short backfire antenna", "Sloper antenna", "Slot antenna", "Space Technology 5", "Sterba antenna", "T-Antenna", "T2FD antenna", "Television antenna", "Turnstile antenna", "Umbrella antenna", "Vivaldi antenna", "Wave impedance", "Whip antenna", "WokFi", "Wullenweber", "X-band", "Yagi-Uda antenna"], "categories": ["Evolutionary algorithms", "Radio frequency antenna types"], "title": "Evolved antenna"}
{"summary": "Gaussian adaptation (GA) (also referred to as normal or natural adaptation and sometimes abbreviated as NA) is an evolutionary algorithm designed for the maximization of manufacturing yield due to statistical deviation of component values of signal processing systems. In short, GA is a stochastic adaptive process where a number of samples of an n-dimensional vector x[xT = (x1, x2, ..., xn)] are taken from a multivariate Gaussian distribution, N(m, M), having mean m and moment matrix M. The samples are tested for fail or pass. The first- and second-order moments of the Gaussian restricted to the pass samples are m* and M*.\nThe outcome of x as a pass sample is determined by a function s(x), 0 < s(x) < q \u2264 1, such that s(x) is the probability that x will be selected as a pass sample. The average probability of finding pass samples (yield) is\n\nThen the theorem of GA states:\n\nFor any s(x) and for any value of P < q, there always exist a Gaussian p. d. f. that is adapted for maximum dispersion. The necessary conditions for a local optimum are m = m* and M proportional to M*. The dual problem is also solved: P is maximized while keeping the dispersion constant (Kjellstr\u00f6m, 1991).\n\nProofs of the theorem may be found in the papers by Kjellstr\u00f6m, 1970, and Kjellstr\u00f6m & Tax\u00e9n, 1981.\nSince dispersion is defined as the exponential of entropy/disorder/average information it immediately follows that the theorem is valid also for those concepts. Altogether, this means that Gaussian adaptation may carry out a simultaneous maximisation of yield and average information (without any need for the yield or the average information to be defined as criterion functions).\nThe theorem is valid for all regions of acceptability and all Gaussian distributions. It may be used by cyclic repetition of random variation and selection (like the natural evolution). In every cycle a sufficiently large number of Gaussian distributed points are sampled and tested for membership in the region of acceptability. The centre of gravity of the Gaussian, m, is then moved to the centre of gravity of the approved (selected) points, m*. Thus, the process converges to a state of equilibrium fulfilling the theorem. A solution is always approximate because the centre of gravity is always determined for a limited number of points.\nIt was used for the first time in 1969 as a pure optimization algorithm making the regions of acceptability smaller and smaller (in analogy to simulated annealing, Kirkpatrick 1983). Since 1970 it has been used for both ordinary optimization and yield maximization.", "links": ["Average information", "CMA-ES", "Central limit theorem", "Covariance matrix", "Developmental Psychobiology", "ETH Zurich", "Entropy in thermodynamics and information theory", "Evolutionary algorithm", "Fisher's fundamental theorem of natural selection", "Free will", "Genetic algorithm", "Hardy\u2013Weinberg law", "Hebbian learning", "Hebbian theory", "Information content", "Information entropy", "Mean fitness", "Multivariate Gaussian distribution", "Normal distribution", "Punctuated equilibrium", "Random search", "Signal processing", "Simulated annealing", "Stochastic optimization", "Swiss Institute of Bioinformatics", "Unit of selection"], "categories": ["All articles lacking reliable references", "All articles needing additional references", "All articles needing expert attention", "Articles lacking reliable references from July 2008", "Articles needing additional references from July 2008", "Articles needing expert attention from January 2015", "Articles needing expert attention with no reason or talk parameter", "Creationism", "Creativity", "Evolutionary algorithms", "Free will", "Mathematics articles needing expert attention", "Wikipedia articles with possible conflicts of interest from March 2009"], "title": "Gaussian adaptation"}
{"summary": "In computer programming, genetic representation is a way of representing solutions/individuals in evolutionary computation methods. Genetic representation can encode appearance, behavior, physical qualities of individuals. Designing a good genetic representation that is expressive and evolvable is a hard problem in evolutionary computation. Difference in genetic representations is one of the major criteria drawing a line between known classes of evolutionary computation.\nGenetic algorithms use linear binary representations. The most standard one is an array of bits. Arrays of other types and structures can be used in essentially the same way. The main property that makes these genetic representations convenient is that their parts are easily aligned due to their fixed size. This facilitates simple crossover operation. Variable length representations were also explored in Genetic algorithms, but crossover implementation is more complex in this case.\nEvolution strategy uses linear real-valued representations, e.g. an array of real values. It uses mostly gaussian mutation and blending/averaging crossover.\nGenetic programming (GP) pioneered tree-like representations and developed genetic operators suitable for such representations. Tree-like representations are used in GP to represent and evolve functional programs with desired properties.\nHuman-based genetic algorithm (HBGA) offers a way to avoid solving hard representation problems by outsourcing all genetic operators to outside agents, in this case, humans. The algorithm has no need for knowledge of a particular fixed genetic representation as long as there are enough external agents capable of handling those representations, allowing for free-form and evolving genetic representations.", "links": ["Binary tree", "Bit", "Computer programming", "Evolution strategy", "Evolutionary computation", "Gaussian", "Genetic algorithm", "Genetic operator", "Genetic programming", "Genetic tree", "HBGA", "Human-based genetic algorithm", "Parse tree"], "categories": ["All articles needing additional references", "Articles needing additional references from December 2009", "Evolutionary algorithms"], "title": "Genetic representation"}
{"summary": "A learning classifier system, or LCS, is a machine learning system with close links to reinforcement learning and genetic algorithms. First described by John Holland, his LCS consisted of a population of binary rules on which a genetic algorithm altered and selected the best rules. Rule fitness was based on a reinforcement learning technique.", "links": ["Digital object identifier", "Genetic algorithms", "John Henry Holland", "Machine learning", "Neural network", "Reinforcement learning", "S-expression"], "categories": ["All articles with unsourced statements", "All stub articles", "Articles with unsourced statements from February 2014", "Articles with unsourced statements from July 2015", "Evolutionary algorithms", "Technology stubs"], "title": "Learning classifier system"}
{"summary": "Melomics (derived from \"genomics of melodies\") is a computational system for the automatic composition of music (with no human intervention), based on bioinspired algorithms.", "links": ["0music (album)", "Alan Turing Year", "Algorithmic composition", "Andalusia Technology Park", "CC0", "Clinical trials", "Commoditization", "Digital object identifier", "Entrepreneur", "Evo-devo", "Evolutionary music", "Hello World! (composition)", "Iamus (album)", "Iamus (computer)", "Internationalization and localization", "London Symphony Orchestra", "MIDI", "MP3", "Melomics109", "MusicXML", "Online advertising", "PDF", "Royalty-free", "Service (economics)", "Slogan", "Sound trademark", "The San Francisco Examiner", "Types of business entity", "University spin-off"], "categories": ["Biotechnology", "Evolutionary algorithms", "Music technology", "Spanish Supercomputing Network"], "title": "Melomics"}
{"summary": "Memetic algorithms (MA) represent one of the recent growing areas of research in evolutionary computation. The term MA is now widely used as a synergy of evolutionary or any population-based approach with separate individual learning or local improvement procedures for problem search. Quite often, MA are also referred to in the literature as Baldwinian evolutionary algorithms (EA), Lamarckian EAs, cultural algorithms, or genetic local search.", "links": ["Algorithm", "Artificial neural network", "Bin packing problem", "Charged particle beam", "Chromosome", "Circuit design", "Cluster analysis", "Digital object identifier", "Dual-phase evolution", "Evolutionary algorithm", "Evolutionary computation", "Expert system", "Expression profiling", "Feature selection", "Generalized Assignment Problem", "Genetic algorithm", "Genotype", "Graph coloring", "Graph partition", "Heuristic", "Hyper-heuristic", "IEEE Computational Intelligence Society", "IEEE Transactions on Evolutionary Computation", "Independent set problem", "International Standard Book Number", "Knapsack problem", "Meme", "Motion planning", "NHL", "NP (complexity)", "Nurse rostering problem", "Pattern recognition", "Processor allocation", "PubMed Identifier", "Quadratic assignment problem", "Richard Dawkins", "Schedule (workplace)", "Set cover problem", "Single machine scheduling", "Springer Science+Business Media", "Travelling salesman problem", "Universal Darwinism", "VLSI"], "categories": ["All Wikipedia articles needing context", "All articles with unsourced statements", "All pages needing cleanup", "Articles with unsourced statements from September 2014", "Evolutionary algorithms", "Wikipedia articles needing context from February 2011", "Wikipedia introduction cleanup from February 2011"], "title": "Memetic algorithm"}
{"summary": "In numerical optimization, meta-optimization is the use of one optimization method to tune another optimization method. Meta-optimization is reported to have been used as early as in the late 1970s by Mercer and Sampson  for finding optimal parameter settings of a genetic algorithm. Meta-optimization is also known in the literature as meta-evolution, super-optimization, automated parameter calibration, hyper-heuristics, etc.", "links": ["Ant colony optimization", "Curse of dimensionality", "Differential evolution", "Digital object identifier", "Discrete mathematics", "Genetic algorithm", "Genetic operators", "Hyper-heuristics", "Meta", "Numerical optimization", "Particle swarm optimization", "PubMed Central", "PubMed Identifier", "Real number", "Statistical models"], "categories": ["Evolutionary algorithms", "Heuristics", "Mathematical optimization", "Optimization algorithms and methods"], "title": "Meta-optimization"}
{"summary": "Natural evolution strategies (NES) are a family of numerical optimization algorithms for black-box problems. Similar in spirit to evolution strategies, they iteratively update the (continuous) parameters of a search distribution by following the natural gradient towards higher expected fitness.\n\n", "links": ["Academic journal", "Algorithms", "Approximation", "Artificial development", "Artificial intelligence", "Black box", "CMA-ES", "Cauchy distribution", "Cellular evolutionary algorithm", "Convergence (evolutionary computing)", "Covariance matrix", "Developmental biology", "Diagonal matrix", "Differential evolution", "Digital organism", "Evolution strategies", "Evolution strategy", "Evolutionary Computation (journal)", "Evolutionary algorithm", "Evolutionary computation", "Evolutionary data mining", "Evolutionary multimodal optimization", "Evolutionary programming", "Evolutionary robotics", "Expected value", "Fisher information", "Fitness approximation", "Fitness function", "Fitness landscape", "Gaussian distribution", "Gene expression programming", "Genetic algorithm", "Genetic operators", "Genetic programming", "Gradient ascent", "Heavy-tailed distribution", "Human-based evolutionary computation", "Information geometry", "Interactive evolutionary computation", "Learning classifier system", "List of digital organism simulators", "Logarithmic derivative", "Machine learning", "Monte Carlo method", "Neuroevolution", "No free lunch in search and optimization", "Numerical optimization", "Plateau (mathematics)", "Probability distribution", "Program synthesis", "Ranking", "Utility"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from March 2015", "Articles with example pseudocode", "Evolutionary algorithms", "Optimization algorithms and methods", "Stochastic optimization"], "title": "Natural evolution strategy"}
{"summary": "Neuroevolution, or neuro-evolution, is a form of machine learning that uses evolutionary algorithms to train artificial neural networks. It is most commonly applied in artificial life, computer games, and evolutionary robotics. A main benefit is that neuroevolution can be applied more widely than supervised learning algorithms, which require a syllabus of correct input-output pairs. In contrast, neuroevolution requires only a measure of a network's performance at a task. For example, the outcome of a game (i.e. whether one player won or lost) can be easily measured without providing labeled examples of desired strategies.", "links": ["Affective neuroscience", "Artificial development", "Artificial life", "Artificial neural network", "Artificial neuron", "Backpropagation", "Basic science", "Behavioral epigenetics", "Behavioral neurology", "Behavioral neuroscience", "Behavioural genetics", "Biological neural network", "Biological neuron models", "Brain\u2013computer interface", "Cellular neuroscience", "Chronobiology", "Clinical neurophysiology", "Clinical neuroscience", "Cognitive neuroscience", "Compositional pattern-producing network", "Computational neuroscience", "Connectomics", "Cultural neuroscience", "Deus Ex Neural Network", "Educational neuroscience", "Embryology", "Evolution of nervous systems", "Evolution strategies", "Evolutionary Acquisition of Neural Topologies", "Evolutionary algorithm", "Evolutionary computation", "Evolutionary neuroscience", "Evolutionary programming", "Evolutionary robotics", "Fitness function", "Games", "Genetic algorithm", "Genetic programming", "Genome", "Genotype", "HyperNEAT", "Hypercube", "Imaging genetics", "Integrative neuroscience", "Interactively Constrained Neuro-Evolution", "Machine learning", "Memetic algorithm", "Molecular cellular cognition", "Molecular neuroscience", "Motor control", "Neural Darwinism", "Neural development", "Neural engineering", "Neuro-ophthalmology", "Neuro-psychoanalysis", "NeuroEvolution of Augmented Topologies", "NeuroEvolution of Augmenting Topologies", "Neuroanatomy", "Neuroanthropology", "Neurobioengineering", "Neurobiology", "Neurobiotics", "Neurocardiology", "Neurochemistry", "Neurochip", "Neurocriminology", "Neurodegeneration", "Neurodevelopmental disorders", "Neurodiversity", "Neuroeconomics", "Neuroeducation", "Neuroembryology", "Neuroendocrinology", "Neuroepidemiology", "Neuroepistemology", "Neuroesthetics", "Neuroethics", "Neuroethology", "Neurogastroenterology", "Neurogenetics", "Neurohistory", "Neuroimaging", "Neuroimmunology", "Neuroinformatics", "Neurointensive care", "Neurolaw", "Neurolinguistics", "Neurology", "Neuromanagement", "Neuromarketing", "Neurometrics", "Neuromodulation (medicine)", "Neuromonitoring", "Neuromorphology", "Neurooncology", "Neuropathology", "Neuropharmacology", "Neurophenomenology", "Neurophilosophy", "Neurophysics", "Neurophysiology", "Neuroplasticity", "Neuropolitics", "Neuroprosthetics", "Neuropsychiatry", "Neuropsychology", "Neuroradiology", "Neurorehabilitation", "Neurorobotics", "Neuroscience", "Neurosociology", "Neurosurgery", "Neurotechnology", "Neurotheology", "Neurotology", "Neurotoxin", "Neurotransmitter", "Neurovirology", "Paleoneurology", "Phenotype", "Psychiatry", "Regeneration (biology)", "S-expressions", "Sensory neuroscience", "Signal processing", "Simulated annealing", "Social neuroscience", "Supervised learning", "Systems neuroscience"], "categories": ["Evolutionary algorithms"], "title": "Neuroevolution"}
{"summary": "In computer science, particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. PSO optimizes a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. Each particle's movement is influenced by its local best known position but, is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions.\nPSO is originally attributed to Kennedy, Eberhart and Shi and was first intended for simulating social behaviour, as a stylized representation of the movement of organisms in a bird flock or fish school. The algorithm was simplified and it was observed to be performing optimization. The book by Kennedy and Eberhart describes many philosophical aspects of PSO and swarm intelligence. An extensive survey of PSO applications is made by Poli.\nPSO is a metaheuristic as it makes few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. However, metaheuristics such as PSO do not guarantee an optimal solution is ever found. More specifically, PSO does not use the gradient of the problem being optimized, which means PSO does not require that the optimization problem be differentiable as is required by classic optimization methods such as gradient descent and quasi-newton methods. PSO can therefore also be used on optimization problems that are partially irregular, noisy, change over time, etc.", "links": ["Active matter", "Agent-based model", "Agent-based model in biology", "Allee effect", "Altitudinal migration", "Animal migration", "Animal migration tracking", "Animal navigation", "Ant colony optimization algorithms", "Ant robotics", "Artificial Ants", "Artificial Bee Colony Algorithm", "Bait ball", "Bat algorithm", "Bees algorithm", "Bird migration", "Boids", "Candidate solution", "Cell migration", "Clustering of self-propelled particles", "Coded wire tag", "Collective animal behavior", "Collective intelligence", "Collective motion", "Combinatorial optimization", "Computer science", "Computer simulation", "Constraint satisfaction", "Convergent sequence", "Convex programming", "Crowd simulation", "Decentralised system", "Derivative-free optimization", "Diel vertical migration", "Differentiable", "Digital object identifier", "Empirical", "Eusociality", "Feeding frenzy", "Firefly algorithm", "Fish migration", "Fish school", "Flock (birds)", "Flocking (behavior)", "Formula", "Genetic algorithm", "Glowworm swarm optimization", "Gradient", "Gradient descent", "Group size measures", "Herd", "Herd behavior", "Homing (biology)", "Infinite-dimensional optimization", "Insect migration", "Integer programming", "Intelligent Small World Autonomous Robots for Micro-manipulation", "International Standard Book Number", "Iterative method", "James Kennedy (social psychologist)", "Lepidoptera migration", "Lessepsian migration", "Local optimum", "Mathematical optimization", "Meta-optimization", "Metaheuristic", "Microbial intelligence", "Microbotics", "Mixed-species foraging flock", "Mobbing (animal behavior)", "Monarch butterfly migration", "Multi-objective optimization", "Multi-swarm optimization", "Multiobjective optimization", "Mutualism (biology)", "Natal homing", "Nonlinear programming", "Occam's razor", "Optimization (mathematics)", "Pack (canine)", "Pack hunter", "Pareto efficiency", "Particle filter", "Patterns of self-organization in ants", "Philopatry", "Point particle", "Position (vector)", "Predator satiation", "Premature convergence", "Program correctness", "PubMed Central", "PubMed Identifier", "Quadratic programming", "Quasi-newton methods", "Quorum sensing", "Real number", "Reverse migration (birds)", "Riccardo Poli", "Robust optimization", "Row vector", "Russell C. Eberhart", "Salmon run", "Sardine run", "Schools of thought", "Sea turtle migration", "Self-propelled particles", "Shoaling and schooling", "Social behaviour", "Sort sol (bird flock)", "Spatial organization", "Stigmergy", "Stochastic programming", "Swarm (simulation)", "Swarm behaviour", "Swarm intelligence", "Swarm robotics", "Swarming (honey bee)", "Swarming (military)", "Swarming motility", "Symbrion", "Symmetry breaking of escaping ants", "Task allocation and partitioning of social insects", "Uniform distribution (continuous)", "Velocity", "Vicsek model"], "categories": ["Evolutionary algorithms", "Optimization algorithms and methods", "Pages using citations with format and no URL"], "title": "Particle swarm optimization"}
{"summary": "Rectangular matrices can be multiplied when the size of the cache and cache line is not known to the algorithm, or cache-obliviously. Cache-oblivious matrix multiplication was first formalized by Harald Prokop.", "links": ["Cache-oblivious algorithm", "Divide and conquer algorithms", "Harald Prokop", "Matrix multiplication", "Row-major order"], "categories": ["Analysis of algorithms", "Cache (computing)", "External memory algorithms", "Matrix multiplication algorithms", "Models of computation"], "title": "Cache-oblivious matrix multiplication"}
{"summary": "In mathematics, given a non-empty set of objects of finite extension in n-dimensional space, for example a set of points, a bounding sphere, enclosing sphere or enclosing ball for that set is an n-dimensional solid sphere containing all of these objects.\nIn the plane the terms bounding or enclosing circle are used.\nUsed in computer graphics and computational geometry, a bounding sphere is a special type of bounding volume. There are several fast and simple bounding sphere construction algorithms with a high practical value in real-time computer graphics applications.\nIn statistics and operations research, the objects are typically points, and generally the sphere of interest is the minimal bounding sphere, that is, the sphere with minimal radius among all bounding spheres. It may be proven that such a sphere is unique: If there are two of them, then the objects in question lie within their intersection. But an intersection of two non-coinciding spheres of equal radius is contained in a sphere of smaller radius.\nThe problem of computing the center of a minimal bounding sphere is also known as the \"unweighted Euclidean 1-center problem\".", "links": ["1-center problem", "Bounding volume", "Cluster analysis", "Computational Geometry Algorithms Library", "Computational geometry", "Computer graphics", "Coreset", "Emo Welzl", "Least squares", "Linear programming", "Mathematics", "Measurement error", "NP-hard", "Nimrod Megiddo", "Operations research", "Piotr Indyk", "Prune and search", "Randomized algorithm", "Scattering (statistics)", "Smallest-circle problem", "Solid sphere", "Space", "Statistical analysis", "Statistics"], "categories": ["Geometric algorithms"], "title": "Bounding sphere"}
{"summary": "In computational geometry, the Bowyer\u2013Watson algorithm is a method for computing the Delaunay triangulation of a finite set of points in any number of dimensions. The algorithm can be used to obtain a Voronoi diagram of the points, which is the dual graph of the Delaunay triangulation.\nThe Bowyer\u2013Watson algorithm is an incremental algorithm. It works by adding points, one at a time, to a valid Delaunay triangulation of a subset of the desired points. After every insertion, any triangles whose circumcircles contain the new point are deleted, leaving a star-shaped polygonal hole which is then re-triangulated using the new point. By using the connectivity of the triangulation to efficiently locate triangles to remove, the algorithm can take O(N log N) operations to triangulate N points, although special degenerate cases exist where this goes up to O(N2).\nThe algorithm is sometimes known just as the Bowyer Algorithm or the Watson Algorithm. Adrian Bowyer and David Watson devised it independently of each other at the same time, and each published a paper on it in the same issue of The Computer Journal (see below).", "links": ["Adrian Bowyer", "Computational geometry", "Delaunay triangulation", "Digital object identifier", "Dimension", "Dual graph", "Fortune's algorithm", "Hilbert curve", "Pseudocode", "Star-shaped polygon", "The Computer Journal", "Voronoi diagram"], "categories": ["Geometric algorithms"], "title": "Bowyer\u2013Watson algorithm"}
{"summary": "In mathematics, a Bregman divergence or Bregman distance is similar to a metric, but does not satisfy the triangle inequality nor symmetry. There are two ways in which Bregman divergences are important. Firstly, they generalize squared Euclidean distance to a class of distances that all share similar properties. Secondly, they bear a strong connection to exponential families of distributions; as has been shown by (Banerjee et al. 2005), there is a bijection between regular exponential families and regular Bregman divergences.\nBregman divergences are named after Lev M. Bregman, who introduced the concept in 1967. More recently researchers in geometric algorithms have shown that many important algorithms can be generalized from Euclidean metrics to distances defined by Bregman divergence (Banerjee et al. 2005; Nielsen and Nock 2006; Boissonnat et al. 2010).", "links": ["ArXiv", "Bijection", "Computational geometry", "Conference on Neural Information Processing Systems", "Convex conjugate", "Convex function", "Convex set", "Delaunay triangulation", "Digital object identifier", "Discrete and Computational Geometry", "Exponential family", "Hamming distance", "IEEE Transactions on Information Theory", "Itakura\u2013Saito distance", "Journal of Machine Learning Research", "Kullback\u2013Leibler divergence", "Lev M. Bregman", "Mahalanobis distance", "Mathematics", "Metric (mathematics)", "Mutual information", "Precision and recall", "Projective duality", "Stein's loss", "Submodular set function", "Taylor expansion", "Triangle inequality", "Von Neumann entropy", "Voronoi diagram"], "categories": ["Geometric algorithms", "Statistical distance measures"], "title": "Bregman divergence"}
{"summary": "In geometry, a centroidal Voronoi tessellation (CVT) is a special type of Voronoi tessellation or Voronoi diagrams. A Voronoi tessellation is called centroidal when the generating point of each Voronoi cell is also its mean (center of mass). It can be viewed as an optimal partition corresponding to an optimal distribution of generators. A number of algorithms can be used to generate centroidal Voronoi tessellations, including Lloyd's algorithm for K-means clustering.\nGersho's conjecture, proven for one and two dimensions, says that \"asymptotically speaking, all cells of the optimal CVT, while forming a tessellation, are congruent to a basic cell which depends on the dimension.\" In two dimensions, the basic cell for the optimal CVT is a regular hexagon.\nCentroidal Voronoi tessellations are useful in data compression, optimal quadrature, optimal quantization, clustering, and optimal mesh generation. Many patterns seen in nature are closely approximated by a Centroidal Voronoi tessellation. Examples of this include the Giant's Causeway, the cells of the cornea, and the breeding pits of the male tilapia.\nA weighted centroidal Voronoi diagrams is a CVT in which each centroid is weighted according to a certain function. For example, a grayscale image can be used as a density function to weight the points of a CVT, as a way to create digital stippling.", "links": ["Congruence (geometry)", "Cornea", "Data clustering", "Data compression", "Digital object identifier", "Geometry", "Giant's Causeway", "Grayscale", "Hexagon", "K-means clustering", "Lloyd's algorithm", "Numerical integration", "Patterns in nature", "Quantization (signal processing)", "Stippling", "Tessellation", "Tilapia", "Vance Faber", "Voronoi diagram"], "categories": ["CS1 maint: Explicit use of et al.", "Diagrams", "Discrete geometry", "Geometric algorithms"], "title": "Centroidal Voronoi tessellation"}
{"summary": "The Computational Geometry Algorithms Library (CGAL) is a software library of computational geometry algorithms. While primarily written in C++, Scilab bindings and bindings generated with SWIG (supporting Python and Java for now) are also available.\nThe software is available under dual licensing scheme. When used for other open source software, it is available under open source licenses (LGPL or GPL depending on the component). In other cases commercial license may be purchased, under different options for academic/research and industrial customers.", "links": ["Algebra", "Algorithm", "Application framework", "Arithmetic", "Arrangement of hyperplanes", "Boost libraries", "C++", "Computational geometry", "Convex hull", "Delaunay triangulation", "ETH Zurich", "European Strategic Program on Research in Information Technology", "European Union", "Free University of Berlin", "GPL", "Geometric primitive", "INRIA", "Interpolation", "Java (programming language)", "Johannes Kepler University Linz", "LGPL", "Library (computing)", "Library of Efficient Data types and Algorithms", "Linux", "List of software categories", "MS Windows", "Mac OS", "Martin-Luther-University Halle-Wittenberg", "Max Planck Institute for Informatics", "Mesh generation", "Microsoft Windows", "OPEN CASCADE", "OpenSCAD", "Open source license", "Operating system", "Polygon", "Polyhedra", "Python (programming language)", "Q Public License", "SWIG", "Saarbr\u00fccken", "Scilab", "Software developer", "Software license", "Software release life cycle", "Solaris (operating system)", "Sophia Antipolis", "Tel-Aviv University", "Triangulation", "Utrecht University", "Voronoi diagram"], "categories": ["All articles containing potentially dated statements", "Articles containing potentially dated statements from 2010", "Articles containing potentially dated statements from 2013", "C++ libraries", "Free computer libraries", "Geometric algorithms", "Max Planck Institute for Informatics", "Python libraries"], "title": "CGAL"}
{"summary": "The closest pair of points problem or closest pair problem is a problem of computational geometry: given n points in metric space, find a pair of points with the smallest distance between them. The closest pair problem for points in the Euclidean plane was among the first geometric problems which were treated at the origins of the systematic study of the computational complexity of geometric algorithms.\nA naive algorithm of finding distances between all pairs of points and selecting the minimum requires O(dn2) time. It turns out that the problem may be solved in O(n log n) time in a Euclidean space or Lp space of fixed dimension d. In the algebraic decision tree model of computation, the O(n log n) algorithm is optimal. The optimality follows from the observation that the element uniqueness problem (with the lower bound of \u03a9(n log n) for time complexity) is reducible to the closest pair problem: checking whether the minimal distance is 0 after the solving of the closest pair problem answers the question whether there are two coinciding points.\nIn the computational model which assumes that the floor function is computable in constant time the problem can be solved in O(n log log n) time. If we allow randomization to be used together with the floor function, the problem can be solved in O(n) time.", "links": ["Algebraic decision tree", "Analysis of algorithms", "Big O notation", "Bounding box", "Brute-force search", "Charles E. Leiserson", "Clifford Stein", "Computational geometry", "Dan Hoey", "Data structure", "Delaunay triangulation", "Divide and conquer (algorithm)", "Dynamic problem (algorithms)", "Element uniqueness problem", "Euclidean space", "Floor function", "GIS", "IEEE", "Introduction to Algorithms", "Lp space", "Master theorem", "Metric space", "Michael Ian Shamos", "Model of computation", "Nearest neighbor search", "Recursion", "Ronald L. Rivest", "SIAM J. Comput.", "Set (abstract data type)", "Set cover problem", "Symposium on Foundations of Computer Science", "Thomas H. Cormen", "Voronoi diagram"], "categories": ["All articles with unsourced statements", "Articles with example pseudocode", "Articles with unsourced statements from October 2015", "Geometric algorithms"], "title": "Closest pair of points problem"}
{"summary": "In computational geometry, the cone algorithm is an algorithm for identifying the particles that are near the surface of an object composed of discrete particles. Its applications include computational surface science and computational nano science. The cone algorithm was first described in a publication about nanogold in 2005.\nThe cone algorithm works well with clusters in condensed phases, including solid and liquid phases. It can handle the situations when one configuration includes multiple clusters or when holes exist inside clusters. It can also be applied to a cluster iteratively to identify multiple sub-surface layers.", "links": ["Algorithm", "Computational geometry", "Digital object identifier", "Nanogold", "Nanotechnology", "Surface science"], "categories": ["Geometric algorithms", "Molecular modelling software"], "title": "Cone algorithm"}
{"summary": "Geometric design (GD), also known as geometric modelling, is a branch of computational geometry. It deals with the construction and representation of free-form curves, surfaces, or volumes. Core problems are curve and surface modelling and representation. GD studies especially the construction and manipulation of curves and surfaces given by a set of points using polynomial, rational, piecewise polynomial, or piecewise rational methods. The most important instruments here are parametric curves and parametric surfaces, such as B\u00e9zier curves, spline curves and surfaces. An important non-parametric approach is the level set method.\nApplication areas include shipbuilding, aircraft, and automotive industries, as well as architectural design. The modern ubiquity and power of computers means that even perfume bottles and shampoo dispensers are designed using techniques unheard of by shipbuilders of 1960s.\nGeometric models can be built for objects of any dimension in any geometric space. Both 2D and 3D geometric models are extensively used in computer graphics. 2D models are important in computer typography and technical drawing. 3D models are central to computer-aided design and manufacturing, and many applied technical fields such as geology and medical image processing.\nGeometric models are usually distinguished from procedural and object-oriented models, which define the shape implicitly by an algorithm. They are also contrasted with digital images and volumetric models; and with implicit mathematical models such as the zero set of an arbitrary polynomial. However, the distinction is often blurred: for instance, geometric shapes can be represented by objects; a digital image can be interpreted as a collection of colored squares; and geometric shapes such as circles are defined by implicit mathematical equations. Also, the modeling of fractal objects often requires a combination of geometric and procedural techniques.\nGeometric problems originating in architecture can lead to interesting research and results in geometry processing, computer-aided geometric design, and discrete differential geometry.", "links": ["2D geometric model", "3D modeling", "Aircraft", "Algorithm", "Architectural design", "Architectural geometry", "Automotive industry", "B\u00e9zier curve", "CAD", "Circle", "Color", "Computational geometry", "Computational topology", "Computer-aided design", "Computer-aided engineering", "Computer-aided manufacturing", "Computer graphics", "Digital geometry", "Digital image", "Dimension", "Fractal", "Geologic modeling", "Implicit model", "Level set method", "List of interactive geometry software", "Medical image processing", "Object-Oriented Modeling", "Object-oriented programming", "Parametric curve", "Parametric surface", "Polynomial", "Procedural modeling", "Shipbuilding", "Solid modeling", "Space", "Space partitioning", "Spline (mathematics)", "Square (geometry)", "Technical drawing", "Typography", "Volumetric model", "Zero set"], "categories": ["Computational science", "Computer-aided design", "Geometric algorithms"], "title": "Geometric design"}
{"summary": "Geometric modeling is a branch of applied mathematics and computational geometry that studies methods and algorithms for the mathematical description of shapes.\nThe shapes studied in geometric modeling are mostly two- or three-dimensional, although many of its tools and principles can be applied to sets of any finite dimension. Today most geometric modeling is done with computers and for computer-based applications. Two-dimensional models are important in computer typography and technical drawing. Three-dimensional models are central to computer-aided design and manufacturing (CAD/CAM), and widely used in many applied technical fields such as civil and mechanical engineering, architecture, geology and medical image processing.\nGeometric models are usually distinguished from procedural and object-oriented models, which define the shape implicitly by an opaque algorithm that generates its appearance. They are also contrasted with digital images and volumetric models which represent the shape as a subset of a fine regular partition of space; and with fractal models that give an infinitely recursive definition of the shape. However, these distinctions are often blurred: for instance, a digital image can be interpreted as a collection of colored squares; and geometric shapes such as circles are defined by implicit mathematical equations. Also, a fractal model yields a parametric or implicit model when its recursive definition is truncated to a finite depth.\nNotable awards of the area are the John A. Gregory Memorial Award and the Bezier award.", "links": ["2D geometric model", "3D modeling", "Algorithm", "Algorithms", "Applied mathematics", "Architectural geometry", "Architecture", "Circle", "Civil engineering", "Color", "Computational geometry", "Computational topology", "Computer-aided design", "Computer-aided engineering", "Computer-aided manufacturing", "Conformal geometry", "Digital geometry", "Digital image", "Dimension", "Fractal", "Geologic modeling", "Geometric modeling kernel", "International Standard Book Number", "Level of detail", "List of interactive geometry software", "Mechanical engineering", "Medical image processing", "Object-oriented model", "Parametric curve", "Parametric surface", "Procedural model", "Pythagorean-hodograph curve", "Solid modeling", "Space partitioning", "Square (geometry)", "Subdivision surface", "Technical drawing", "Typography", "Volumetric model"], "categories": ["All articles needing additional references", "All articles with unsourced statements", "All stub articles", "Applied mathematics stubs", "Articles needing additional references from August 2014", "Articles with unsourced statements from August 2014", "Computer-aided design", "Geometric algorithms"], "title": "Geometric modeling"}
{"summary": "The Gilbert\u2013Johnson\u2013Keerthi distance algorithm is a method of determining the minimum distance between two convex sets. Unlike many other distance algorithms, it does not require that the geometry data be stored in any specific format, but instead relies solely on a support function to iteratively generate closer simplices to the correct answer using the Minkowski sum (CSO) of two convex shapes.\n\"Enhanced GJK\" algorithms use edge information to speed up the algorithm by following edges when looking for the next simplex. This improves performance substantially for polytopes with large numbers of vertices.\nGJK algorithms are often used incrementally in simulation systems and video games. In this mode, the final simplex from a previous solution is used as the initial guess in the next iteration, or \"frame\". If the positions in the new frame are close to those in the old frame, the algorithm will converge in one or two iterations. This yields collision detection systems which operate in near-constant time.\nThe algorithm's stability, speed, and small storage footprint make it popular for realtime collision detection, especially in physics engines for video games.", "links": ["Algorithm", "Applied mathematics", "Collision detection", "Convex set", "Minkowski sum", "Physics engine", "Simplex", "Support (mathematics)", "Video games"], "categories": ["All stub articles", "Applied mathematics stubs", "Convex geometry", "Geometric algorithms"], "title": "Gilbert\u2013Johnson\u2013Keerthi distance algorithm"}
{"summary": "The Java Topology Suite (JTS) is an open source Java software library that provides an object model for Euclidean planar linear geometry together with a set of fundamental geometric functions. JTS is primarily intended to be used as a core component of vector-based geomatics software such as geographical information systems. It can also be used as a general-purpose library providing algorithms in computational geometry.\nJTS implements the geometry model and API defined in the OpenGIS Consortium Simple Features Specification for SQL.\nJTS defines a standards-compliant geometry system for building spatial applications; examples include viewers, spatial query processors, and tools for performing data validation, cleaning and integration. In addition to the Java library, the foundations of JTS and selected functions are maintained in a C++ port, for use in C-style linking on all major operating systems, in the form of the GEOS software library.\nJTS, and the GEOS port, are published under the GNU Lesser General Public License (LGPL).", "links": [".NET Framework", "Algorithm", "Batik (software)", "Buffer (GIS)", "C++", "Cartographic generalization", "Complement (set theory)", "Computational geometry", "Computing platform", "Convex hull", "DE-9IM", "Delaunay triangulation", "Euclidean geometry", "GDAL", "GIS", "GNU Lesser General Public License", "GRASS", "GeoConnections", "GeoServer", "GeoTools", "Geographical information system", "Geography Markup Language", "Geomatics", "Geometry", "Google Earth", "GvSIG", "Hausdorff distance", "Intersection (set theory)", "Java (programming language)", "Library (computer science)", "Line segment intersection", "Linear Reference System", "List of software categories", "MapServer", "NASA World Wind", "OpenGIS", "OpenJUMP", "Orfeo toolbox", "Planar graph", "Point in polygon", "PostGIS", "QGis", "Quadtree", "R-tree", "R (programming language)", "Ramer\u2013Douglas\u2013Peucker algorithm", "Robust geometric computation", "Simple feature access", "Smallest enclosing rectangle", "Software developer", "Software library", "Software license", "Software release life cycle", "Spatial index", "Symmetric difference", "Topology", "UDig", "Union (set theory)", "Voronoi diagram", "Well-known text", "Whitebox Geospatial Analysis Tools"], "categories": ["Application programming interfaces", "Free software programmed in Java (programming language)", "Geometric algorithms"], "title": "JTS Topology Suite"}
{"summary": "In computational geometry, the largest empty rectangle problem, maximal empty rectangle problem or maximum empty rectangle problem, is the problem of finding a rectangle of maximal size to be placed among obstacles in the plane. There are a number of variants of the problem, depending on the particularities of this generic formulation, in particular, depending on the measure of the \"size\", domain (type of obstacles), and the orientation of the rectangle.\nThe problems of this kind arise e.g., in electronic design automation, in design and verification of physical layout of integrated circuits.\nA maximal empty rectangle (MER) is a rectangle which is not contained in another empty rectangle. Each side of a MER abuts an obstacle (otherwise the side may be shifted outwards, increasing the empty rectangle). An application of this kind is enumeration of \"maximal white rectangles\" in image segmentation R&D of image processing and pattern recognition. In the contexts of many algorithms for largest empty rectangles, \"maximal empty rectangles\" are candidate solutions to be considered by the algorithm, since it is easily proven that, e.g., a maximum-area empty rectangle is a maximal empty rectangle.", "links": ["Alok Aggearwal", "Axis-oriented", "Bernard Chazelle", "Circuit extraction", "Computational geometry", "Cuboid", "D. T. Lee", "Design rule checking", "Digital object identifier", "Electronic design automation", "Image processing", "Image segmentation", "Integrated circuit", "International Standard Book Number", "Isothetic polygon", "Jeffrey Ullman", "Largest empty circle", "Largest empty sphere", "Lecture Notes in Computer Science", "Minimum bounding box", "Minimum bounding rectangle", "Pattern Recognition", "Pattern recognition", "Physical layout", "Placement and routing", "Polygon operations", "Rectangle", "STACS", "Subhash Suri", "Symposium on Computational Geometry", "Time complexity", "VLSI", "Voronoi diagram"], "categories": ["Geometric algorithms"], "title": "Largest empty rectangle"}
{"summary": "In computational geometry, the line segment intersection problem supplies a list of line segments in the Euclidean plane and asks whether any two of them intersect, or cross.\nSimple algorithms examine each pair of segments. However, if a large number of possibly intersecting segments are to be checked, this becomes increasingly inefficient since most pairs of segments are not close to one another in a typical input sequence. The most common, more efficient way to solve this problem for a high number of segments is to use a sweep line algorithm, where we imagine a line sliding across the line segments and we track which line segments it intersects at each point in time using a dynamic data structure based on binary search trees. The Shamos\u2013Hoey algorithm applies this principle to solve the line segment intersection detection problem, as stated above, of determining whether or not a set of line segments has an intersection; the Bentley\u2013Ottmann algorithm works by the same principle to list all intersections in logarithmic time per intersection.", "links": ["Algorithm", "Bentley\u2013Ottmann algorithm", "Binary search tree", "CGAL", "Charles E. Leiserson", "Clifford Stein", "Computational geometry", "Data structure", "Digital object identifier", "Euclidean plane", "International Standard Book Number", "Introduction to Algorithms", "Line-line intersection", "Line segment", "Ronald L. Rivest", "Shamos\u2013Hoey algorithm", "Sweep line algorithm", "Thomas H. Cormen", "Washington University in St. Louis"], "categories": ["Algorithms and data structures stubs", "All stub articles", "CS1 errors: chapter ignored", "Computer science stubs", "Geometric algorithms"], "title": "Line segment intersection"}
{"summary": "List of numerical computational geometry topics enumerates the topics of computational geometry that deals with geometric objects as continuous entities and applies methods and algorithms of nature characteristic to numerical analysis. This area is also called \"machine geometry\", computer-aided geometric design, and geometric modelling.\nSee List of combinatorial computational geometry topics for another flavor of computational geometry that states problems in terms of geometric objects as discrete entities and hence the methods of their solution are mostly theories and algorithms of combinatorial character.", "links": ["Algorithm", "B-spline", "Beta spline", "B\u00e9zier curve", "B\u00e9zier surface", "Combinatorics", "Computational geometry", "Computational topology", "Contour line", "Discrete mathematics", "Geometric modelling", "Hermite spline", "Higher-order spline", "Isosurface", "Level set method", "List of combinatorial computational geometry topics", "List of curves topics", "NURBS", "Numerical analysis", "Parametric curve", "Parametric surface", "Spline (mathematics)"], "categories": ["Geometric algorithms", "Mathematics-related lists"], "title": "List of numerical computational geometry topics"}
{"summary": "In computer science and electrical engineering, Lloyd's algorithm, also known as Voronoi iteration or relaxation, is an algorithm named after Stuart P. Lloyd for finding evenly spaced sets of points in subsets of Euclidean spaces, and partitions of these subsets into well-shaped and uniformly sized convex cells. Like the closely related k-means clustering algorithm, it repeatedly finds the centroid of each set in the partition, and then re-partitions the input according to which of these centroids is closest. However, Lloyd's algorithm differs from k-means clustering in that its input is a continuous geometric region rather than a discrete set of points. Thus, when re-partitioning the input, Lloyd's algorithm uses Voronoi diagrams rather than simply determining the nearest center to each of a finite set of points as the k-means algorithm does.\nAlthough the algorithm may be applied most directly to the Euclidean plane, similar algorithms may also be applied to higher-dimensional spaces or to spaces with other non-Euclidean metrics. Lloyd's algorithm can be used to construct close approximations to centroidal Voronoi tessellations of the input, which can be used for quantization, dithering, and stippling. Other applications of Lloyd's algorithm include smoothing of triangle meshes in the finite element method.", "links": ["ACM SIGGRAPH", "ArXiv", "Blue noise", "Centroid", "Centroidal Voronoi tessellation", "Colors of noise", "Computer science", "Data compression", "David Eppstein", "Digital object identifier", "Dithering", "Electrical engineering", "Euclidean plane", "Euclidean space", "Eurographics", "Farthest-first traversal", "Finite element method", "IEEE Transactions on Information Theory", "Information theory", "K-means clustering", "Laplacian smoothing", "Linde\u2013Buzo\u2013Gray algorithm", "Manhattan metric", "Matthew T. Dickerson", "Mean shift", "Monte Carlo methods", "Mosaic", "Non-Euclidean geometry", "Quantization (signal processing)", "Stippling", "Triangle mesh", "Voronoi diagram"], "categories": ["Geometric algorithms", "Mathematical optimization"], "title": "Lloyd's algorithm"}
{"summary": "In computer graphics, the midpoint circle algorithm is an algorithm used to determine the points needed for drawing a circle. Bresenham's circle algorithm is derived from the midpoint circle algorithm. The algorithm can be generalized to conic sections.\n\nThe algorithm is related to work by Pitteway and Van Aken.", "links": ["Arc (geometry)", "Basis (linear algebra)", "Bresenham's line algorithm", "Circle", "Computer graphics", "Conic section", "Ellipse", "International Standard Book Number", "Methods of computing square roots", "Minecraft", "Performance tuning", "Slope", "Square (algebra)", "Square root", "Trigonometry"], "categories": ["All Wikipedia articles needing clarification", "Articles with example C code", "Articles with example JavaScript code", "Digital geometry", "Geometric algorithms", "Wikipedia articles needing clarification from February 2009"], "title": "Midpoint circle algorithm"}
{"summary": "The Minkowski Portal Refinement collision detection algorithm is a technique for determining whether two convex shapes overlap.\nThe algorithm was created by Gary Snethen in 2006 and was first published in Game Programming Gems 7. The algorithm was used in Tomb Raider: Underworld and other games created by Crystal Dynamics and its sister studios within Eidos Interactive.\nMPR, like its cousin GJK, relies on shapes that are defined using support mappings. This allows the algorithm to support a limitless variety of shapes that are problematic for other algorithms. Support mappings require only a single mathematical function to represent a point, line segment, disc, cylinder, cone, ellipsoid, football, bullet, frustum or most any other common convex shape. Once a set of basic primitives have been created, they can easily be combined with one another using operations such as sweep, shrink-wrap and affine transformation.\nUnlike GJK, MPR does not provide the shortest distance between separated shapes. However, according to its author, MPR is simpler, more numerically robust and handles translational sweeping with very little modification. This makes it well-suited for games and other real-time applications.", "links": ["Affine transformation", "Algorithm", "Collision detection", "Crystal Dynamics", "Eidos Interactive", "GJK", "Gary Snethen", "Geometry", "Support (mathematics)"], "categories": ["All stub articles", "Convex geometry", "Geometric algorithms", "Geometry stubs"], "title": "Minkowski Portal Refinement"}
{"summary": "The M\u00f6ller\u2013Trumbore ray-triangle intersection algorithm, named after its inventors Tomas M\u00f6ller and Ben Trumbore, is a fast method for calculating the intersection of a ray and a triangle in three dimensions without needing precomputation of the plane equation of the plane containing the triangle. Among other uses, it can be used in computer graphics to implement ray tracing computations involving triangle meshes.", "links": ["C (programming language)", "Computer graphics", "Computer science", "Geometry", "Ray (geometry)", "Ray tracing (graphics)", "Triangle", "Triangle mesh"], "categories": ["All stub articles", "Computer science stubs", "Geometric algorithms", "Geometry", "Geometry stubs"], "title": "M\u00f6ller\u2013Trumbore intersection algorithm"}
{"summary": "Nesting algorithms are used to make the most efficient use of material or space by evaluating many different possible combinations via recursion.\nLinear (1-dimensional): The simplest of the algorithms illustrated here. For an existing set there is only one position where a new cut can be placed \u2013 at the end of the last cut. Validation of a combination involves a simple Stock - Yield - Kerf = Scrap calculation.\nPlate (2-dimensional): These algorithms are significantly more complex. For an existing set, there may be as many as eight positions where a new cut may be introduced next to each existing cut, and if the new cut is not perfectly square then different rotations may need to be checked. Validation of a potential combination involves checking for intersections between two-dimensional objects.\nPacking (3-dimensional): These algorithms are the most complex illustrated here due to the larger number of possible combinations. Validation of a potential combination involves checking for intersections between three-dimensional objects.", "links": ["Algorithms", "Kerf", "Recursion (computer science)", "Scientific software", "Three-dimensional space", "Two-dimensional"], "categories": ["All articles needing additional references", "All orphaned articles", "All stub articles", "Articles needing additional references from September 2015", "Geometric algorithms", "Orphaned articles from February 2013", "Science software stubs"], "title": "Nesting algorithm"}
{"summary": "Planar straight-line graph (PSLG) is a term used in computational geometry for an embedding of a planar graph in the plane such that its edges are mapped into straight line segments. F\u00e1ry's theorem (1948) states that every planar graph has this kind of embedding.\nIn computational geometry PSLGs have often been called planar subdivisions, with an assumption or assertion that subdivisions are polygonal.\nA PSLG without vertices of degree 1 defines a subdivision of the plane into polygonal regions and vice versa. The absence of vertices of degree 1 simplifies descriptions of various algorithms, but it is not essential.\nPSLGs may serve as representations of various maps, e.g., geographical maps in geographical information systems.\nSpecial cases of PSLGs are triangulations (polygon triangulation, point set triangulation). Point set triangulations are maximal PSLGs in the sense that it is impossible to add straight edges to them. Triangulations have numerous applications in various areas.\nPSLGs may be seen as a special kind of Euclidean graphs. However in discussions involving Euclidean graphs the primary interest is their metric properties, i.e., distances between vertices, while for PSLGs the primary interest is the topological properties. For some graphs, such as Delaunay triangulations, both metric and topological properties are of importance.", "links": ["Algorithm", "Computational geometry", "Delaunay triangulation", "Doubly connected edge list", "Euclidean graph", "Franco P. Preparata", "F\u00e1ry's theorem", "Geographical information systems", "Geographical map", "Graph embedding", "Local feature size", "Map", "Map overlay", "Michael Ian Shamos", "Planar graph", "Plane (mathematics)", "Point location", "Point set triangulation", "Polygon triangulation", "Polygonal region", "Springer-Verlag", "Thematic map"], "categories": ["Geometric algorithms", "Geometric graphs", "Planar graphs"], "title": "Planar straight-line graph"}
{"summary": "Proximity problems is a class of problems in computational geometry which involve estimation of distances between geometric objects.\nA subset of these problems stated in terms of points only are sometimes referred to as closest point problems, although the term \"closest point problem\" is also used synonymously to the nearest neighbor search.\nA common trait for many of these problems is the possibility to establish the \u0398(n log n) lower bound on their computational complexity by reduction from the element uniqueness problem basing on an observation that if there is an efficient algorithm to compute some kind of minimal distance for a set of objects, it is trivial to check whether this distance equals to 0.", "links": ["Big O notation", "Bounding box", "Closest pair of points", "Closest point query", "Computational complexity theory", "Computational geometry", "Convex hull", "Delaunay triangulation", "Diameter of a point set", "Digital object identifier", "Distance", "Distance from a point to a line", "Distance of closest approach of ellipses and ellipsoids", "Element uniqueness problem", "Euclidean minimum spanning tree", "Franco P. Preparata", "Geometric spanner", "Hyperrectangle", "Information Processing Letters", "International Standard Book Number", "J. R. Sack", "Jorge Urrutia (professor)", "Largest empty circle", "Largest empty rectangle", "Line segment", "Lower bound", "Michael Ian Shamos", "Minimum spanning tree", "Nearest-neighbor graph", "Nearest neighbor query", "Nearest neighbor search", "North Holland", "Shortest path among obstacles", "Smallest enclosing rectangle", "Smallest enclosing sphere", "Springer-Verlag", "Vladimir Lumelsky", "Voronoi diagram", "Weighted graph", "Width of a point set"], "categories": ["Geometric algorithms"], "title": "Proximity problems"}
{"summary": "The shoelace formula or shoelace algorithm (also known as Gauss's area formula and the surveyor's formula) is a mathematical algorithm to determine the area of a simple polygon whose vertices are described by ordered pairs in the plane. The user cross-multiplies corresponding coordinates to find the area encompassing the polygon, and subtracts it from the surrounding polygon to find the area of the polygon within. It is called the shoelace formula because of the constant cross-multiplying for the coordinates making up the polygon, like tying shoelaces. It is also sometimes called the shoelace method. It has applications in surveying and forestry, among other areas.\nThe formula was described by Meister (1724-1788) in 1769 and by Gauss in 1795. It can be verified by dividing the polygon into triangles, but it can also be seen as a special case of Green's theorem.\nThe area formula is derived by taking each edge AB, and calculating the (signed) area of triangle ABO with a vertex at the origin O, by taking the cross-product (which gives the area of a parallelogram) and dividing by 2. As one wraps around the polygon, these triangles with positive and negative area will overlap, and the areas between the origin and the polygon will be cancelled out and sum to 0, while only the area inside the reference triangle remains. This is why the formula is called the Surveyor's Formula, since the \"surveyor\" is at the origin; if going counterclockwise, positive area is added when going from left to right and negative area is added when going from right to left, from the perspective of the origin.\nThe area formula is valid for any non-self-intersecting (simple) polygon, which can be convex or concave.", "links": ["Absolute value", "Algorithm", "Area", "Carl Friedrich Gauss", "Determinant", "Digital object identifier", "Eric W. Weisstein", "Green's Theorem", "Green's theorem", "International Standard Book Number", "Matrix (mathematics)", "Ordered pair", "Pentagon", "Planimeter", "Polygon", "Quadrilateral", "Simple polygon", "Surveying", "Triangle"], "categories": ["CS1 Latin-language sources (la)", "Geometric algorithms", "Surveying"], "title": "Shoelace formula"}
{"summary": "Stencil jumping, at times called stencil walking, is an algorithm to locate the grid element enclosing a given point for any structured mesh. In simple words, given a point and a structured mesh, this algorithm will help locate the grid element that will enclose the given point.\nThis algorithm finds extensive use in Computational Fluid Dynamics (CFD) in terms of holecutting and interpolation when two meshes lie one inside the other. The other variations of the problem would be something like this: Given a place, at which latitude and longitude does it lie? The brute force algorithm would find the distance of the point from every mesh point and see which is smallest. Another approach would be to use a binary search algorithm which would yield a result comparable in speed to the stencil jumping algorithm. A combination of both the binary search and the stencil jumping algorithm will yield an optimum result in the minimum possible time.", "links": ["Algorithm", "Binary search algorithm", "Computational Fluid Dynamics", "Cross product", "Digital object identifier", "Five-point stencil", "Unstructured grid", "Wei Shyy"], "categories": ["All articles with dead external links", "Articles with dead external links from October 2010", "Geometric algorithms"], "title": "Stencil jumping"}
{"summary": "In computational geometry, a sweep line algorithm or plane sweep algorithm is a type of algorithm that uses a conceptual sweep line or sweep surface to solve various problems in Euclidean space. It is one of the key techniques in computational geometry.\nThe idea behind algorithms of this type is to imagine that a line (often a vertical line) is swept or moved across the plane, stopping at some points. Geometric operations are restricted to geometric objects that either intersect or are in the immediate vicinity of the sweep line whenever it stops, and the complete solution is available once the line has passed over all objects.", "links": ["Analysis of algorithms", "Bentley\u2013Ottmann algorithm", "Big O notation", "Boolean operations on polygons", "Computational geometry", "Computer graphics", "Delaunay triangulation", "Diane Souvaine", "Digital object identifier", "Euclidean space", "Fortune's algorithm", "Integrated circuit layout", "Line segment intersection", "Michael Ian Shamos", "Projective dual", "Rotating calipers", "Scanline algorithm", "Self-balancing binary search tree", "Voronoi diagram"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from May 2009", "Geometric algorithms"], "title": "Sweep line algorithm"}
{"summary": "In robotics and motion planning, a velocity obstacle, commonly abbreviated VO, is the set of all velocities of a robot that will result in a collision with another robot at some moment in time, assuming that the other robot maintains its current velocity. If the robot chooses a velocity inside the velocity obstacle then the two robots will eventually collide, if it chooses a velocity outside the velocity obstacle, such a collision is guaranteed not to occur.\nThis algorithm for robot collision avoidance has been repeatedly rediscovered and published under different names: in 1989 as a maneuvering-board approach, in 1993 it was first introduced as the \"velocity obstacle\", in 1998 as collision cones, and in 2009 as forbidden velocity maps. The same algorithm has been used in maritime port navigation since at least 1903.\nThe velocity obstacle for a robot  induced by a robot  may be formally written as\n\nwhere  has position  and radius , and  has position , radius , and velocity . The notation  represents a disc with center  and radius .\nVariations include common velocity obstacles (CVO), finite-time-interval velocity obstacles (FVO), generalized velocity obstacles (GVO), hybrid reciprocal velocity obstacles (HRVO), nonlinear velocity obstacles (NLVO), reciprocal velocity obstacles (RVO), and recursive probabilistic velocity obstacles (PVO).", "links": ["Association for Computing Machinery", "Centroid", "Collision", "Digital object identifier", "Disk (mathematics)", "Generalization", "Hybrid (biology)", "Institute of Electrical and Electronics Engineers", "International Standard Serial Number", "Motion planning", "New York City", "Nonlinear system", "Position (vector)", "Probability", "Radius", "Recursion", "Robot", "Robotics", "SAGE Publications", "Thousand Oaks, California", "Velocity"], "categories": ["All stub articles", "Geometric algorithms", "Multi-robot systems", "Robot kinematics", "Robotics stubs"], "title": "Velocity obstacle"}
{"summary": "Image morphing is a technique to synthesize a fluid transformation from one image (source image) to another (destination image). Source image can be one or more than one images. There are two parts in the image morphing implementation. The first part is warping and the second part is cross-dissolving.\nThe algorithm of Beier and Neely is a method to compute a mapping of coordinates between 2 images from a set of lines; i.e., the warp is specified by a set of line pairs where the start-points and end-points are given for both images. The algorithm is widely used within morphing software.\nAlso noteworthy, this algorithm only discussed about the situation with at most 2 source images as there are other algorithms introducing multiple source images.", "links": ["Algorithm", "Data structure", "Digital object identifier", "Image processing", "Image warping", "Morphing"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer graphics algorithms", "Computer science stubs"], "title": "Beier\u2013Neely morphing algorithm"}
{"summary": "Bresenham's line algorithm is an algorithm that determines the points of an n-dimensional raster that should be selected in order to form a close approximation to a straight line between two points. It is commonly used to draw lines on a computer screen, as it uses only integer addition, subtraction and bit shifting, all of which are very cheap operations in standard computer architectures. It is one of the earliest algorithms developed in the field of computer graphics. An extension to the original algorithm may be used for drawing circles.\nWhile algorithms such as Wu's algorithm are also frequently used in modern computer graphics because they can support antialiasing, the speed and simplicity of Bresenham's line algorithm means that it is still important. The algorithm is used in hardware such as plotters and in the graphics chips of modern graphics cards. It can also be found in many software graphics libraries. Because the algorithm is very simple, it is often implemented in either the firmware or the graphics hardware of modern graphics cards.\nThe label \"Bresenham\" is used today for a family of algorithms extending or modifying Bresenham's original algorithm.", "links": ["Absolute value", "Algorithm", "Association for Computing Machinery", "Bitwise operation", "Calcomp plotter", "Computer architecture", "Computer graphics", "Digital differential analyzer (graphics algorithm)", "Digital object identifier", "Firmware", "Graphics card", "Graphics hardware", "Graphics library", "Graphics processing unit", "IBM 1401", "International Business Machines", "International Standard Book Number", "Jack Elton Bresenham", "Midpoint circle algorithm", "National Institute of Standards and Technology", "Octant (plane geometry)", "Plotter", "Pseudocode", "Raster graphics", "Slope", "Spatial anti-aliasing", "Uv mapping", "Voxel", "Xiaolin Wu's line algorithm"], "categories": ["All articles needing additional references", "All articles to be expanded", "All articles with unsourced statements", "Articles needing additional references from August 2012", "Articles to be expanded from September 2011", "Articles with example pseudocode", "Articles with unsourced statements from December 2010", "Commons category with local link same as on Wikidata", "Computer graphics algorithms", "Digital geometry"], "title": "Bresenham's line algorithm"}
{"summary": "The diamond-square algorithm is a method for generating heightmaps for computer graphics. It is a slightly better algorithm than the three-dimensional implementation of the midpoint displacement algorithm which produces two-dimensional landscapes. It is also known as the random midpoint displacement fractal, the cloud fractal or the plasma fractal, because of the plasma effect produced when applied.\nThe idea was first introduced by Fournier, Fussell and Carpenter at SIGGRAPH 1982. It was later analyzed by Gavin S. P. Miller in SIGGRAPH 1986 who described it as flawed because the algorithm produces noticeable vertical and horizontal \"creases\" due to the most significant perturbation taking place in a rectangular grid.\nThe algorithm starts with a 2D grid then randomly generates terrain height from four seed values arranged in a grid of points so that the entire plane is covered in squares.", "links": ["Alain Fournier", "Algorithm", "Computer graphics", "Digital object identifier", "Don Fussell", "Fractal landscape", "Gavin S. P. Miller", "Heightmap", "Loren Carpenter", "Plasma effect", "SIGGRAPH", "Terragen", "Terrain generation"], "categories": ["Computer graphics algorithms", "Fractals", "Pages using citations with accessdate and no URL", "Procedural generation"], "title": "Diamond-square algorithm"}
{"summary": "In computer graphics, a digital differential analyzer (DDA) is hardware or software used for linear interpolation of variables over an interval between start and end point. DDAs are used for rasterization of lines, triangles and polygons. In its simplest implementation, the DDA algorithm interpolates values in interval by computing for each xi the equations xi = xi\u22121+1/m, yi = yi\u22121 + m, where \u0394x = xend \u2212 xstart and \u0394y = yend \u2212 ystart and m = \u0394y/\u0394x", "links": ["Bresenham's line algorithm", "Computer graphics", "Digital differential analyzer", "Fixed-point arithmetic", "Floating-point", "Floating-point unit", "Integer", "Interval (mathematics)", "Linear interpolation", "Rasterization", "Variable (computer science)", "Xiaolin Wu's line algorithm"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from June 2011", "Articles with example C code", "Computer graphics algorithms", "Digital geometry"], "title": "Digital differential analyzer (graphics algorithm)"}
{"summary": "The even\u2013odd rule is an algorithm implemented in vector-based graphic software, like the PostScript language and Scalable Vector Graphics (SVG), which determines how a graphical shape with more than one closed outline will be filled. Unlike the nonzero-rule algorithm, this algorithm will alternatively color and leave uncolored shapes defined by nested closed paths irrespective of their winding.\nThe SVG specification says: \"This rule determines the \"insideness\" of a point on the canvas by drawing a ray from that point to infinity in any direction and counting the number of path segments from the given shape that the ray crosses. If this number is odd, the point is inside; if even, the point is outside.\"\nThe rule can be seen in effect in many vector graphic programs (like Freehand or Illustrator), where a crossing of an outline with itself causes shapes to fill in strange ways.\nOn a simple curve, the even\u2013odd rule reduces to a decision algorithm for the point in polygon problem.", "links": ["Adobe Illustrator", "Algorithm", "Computer programming", "Macromedia FreeHand", "Nonzero-rule", "Point in polygon", "PostScript", "Python (programming language)", "Scalable Vector Graphics"], "categories": ["All stub articles", "Computer graphics algorithms", "Computer programming stubs", "Parity (mathematics)"], "title": "Even\u2013odd rule"}
{"summary": "Flood fill, also called seed fill, is an algorithm that determines the area connected to a given node in a multi-dimensional array. It is used in the \"bucket\" fill tool of paint programs to fill connected, similarly-colored areas with a different color, and in games such as Go and Minesweeper for determining which pieces are cleared. When applied on an image to fill a particular bounded area with color, it is also known as boundary fill.", "links": ["Algorithm", "Alpha compositing", "Array data structure", "Boundary value problem", "Connected-component labeling", "Dijkstra's algorithm", "Glossary of graph theory", "Go (game)", "Graph traversal", "Graphic Adventure Creator", "Inkscape", "Java applet", "Minesweeper (video game)", "Paint program", "Pixel connectivity", "Queue (abstract data type)", "Queue (data structure)", "Recursion", "Stack (data structure)"], "categories": ["All articles lacking sources", "Articles lacking sources from August 2009", "Articles with example pseudocode", "Computer graphics algorithms"], "title": "Flood fill"}
{"summary": "Geomipmapping or geometrical mipmapping is a real-time block-based terrain rendering algorithm developed by W.H. de Boer in 2000 that aims to reduce CPU processing time which is a common bottleneck in level of detail approaches to terrain rendering. [1]\nPrior to geomipmapping, techniques such as quadtree rendering were used to divide the terrain into square tiles created by binary division with quadratically diminishing size. The subdivision step is typically performed on the CPU which creates a bottleneck as geometry commands are buffered to the GPU. Unlike quadtrees which send 1x1 polygon units to the GPU, to reduce the CPU processing time geomipmapping divides the terrain into grid-based tiles which are themselves regularly subdivided. Typically, a fixed number of vertex buffer objects (VBOs) are stored on the GPU at different grid resolutions, such as 10x10 and 20x20, and then placed at major terrain regions selectively chosen by the CPU. A vertex shader is then used to reposition the vertices for a given VBO, all on the GPU. Overall, this results in a major reduction in CPU processing, and reduced CPU-to-GPU bandwidth as the GPU then performs most of the work. Geoclipmaps and GPU raycasting are two other modern alternatives to geomipmapping for interactive rendering of terrain.", "links": ["CPU", "Computer graphics", "GPU raycasting", "Geoclipmap", "Level of detail", "Quadtree rendering", "Terrain rendering"], "categories": ["All stub articles", "Computer graphics algorithms", "Computer graphics stubs"], "title": "Geomipmapping"}
{"summary": "A line drawing algorithm is a graphical algorithm for approximating a line segment on discrete graphical media. On discrete media, such as pixel-based displays and printers, line drawing requires such an approximation (in nontrivial cases). Basic algorithms rasterize lines in one color. A better representation with multiple color gradations requires an advanced process, anti-aliasing.\nOn continuous media, by contrast, no algorithm is necessary to draw a line. For example, oscilloscopes use natural phenomena to draw lines and curves.\nThe Cartesian slope-intercept equation for a straight line is  With m representing the slope of the line and b as the y intercept. Given that the two endpoints of the line segment are specified at positions  and . we can determine values for the slope m and y intercept b with the following calculations,  so, .", "links": ["Algorithm", "Anti-aliasing", "Bresenham's line algorithm", "Computer display", "Computer printer", "Digital Differential Analyzer (graphics algorithm)", "Gupta-Sproull algorithm", "Oscilloscope", "Pixel", "Spatial anti-aliasing", "Xiaolin Wu's line algorithm"], "categories": ["All articles to be expanded", "Articles to be expanded from December 2009", "Computer graphics algorithms", "Featured articles needing translation from German Wikipedia", "Science articles needing translation from German Wikipedia"], "title": "Line drawing algorithm"}
{"summary": "Marching cubes is a computer graphics algorithm, published in the 1987 SIGGRAPH proceedings by Lorensen and Cline, for extracting a polygonal mesh of an isosurface from a three-dimensional discrete scalar field (sometimes called voxels). This paper is one of the most cited papers in the computer graphics field. The applications of this algorithm are mainly concerned with medical visualizations such as CT and MRI scan data images, and special effects or 3-D modelling with what is usually called metaballs or other metasurfaces. An analogous two-dimensional method is called the marching squares algorithm.\n\n", "links": ["Algorithm", "Asymptotic decider", "Computed axial tomography", "Computer graphics", "Digital object identifier", "General Electric", "Gradient", "Illumination model", "Image-based meshing", "International Standard Book Number", "Isosurface", "Linear interpolation", "MRI", "Magnetic resonance imaging", "Marching squares", "Marching tetrahedra", "Marching tetrahedrons", "Medical visualization", "Metaballs", "Metasurface", "Polygonal mesh", "SIGGRAPH", "Scalar field", "Voxel"], "categories": ["3D computer graphics", "All articles with unsourced statements", "Articles with unsourced statements from August 2015", "Commons category template with no category set", "Commons category without a link on Wikidata", "Computer graphics algorithms", "Mesh generation"], "title": "Marching cubes"}
{"summary": "Marching squares is a computer graphics algorithm that generates contours for a two-dimensional scalar field (rectangular array of individual numerical values). A similar method can be used to contour 2D triangle meshes.\nThe contours can be of two kinds:\nIsolines - lines following a single data level, or isovalue.\nIsobands - filled areas between isolines.\nTypical applications include the Contour lines on topographic maps or the generation of isobars for weather maps.\nMarching squares takes a similar approach to the 3D marching cubes algorithm:\nProcess each cell in the grid independently.\nCalculate a cell index using comparisons of the contour level(s) with the data values at the cell corners.\nUse a pre-built lookup table, keyed on the cell index, to describe the output geometry for the cell.\nApply linear interpolation along the boundaries of the cell to calculate the exact contour position.", "links": ["Algorithm", "Array data structure", "Average", "Binary numeral system", "Bit", "Bitwise OR", "C language", "Cache (computing)", "Clockwise", "Computer graphics", "Concave (disambiguation)", "Contour line", "Contour lines", "Delaunay triangulation", "Digital object identifier", "Embarrassingly parallel", "Fortran", "Heptagon", "Least significant bit", "Linear interpolation", "Logical shift", "Lookup table", "Marching cubes", "Most significant bit", "Parallel algorithm", "Plane (geometry)", "Quadrilateral", "Saddle points", "Scalar field", "Scientific visualization", "Short integer", "Simplex", "Ternary numeral system", "Three-dimensional space", "Topology", "Triangulated irregular network"], "categories": ["Computer graphics algorithms"], "title": "Marching squares"}
{"summary": "Marching tetrahedra is an algorithm in the field of computer graphics to render implicit surfaces. It clarifies a minor ambiguity problem of the marching cubes algorithm with some cube configurations.\nSince more than 20 years have passed from the patent filing date of the marching cubes (June 5, 1985), the original algorithm can be used freely again, adding only the minor modification to circumvent the aforementioned ambiguity in some configurations.\nIn marching tetrahedra, each cube is split into six irregular tetrahedra by cutting the cube in half three times, cutting diagonally through each of the three pairs of opposing faces. In this way, the tetrahedra all share one of the main diagonals of the cube. Instead of the twelve edges of the cube, we now have nineteen edges: the original twelve, six face diagonals, and the main diagonal. Just like in marching cubes, the intersections of these edges with the isosurface are approximated by linearly interpolating the values at the grid points.\nAdjacent cubes share all edges in the connecting face, including the same diagonal. This is an important property to prevent cracks in the rendered surface, because interpolation of the two distinct diagonals of a face usually gives slightly different intersection points. An added benefit is that up to five computed intersection points can be reused when handling the neighbor cube. This includes the computed surface normals and other graphics attributes at the intersection points.\nEach tetrahedron has sixteen possible configurations, falling into three classes: no intersection, intersection in one triangle and intersection in two (adjacent) triangles. It is straightforward to enumerate all sixteen configurations and map them to vertex index lists defining the appropriate triangle strips.", "links": ["Asymptotic decider", "Computer graphics", "Image-based meshing", "Implicit surface", "International Standard Book Number", "Isosurface", "Lookup table", "Marching cubes", "Surface normal", "Tessellation", "Tetrahedron", "Triangle strip"], "categories": ["All articles needing additional references", "Articles needing additional references from September 2012", "Computer graphics algorithms"], "title": "Marching tetrahedra"}
{"summary": "MClone, or Clonal Mosaic, is a pattern formation algorithm proposed in 1998 used specially for simulating the visible patches of color in the fur of giraffes and members of the Felidae family of the mammalians. It was primarily proposed as a 2D model and lately was extended to 3D. An important feature of the algorithm is that it is biologically plausible.\nSince the algorithm was created in order to address some of the problems with texture mapping, its main goal is to produce, with the same set of parameters, a variable number of color patterns for a 2D or 3D object model. This way, for a relatively big amount of different entities represented by the same model, instead of using the same texture (and, doing so, each object would be equal to the others), one could use the different color patterns created by the MClone algorithm. Another useful feature of MClone is that it can be used to create patterns along with growing data of the object model.", "links": ["Algorithm", "CiteSeer", "Google Code", "Mitosis", "Pattern formation", "Texture mapping"], "categories": ["All articles needing additional references", "Articles needing additional references from June 2011", "Computer graphics algorithms"], "title": "MClone"}
{"summary": "Newell's Algorithm is a 3D computer graphics procedure for elimination of polygon cycles in the depth sorting required in hidden surface removal. It was proposed in 1972 by brothers Martin Newell and Dick Newell, and Tom Sancha, while all three were working at CADCentre.\nIn the depth sorting phase of hidden surface removal, if two polygons have no overlapping extents or extreme minimum and maximum values in the x, y, and z directions, then they can be easily sorted. If two polygons, Q and P, do have overlapping extents in the Z direction, then it is possible that cutting is necessary.\n\nIn that case Newell's algorithm tests the following:\nTest for Z overlap; implied in the selection of the face Q from the sort list\nThe extreme coordinate values in X of the two faces do not overlap (minimax test in X)\nThe extreme coordinate values in Y of the two faces do not overlap (minimax test in Y)\nAll vertices of P lie deeper than the plane of Q\nAll vertices of Q lie closer to the viewpoint than the plane of P\nThe rasterisation of P and Q do not overlap\nNote that the tests are given in order of increasing computational difficulty.\nNote also that the polygons must be planar.\nIf the tests are all false, then the polygons must be split. Splitting is accomplished by selecting one polygon and cutting it along the line of intersection with the other polygon. The above tests are again performed, and the algorithm continues until all polygons pass the above tests.", "links": ["3D computer graphics", "Bob Sproull", "Boolean operations on polygons", "CADCentre", "Computer graphics", "Dick Newell", "Digital object identifier", "Hidden surface determination", "Ivan Sutherland", "Martin Newell (computer scientist)", "Minimax", "Painter's algorithm", "Plane (geometry)", "Polygon", "Rasterisation"], "categories": ["3D computer graphics", "All stub articles", "Computer graphics algorithms", "Computer graphics stubs"], "title": "Newell's algorithm"}
{"summary": "The painter's algorithm, also known as a priority fill, is one of the simplest solutions to the visibility problem in 3D computer graphics. When projecting a 3D scene onto a 2D plane, it is necessary at some point to decide which polygons are visible, and which are hidden.\nThe name \"painter's algorithm\" refers to the technique employed by many painters of painting distant parts of a scene before parts which are nearer thereby covering some areas of distant parts. The painter's algorithm sorts all the polygons in a scene by their depth and then paints them in this order, farthest to closest. It will paint over the parts that are normally not visible \u2014 thus solving the visibility problem \u2014 at the cost of having painted invisible areas of distant objects. The ordering used by the algorithm is called a 'depth order', and does not have to respect the numerical distances to the parts of the scene: the essential property of this ordering is, rather, that if one object obscures part of another then the first object is painted after the object that it obscures. Thus, a valid ordering can be described as a topological ordering of a directed acyclic graph representing occlusions between objects.\n\nThe algorithm can fail in some cases, including cyclic overlap or piercing polygons. In the case of cyclic overlap, as shown in the figure to the right, Polygons A, B, and C overlap each other in such a way that it is impossible to determine which polygon is above the others. In this case, the offending polygons must be cut to allow sorting. Newell's algorithm, proposed in 1972, provides a method for cutting such polygons. Numerous methods have also been proposed in the field of computational geometry.\nThe case of piercing polygons arises when one polygon intersects another. As with cyclic overlap, this problem may be resolved by cutting the offending polygons.\nIn basic implementations, the painter's algorithm can be inefficient. It forces the system to render each point on every polygon in the visible set, even if that polygon is occluded in the finished scene. This means that, for detailed scenes, the painter's algorithm can overly tax the computer hardware.\nA reverse painter's algorithm is sometimes used, in which objects nearest to the viewer are painted first \u2014 with the rule that paint must never be applied to parts of the image that are already painted (unless they are partially transparent). In a computer graphic system, this can be very efficient, since it is not necessary to calculate the colors (using lighting, texturing and such) for parts of the more distant scene that are hidden by nearby objects. However, the reverse algorithm suffers from many of the same problems as the standard version.\nThese and other flaws with the algorithm led to the development of Z-buffer techniques, which can be viewed as a development of the painter's algorithm, by resolving depth conflicts on a pixel-by-pixel basis, reducing the need for a depth-based rendering order. Even in such systems, a variant of the painter's algorithm is sometimes employed. As Z-buffer implementations generally rely on fixed-precision depth-buffer registers implemented in hardware, there is scope for visibility problems due to rounding error. These are overlaps or gaps at joins between polygons. To avoid this, some graphics engine implementations \"overrender\", drawing the affected edges of both polygons in the order given by painter's algorithm. This means that some pixels are actually drawn twice (as in the full painter's algorithm) but this happens on only small parts of the image and has a negligible performance effect.", "links": ["3D computer graphics", "Addison-Wesley", "Computational geometry", "Computer Graphics: Principles and Practice", "Directed acyclic graph", "Hidden surface determination", "International Standard Book Number", "James D. Foley", "John F. Hughes (computer scientist)", "Newell's algorithm", "Polygon", "Rendering (computer graphics)", "Schlemiel the Painter's algorithm", "Topological ordering", "Visibility problem", "Z-buffering"], "categories": ["3D computer graphics", "All articles with unsourced statements", "Articles with unsourced statements from January 2008", "Commons category with local link same as on Wikidata", "Computer graphics algorithms"], "title": "Painter's algorithm"}
{"summary": "Progressive refinement is a ray tracing algorithm that quickly reveals coarse structure of an image, and gradually reveals additional detail over time.\nThe first pixel is rendered as a single rectangle occupying the entire work area. The second through fourth each occupy a quarter of the work area. Sufficient progression will refine the image until the rendered rectangles correspond to a target resolution (for example, a screen resolution).\nRectangles are laid out with an overlap pattern so as to avoid unnecessary rendering.", "links": ["Algorithm", "Computer graphics", "Display resolution", "Pixel", "Ray tracing (graphics)"], "categories": ["All articles lacking sources", "All stub articles", "Articles lacking sources from December 2009", "Computer graphics algorithms", "Computer graphics stubs"], "title": "Progressive refinement"}
{"summary": "Ray casting is the use of ray-surface intersection tests to solve a variety of problems in computer graphics and computational geometry. The term was first used in computer graphics in a 1982 paper by Scott Roth to describe a method for rendering constructive solid geometry models.\nRay casting can refer to a variety of problems and techniques:\nthe general problem of determining the first object intersected by a ray,\na technique for hidden surface removal based on finding the first intersection of a ray cast from the eye through each pixel of an image,\na non-recursive ray tracing rendering algorithm that only casts primary rays, or\na direct volume rendering method, also called volume ray casting, in which the ray is \"pushed through\" the object and the 3D scalar field of interest is sampled along the ray inside the object. No secondary rays are spawned in this method.\nAlthough \"ray casting\" and \"ray tracing\" were often used interchangeably in early computer graphics literature, more recent usage tries to distinguish the two. The distinction is that ray casting is a rendering algorithm that never recursively traces secondary rays, whereas other ray tracing-based rendering algorithms may do so.", "links": ["2.5D", "Absorption (electromagnetic radiation)", "Adaptive tile refresh", "Andre LaMothe", "Andries van Dam", "Arthur Appel", "Comanche series", "Computational geometry", "Computer Graphics: Principles and Practice", "Computer graphics", "Cone (geometry)", "Constructive solid geometry", "Data structure", "Digital object identifier", "Elmsford, New York", "Heightmap", "Hidden surface removal", "Image and object order rendering", "International Standard Book Number", "James D. Foley", "Mark de Berg", "Mathematical Applications Group, Inc.", "Mode 7", "NovaLogic", "Optical spectrum", "Photon", "Pixel", "Projective geometry", "Radiance", "Ray (optics)", "Ray tracing (graphics)", "Reflection (physics)", "Refractions", "Rendering (computer graphics)", "Scalar field", "Scanline rendering", "Shadow", "Solid modelling", "Sparse voxel octree", "Sphere", "Sprite (computer graphics)", "Texture (computer graphics)", "Texture mapping", "Translucent", "Transparency (optics)", "University of Bournemouth", "Volume ray casting", "Volume rendering", "Voxel Space", "Wolfenstein 3-D"], "categories": ["All articles to be expanded", "Articles to be expanded from May 2010", "CS1 errors: chapter ignored", "Computer graphics algorithms"], "title": "Ray casting"}
{"summary": "The recursive X-Y cut is a top-down page segmentation technique that decomposes a document image recursively into a set of rectangular blocks. The algorithm works by projecting the document bitmap (i.e. summing up all the pixels in a line) to the sides of the document page. By this method, a white space density graph is produced, with peaks for vertical or horizontal whitespace lines. These peaks define the cuts of the document and are used top-down to segment the document into smaller pieces.", "links": ["Algorithm", "Bitmap", "Computer science", "Pixels"], "categories": ["All orphaned articles", "All stub articles", "Computer graphics algorithms", "Computer science stubs", "Orphaned articles from February 2009"], "title": "Recursive XY-cut"}
{"summary": "Real-time optimally adapting mesh (ROAM), is a continuous level of detail algorithm that optimizes terrain meshes. On modern computers, sometimes it is more effective to send a small amount of unneeded polygons to the GPU, rather than burden the CPU with LOD (Level of Detail) calculations\u2014making algorithms like geomipmapping more effective than ROAM. This technique is used by graphics programmers in order to produce high quality displays while being able to maintain real-time frame rates. Algorithms such as ROAM exist to provide a control over scene quality versus performance in order to provide HQ scenes while retaining real-time frame rates on hardware. ROAM largely aims toward terrain visualization, but various elements from ROAM are difficult to place within a game system.\nTo assist regional geological mapping, more abundant and visualized expression forms are highly needs. Thus, the 3D terrain model is adopted as the carrier for the demands in many correlative fields. Based on the regular grid DEM (Digital Elevation Model) in DRGS, ROAM algorithm is applied to create a more dynamic model, which will give consideration to the importance of different features and select correspondence level of detail.", "links": ["Algorithm", "Central processing unit", "Computer graphics", "Computer science", "Geomipmapping", "Graphics processing unit", "Level of detail (programming)", "Polygon mesh"], "categories": ["All stub articles", "Computer graphics algorithms", "Computer graphics stubs", "Computer science stubs"], "title": "ROAM"}
{"summary": "Scanline rendering is an algorithm for visible surface determination, in 3D computer graphics, that works on a row-by-row basis rather than a polygon-by-polygon or pixel-by-pixel basis. All of the polygons to be rendered are first sorted by the top y coordinate at which they first appear, then each row or scanline of the image is computed using the intersection of a scanline with the polygons on the front of the sorted list, while the sorted list is updated to discard no-longer-visible polygons as the active scan line is advanced down the picture.\nThe main advantage of this method is that sorting vertices along the normal of the scanning plane reduces the number of comparisons between edges. Another advantage is that it is not necessary to translate the coordinates of all vertices from the main memory into the working memory\u2014only vertices defining edges that intersect the current scan line need to be in active memory, and each vertex is read in only once. The main memory is often very slow compared to the link between the central processing unit and cache memory, and thus avoiding re-accessing vertices in main memory can provide a substantial speedup.\nThis kind of algorithm can be easily integrated with many other graphics techniques, such as the Phong reflection model or the Z-buffer algorithm.", "links": ["3D computer graphics", "Binary space partitioning", "Bresenham's line algorithm", "Bubble sort", "Cache memory", "Caulking (video games)", "Cell (microprocessor)", "Deferred shading", "Dreamcast", "Evans & Sutherland", "Framebuffer", "Hidden surface determination", "Ivan Sutherland", "Nintendo DS", "Painter's algorithm", "Phong reflection model", "Pixel", "PlayStation 3", "Polygon", "PowerVR", "Raster scan", "Ray tracing (graphics)", "Salt Lake City", "Scanline", "Sprite (computer graphics)", "Tiled rendering", "University of Utah", "Vertex (geometry)", "Z-buffer", "Z-buffering"], "categories": ["3D rendering", "Computer graphics algorithms", "Optics"], "title": "Scanline rendering"}
{"summary": "The SGI algorithm creates triangle strips from a set of triangles. It was published by K. Akeley, P. Haeberli, and D. Burns as a C program named \"tomesh.c\" for use with Silicon Graphics' IRIS GL API.\nThe algorithm operates on the set of triangles that have not yet been added to a triangle strip, starting with the entire set of input triangles. Triangles are greedily added to a strip until no triangle is available that can be appended to the strip; a new strip will be started in this case. When choosing a triangle for starting or continuing a triangle strip, the selection is based on a triangle's degree (i.e. the number of triangles adjacent to it), with smaller degrees being preferred.\nIf implemented using a priority queue to quickly identify triangles that can start a new strip, the algorithm runs in linear time.", "links": ["C (programming language)", "Greedy algorithm", "IRIS GL", "Silicon Graphics International", "Triangle strip"], "categories": ["Computer graphics algorithms"], "title": "SGI algorithm"}
{"summary": "The Simulated Fluorescence Process (SFP) is a computing algorithm used for scientific visualization of 3D data from, for example, fluorescence microscopes. By modeling a physical light/matter interaction process an image is computed showing the data as it would have appeared in reality when viewed under these conditions.", "links": ["Computer simulation", "Fluorescence microscope", "Scientific visualization"], "categories": ["All articles covered by WikiProject Wikify", "All articles lacking in-text citations", "All articles with too few wikilinks", "All stub articles", "Articles covered by WikiProject Wikify from March 2015", "Articles lacking in-text citations from December 2010", "Articles with too few wikilinks from March 2015", "Computational science", "Computer graphics algorithms", "Fluorescence", "Microscopes", "Microscopy", "Simulation software stubs", "Visualization (graphic)"], "title": "Simulated fluorescence process algorithm"}
{"summary": "The Warnock algorithm is a hidden surface algorithm invented by John Warnock that is typically used in the field of computer graphics. It solves the problem of rendering a complicated image by recursive subdivision of a scene until areas are obtained that are trivial to compute. In other words, if the scene is simple enough to compute efficiently then it is rendered; otherwise it is divided into smaller parts which are likewise tested for simplicity.\nThis is a divide and conquer algorithm with run-time of , where n is the number of polygons and p is the number of pixels in the viewport.\nThe inputs are a list of polygons and a viewport. The best case is that if the list of polygons is simple, then draw the polygons in the viewport. Simple is defined as one polygon (then the polygon or its part is drawn in appropriate part of a viewport) or a viewport that is one pixel in size (then that pixel gets a color of the polygon closest to the observer). The continuous step is to split the viewport into 4 equally sized quadrants and to recursively call the algorithm for each quadrant, with a polygon list modified such that it only contains polygons that are visible in that quadrant.", "links": ["Analysis of algorithms", "Computer graphics", "Computer programming", "Divide and conquer algorithm", "Hidden surface determination", "International Standard Book Number", "John Warnock"], "categories": ["All stub articles", "Computer graphics algorithms", "Computer programming stubs"], "title": "Warnock algorithm"}
{"summary": "Xiaolin Wu's line algorithm is an algorithm for line antialiasing, which was presented in the article An Efficient Antialiasing Technique in the July 1991 issue of Computer Graphics, as well as in the article Fast Antialiasing in the June 1992 issue of Dr. Dobb's Journal.\nBresenham's algorithm draws lines extremely quickly, but it does not perform anti-aliasing. In addition, it cannot handle any cases where the line endpoints do not lie exactly on integer points of the pixel grid. A naive approach to anti-aliasing the line would take an extremely long time. Wu's algorithm is comparatively fast, but is still slower than Bresenham's algorithm. The algorithm consists of drawing pairs of pixels straddling the line, each coloured according to its distance from the line. Pixels at the line ends are handled separately. Lines less than one pixel long are handled as a special case.\nAn extension to the algorithm for circle drawing was presented by Xiaolin Wu in the book Graphics Gems II. Just like the line drawing algorithm is a replacement for Bresenham's line drawing algorithm, the circle drawing algorithm is a replacement for Bresenham's circle drawing algorithm.", "links": ["Algorithm", "Bresenham's line algorithm", "Computer Graphics", "Computer Graphics (publication)", "Digital object identifier", "Dr. Dobb's Journal", "Graphics Gems", "International Standard Book Number", "Spatial anti-aliasing"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from January 2013", "Articles with example pseudocode", "Computer graphics algorithms"], "title": "Xiaolin Wu's line algorithm"}
{"summary": "Fast Approximate Anti-Aliasing (FXAA) is an anti-aliasing algorithm created by Timothy Lottes under NVIDIA.\nThe main advantage of this technique over conventional anti-aliasing is that it does not require large amounts of computing power. It achieves this by smoothing jagged edges (\"jaggies\") according to how they appear on screen as pixels, rather than analyzing the 3D models itself as in conventional anti-aliasing. In addition it smooths edges in all pixels on the screen, including those inside alpha-blended textures and those resulting from pixel shader effects, which were previously immune to the effects of multisample anti-aliasing (MSAA).\n\nThe downsides are that textures may not appear as sharp if they are included in the edge detection, and it must be applied before rendering the HUD elements of a game, lest it affect them too.\nIt is very similar to a patent Image Processor, submitted by Stephen Todd at IBM in 1991.", "links": ["Graphics software", "Image Processor", "Jaggies", "Multisample anti-aliasing", "NVIDIA", "Spatial anti-aliasing", "Timothy Lottes"], "categories": ["All stub articles", "Anti-aliasing algorithms", "Graphics software stubs"], "title": "Fast approximate anti-aliasing"}
{"summary": "Beam tracing is an algorithm to simulate wave propagation. It was developed in the context of computer graphics to render 3D scenes, but it has been also used in other similar areas such as acoustics and electromagnetism simulations.\nBeam tracing is a derivative of the ray tracing algorithm that replaces rays, which have no thickness, with beams. Beams are shaped like unbounded pyramids, with (possibly complex) polygonal cross sections. Beam tracing was first proposed by Paul Heckbert and Pat Hanrahan.\nIn beam tracing, a pyramidal beam is initially cast through the entire viewing frustum. This initial viewing beam is intersected with each polygon in the environment, typically from nearest to farthest. Each polygon that intersects with the beam must be visible, and is removed from the shape of the beam and added to a render queue. When a beam intersects with a reflective or refractive polygon, a new beam is created in a similar fashion to ray-tracing.\nA variant of beam tracing casts a pyramidal beam through each pixel of the image plane. This is then split up into sub-beams based on its intersection with scene geometry. Reflection and transmission (refraction) rays are also replaced by beams.This sort of implementation is rarely used, as the geometric processes involved are much more complex and therefore expensive than simply casting more rays through the pixel. Cone tracing is a similar technique using a cone instead of a complex pyramid.\nBeam tracing solves certain problems related to sampling and aliasing, which can plague conventional ray tracing approaches. Since beam tracing effectively calculates the path of every possible ray within each beam (which can be viewed as a dense bundle of adjacent rays), it is not as prone to under-sampling (missing rays) or over-sampling (wasted computational resources). The computational complexity associated with beams has made them unpopular for many visualization applications. In recent years, Monte Carlo algorithms like distributed ray tracing (and Metropolis light transport?) have become more popular for rendering calculations.\nA 'backwards' variant of beam tracing casts beams from the light source into the environment. Similar to backwards raytracing and photon mapping, backwards beam tracing may be used to efficiently model lighting effects such as caustics. Recently the backwards beam tracing technique has also been extended to handle glossy to diffuse material interactions (glossy backward beam tracing) such as from polished metal surfaces.\nBeam tracing has been successfully applied to the fields of acoustic modelling and electromagnetic propagation modelling. In both of these applications, beams are used as an efficient way to track deep reflections from a source to a receiver (or vice versa). Beams can provide a convenient and compact way to represent visibility. Once a beam tree has been calculated, one can use it to readily account for moving transmitters or receivers.\nBeam tracing is related in concept to cone tracing.", "links": ["3D computer graphics", "Acoustics", "Algorithm", "Aliasing", "Caustic (optics)", "Complex polygon", "Computer graphics", "Cone tracing", "Distributed ray tracing", "Electromagnetism", "Image plane", "Metropolis light transport", "Monte Carlo method", "Pat Hanrahan", "Paul Heckbert", "Photon mapping", "Pixel", "Polygon", "Ray tracing (graphics)", "Reflection (physics)", "Refraction", "Rendering (computer graphics)", "Sample (signal)", "Viewing frustum", "Wave propagation"], "categories": ["Global illumination algorithms"], "title": "Beam tracing"}
{"summary": "Cone tracing and beam tracing are a derivative of the ray tracing algorithm that replaces rays, which have no thickness, with thick rays.\n\n", "links": ["Airy disc", "Algorithm", "Aliasing", "Beam tracing", "Computer graphics", "Depth of field", "Distributed ray tracing", "Focal plane", "Gaussian filter", "Global illumination", "Image noise", "Lanczos resampling", "Light transport theory", "Mipmap", "Monte Carlo method", "Nyquist\u2013Shannon sampling theorem", "Pinhole camera", "Point spread function", "Ray tracing (graphics)", "Rectangular function", "Sample (signal)", "Sinc function", "Solid angle", "Sparse voxel octree"], "categories": ["Computer graphics", "Global illumination algorithms"], "title": "Cone tracing"}
{"summary": "Global illumination (shortened as GI) or indirect illumination is a general name for a group of algorithms used in 3D computer graphics that are meant to add more realistic lighting to 3D scenes. Such algorithms take into account not only the light which comes directly from a light source (direct illumination), but also subsequent cases in which light rays from the same source are reflected by other surfaces in the scene, whether reflective or not (indirect illumination).\nTheoretically reflections, refractions, and shadows are all examples of global illumination, because when simulating them, one object affects the rendering of another object (as opposed to an object being affected only by a direct light). In practice, however, only the simulation of diffuse inter-reflection or caustics is called global illumination.\nImages rendered using global illumination algorithms often appear more photorealistic than images rendered using only direct illumination algorithms. However, such images are computationally more expensive and consequently much slower to generate. One common approach is to compute the global illumination of a scene and store that information with the geometry, e.g., radiosity. That stored data can then be used to generate images from different viewpoints for generating walkthroughs of a scene without having to go through expensive lighting calculations repeatedly.\nRadiosity, ray tracing, beam tracing, cone tracing, path tracing, Metropolis light transport, ambient occlusion, photon mapping, and image based lighting are examples of algorithms used in global illumination, some of which may be used together to yield results that are not fast, but accurate.\nThese algorithms model diffuse inter-reflection which is a very important part of global illumination; however most of these (excluding radiosity) also model specular reflection, which makes them more accurate algorithms to solve the lighting equation and provide a more realistically illuminated scene.\nThe algorithms used to calculate the distribution of light energy between surfaces of a scene are closely related to heat transfer simulations performed using finite-element methods in engineering design.\nIn real-time 3D graphics, the diffuse inter-reflection component of global illumination is sometimes approximated by an \"ambient\" term in the lighting equation, which is also called \"ambient lighting\" or \"ambient color\" in 3D software packages. Though this method of approximation (also known as a \"cheat\" because it's not really a global illumination method) is easy to perform computationally, when used alone it does not provide an adequately realistic effect. Ambient lighting is known to \"flatten\" shadows in 3D scenes, making the overall visual effect more bland. However, used properly, ambient lighting can be an efficient way to make up for a lack of processing power.", "links": ["3D computer graphics", "Algorithm", "Ambient occlusion", "Beam tracing", "Caustic (optics)", "Cone tracing", "Diffuse inter-reflection", "Distributed ray tracing", "Finite element analysis", "Heat transfer", "High dynamic range imaging", "Image-based lighting", "Image based lighting", "International Standard Book Number", "Lightcuts", "Metropolis light transport", "Path tracing", "Photon mapping", "Point Based Global Illumination", "Radiosity (computer graphics)", "Ray tracing (graphics)", "Real-time rendering", "Rendering equation", "Specular reflection", "Spherical harmonic lighting"], "categories": ["All articles needing additional references", "Articles needing additional references from May 2013", "Global illumination algorithms"], "title": "Global illumination"}
{"summary": "The Metropolis light transport (MLT) is an application of a variant of the Monte Carlo method called the Metropolis-Hastings algorithm to the rendering equation for generating images from detailed physical descriptions of three-dimensional scenes.\nThe procedure constructs paths from the eye to a light source using bidirectional path tracing, then constructs slight modifications to the path. Some careful statistical calculation (the Metropolis algorithm) is used to compute the appropriate distribution of brightness over the image. This procedure has the advantage, relative to bidirectional path tracing, that once a path has been found from light to eye, the algorithm can then explore nearby paths; thus difficult-to-find light paths can be explored more thoroughly with the same number of simulated photons. In short, the algorithm generates a path and stores the path's 'nodes' in a list. It can then modify the path by adding extra nodes and creating a new light path. While creating this new path, the algorithm decides how many new 'nodes' to add and whether or not these new nodes will actually create a new path.\nMetropolis Light Transport is an unbiased method that, in some cases (but not always), converges to a solution of the rendering equation faster than other unbiased algorithms such as path tracing or bidirectional path tracing.", "links": ["3D computer graphics", "Arion Render", "Digital object identifier", "Eric Veach", "Indigo Renderer", "International Standard Book Number", "Iray", "Kerkythea", "Leonidas J. Guibas", "LuxRender", "Metropolis-Hastings algorithm", "Mitsuba Renderer", "Monte Carlo method", "Nicholas Metropolis", "Path tracing", "Rendering equation", "Stanford University"], "categories": ["All articles lacking in-text citations", "All articles with unsourced statements", "All stub articles", "Articles lacking in-text citations from February 2014", "Articles with unsourced statements from July 2010", "Computing stubs", "Global illumination algorithms", "Monte Carlo methods"], "title": "Metropolis light transport"}
{"summary": "Path tracing is a computer graphics Monte Carlo method of rendering images of three-dimensional scenes such that the global illumination is faithful to reality. Fundamentally, the algorithm is integrating over all the illuminance arriving to a single point on the surface of an object. This illuminance is then reduced by a surface reflectance function (BRDF) to determine how much of it will go towards the viewpoint camera. This integration procedure is repeated for every pixel in the output image. When combined with physically accurate models of surfaces, accurate models of real light sources (light bulbs), and optically-correct cameras, path tracing can produce still images that are indistinguishable from photographs.\nPath tracing naturally simulates many effects that have to be specifically added to other methods (conventional ray tracing or scanline rendering), such as soft shadows, depth of field, motion blur, caustics, ambient occlusion, and indirect lighting. Implementation of a renderer including these effects is correspondingly simpler. An extended version of the algorithm is realized by volumetric path tracing, which considers the light scattering of a scene.\nDue to its accuracy and unbiased nature, path tracing is used to generate reference images when testing the quality of other rendering algorithms. In order to get high quality images from path tracing, a large number of rays must be traced to avoid visible noisy artifacts.", "links": ["Algorithm", "Ambient occlusion", "Arithmetic mean", "Arnold (software)", "BRDF", "Bidirectional reflectance distribution function", "Bidirectional scattering distribution function", "Blender (software)", "CPU", "CUDA", "Caustic (optics)", "Chromatic aberration", "CiteSeer", "Computer graphics", "Depth of field", "Fluorescence", "GPGPU", "GPU", "Global illumination", "Illuminance", "Image", "Image noise", "Iridescence", "Lambert's cosine law", "Lambertian", "Leonidas J. Guibas", "Light scattering", "Luminance", "Metropolis light transport", "Monte Carlo integration", "Monte Carlo method", "Motion blur", "Nvidia", "Octane Render", "OpenCL", "OptiX", "Pathological (mathematics)", "Pixel", "Probability Density Function", "Radiance", "Radiosity (computer graphics)", "Ray tracing (graphics)", "Rendering (computer graphics)", "Rendering equation", "Scanline rendering", "Shadows", "Simulate", "Subsurface scattering", "Traceroute", "Tracing (disambiguation)", "Unbiased rendering", "Volumetric path tracing"], "categories": ["Global illumination algorithms"], "title": "Path tracing"}
{"summary": "In computer graphics, photon mapping is a two-pass global illumination algorithm developed by Henrik Wann Jensen that approximately solves the rendering equation. Rays from the light source and rays from the camera are traced independently until some termination criterion is met, then they are connected in a second step to produce a radiance value. It is used to realistically simulate the interaction of light with different objects. Specifically, it is capable of simulating the refraction of light through a transparent substance such as glass or water, diffuse interreflection between illuminated objects, the subsurface scattering of light in translucent materials, and some of the effects caused by particulate matter such as smoke or water vapor. It can also be extended to more accurate simulations of light such as spectral rendering.\nUnlike path tracing, bidirectional path tracing, volumetric path tracing and Metropolis light transport, photon mapping is a \"biased\" rendering algorithm, which means that averaging many renders using this method does not converge to a correct solution to the rendering equation. However, since it is a consistent method, any desired accuracy can be achieved by increasing the number of photons.", "links": ["Bidirectional reflectance distribution function", "Bidirectional surface scattering reflectance distribution function", "Caustic (optics)", "Color bleeding (computer graphics)", "Computer graphics", "Diffuse interreflection", "Global illumination", "Henrik Wann Jensen", "Irradiance", "Irradiance caching", "K-nearest neighbor algorithm", "Kd-tree", "Lambertian", "Light", "Metropolis light transport", "Monte Carlo method", "Path tracing", "Radiance", "Radiosity (computer graphics)", "Ray tracing (graphics)", "Refraction", "Rendering equation", "Scanline rendering", "Spectral rendering", "Specular reflection", "Subsurface scattering", "Volumetric path tracing", "Worcester Polytechnic Institute"], "categories": ["3D computer graphics", "Global illumination algorithms", "Infographics"], "title": "Photon mapping"}
{"summary": "In 3D computer graphics, radiosity is an application of the finite element method to solving the rendering equation for scenes with surfaces that reflect light diffusely. Unlike rendering methods that use Monte Carlo algorithms (such as path tracing), which handle all types of light paths, typical radiosity only account for paths (represented by the code \"LD*E\") which leave a light source and are reflected diffusely some number of times (possibly zero) before hitting the eye. Radiosity is a global illumination algorithm in the sense that the illumination arriving on a surface comes not just directly from the light sources, but also from other surfaces reflecting light. Radiosity is viewpoint independent, which increases the calculations involved, but makes them useful for all viewpoints.\nRadiosity methods were first developed in about 1950 in the engineering field of heat transfer. They were later refined specifically for the problem of rendering computer graphics in 1984 by researchers at Cornell University.\nNotable commercial radiosity engines are Enlighten by Geomerics (used for games including Battlefield 3 and Need for Speed: The Run); 3ds Max; form\u2022Z; LightWave 3D and the Electric Image Animation System.", "links": ["3D computer graphics", "3ds Max", "Algorithm", "Ambient occlusion", "Battlefield 3", "Binary space partitioning", "Cache (computing)", "Computation", "Computer Graphics (Publication)", "Cornell University", "Diffuse reflection", "Dimensionless number", "Electric Image Animation System", "False radiosity", "Finite element method", "Form-Z", "Form factor (radiative transfer)", "Gauss\u2013Seidel method", "Geomerics", "Global illumination", "Graphics accelerator", "Heat", "Heat transfer", "Hemicube (computer graphics)", "Hidden surface removal", "J. Turner Whitted", "Jacobi iteration", "Lambert's cosine law", "LightWave 3D", "Lightmap", "Low-pass filter", "Monte Carlo method", "Need for Speed: The Run", "OpenGL", "Parlance", "Path-tracing", "Path tracing", "Perspective transform", "Quadratic function", "Radiant exitance", "Radiosity (heat transfer)", "Ray tracing (graphics)", "Raytracing", "Rendering (computer graphics)", "Rendering equation", "Specular reflection", "Texture mapping", "Utah teapot", "View factor"], "categories": ["3D computer graphics", "All articles needing expert attention", "All articles that are too technical", "All articles with unsourced statements", "Articles needing expert attention from July 2009", "Articles with unsourced statements from March 2011", "Global illumination algorithms", "Heat transfer", "Wikipedia articles that are too technical from July 2009"], "title": "Radiosity (computer graphics)"}
{"summary": "Pankaj Kumar Agarwal is an Indian computer scientist and mathematician researching algorithms in computational geometry and related areas. He is the RJR Nabisco Professor of Computer Science and Mathematics at Duke University, where he has been chair of the computer science department since 2004. He obtained his Ph.D. in Computer Science in 1989 from the Courant Institute, New York, under the supervision of Micha Sharir.", "links": ["Algorithm", "Arrangement of lines", "Association for Computing Machinery", "Circle packing theorem", "Computational geometry", "Computer scientist", "Courant Institute", "Davenport\u2013Schinzel sequence", "Discrepancy theory", "Duke University", "Euclidean plane", "Extremal graph theory", "Fellow", "J\u00e1nos Pach", "K-set (geometry)", "Mathematical Reviews", "Mathematician", "Mathematics Genealogy Project", "Micha Sharir", "Minkowski's theorem", "New York", "Planar separator theorem", "Projective plane", "Ray casting", "Shortest path", "Sphere packing", "Vapnik\u2013Chervonenkis dimension"], "categories": ["Duke University faculty", "Fellows of the Association for Computing Machinery", "Indian mathematicians", "Living people", "New York University alumni", "Researchers in geometric algorithms", "Year of birth missing (living people)"], "title": "Pankaj K. Agarwal"}
{"summary": "Lars Allan Arge is a Danish computer scientist, the head of the Center for Massive Data Algorithmics (MADALGO) at Aarhus University, where he is also a professor of computer science. His research involves the study of algorithms and data structures for handling massive data, especially in graph algorithms and computational geometry.\nArge earned his Ph.D. in 1996 from Aarhus University, under the supervision of Erik Meineche Schmidt. He was a professor at Duke University before returning to Aarhus as a professor in 2004, and he continues to hold an adjunct professorship at Duke.\nArge is a member of the Royal Danish Academy of Sciences and Letters, and was elected to the presidium of the academy in 2015. In 2012, he was elected as a Fellow of the Association for Computing Machinery \"for contributions to massive data algorithmics\", becoming only the second ACM Fellow in Denmark. He also belongs to the Danish Academy of Technical Sciences.", "links": ["Aarhus University", "Akademiet for de Tekniske Videnskaber", "Algorithm", "Association for Computing Machinery", "Computational geometry", "Data structure", "Duke University", "Fellow", "Graph algorithm", "Mathematics Genealogy Project", "Royal Danish Academy of Sciences and Letters"], "categories": ["Aarhus University alumni", "Aarhus University faculty", "Danish computer scientists", "Duke University faculty", "Fellows of the Association for Computing Machinery", "Living people", "Researchers in geometric algorithms", "Year of birth missing (living people)"], "title": "Lars Arge"}
{"summary": "David Michael Avis (born March 20, 1951) is a Canadian and British computer scientist known for his contributions to geometric computations. Avis is a professor in computational geometry and applied mathematics in the School of Computer Science, McGill University, in Montreal. Since 2010, he belongs to Department of Communications and Computer Engineering, School of Informatics, Kyoto University.\nAvis received his Ph.D. in 1977 from Stanford University. He has published more than 70 journal papers and articles. Writing with Komei Fukuda, Avis proposed a reverse-search algorithm for the vertex enumeration problem; their algorithm generates all of the vertices of a convex polytope.\nHe has a collaboration article with Paul Erd\u0151s. Therefore, his Erd\u0151s number is 1.\n^ David Avis at the Mathematics Genealogy Project\n^ Avis & Fukuda (1992)\n^ Avis & Fukuda (1996)\n^ David Avis, Paul Erd\u00f6s and J\u00e1nos Pach: \"Repeated distances in space\"(1988)", "links": ["Alma mater", "Applied mathematics", "Canada", "Computational geometry", "Computer Science", "Convex polytope", "David Rappaport (mathematician)", "Digital object identifier", "Discrete Applied Mathematics", "Discrete and Computational Geometry", "Doctoral advisor", "Erd\u0151s number", "International Standard Name Identifier", "Jean-Marc Robert", "Komei Fukuda", "Kyoto University", "Mathematical Reviews", "Mathematics", "Mathematics Genealogy Project", "McGill University", "Montreal", "Paul Erd\u0151s", "Rephael Wenger", "Reverse-search algorithm", "Stanford University", "Syst\u00e8me universitaire de documentation", "Thomas Shermer", "United Kingdom", "Vertex (geometry)", "Vertex enumeration problem", "Virtual International Authority File", "V\u00e1clav Chv\u00e1tal"], "categories": ["1951 births", "20th-century British mathematicians", "21st-century British mathematicians", "All stub articles", "Anglophone Quebec people", "British mathematician stubs", "Living people", "McGill University faculty", "Researchers in geometric algorithms", "Stanford University alumni", "Wikipedia articles with ISNI identifiers", "Wikipedia articles with VIAF identifiers"], "title": "David Avis"}
{"summary": "Jon Louis Bentley (born February 20, 1953 in Long Beach, California) is an American computer scientist who is credited with the heuristic based partitioning algorithm k-d tree.\nBentley received a B.S. in mathematical sciences from Stanford University in 1974, and M.S. and Ph.D in 1976 from the University of North Carolina at Chapel Hill; while a student, he also held internships at the Xerox Palo Alto Research Center and Stanford Linear Accelerator Center. After receiving his Ph.D., he joined the faculty at Carnegie Mellon University as an assistant professor of computer science and mathematics. At CMU, his students included Brian Reid, John Ousterhout, Jeff Eppinger, Joshua Bloch, and James Gosling, and he was one of Charles Leiserson's advisors. Later, Bentley moved to Bell Laboratories.\nHe found an optimal solution for the two dimensional case of Klee's measure problem: given a set of n rectangles, find the area of their union. He and Thomas Ottmann invented the Bentley\u2013Ottmann algorithm, an efficient algorithm for finding all intersecting pairs among a collection of line segments. He wrote the Programming Pearls column for the Communications of the ACM magazine, and later collected the articles into two books of the same name.\nBentley received the Dr. Dobb's Excellence in Programming award in 2004.", "links": ["Algorithm", "Alma mater", "Area (geometry)", "Bell Laboratories", "Bentley\u2013Ottmann algorithm", "Biblioth\u00e8que nationale de France", "Brian Reid (computer scientist)", "Carnegie Mellon University", "Charles E. Leiserson", "Communications of the ACM", "Computer science", "Computer scientist", "Digital object identifier", "Doctoral advisor", "Dr. Dobb's Journal", "GitHub", "International Standard Name Identifier", "James Gosling", "Jeff Eppinger", "John Ousterhout", "Jon Bentley (TV presenter)", "Joshua Bloch", "K-d tree", "Klee's measure problem", "Library of Congress Control Number", "Long Beach, California", "Mathematics", "Mathematics Genealogy Project", "Rectangle", "Stanford Linear Accelerator Center", "Stanford University", "Syst\u00e8me universitaire de documentation", "The C Programming Language", "Thesis", "University of North Carolina at Chapel Hill", "Virtual International Authority File", "Xerox Palo Alto Research Center"], "categories": ["1953 births", "American computer programmers", "American computer scientists", "Carnegie Mellon University faculty", "Living people", "People from Long Beach, California", "Researchers in geometric algorithms", "Stanford University alumni", "University of North Carolina at Chapel Hill alumni", "Wikipedia articles with BNF identifiers", "Wikipedia articles with ISNI identifiers", "Wikipedia articles with LCCN identifiers", "Wikipedia articles with VIAF identifiers"], "title": "Jon Bentley"}
{"summary": "Prosenjit K. \"Jit\" Bose is a Canadian mathematician and computer scientist who works at Carleton University as a professor in the School of Computer Science and associate dean of research and graduate studies for the Faculty of Science. His research concerns graph algorithms and computational geometry, including work on geometric spanners and geographic routing in wireless ad hoc networks.\nBose did his undergraduate studies in mathematics at the University of Waterloo, graduating in 1990, and earned a master's degree from Waterloo in 1991. He earned his Ph.D. in computer science from McGill University in 1994 under the supervision of Godfried Toussaint. After postdoctoral studies at the University of British Columbia, he became an assistant professor at the Universit\u00e9 du Qu\u00e9bec \u00e0 Trois-Rivi\u00e8res in 1995, and moved to Carleton in 1997.", "links": ["Carleton University", "Computational geometry", "David G. Kirkpatrick", "Digital object identifier", "Geographic routing", "Geometric spanner", "Godfried Toussaint", "Graph algorithm", "Jorge Urrutia Galicia", "Luc Devroye", "Mathematical Reviews", "Mathematics Genealogy Project", "McGill University", "SIAM Journal on Computing", "SIAM Journal on Discrete Mathematics", "University of British Columbia", "University of Waterloo", "Universit\u00e9 du Qu\u00e9bec \u00e0 Trois-Rivi\u00e8res", "Wireless ad hoc network"], "categories": ["20th-century mathematicians", "21st-century mathematicians", "Canadian computer scientists", "Canadian mathematicians", "Carleton University faculty", "Graph drawing people", "Living people", "McGill University alumni", "Researchers in geometric algorithms", "University of Waterloo alumni", "Universit\u00e9 du Qu\u00e9bec \u00e0 Trois-Rivi\u00e8res faculty", "Year of birth missing (living people)"], "title": "Jit Bose"}
{"summary": "Timothy Moon-Yew Chan is Professor and University Research Chair in the David R. Cheriton School of Computer Science, University of Waterloo, Canada.\nHe graduated with BA (summa cum laude) from Rice University in 1992, and completed his Ph.D. in Computer Science at UBC in 1995 at the age of 19.\nHe was awarded the Governor General's Gold Medal (as Head of Graduating Class in the Faculty of Graduate Studies at the University of British Columbia during convocation), the NSERC doctoral prize, and the Premier's Research Excellence Award (PREA) of Ontario, Canada.\nHe is currently an associate editor for the ACM Transactions on Algorithms (TALG), and the International Journal of Computational Geometry and Applications. He is also a member of the editorial board of Algorithmica, Discrete and Computational Geometry, as well as Computational Geometry: Theory and Applications.\nChan has published extensively. His research covers Data Structures, Algorithms and Computational geometry.", "links": ["Algorithmica", "Algorithms", "Bachelor of Arts", "Canada", "Chan's algorithm", "Computational geometry", "Computer Science", "Convex hull", "Data Structures", "David R. Cheriton School of Computer Science", "Discrete and Computational Geometry", "International Journal of Computational Geometry and Applications", "NSERC", "Ontario", "Output-sensitive algorithm", "Ph.D.", "Professor", "Rice University", "Summa cum laude", "University of British Columbia", "University of Waterloo"], "categories": ["1976 births", "Living people", "Researchers in geometric algorithms", "Rice University alumni", "University of British Columbia alumni", "University of Waterloo faculty"], "title": "Timothy M. Chan"}
{"summary": "Bernard Chazelle (born November 5, 1955) is the Eugene Higgins Professor of Computer Science at Princeton University. Much of his work is in computational geometry, where he has found many of the best-known algorithms, such as linear-time triangulation of a simple polygon, as well as major complexity results, such as lower bound techniques based on discrepancy theory. He is also known for his invention of the soft heap data structure and the most asymptotically efficient known algorithm for finding minimum spanning trees.\n\n", "links": ["American Academy of Arts and Sciences", "Applied mathematics", "Association for Computing Machinery", "Asymptotically", "Biblioth\u00e8que nationale de France", "Brown University", "Carnegie Mellon University", "Coll\u00e8ge de France", "Computational geometry", "Computer Science", "Computer science", "Damien Chazelle", "David P. Dobkin", "Defamation", "Digital object identifier", "Discrepancy theory", "Ecole des Mines de Paris", "European Academy of Sciences", "France", "INRIA", "Institute for Advanced Study", "International Standard Book Number", "International Standard Name Identifier", "International Standard Serial Number", "John Simon Guggenheim Memorial Foundation", "Journal of the Association for Computing Machinery", "Linear-time", "Lower bound", "Mathematical Reviews", "Mathematics Genealogy Project", "Minimum spanning tree", "NEC Research Institute", "Paris, France", "Polygon triangulation", "Princeton University", "Simple polygon", "Soft heap", "Syst\u00e8me universitaire de documentation", "Virtual International Authority File", "Xerox PARC", "Yale University", "YouTube", "\u00c9cole Normale Sup\u00e9rieure", "\u00c9cole Polytechnique"], "categories": ["1955 births", "American computer scientists", "Articles with hCards", "BLP articles lacking sources from October 2012", "Fellows of the Association for Computing Machinery", "French computer scientists", "Guggenheim Fellows", "Infobox person using numbered parameter", "Living people", "People from Paris", "Princeton University faculty", "Researchers in geometric algorithms", "Wikipedia articles with BNF identifiers", "Wikipedia articles with ISNI identifiers", "Wikipedia articles with VIAF identifiers", "Yale University alumni"], "title": "Bernard Chazelle"}
{"summary": "Matthew T. Dickerson is a professor of computer science at Middlebury College in Vermont, a scholar of the fiction of J. R. R. Tolkien and the Inklings, a novelist, a blues musician and historian of music, a fly fisherman, a maple sugar farmer, and a beekeeper.\nDickerson received an A.B. from Dartmouth College in 1985, and a Ph.D. in Computer Science from Cornell University, under the supervision of Dexter Kozen, in 1989. His Ph.D. research was in symbolic computation, but since then he has worked primarily in computational geometry; his most frequently cited computer science papers concern k-nearest neighbor algorithms and minimum-weight triangulation. He has been on the Middlebury faculty since receiving his Ph.D.\nHe is also the author of six non-technical books, most of them about fantasy fiction. His 2003 book Following Gandalf: Epic Battles and Moral Victory in The Lord of the Rings (Brazos Press, 2003, ISBN 978-1-58743-085-5), a study of the moral and Christian values expressed by Tolkien's works, highlights the contrasts between moral and physical victories, and between heroism and violence; it points out the necessity of having free will in order to make moral choices. It was shortlisted for the Mythopoeic Society's 2004 and 2005 Mythopoeic Scholarship Awards. He has also written a pair of books on Tolkien, C. S. Lewis, and environmentalism, Ents, Elves, and Eriador: The Environmental Vision of J.R.R. Tolkien (with Jonathan Evans, The University Press of Kentucky, 2006, ISBN 978-0-8131-2418-6) and Narnia and the Fields of Arbol: The Environmental Vision of C. S. Lewis (with David L. O'Hara, The University Press of Kentucky, 2009, ISBN 978-0-8131-2522-0). Despite giving the first of these two books an overall negative review, reviewer Patrick Curry writes that it is \"a major new contribution to the subject of Tolkien's work\". His other books include The Finnsburg Encounter (Crossway Books, 1991, ISBN 978-0-89107-604-9), a work of historical fiction, translated into German as Licht uber Friesland (Verlag Schulte & Gerth, 1996, ISBN 3-89437-422-5), Hammers and Nails: The Life and Music of Mark Heard (Cornerstone Press, 2003, ISBN 978-0-940895-49-2), a biography of musician Mark Heard, and From Homer to Harry Potter: A Handbook on Myth and Fantasy (with David L. O'Hara, Brazos Press, 2006, ISBN 978-1-58743-133-3).\nFrom 1997 to 2001 Dickerson published a biweekly column on fishing and the outdoors in the Addison Independent, a local newspaper. Since 2002 he has been the director of the New England Young Writers Conference, an annual four-day conference for high school students in Bread Loaf, Vermont that is associated with Middlebury College. He is also the founding director of the Vermont Conference on Christianity and the Arts. He plays bass in a Vermont-based blues band, Deep Freyed.", "links": ["Association for Computing Machinery", "C. S. Lewis", "Computational geometry", "Computer science", "Cornell University", "Dartmouth College", "Dexter Kozen", "Digital object identifier", "Environmentalism", "Inklings", "International Journal of Computational Geometry and Applications", "J. R. R. Tolkien", "Journal of Religion and Popular Culture", "J\u00f6rg-R\u00fcdiger Sack", "K-nearest neighbor algorithm", "Mark Heard", "Middlebury College", "Minimum-weight triangulation", "Mythopoeic Awards", "Mythopoeic Society", "Symbolic computation", "The Lord of the Rings", "Vermont"], "categories": ["American computer scientists", "Cornell University alumni", "Dartmouth College alumni", "Living people", "Middlebury College faculty", "Researchers in geometric algorithms", "Tolkien studies", "Year of birth missing (living people)"], "title": "Matthew T. Dickerson"}
{"summary": "Gy\u00f6rgy Elekes (19 May 1949 \u2013 29 September 2008) was a Hungarian mathematician and computer scientist who specialized in Combinatorial geometry and Combinatorial set theory. He may be best known for his work in the field that would eventually be called Additive Combinatorics. Particularly notable was his \"ingenious\" application of the Szemer\u00e9di\u2013Trotter theorem to improve the best known lower bound for the sum-product problem. He also proved that any polynomial-time algorithm approximating the volume of convex bodies must have a multiplicative error, and the error grows exponentially on the dimension. With Micha Sharir he set up a framework which eventually led Guth and Katz to the solution of the Erd\u0151s distinct distances problem. (See below.)", "links": ["Additive number theory", "Algebraic geometry", "Alma mater", "Andr\u00e1s Hajnal", "Budapest", "Combinatorial geometry", "Combinatorial set theory", "Computational geometry", "Computer Science", "Computer science", "Computer scientist", "Convex body", "Digital object identifier", "Discrete and Computational Geometry", "Discrete geometry", "Doktor nauk", "Erd\u0151s distinct distances problem", "Exponential growth", "E\u00f6tv\u00f6s Lor\u00e1nd University", "Fazekas Mih\u00e1ly", "Fazekas Mih\u00e1ly Gimn\u00e1zium (Budapest)", "F\u00f3t", "George B. Purdy", "Hungarian Academy of Sciences", "Hungary", "International Standard Book Number", "Larry Guth", "L\u00e1szl\u00f3 Lov\u00e1sz", "Mathematical analysis", "Mathematician", "Mathematics", "Micha Sharir", "Nets Hawk Katz", "Nets Katz", "Number theory", "Numerical stability", "Paul Erd\u0151s", "Professor", "Sum-product problem", "Szemer\u00e9di\u2013Trotter theorem", "Terence Tao", "Time complexity", "Van H. Vu", "Volume"], "categories": ["1949 births", "2008 deaths", "Combinatorialists", "Hungarian computer scientists", "Hungarian mathematicians", "Number theorists", "Researchers in geometric algorithms"], "title": "Gy\u00f6rgy Elekes"}
{"summary": "David Arthur Eppstein (born 1963) is an American computer scientist and mathematician. He is a Chancellor's Professor of computer science at University of California, Irvine. He is known for his work in computational geometry, graph algorithms, and recreational mathematics.", "links": ["ACM Fellow", "Alma mater", "Association for Computing Machinery", "Bachelor of Science", "Biblioth\u00e8que nationale de France", "Columbia University", "Computational geometry", "Computational statistics", "Computer science", "Computer scientist", "David Epstein (disambiguation)", "Digital Bibliography & Library Project", "Digital object identifier", "Doctoral advisor", "Donald Bren School of Information and Computer Sciences", "England", "Finite element meshing", "Geometric", "Graph coloring", "Graph data structure", "Graph drawing", "Graph theory", "Integrated Authority File", "International Standard Book Number", "International Standard Name Identifier", "International Symposium on Graph Drawing", "Irvine, California", "Jean-Claude Falmagne", "Journal of the ACM", "Library of Congress Control Number", "Los Angeles Times", "Master of Science", "Mathematician", "Mathematics", "Mathematics Genealogy Project", "Microsoft Academic Search", "Minimum spanning tree", "Multivariate statistics", "National Science Foundation", "Nonparametric statistics", "Optimization (mathematics)", "Palo Alto Research Center", "Ph.D.", "Recreational mathematics", "Robust statistics", "SIAM Journal on Computing", "Shortest path", "Stanford University", "Symposium on Computational Geometry", "Syst\u00e8me universitaire de documentation", "Thesis", "University of California, Irvine", "Virtual International Authority File", "Xerox", "Zvi Galil"], "categories": ["1963 births", "American computer scientists", "British emigrants to the United States", "Cellular automatists", "Columbia School of Engineering and Applied Science alumni", "Fellows of the Association for Computing Machinery", "Graph drawing people", "Graph theorists", "Living people", "Palo Alto High School alumni", "People from Irvine, California", "Recreational mathematicians", "Researchers in geometric algorithms", "Stanford University alumni", "University of California, Irvine faculty", "Wikipedia articles with BNF identifiers", "Wikipedia articles with GND identifiers", "Wikipedia articles with ISNI identifiers", "Wikipedia articles with LCCN identifiers", "Wikipedia articles with VIAF identifiers", "Wikipedia indefinitely semi-protected biographies of living people"], "title": "David Eppstein"}
{"summary": "Leonidas John Guibas (Greek: \u039b\u03b5\u03c9\u03bd\u03af\u03b4\u03b1\u03c2 \u0393\u03ba\u03af\u03bc\u03c0\u03b1\u03c2) is a professor of computer science at Stanford University, where he heads the geometric computation group and is a member of the computer graphics and artificial intelligence laboratories. Guibas was a student of Donald Knuth at Stanford, where he received his Ph.D. in 1976. He has worked for several industrial research laboratories, and joined the Stanford faculty in 1984. He was program chair for the ACM Symposium on Computational Geometry in 1996, is a Fellow of the ACM and the IEEE, and was awarded the ACM - AAAI Allen Newell Award for 2007 \u201cfor his pioneering contributions in applying algorithms to a wide range of computer science disciplines.\u201c He has Erd\u0151s number 2 due to his collaborations with Boris Aronov, Andrew Odlyzko, J\u00e1nos Pach, Richard M. Pollack, Endre Szemer\u00e9di, and Frances Yao. The research contributions he is known for include finger trees, red-black trees, fractional cascading, the Guibas\u2013Stolfi algorithm for Delaunay triangulation, an optimal data structure for point location, the quad-edge data structure for representing planar subdivisions, Metropolis light transport, and kinetic data structures for keeping track of objects in motion.", "links": ["ACM - AAAI Allen Newell Award", "Andrew Odlyzko", "Association for Computing Machinery", "Computer Science", "Computer science", "Delaunay triangulation", "Doctoral advisor", "Donald Knuth", "Endre Szemer\u00e9di", "Erd\u0151s number", "Fellow", "Finger tree", "Fractional cascading", "Frances Yao", "Greece", "Greek language", "IEEE", "Jorge Stolfi", "J\u00e1nos Pach", "Kinetic data structure", "Mathematics Genealogy Project", "Metropolis light transport", "Point location", "Quad-edge", "Red-black tree", "Stanford University", "Symposium on Computational Geometry", "United States"], "categories": ["American computer scientists", "Articles containing Greek-language text", "Fellow Members of the IEEE", "Fellows of the Association for Computing Machinery", "Greek computer scientists", "Living people", "Researchers in geometric algorithms", "Stanford University Department of Computer Science faculty", "Stanford University School of Engineering faculty", "Stanford University alumni", "Year of birth missing (living people)"], "title": "Leonidas J. Guibas"}
{"summary": "John E. Hershberger (born 1959) is an American computer scientist and software professional, a principal engineer at Mentor Graphics Corporation since 1993. He is known for his research in computational geometry and algorithm engineering.", "links": ["Algorithm engineering", "Association for Computing Machinery", "California", "California Institute of Technology", "Computational geometry", "Computer science", "DBLP", "DEC Systems Research Center", "Defamation", "Digital Equipment Corporation", "Digital object identifier", "Homotopic", "Jack Snoeyink", "Julien Basch", "Kinetic data structure", "Leonidas Guibas", "Leonidas J. Guibas", "Line segment", "Mathematical Reviews", "Mentor Graphics", "Oregon", "Palo Alto", "Parallel algorithms", "Ph.D.", "Plane (geometry)", "Polygon", "Shortest path", "Shortest path tree", "Simple polygon", "Stanford University", "Subhash Suri", "Symposium on Computational Geometry", "Tigard", "Time complexity", "Triangulation", "University of Trier", "Visibility graph", "Visibility polygon", "Visibility problem"], "categories": ["1959 births", "American computer scientists", "BLP articles lacking sources from January 2013", "CS1 errors: chapter ignored", "California Institute of Technology alumni", "Fellows of the Association for Computing Machinery", "Living people", "People from Tigard, Oregon", "Researchers in geometric algorithms"], "title": "John Hershberger"}
{"summary": "David Galer Kirkpatrick is a professor of computer science at the University of British Columbia. He is known for the Kirkpatrick\u2013Seidel algorithm and his work on polygon triangulation, and for co-inventing \u03b1-shapes and the \u03b2-skeleton. He received his PhD from the University of Toronto in 1974.", "links": ["Beta skeleton", "Computer science", "Digital object identifier", "Herbert Edelsbrunner", "Kirkpatrick\u2013Seidel algorithm", "Polygon triangulation", "Raimund Seidel", "SWAT and WADS conferences", "University of British Columbia", "University of Toronto", "Virtual International Authority File"], "categories": ["All stub articles", "Canadian computer scientists", "Canadian computer specialist stubs", "Canadian people stubs", "Fellows of the Royal Society of Canada", "Living people", "Researchers in geometric algorithms", "University of British Columbia faculty", "University of Toronto alumni", "Wikipedia articles with VIAF identifiers", "Year of birth missing (living people)"], "title": "David G. Kirkpatrick"}
{"summary": "Ji\u0159\u00ed (Jirka) Matou\u0161ek (10 March 1963 \u2013 9 March 2015) was a Czech mathematician working in computational geometry and algebraic topology. He was a professor at Charles University in Prague and the author of several textbooks and research monographs.\nMatou\u0161ek was born in Prague. In 1986, he received his Master's degree at Charles University under Miroslav Kat\u011btov. From 1986 until his death he was employed at the Department of Applied Mathematics of Charles University in Prague, holding a professor position since 2000. He was also a visiting and later full professor at ETH Zurich.\nIn 1996, he won the European Mathematical Society prize and in 2000 he won the Scientist award of the Learned Society of the Czech Republic. He became a fellow of the Learned Society of the Czech Republic in 2005.\nMatou\u0161ek's paper on computational aspects of algebraic topology won the Best Paper award at the 2012 ACM Symposium on Discrete Algorithms.\nAside from his own academic writing, he has translated the popularization book Mathematics: A Very Short Introduction by Timothy Gowers into Czech.\nHe was a supporter and signatory of the Cost of Knowledge protest. He died in 2015, aged 51.", "links": ["Algebraic topology", "American Mathematical Monthly", "American Mathematical Society", "Biblioth\u00e8que nationale de France", "Charles University in Prague", "Computational geometry", "Cost of Knowledge", "Czech people", "Digital object identifier", "Discrepancy theory", "ETH Zurich", "European Mathematical Society", "Graduate Texts in Mathematics", "Ham sandwich theorem", "Imre B\u00e1r\u00e1ny", "Integrated Authority File", "International Standard Book Number", "International Standard Name Identifier", "JSTOR", "Jan Kratochv\u00edl", "Jaroslav Ne\u0161et\u0159il", "Ji\u0159\u00ed Matou\u0161ek (disambiguation)", "Kneser graph", "Learned Society of the Czech Republic", "Library of Congress Control Number", "Master's degree", "Mathematical Research Institute of Oberwolfach", "Mathematical Reviews", "Mathematician", "Mathematics", "Microsoft Academic Search", "Miroslav Kat\u011btov", "Oxford University Press", "Prague", "Robin Thomas (mathematician)", "Springer-Verlag", "Syst\u00e8me universitaire de documentation", "Timothy Gowers", "Virtual International Authority File"], "categories": ["1963 births", "2015 deaths", "All stub articles", "CS1 Czech-language sources (cs)", "Charles University in Prague faculty", "Czech mathematicians", "European mathematician stubs", "People from Prague", "Researchers in geometric algorithms", "Wikipedia articles with BNF identifiers", "Wikipedia articles with GND identifiers", "Wikipedia articles with ISNI identifiers", "Wikipedia articles with LCCN identifiers", "Wikipedia articles with VIAF identifiers"], "title": "Ji\u0159\u00ed Matou\u0161ek (mathematician)"}
{"summary": "Nimrod Megiddo (Hebrew: \u05e0\u05de\u05e8\u05d5\u05d3 \u05de\u05d2\u05d9\u05d3\u05d5\u200e) is a mathematician and computer scientist. He is a research scientist at the IBM Almaden Research Center.\nHis interests include optimization, algorithm design and analysis, game theory, and machine learning.\nMegiddo received his Ph.D. in mathematics from the Hebrew University of Jerusalem.\nMegiddo received the 2014 John von Neumann Theory Prize and is a 1992 Frederick W. Lanchester Prize recipient.", "links": ["Algorithm design", "Almaden Research Center", "Computational geometry", "Computer (journal)", "Computer scientist", "DBLP", "Frederick W. Lanchester Prize", "Game theory", "Hebrew University of Jerusalem", "Hebrew language", "IBM", "INFORMS", "Integrated Authority File", "International Standard Name Identifier", "John von Neumann Theory Prize", "Library of Congress Control Number", "Machine learning", "Mathematician", "Prune and search", "Smallest-circle problem", "Virtual International Authority File"], "categories": ["American computer scientists", "American operations researchers", "Articles containing Hebrew-language text", "Game theorists", "Hebrew University of Jerusalem alumni", "Israeli operations researchers", "John von Neumann Theory Prize winners", "Living people", "Numerical analysts", "Researchers in geometric algorithms", "Wikipedia articles with GND identifiers", "Wikipedia articles with ISNI identifiers", "Wikipedia articles with LCCN identifiers", "Wikipedia articles with VIAF identifiers", "Year of birth missing (living people)"], "title": "Nimrod Megiddo"}
{"summary": "Joseph O'Rourke is the Olin Professor of Computer Science at Smith College and the founding chair of the Smith computer science department. His main research interest is computational geometry.\nO'Rourke was the first person to publish an algorithm to determine the minimum bounding box of a point set in three dimensions.\nIn 1985, O'Rourke was the program chair of the first annual Symposium on Computational Geometry. He was formerly the arXiv moderator for computational geometry and discrete mathematics.\nIn 2012 O'Rourke was named a fellow of the Association for Computing Machinery.", "links": ["ArXiv", "Association for Computing Machinery", "Biblioth\u00e8que nationale de France", "Computational geometry", "Computer scientist", "Defamation", "Discrete mathematics", "Erik Demaine", "Forum moderator", "International Standard Name Identifier", "Jacob E. Goodman", "Minimum bounding box", "Minimum bounding box algorithms", "Satyan Devadoss", "Smith College", "Symposium on Computational Geometry", "Syst\u00e8me universitaire de documentation", "Virtual International Authority File"], "categories": ["All stub articles", "American computer scientists", "BLP articles lacking sources from December 2012", "Computer scientist stubs", "Fellows of the Association for Computing Machinery", "Guggenheim Fellows", "Living people", "Researchers in geometric algorithms", "Smith College faculty", "Wikipedia articles with BNF identifiers", "Wikipedia articles with ISNI identifiers", "Wikipedia articles with VIAF identifiers", "Year of birth missing (living people)"], "title": "Joseph O'Rourke (professor)"}
{"summary": "Markus Hendrik \"Mark\" Overmars (Dutch pronunciation: [\u02c8m\u0251rk\u028fs \u02c8\u0266\u025bndr\u026ak \u02c8m\u0251rk \u02c8o\u02d0v\u0259r\u02ccm\u0251rs], born 29 September 1958 in Zeist, Netherlands) is a Dutch computer scientist and teacher of game programming known for his game development application Game Maker. Game Maker lets people create computer games using a drag-and-drop interface. He is the former head of the Center for Geometry, Imaging, and Virtual Environments at Utrecht University, in the Netherlands. This research center concentrates on computational geometry and its application in areas like computer graphics, robotics, geographic information systems, imaging, multimedia, virtual environments, and games.\nOvermars received his Ph.D. in 1983 from Utrecht University under the supervision of Jan van Leeuwen, and has since been a member of the faculty of the same university. Overmars has published over 100 journal papers, largely on computational geometry, and is the co-author of several books including a widely used computational geometry text.\nOvermars has also worked in robotics. He was the first to develop the probabilistic roadmap method in 1992, which was later independently discovered by Kavraki and Latombe in 1994. Their joint paper, Probabilistic roadmaps for path planning in high-dimensional configuration spaces, is considered one of the most influential studies in motion planning, and has been widely cited (more than 2500 times as of 2014 according to Google Scholar).\n^ Curriculum vitae, archived from the Utrecht University web site on October 2, 2011.\n^ Former colleagues, GIVE Center, retrieved 2014-01-16.\n^ Markus (Mark) Hendrik Overmars at the Mathematics Genealogy Project.\n^ Kavraki, L.E.; Svestka, P.; Latombe, J.C.; Overmars, M.H. (1996). \"Probabilistic roadmaps for path planning in high-dimensional configuration spaces\". Robotics and Automation, IEEE Transactions on 12 (4): 566\u2013580. doi:10.1109/70.508439. \n^ Karaman, Sertac; Frazzoli, Emilio (2011), \"Sampling-based algorithms for optimal motion planning\", International Journal of Robotics Research 30 (7): 846\u2013894, doi:10.1177/0278364911406761, Arguably, the most influential sampling-based motion planning algorithms to date include probabilistic roadmaps \n^ Citations to probabilistic roadmaps, Google Scholar, retrieved 2014-01-17.", "links": ["Alma mater", "Computational geometry", "Computer graphics", "Computer science", "Der-Tsai Lee", "Digital object identifier", "Doctoral advisor", "GameMaker: Studio", "Geographic information system", "Google Scholar", "International Standard Book Number", "International Standard Name Identifier", "Jan van Leeuwen", "Jean-Claude Latombe", "Library of Congress Control Number", "Lydia Kavraki", "Marc Overmars", "Mathematical Reviews", "Mathematics Genealogy Project", "Motion planning", "Multimedia", "Netherlands", "Probabilistic Roadmap Method", "Probabilistic roadmap method", "Robotics", "Syst\u00e8me universitaire de documentation", "Thesis", "Utrecht (city)", "Utrecht University", "Virtual International Authority File", "Zeist"], "categories": ["1958 births", "Dutch computer programmers", "Dutch computer scientists", "Living people", "People from Zeist", "Researchers in geometric algorithms", "Roboticists", "Utrecht University alumni", "Utrecht University faculty", "Wikipedia articles with ISNI identifiers", "Wikipedia articles with LCCN identifiers", "Wikipedia articles with VIAF identifiers"], "title": "Mark Overmars"}
{"summary": "John H. Reif (born 1951) is an American academic, and Professor of Computer Science at Duke University, who has made contributions to large number of fields in computer science: ranging from algorithms and computational complexity theory to robotics and to game theory.", "links": ["Algorithms", "American Association for the Advancement of Science", "Arnold E. Reif", "Association for Computing Machinery", "Computational complexity theory", "Computational geometry", "Computer science", "DNA computing", "DNA nanotechnology", "Duke University", "Game theory", "Graph algorithm", "H-index", "IEEE", "John F. Reif", "Kinodynamic planning", "Motion planning", "Nanorobotics", "Nanoscience", "Optical computing", "Parallel algorithms", "Randomized algorithms", "Robotics", "United States"], "categories": ["1951 births", "American academics", "DNA nanotechnology", "Duke University faculty", "Fellow Members of the IEEE", "Fellows of the American Association for the Advancement of Science", "Fellows of the Association for Computing Machinery", "Harvard University alumni", "Harvard University faculty", "Living people", "Researchers in geometric algorithms", "Theoretical computer scientists", "Tufts University alumni"], "title": "John Reif"}
{"summary": "Pierre Rosenstiehl (born 1933) is a French mathematician recognized for his work in graph theory, planar graphs, and graph drawing.\nThe Fraysseix-Rosenstiehl's planarity criterion is at the origin of the left-right planarity algorithm implemented in Pigale software, which is considered as the fastest implemented planarity testing algorithm.\nRosenstiehl was directeur d\u2019\u00e9tudes at the \u00c9cole des Hautes \u00c9tudes en Sciences Sociales in Paris, before his retirement. He is co-editor in chief of the European Journal of Combinatorics. Rosenstiehl, Giuseppe Di Battista, Peter Eades and Roberto Tamassia organized in 1992 at Marino (Italy) a meeting devoted to graph drawing which initiated a long series of international conferences, the International Symposia on Graph Drawing.\nHe has been a member of the French literary group Oulipo since 1992. He married the French author and illustrator Agn\u00e8s Rosenstiehl.", "links": ["Biblioth\u00e8que nationale de France", "Digital object identifier", "European Journal of Combinatorics", "Fraysseix-Rosenstiehl's planarity criterion", "French people", "Graph drawing", "Graph theory", "International Symposium on Graph Drawing", "Italy", "Library of Congress Control Number", "Mathematician", "Oulipo", "Paris", "Peter Eades", "Planar graph", "Planarity testing", "Roberto Tamassia", "Syst\u00e8me universitaire de documentation", "Virtual International Authority File", "\u00c9cole des Hautes \u00c9tudes en Sciences Sociales"], "categories": ["1933 births", "20th-century mathematicians", "21st-century mathematicians", "Academic journal editors", "All stub articles", "Combinatorialists", "French male writers", "French mathematician stubs", "French mathematicians", "Graph drawing people", "Graph theorists", "Living people", "Oulipo members", "Researchers in geometric algorithms", "Wikipedia articles with BNF identifiers", "Wikipedia articles with LCCN identifiers", "Wikipedia articles with VIAF identifiers"], "title": "Pierre Rosenstiehl"}
{"summary": "J\u00f6rg-R\u00fcdiger Wolfgang Sack (born in Duisburg, Germany) is a professor of computer science at Carleton University, where he holds the SUN\u2013NSERC chair in Applied Parallel Computing. Sack received a masters degree from the University of Bonn in 1979 and a Ph.D. in 1984 from McGill University, under the supervision of Godfried Toussaint. He is co-editor-in-chief of the journals Computational Geometry: Theory and Applications and the Journal of Spatial Information Science, co-editor of the Handbook of Computational Geometry (Elsevier, 2000, ISBN 978-0-444-82537-7), and co-editor of the proceedings of the biennial Algorithms and Data Structures Symposium (WADS). Sack's research interests include computational geometry, parallel algorithms, and geographic information systems.", "links": ["Carleton University", "Computational geometry", "Computer science", "Computer scientist", "Duisburg", "Geographic information system", "Godfried Toussaint", "Mathematics Genealogy Project", "McGill University", "Parallel algorithm", "SWAT and WADS conferences", "University of Bonn"], "categories": ["All stub articles", "Canadian computer scientists", "Canadian computer specialist stubs", "Canadian people stubs", "Carleton University faculty", "Computer scientist stubs", "German computer scientists", "Living people", "McGill University alumni", "Researchers in geometric algorithms", "University of Bonn alumni", "Year of birth missing (living people)"], "title": "J\u00f6rg-R\u00fcdiger Sack"}
{"summary": "Michael Segal (Hebrew: \u05de\u05d9\u05db\u05d0\u05dc \u05e1\u05d2\u05dc; Russian: \u041c\u0438\u0445\u0430\u0438\u043b \u0421\u0435\u0433\u0430\u043b, born 1972 in Kishinev, USSR) is a Professor of Communication Systems Engineering at Ben-Gurion University of the Negev, known for his work in ad-hoc and sensor networks.\nAfter completing his undergraduate studies at Ben-Gurion University in 1994, Segal received a Ph.D. in Mathematics and Computer Science from Ben-Gurion University in 2000 under the supervision of Klara Kedem. The topic of his PhD Dissertation was: Covering point sets and accompanying problems.\nAfter continuing his studies with David G. Kirkpatrick at University of British Columbia, and Pacific Istitute for the Mathematical Studies  he joined the faculty at Ben-Gurion University in 2000, where he also served as the head of the Communication Systems Engineering department between 2005-2010. He is known (equally with his coauthors) for being first to analyze the analytical performance of the well-known Least Cluster Change (LCC) algorithm that is widely used in ad hoc networks for re-clustering in order to reduce the number of modifications. He also was one of the first to introduce and analyze the construction of multi-criteria spanners for ad hoc networks.\nSegal has published over 140 scientific papers and was a recipient of the Toronto Prize for Research in 2010. He is serving as the Editor-in-Chief for the Journal of Computer and System Sciences. Along with his Ben-Gurion University professorship, he also is visiting professor at Cambridge University.", "links": ["Alma mater", "Ben-Gurion University of the Negev", "Cambridge University", "Chi\u0219in\u0103u", "Computer Science", "David G. Kirkpatrick", "Digital object identifier", "Doctoral advisor", "Journal of Computer and System Sciences", "Klara Kedem", "Mathematics Genealogy Project", "Soviet Union", "Thesis", "Wireless Sensor Networks", "Wireless ad hoc networks"], "categories": ["1972 births", "Israeli computer scientists", "Israeli mathematicians", "Living people", "Pages using infoboxes with thumbnail images", "Pages using web citations with no URL", "Pages with citations lacking titles", "Researchers in geometric algorithms"], "title": "Michael Segal"}
{"summary": "Raimund G. Seidel is a German and Austrian computer scientist and an expert in computational geometry.\nSeidel was born in Graz, Austria, and studied with Hermann Maurer at the Graz University of Technology. He received his Ph.D. in 1987 from Cornell University under the supervision of John Gilbert. After teaching at the University of California, Berkeley, he moved in 1994 to Saarland University. In 1997 he and Christoph M. Hoffmann were program chairs for the Symposium on Computational Geometry. In 2014, he became director of the Leibniz Center for Informatics at Schloss Dagstuhl.\nSeidel invented backwards analysis of randomized algorithms and used it to analyze a simple linear programming algorithm that runs in linear time for problems of bounded dimension. With his student Cecilia R. Aragon in 1989 he devised the treap data structure, and he is also known for the Kirkpatrick\u2013Seidel algorithm for computing two-dimensional convex hulls.", "links": ["Austria", "Cecilia R. Aragon", "Christoph M. Hoffmann", "Computational geometry", "Computer scientist", "Convex hull", "Cornell University", "Dagstuhl", "Data structure", "Digital object identifier", "Graz", "Graz University of Technology", "Hermann Maurer", "International Standard Book Number", "Kirkpatrick\u2013Seidel algorithm", "Linear programming", "Mathematics Genealogy Project", "Randomized algorithm", "SIAM Journal on Computing", "Saarland University", "Symposium on Computational Geometry", "Symposium on Foundations of Computer Science", "Treap", "University of California, Berkeley"], "categories": ["Austrian computer scientists", "Cornell University alumni", "German computer scientists", "Living people", "Researchers in geometric algorithms", "University of California, Berkeley faculty", "Year of birth missing (living people)"], "title": "Raimund Seidel"}
{"summary": "Michael Ian \"Mike\" Shamos (born April 21, 1947) is an American mathematician, attorney, book author, journal editor, consultant and company director. He is (with Franco P. Preparata) the author of Computational Geometry (Springer-Verlag, 1985), which was for many years the standard textbook in computational geometry, and is known for the Shamos\u2013Hoey sweep line algorithm for line segment intersection detection and for the rotating calipers technique for finding the width and diameter of a geometric figure. His publications also include works on electronic voting, the game of billiards, and intellectual property law in the digital age.\nHe was a fellow of Sigma Xi (1974\u201383), had an IBM Fellowship at Yale University (1974\u201375), was SIAM National Lecturer (1977\u201378), distinguished lecturer in computer science at the University of Rochester (1978), visited McGill University (1979), and belonged to the Duquesne University Law Review (1980\u201381). He won the first annual Black & White Scotch Achiever\u2019s Award in 1991 for contributions to bagpipe musicography, and the Industry Service Award of the Billiard and Bowling Institute of America, 1996, for contributions to billiard history. Since 2001 he is a Billiard Worldcup Association official referee.\nHe has been editor in chief of the Journal of Privacy Technology (2003\u20132006), a member of the editorial boards of Electronic Commerce Research Journal and the Pittsburgh Journal of Technology, Law and Policy, and a contributing editor of Billiards Digest magazine.\nShamos is the author of The New Illustrated Encyclopedia of Billiards (Lyons, 1999) among other related works, and is the curator of the Billiards Museum and Archive.\nMichael Shamos is the Director of the MS in IT eBusiness Technology program at Carnegie Mellon University.", "links": ["Author", "Biblioth\u00e8que nationale de France", "Billiards", "Company director", "Computational geometry", "Consultant", "Diameter", "Duquesne University", "Editing", "Electronic voting", "Fellow", "Franco P. Preparata", "Intellectual property", "International Standard Name Identifier", "Lawyer", "Mathematician", "McGill University", "Rotating calipers", "Sigma Xi", "Society for Industrial and Applied Mathematics", "Sweep line algorithm", "Syst\u00e8me universitaire de documentation", "United States", "University of Rochester", "Virtual International Authority File", "Yale University"], "categories": ["1947 births", "20th-century American mathematicians", "21st-century American mathematicians", "Cue sports writers and broadcasters", "Duquesne University faculty", "Living people", "Researchers in geometric algorithms", "University of Rochester faculty", "Voting theorists", "Wikipedia articles with BNF identifiers", "Wikipedia articles with ISNI identifiers", "Wikipedia articles with VIAF identifiers"], "title": "Michael Ian Shamos"}
{"summary": "Subhash Suri (born July 7, 1960) is an Indian-American computer scientist, a professor at the University of California, Santa Barbara. He is known for his research in computational geometry, computer networks, and algorithmic game theory.", "links": ["Algorithmic game theory", "American Association for the Advancement of Science", "Association for Computing Machinery", "Bellcore", "Computational geometry", "Computer network", "Digital object identifier", "George Varghese", "IEEE", "Indian Institute of Technology Roorkee", "John Hershberger", "Johns Hopkins University", "Joseph O'Rourke (professor)", "Joseph S. B. Mitchell", "Mathematical Reviews", "Symposium on Computational Geometry", "University of California, Santa Barbara", "Washington University in St. Louis"], "categories": ["1960 births", "American computer scientists", "Fellow Members of the IEEE", "Fellows of the American Association for the Advancement of Science", "Fellows of the Association for Computing Machinery", "Indian computer scientists", "Indian mathematicians", "Johns Hopkins University alumni", "Living people", "Researchers in geometric algorithms", "University of California, Santa Barbara faculty", "Washington University in St. Louis faculty"], "title": "Subhash Suri"}
{"summary": "Roberto Tamassia is a computer scientist, the Plastech Professor of Computer Science at Brown University, and since 2007 has been chair of the Brown Computer Science department. His research specialty is in the design and analysis of algorithms for graph drawing, computational geometry, and computer security; he is also the author of several textbooks.", "links": ["ACM SIGACT", "Algorithms", "American Association for the Advancement of Science", "Association for Computing Machinery", "Brown University", "Computational geometry", "Computer science", "Computer security", "Consiglio Nazionale delle Ricerche", "Digital object identifier", "Franco Preparata", "Graph drawing", "IEEE Computer Society", "IEEE Fellow", "ISI highly cited researcher", "International Symposium on Graph Drawing", "Journal of Graph Algorithms and Applications", "Michael T. Goodrich", "Peter Eades", "Sapienza University of Rome", "Science (journal)", "University of Illinois at Urbana-Champaign", "University of Texas at Dallas", "Workshop on Algorithms and Data Structures"], "categories": ["American computer scientists", "Brown University faculty", "Fellow Members of the IEEE", "Fellows of the American Association for the Advancement of Science", "Fellows of the Association for Computing Machinery", "Graph drawing people", "Italian computer scientists", "Living people", "People associated with computer security", "Researchers in geometric algorithms", "University of Illinois at Urbana\u2013Champaign alumni", "Year of birth missing (living people)"], "title": "Roberto Tamassia"}
{"summary": "Shang-Hua Teng (Chinese: \u6ed5\u5c1a\u534e; pinyin: T\u00e9ng Sh\u00e0ng-hu\u00e1, born 1964) is a Chinese-American computer scientist. He is the Seeley G. Mudd Professor of Computer Science and Mathematics at the University of Southern California. Previously, he was the chairman of the Computer Science Department at the Viterbi School of Engineering of the University of Southern California. In 2008 he was awarded the G\u00f6del Prize for his joint work on smoothed analysis of algorithms with Daniel Spielman. They went to win the prize again in 2015 for their contribution on \"nearly-linear-time Laplacian solvers\". In 2009, he received the Fulkerson Prize given by the American Mathematical Society and the Mathematical Programming Society.", "links": ["ACM SIGACT", "Akamai Technologies", "Alexander Razborov", "Algorithm", "Alistair Sinclair", "Alma mater", "American Mathematical Society", "Amir Ronen", "Amnon Lotem", "Antoine Joux", "Association for Computing Machinery", "Avi Wigderson", "Beijing", "Boston University", "Carnegie Mellon", "Carnegie Mellon University", "Carsten Lund", "Charles Rackoff", "China", "Chinese language", "Chinese name", "Chinese surname", "Christos Papadimitriou", "Computer Science", "Computer science", "Dan Boneh", "Daniel Spielman", "Doctoral advisor", "Electrical engineering", "Elias Koutsoupias", "Fotios Zaharoglou", "Fulkerson Prize", "Gary Miller (professor)", "G\u00e9raud S\u00e9nizergues", "G\u00f6del Prize", "IBM", "Integrated Authority File", "Intel Corporation", "Johan H\u00e5stad", "Joseph Halpern", "Joseph S. B. Mitchell", "Library of Congress Control Number", "L\u00e1szl\u00f3 Babai", "L\u00e1szl\u00f3 Lov\u00e1sz", "Madhu Sudan", "Manindra Agrawal", "Mario Szegedy", "Mark Jerrum", "Massachusetts Institute of Technology", "Mathematical Programming Society", "Mathematics Genealogy Project", "Matthew K. Franklin", "Maurice Herlihy", "Michael Saks (mathematician)", "Microsoft Research", "Microsoft Research Asia", "Microsoft Research New England", "Moni Naor", "Moshe Y. Vardi", "NASA Ames Research Center", "Neeraj Kayal", "Neil Immerman", "Nir Shavit", "Nitin Saxena", "Noam Nisan", "Noga Alon", "Omer Reingold", "PARC (company)", "Peter Shor", "Pierre Wolper", "Pinyin", "Rajeev Motwani", "Robert Schapire", "Ronald Fagin", "R\u00f3bert Szelepcs\u00e9nyi", "Salil Vadhan", "Sanjeev Arora", "Seinosuke Toda", "Shafi Goldwasser", "Shanghai Jiao Tong University", "Shlomo Moran", "Shmuel Safra", "Silvio Micali", "Sloan Fellowship", "Smoothed analysis", "Steven Rudich", "Thesis", "Tim Roughgarden", "United States", "United States of America", "University of Illinois at Urbana-Champaign", "University of Minnesota", "University of Southern California", "Uriel Feige", "Virtual International Authority File", "Viterbi School of Engineering", "Yoav Freund", "Yoram Moses", "Yossi Matias", "\u00c9va Tardos"], "categories": ["1964 births", "All articles with dead external links", "All stub articles", "American computer scientists", "Articles containing Chinese-language text", "Articles with dead external links from October 2010", "Boston University faculty", "Carnegie Mellon University alumni", "Chinese computer scientists", "Chinese emigrants to the United States", "Computer specialist stubs", "Educators from Beijing", "Fellows of the Association for Computing Machinery", "G\u00f6del Prize laureates", "IBM employees", "Intel people", "Living people", "Massachusetts Institute of Technology faculty", "Microsoft people", "Researchers in geometric algorithms", "Shanghai Jiao Tong University alumni", "Sloan Fellows", "USC Viterbi School of Engineering alumni", "University of Illinois at Urbana\u2013Champaign faculty", "University of Minnesota faculty", "University of Southern California faculty", "Wikipedia articles with GND identifiers", "Wikipedia articles with LCCN identifiers", "Wikipedia articles with VIAF identifiers", "Xerox people"], "title": "Shang-Hua Teng"}
{"summary": "Godfried T. Toussaint is a Professor of Computer Science and the Head of the Computer Science Program at New York University Abu Dhabi (NYUAD) in Abu Dhabi, United Arab Emirates. He does research on various aspects of computational geometry, discrete geometry, and their applications: pattern recognition (k-nearest neighbor algorithm, cluster analysis), motion planning, visualization (computer graphics), knot theory (stuck unknot problem), linkage (mechanical) reconfiguration, the art gallery problem, polygon triangulation, the largest empty circle problem, unimodality (unimodal function), and others. Other interests include meander (art), compass and straightedge constructions, instance-based learning, music information retrieval, and computational music theory.\nHe is a co-founder of the Annual ACM Symposium on Computational Geometry, and the Annual Canadian Conference on Computational Geometry.\nAlong with Selim Akl, he is an author and namesake of the efficient \"Akl\u2013Toussaint algorithm\" for the construction of the convex hull of a planar point set. This algorithm exhibits a computational complexity with expected value linear in the size of the input. In 1980 he introduced the relative neighborhood graph (RNG) to the fields of pattern recognition and machine learning, and showed that it contained the minimum spanning tree, and was a subgraph of the Delaunay triangulation. Three other well known proximity graphs are the nearest neighbor graph, the Urquhart graph, and the Gabriel graph. The first is contained in the minimum spanning tree, and the Urquhart graph contains the RNG, and is contained in the Delaunay triangulation. Since all these graphs are nested together they are referred to as the Toussaint hierarchy.", "links": ["Abu Dhabi", "Analysis of algorithms", "Art gallery problem", "Canada Council for the Arts", "Canadian Image Processing and Pattern Recognition Society", "Cluster analysis", "Compass and straightedge constructions", "Computational geometry", "Convex hull", "Convex hull algorithms", "Delaunay triangulation", "Discrete geometry", "Discrete mathematics", "Eric Demaine", "Euclidean algorithm", "Expected value", "Gabriel graph", "Greatest common divisor", "Harvard University", "Instance-based learning", "International Standard Name Identifier", "Izaak Walton Killam", "Jacob E. Goodman", "K-nearest neighbor algorithm", "Kenneth Millett", "Knot theory", "Largest empty circle", "Linkage (mechanical)", "Machine learning", "McGill University", "Meander (art)", "Minimum spanning tree", "Motion planning", "Music cognition", "Music information retrieval", "Music theory", "Musical similarity", "Nearest neighbor graph", "New York University Abu Dhabi", "Pattern Recognition Society", "Pattern recognition", "Phylogenetics", "Polygon triangulation", "Radcliffe Institute for Advanced Study", "Relative neighborhood graph", "Rhythm", "Schulich School of Music", "Selim Akl", "Stuck unknot", "Symposium on Computational Geometry", "Syst\u00e8me universitaire de documentation", "Toussaint hierarchy", "Unimodal function", "University of Newcastle, Australia", "Urquhart graph", "Virtual International Authority File", "Visualization (computer graphics)"], "categories": ["Canadian computer scientists", "Living people", "McGill University faculty", "New York University Abu Dhabi faculty", "Researchers in geometric algorithms", "Wikipedia articles with ISNI identifiers", "Wikipedia articles with VIAF identifiers", "Wikipedia articles with possible conflicts of interest from August 2010"], "title": "Godfried Toussaint"}
{"summary": "Frances Foong Chu Yao (\u5132\u6953) is a Chinese-born American mathematician and computer scientist. She was Chair Professor and Head of the Department of computer science at the City University of Hong Kong, where she is now an honorary professor.\nAfter receiving a B.S. in mathematics from National Taiwan University in 1969, Yao did her Ph.D. studies under the supervision of Michael J. Fischer at the Massachusetts Institute of Technology, receiving her Ph.D. in 1973. She then held positions at the University of Illinois at Urbana-Champaign, Brown University, and Stanford University, before joining the staff at the Xerox Palo Alto Research Center in 1979 where she stayed until her retirement in 1999.\nIn 2003, she came out of retirement to become the Head and a Chair Professor of the Department of Computer Science at City University, which she held until June 2011. She is a Fellow of the American Association for the Advancement of Science; in 1991, she and Ronald Graham won the Lester R. Ford Award of the Mathematical Association of America for their expository article, A Whirlwind Tour of Computational Geometry.\nYao's husband, Andrew Yao, is also a well-known theoretical computer scientist and Turing Award winner.\nMuch of Yao's research has been in the subject of computational geometry and combinatorial algorithms; she is known for her work with Mike Paterson on binary space partitioning, her work with Dan Greene on finite-resolution computational geometry, and her work with Alan Demers and Scott Shenker on scheduling algorithms for energy-efficient power management.\nMore recently she has been working in cryptography. Along with her husband Andrew Yao and Wang Xiaoyun, they found new attacks on the SHA-1 cryptographic hash function.", "links": ["American Association for the Advancement of Science", "Andrew Yao", "Binary space partitioning", "Brown University", "City University of Hong Kong", "Computational geometry", "Computer science", "Conference on Computer Communications", "Cryptography", "Digital object identifier", "Discrete and Computational Geometry", "Fan Chung", "Fellow", "International Standard Book Number", "L. R. Ford", "Massachusetts Institute of Technology", "Mathematical Association of America", "Mathematical Reviews", "Mathematics Genealogy Project", "Michael J. Fischer", "Mike Paterson", "National Taiwan University", "New Scientist", "Paul Erd\u0151s", "Power management", "Ronald Graham", "SHA-1", "Scheduling algorithm", "Scott Shenker", "Stanford University", "Stanislaw Ulam", "Symposium on Foundations of Computer Science", "Symposium on Theory of Computing", "The Register", "Turing Award", "University of Illinois at Urbana-Champaign", "Wang Xiaoyun", "Xerox Palo Alto Research Center"], "categories": ["American computer scientists", "Brown University faculty", "Chinese emigrants to the United States", "Faculty of the City University of Hong Kong", "Fellows of the American Association for the Advancement of Science", "Living people", "Massachusetts Institute of Technology alumni", "National Taiwan University alumni", "Researchers in geometric algorithms", "Stanford University Department of Computer Science faculty", "University of Illinois at Urbana\u2013Champaign faculty", "Women computer scientists"], "title": "Frances Yao"}
{"summary": "HeuristicLab   is a software environment for heuristic and evolutionary algorithms, developed by members of the Heuristic and Evolutionary Algorithm Laboratory (HEAL) at the University of Applied Sciences Upper Austria, Campus Hagenberg.\nHeuristicLab has a strong focus on providing a graphical user interface so that users are not required to have comprehensive programming skills to adjust and extend the algorithms for a particular problem. In HeuristicLab algorithms are represented as operator graphs and changing or rearranging operators can be done by drag-and-drop without actually writing code. The software thereby tries to shift algorithm development capability from the software engineer to the user and practitioner. Developers can still extend the functionality on code level and can use HeuristicLab's plug-in mechanism that allows them to integrate custom algorithms, solution representations or optimization problems.\n^ Wagner, Stefan; Kronberger G.; Beham A.; Kommenda M.; Scheibenpflug A.; Pitzer E.; Vonolfen S.; Kofler M.; Winkler S.; Dorfer V.; Affenzeller M. (2014). \"Architecture and Design of the HeuristicLab Optimization Environment\". Topics in Intelligent Engineering and Informatics 6: 197\u2013261. doi:10.1007/978-3-319-01436-4_10. \n^ Wagner, Stefan (2009). Heuristic Optimization Software Systems - Modeling of Heuristic Optimization Algorithms in the HeuristicLab Software Environment, PhD Thesis. Johannes Kepler University Linz.", "links": [".NET Framework", "Boinc", "CMA-ES", "C Sharp (programming language)", "Cross-validation (statistics)", "Digital object identifier", "Ensemble Modeling", "Evolution strategy", "Evolutionary algorithm", "GNU General Public License", "Gaussian process", "Genetic Algorithm", "Genetic Programming", "Grammatical Evolution", "Java Evolutionary Computation Toolkit", "Job shop scheduling", "K means", "Knapsack problem", "Linear Discriminant Analysis", "Linear regression", "List of software categories", "Local search (optimization)", "MATLAB", "Master/slave (technology)", "Metaheuristic", "Microsoft Windows", "Multi-objective optimization", "Multinomial logistic regression", "Nearest Neighbor Regression", "Open-source software", "Operating system", "Paradiseo", "Particle swarm optimization", "Quadratic assignment problem", "Random forest", "Relevant Alleles Preserving Genetic Algorithm", "Scilab", "Simulated annealing", "Software developer", "Software license", "Software release life cycle", "Support vector machine", "Symbolic Regression", "Tabu search", "Travelling salesman problem", "Vehicle routing problem"], "categories": ["Heuristic algorithms"], "title": "HeuristicLab"}
{"summary": "In computational engineering, Luus\u2013Jaakola (LJ) denotes a heuristic for global optimization of a real-valued function. In engineering use, LJ is not an algorithm that terminates with an optimal solution; nor is it an iterative method that generates a sequence of points that converges to an optimal solution (when one exists). However, when applied to a twice continuously differentiable function, the LJ heuristic is a proper iterative method, that generates a sequence that has a convergent subsequence; for this class of problems, Newton's method is recommended and enjoys a quadratic rate of convergence, while no convergence rate analysis has been given for the LJ heuristic. In practice, the LJ heuristic has been recommended for functions that need be neither convex nor differentiable nor locally Lipschitz: The LJ heuristic does not use a gradient or subgradient when one be available, which allows its application to non-differentiable and non-convex problems.\nProposed by Luus and Jaakola, LJ generates a sequence of iterates. The next iterate is selected from a sample from a neighborhood of the current position using a uniform distribution. With each iteration, the neighborhood decreases, which forces a subsequence of iterates to converge to a cluster point.\nLuus has applied LJ in optimal control, transformer design, metallurgical processes, and chemical engineering.", "links": ["Algorithm", "Banach space", "Chemical engineering", "Combinatorial optimization", "Computational engineering", "Constraint satisfaction", "Convex function", "Convex programming", "Differentiable", "Digital object identifier", "Global optimization", "Gradient", "Heuristic algorithm", "Hypersphere", "Infinite-dimensional optimization", "Integer programming", "International Standard Book Number", "Iterative method", "Kantorovich", "Lipschitz continuity", "Mathematical Reviews", "Metaheuristic", "Metallurgy", "Multiobjective optimization", "Newton's method", "Nonlinear programming", "Optimal control", "Optimization (mathematics)", "Pattern search (optimization)", "Quadratic convergence", "Quadratic programming", "Random optimization", "Random search", "Response surface methodology", "Robust optimization", "Stochastic programming", "Subgradient", "Transformer", "Uniform distribution (continuous)", "Unimodal function", "Unit sphere"], "categories": ["Heuristic algorithms", "Optimization algorithms and methods"], "title": "Luus\u2013Jaakola"}
{"summary": "In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm of making decisions in some decision processes, most notably employed in game playing. The leading example of its use is in contemporary computer Go programs, but it is also used in other board games, as well as real-time video games and non-deterministic games such as poker (see history section).", "links": ["Alpha\u2013beta pruning", "Anytime algorithm", "Arimaa", "Board game", "Branching factor", "Chess", "Computer Go", "Computer science", "Dan (rank)", "Decision process", "Digital object identifier", "EinStein w\u00fcrfelt nicht!", "Evaluation function", "Fable Legends", "Game of the Amazons", "General game playing", "Havannah", "Heuristic (computer science)", "Hex (board game)", "International Standard Book Number", "Magic: The Gathering", "Minimax", "Monte Carlo method", "Ms. Pac-Man", "Mutex", "Non-blocking algorithm", "Parallel computing", "Poker", "Process (computing)", "Reversi", "Search algorithm", "Search tree", "Settlers of Catan", "Skat (card game)", "Thread (computing)", "Tic-tac-toe", "Total War: Rome II"], "categories": ["Combinatorial game theory", "Heuristic algorithms", "Monte Carlo methods", "Wikipedia articles needing clarification from November 2015"], "title": "Monte Carlo tree search"}
{"summary": "Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic for approximate global optimization in a large search space. It is often used when the search space is discrete (e.g., all tours that visit a given set of cities). For problems where finding the precise global optimum is less important than finding an acceptable global optimum in a fixed amount of time, simulated annealing may be preferable to alternatives such as brute-force search or gradient descent.\nThe name and inspiration come from annealing in metallurgy, a technique involving heating and controlled cooling of a material to increase the size of its crystals and reduce defects. Both are attributes of the material that depend on its thermodynamic free energy. Heating and cooling the material affects both the temperature and the thermodynamic free energy. While the same amount of cooling brings the same decrease in temperature, the rate of cooling dictates the magnitude of decrease in the thermodynamic free energy, with slower cooling producing a bigger decrease. Simulated annealing interprets slow cooling as a slow decrease in the probability of accepting worse solutions as it explores the solution space. Accepting worse solutions is a fundamental property of metaheuristics because it allows for a more extensive search for the optimal solution.\nThe method was independently described by Scott Kirkpatrick, C. Daniel Gelatt and Mario P. Vecchi in 1983, and by Vlado \u010cern\u00fd in 1985. The method is an adaptation of the Metropolis\u2013Hastings algorithm, a Monte Carlo method to generate sample states of a thermodynamic system, invented by M.N. Rosenbluth and published by N. Metropolis et al. in 1953.", "links": ["Adaptive simulated annealing", "Amorphous solid", "Annealing (metallurgy)", "Ant colony optimization", "Automatic label placement", "Bibcode", "Brute-force search", "Colour", "Combinatorial optimization", "Constraint satisfaction", "Convex programming", "Cross-entropy method", "Crystal", "Crystalline solid", "Crystallographic defect", "Diameter (graph theory)", "Digital object identifier", "Drainage basin", "Dual-phase evolution", "Factorial", "Function (mathematics)", "Genetic algorithms", "Global optimization", "Global optimum", "Gradient descent", "Graduated optimization", "Graph cuts in computer vision", "Graph theory", "Greedy algorithm", "Hamiltonian (quantum mechanics)", "Harmony search", "Heuristic", "Hill climbing", "Hill climbing algorithm", "Infinite-dimensional optimization", "Integer programming", "Intelligent Water Drops", "Internal energy", "International Standard Book Number", "JSTOR", "Local optimum", "Markov chain", "Marshall Rosenbluth", "Mathematical optimization", "Metaheuristic", "Metropolis-Hastings algorithm", "Metropolis\u2013Hastings algorithm", "Molecular dynamics", "Monte Carlo method", "Multidisciplinary optimization", "Multiobjective optimization", "Nicholas Metropolis", "Nonlinear programming", "Optimization (mathematics)", "Parallel tempering", "Particle filter", "Particle swarm optimization", "Permutation", "Physical system", "Pixel", "Place and route", "Potential energy", "Probabilistic", "Probabilistic algorithm", "Procedural parameter", "Pseudocode", "PubMed Identifier", "Quadratic programming", "Quantum annealing", "Quintillion", "Reactive search optimization", "Robust optimization", "Solution space", "State transition", "Steepest descent", "Stochastic gradient descent", "Stochastic optimization", "Stochastic programming", "Stochastic tunneling", "Tabu search", "Thermodynamic equilibrium", "Thermodynamic free energy", "Thermodynamic state", "Traveling salesman problem", "Uniform distribution (continuous)"], "categories": ["All articles needing additional references", "All articles with unsourced statements", "Articles needing additional references from December 2009", "Articles with inconsistent citation formats", "Articles with unsourced statements from June 2011", "Heuristic algorithms", "Monte Carlo methods", "Optimization algorithms and methods"], "title": "Simulated annealing"}
{"summary": "Social cognitive optimization (SCO) is a population-based metaheuristic optimization algorithm which was developed in 2002. This algorithm is based on the social cognitive theory, and the key point of the ergodicity is the process of individual learning of a set of agents with their own memory and their social learning with the knowledge points in the social sharing library. It has been used for solving continuous optimization, integer programming, and combinatorial optimization problems. It has been incorporated into the NLPSolver extension of Calc in Apache OpenOffice.", "links": ["Apache OpenOffice", "Combinatorial optimization", "Continuous optimization", "Integer programming", "Learning", "Markov chain", "Memory", "Metaheuristic", "Optimization", "Particle swarm optimization", "Premature convergence", "Social cognitive theory", "Social learning", "Tournament selection"], "categories": ["Collective intelligence", "Heuristic algorithms", "Metaheuristics", "Optimization algorithms and methods"], "title": "Social cognitive optimization"}
{"summary": "Almeida\u2013Pineda recurrent backpropagation is an extension to the backpropagation algorithm that is applicable to recurrent neural networks. It is a type of supervised learning.\nA recurrent neural network for this algorithm consists of some input units, some output units and eventually some hidden units.\nFor a given set of (input, target) states, the network is trained to settle into a stable activation state with the output units in the target state, based on a given input state clamped on the input units.", "links": ["Algorithm", "Backpropagation", "Neuroscience", "Recurrent neural networks", "Supervised learning"], "categories": ["All stub articles", "Machine learning algorithms", "Neuroscience", "Neuroscience stubs"], "title": "Almeida\u2013Pineda recurrent backpropagation"}
{"summary": "The CN2 induction algorithm is a learning algorithm for rule induction. It is designed to work even when the training data is imperfect. It is based on ideas from the AQ algorithm and the ID3 algorithm. As a consequence it creates a rule set like that created by AQ but is able to handle noisy data like ID3.", "links": ["AQ algorithm", "Algorithmic learning theory", "Artificial intelligence", "ID3 algorithm", "Rule induction"], "categories": ["All articles needing additional references", "All stub articles", "Articles needing additional references from September 2012", "Artificial intelligence stubs", "Machine learning algorithms"], "title": "CN2 algorithm"}
{"summary": "Constructing skill trees (CST) is a hierarchical reinforcement learning algorithm which can build skill trees from a set of sample solution trajectories obtained from demonstration. CST uses an incremental MAP(maximum a posteriori ) change point detection algorithm to segment each demonstration trajectory into skills and integrate the results into a skill tree. CST was introduced by George Konidaris, Scott Kuindersma, Andrew Barto and Roderic Grupen in 2010.", "links": ["Andrew Barto", "George Konidaris", "Maximum a posteriori", "Particle filter", "Paul Fearnhead", "PinBall", "Pseudocode", "Reinforcement learning", "Roderic Grupen", "Scott Kuindersma", "Skill chaining", "Viterbi algorithm"], "categories": ["All articles needing additional references", "Articles needing additional references from January 2012", "CS1 errors: missing author or editor", "Machine learning algorithms"], "title": "Constructing skill trees"}
{"summary": "Diffusion maps is a dimensionality reduction or feature extraction algorithm introduced by R. R. Coifman and S. Lafon. It computes a family of embeddings of a data set into Euclidean space (often low-dimensional) whose coordinates can be computed from the eigenvectors and eigenvalues of a diffusion operator on the data. The Euclidean distance between points in the embedded space is equal to the \"diffusion distance\" between probability distributions centered at those points. Different from linear dimensionality reduction methods such as principal component analysis (PCA) and multi-dimensional scaling (MDS), diffusion maps is part of the family of nonlinear dimensionality reduction methods which focus on discovering the underlying manifold that the data has been sampled from. By integrating local similarities at different scales, diffusion maps gives a global description of the data-set. Compared with other methods, the diffusion maps algorithm is robust to noise perturbation and is computationally inexpensive.", "links": ["Digital object identifier", "Dimensionality reduction", "Feature extraction", "Fokker-Planck equation", "Heat diffusion", "Integral kernel", "Laplace-Beltrami operator", "Manifold", "Markov chain", "Multi-dimensional scaling", "Nonlinear dimensionality reduction", "Principal component analysis", "Ronald Coifman"], "categories": ["Machine learning algorithms", "Pages containing cite templates with deprecated parameters"], "title": "Diffusion map"}
{"summary": "In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences which may vary in time or speed. For instance, similarities in walking patterns could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data \u2014 indeed, any data which can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. Also it is seen that it can be used in partial shape matching application.\nIn general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restrictions. The sequences are \"warped\" non-linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. This sequence alignment method is often used in time series classification. Although DTW measures a distance-like quantity between two given sequences, it doesn't guarantee the triangle inequality to hold.\n\n", "links": ["Acceleration", "Algorithm", "Digital object identifier", "Elastic matching", "Functional data analysis", "International Standard Book Number", "International Standard Serial Number", "Levenshtein distance", "Multiple alignment", "Nearest neighbour classifiers", "Non-linear", "Optimal matching", "PubMed Central", "PubMed Identifier", "Sequence alignment", "Shape analysis (digital geometry)", "Signature recognition", "Speaker recognition", "Speech recognition", "Time series", "Time series analysis", "Triangle inequality"], "categories": ["Articles with example pseudocode", "CS1 maint: Explicit use of et al.", "Dynamic programming", "Machine learning algorithms", "Time series analysis"], "title": "Dynamic time warping"}
{"summary": "FastICA is an efficient and popular algorithm for independent component analysis invented by Aapo Hyv\u00e4rinen at Helsinki University of Technology. Like most ICA algorithms, FastICA seeks an orthogonal rotation of prewhitened data, through a fixed-point iteration scheme, that maximizes a measure of non-Gaussianity of the rotated components. Non-gaussianity serves as a proxy for statistical independence, which is a very strong condition and requires infinite data to verify. FastICA can also be alternatively derived as an approximative Newton iteration.", "links": ["C++", "Covariance matrix", "Digital object identifier", "Eigenvalue decomposition", "Expected value", "FastICA", "Function", "Helsinki University of Technology", "IT++", "Independence (probability theory)", "Independent component analysis", "Iterative method", "Linear transformation", "Machine learning", "Non-Gaussianity", "Nonlinearity", "PubMed Identifier", "R programming language", "RapidMiner", "SourceForge", "Statistical independence", "Unsupervised learning"], "categories": ["All articles needing additional references", "Articles needing additional references from April 2013", "Computational statistics", "Machine learning algorithms", "Multivariate statistics"], "title": "FastICA"}
{"summary": "GeneRec is a generalization of the Recirculation algorithm, and approximates Almeida-Pineda recurrent backpropagation. It is used as part of the Leabra algorithm for error-driven learning.\nThe symmetric, midpoint version of GeneRec is equivalent to the contrastive Hebbian learning algorithm (CHL).", "links": ["Almeida-Pineda recurrent backpropagation", "Contrastive Hebbian learning", "Error-driven learning", "Leabra", "Neuroscience", "Recirculation algorithm"], "categories": ["All stub articles", "Machine learning algorithms", "Neuroscience", "Neuroscience stubs"], "title": "GeneRec"}
{"summary": "Genetic Algorithm for Rule Set Production (GARP) is a computer program based on genetic algorithm that creates ecological niche models for species. The generated models describe environmental conditions (precipitation, temperatures, elevation, etc.) under which the species should be able to maintain populations. As input, local observations of species and related environmental parameters are used which describe potential limits of the species' capabilities to survive. Such environmental parameters are commonly stored in geographical information systems. A GARP model is a random set of mathematical rules which can be read as limiting environmental conditions. Each rule is considered as a gene; the set of genes is combined in random ways to further generate many possible models describing the potential of the species to occur.", "links": ["Ecology", "Environmental niche modelling", "Genetic algorithm", "Geographical information systems", "Lifemapper", "OpenModeller"], "categories": ["All stub articles", "Ecology stubs", "Machine learning algorithms"], "title": "Genetic Algorithm for Rule Set Production"}
{"summary": "HEXQ is a reinforcement learning algorithm created by Bernhard Hengst, which attempts to solve a Markov Decision Process by decomposing it hierarchically.\nBernhard Hengst (2002). \"Discovering Hierarchy in Reinforcement Learning with HEXQ\".", "links": ["Algorithm", "Computer science", "Hierarchically", "Markov Decision Process", "Reinforcement learning"], "categories": ["All orphaned articles", "All stub articles", "Computer science stubs", "Machine learning algorithms", "Orphaned articles from February 2009"], "title": "HEXQ"}
{"summary": "Kernel methods are a well-established tool to analyze the relationship between input data and the corresponding output of a function. Kernels encapsulate the properties of functions in a computationally efficient way and allow algorithms to easily swap functions of varying complexity.\nIn typical machine learning algorithms, these functions produce a scalar output. Recent development of kernel methods for functions with vector-valued output is due, at least in part, to interest in simultaneously solving related problems. Kernels which capture the relationship between the problems allow them to borrow strength from each other. Algorithms of this type include multi-task learning (also called multi-output learning or vector-valued learning), transfer learning, and co-kriging. Multi-label classification can be interpreted as mapping inputs to (binary) coding vectors with length equal to the number of classes.\nIn Gaussian processes, kernels are called covariance functions. Multiple-output functions correspond to considering multiple processes. See Bayesian interpretation of regularization for the connection between the two perspectives.", "links": ["Bayesian interpretation of regularization", "Block matrix", "Covariance function", "Cross-validation (statistics)", "Curl (mathematics)", "Divergence", "Eigen decomposition", "Gaussian process", "Graph kernel", "Inductive transfer", "Isometry", "Kernel methods", "Kernel trick", "Kriging", "Laplacian matrix", "Machine learning", "Multi-label classification", "Multi-task learning", "Regularization (mathematics)", "Representer theorem", "Reproducing kernel Hilbert space", "Tikhonov regularization", "Transformation (function)"], "categories": ["Kernel methods for machine learning", "Machine learning algorithms"], "title": "Kernel methods for vector output"}
{"summary": "Leabra stands for \"Local, Error-driven and Associative, Biologically Realistic Algorithm\". It is a model of learning which is a balance between Hebbian and error-driven learning with other network-derived characteristics. This model is used to mathematically predict outcomes based on inputs and previous learning influences. This model is heavily influenced by and contributes to neural network designs and models. This algorithm is the default algorithm in Emergent (successor of PDP++) when making a new project, and is extensively used in various simulations.\nHebbian learning is performed using conditional principal components analysis (CPCA) algorithm with correction factor for sparse expected activity levels.\nError-driven learning is performed using GeneRec, which is a generalization of the Recirculation algorithm, and approximates Almeida-Pineda recurrent backpropagation. The symmetric, midpoint version of GeneRec is used, which is equivalent to the contrastive Hebbian learning algorithm (CHL). See O'Reilly (1996; Neural Computation) for more details.\nThe activation function is a point-neuron approximation with both discrete spiking and continuous rate-code output.\nLayer or unit-group level inhibition can be computed directly using a k-winners-take-all (KWTA) function, producing sparse distributed representations.\nThe net input is computed as an average, not a sum, over connections, based on normalized, sigmoidally transformed weight values, which are subject to scaling on a connection-group level to alter relative contributions. Automatic scaling is performed to compensate for differences in expected activity level in the different projections.\nDocumentation about this algorithm can be found in the book \"Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain\" published by MIT press. and in the Emergent Documentation", "links": ["Almeida-Pineda recurrent backpropagation", "Basal ganglia", "Classical conditioning", "Computer simulation", "Conditional principal components analysis", "Contrastive Hebbian learning", "Dopaminergic", "Emergent (software)", "Error-driven learning", "GeneRec", "Hebbian learning", "International Standard Book Number", "Learning", "Midbrain", "Neural network", "Neurons", "PBWM", "PVLV", "Prefrontal cortex", "Recirculation algorithm", "Spiking neural network", "Temporal difference learning", "Winner-take-all (computing)", "Working memory"], "categories": ["Artificial neural networks", "Machine learning algorithms"], "title": "Leabra"}
{"summary": "In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and J\u00f6rg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.\nLOF shares some concepts with DBSCAN and OPTICS such as the concepts of \"core distance\" and \"reachability distance\", which are used for local density estimation.", "links": ["Anomaly detection", "DBSCAN", "Digital object identifier", "Distance", "Ensemble learning", "Environment for DeveLoping KDD-Applications Supported by Index-Structures", "Hans-Peter Kriegel", "International Standard Book Number", "Network intrusion detection system", "OPTICS algorithm", "Outlier", "Quotient", "SIGMOD", "Usability"], "categories": ["Data mining", "Machine learning algorithms", "Statistical outliers"], "title": "Local outlier factor"}
{"summary": "In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems. Given  as the vector space of all possible inputs, and Y = {\u20131,1} as the vector space of all possible outputs, we wish to find a function  which best maps  to . However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same  to generate different . As a result, the goal of the learning problem is to minimize expected risk, defined as\n\nwhere  represents the loss function, and  represents the probability distribution of the data, which can equivalently be written using Bayes' theorem as\n\nIn practice, the probability distribution  is unknown. Consequently, utilizing a training set of  independently and identically distributed samples\n\ndrawn from the data sample space, one seeks to minimize empirical risk\n\nas a proxy for expected risk. (See statistical learning theory for a more detailed description.)\nFor computational ease, it is standard practice to write loss functions as functions of only one variable. Within classification, loss functions are generally written solely in terms of the product of the true classifier  and the predicted value . Selection of a loss function within this framework\n\nimpacts the optimal  which minimizes empirical risk, as well as the computational complexity of the learning algorithm.\nGiven the binary nature of classification, a natural selection for a loss function (assuming equal cost for false positives and false negatives) would be the 0\u20131 indicator function which takes the value of 0 if the predicted classification equals that of the true class or a 1 if the predicted classification does not match the true class. This selection is modeled by\n\nwhere  indicates the Heaviside step function. However, this loss function is non-convex and non-smooth, and solving for the optimal solution is an NP-hard combinatorial optimization problem. As a result, it is better to substitute continuous, convex loss function surrogates which are tractable for commonly used learning algorithms. In addition to their computational tractability, one can show that the solutions to the learning problem using these loss surrogates allows for the recovery of the actual solution to the original classification problem. Some of these surrogates are described below.", "links": ["Bayes' theorem", "Cross-validation (statistics)", "Cross entropy", "Deep learning", "Digital object identifier", "Empirical risk minimization", "False positives and false negatives", "Gradient descent", "Heaviside step function", "Hinge loss", "Iid", "Indicator function", "Kullback-Leibler divergence", "Loss function", "Loss functions", "Machine learning", "Mathematical optimization", "NP-hard", "PubMed Identifier", "Quadratic programming", "Sample space", "Sign function", "Statistical learning theory", "Stochastic gradient descent", "Subgradient method", "Support vector machine", "Support vector machines", "Tikhonov regularization"], "categories": ["Machine learning algorithms"], "title": "Loss functions for classification"}
{"summary": "Manifold alignment is a class of machine learning algorithms that produce projections between sets of data, given that the original data sets lie on a common manifold. The concept was first introduced as such by Ham, Lee, and Saul in 2003, adding a manifold constraint to the general problem of correlating sets of high-dimensional vectors.", "links": ["Adjacency matrix", "CiteSeer", "European Union", "Generalized eigenvalue problem", "Heat kernel", "International Standard Book Number", "Laplacian matrix", "Loss function", "Machine learning", "Manifold", "Manifold learning", "Multiple-instance learning", "Nearest neighbor graph", "Neural Computation (journal)", "Nonlinear dimensionality reduction", "Nuclear magnetic resonance spectroscopy of proteins", "Semi-supervised learning", "Supervised learning", "Transfer learning", "Unsupervised learning"], "categories": ["Artificial intelligence", "Machine learning algorithms"], "title": "Manifold alignment"}
{"summary": "Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of genes and phenotypes and narrow down their relevance and is usually described in its pairing with relevant feature selection as Minimum Redundancy Maximum Relevance (mRMR).\nFeature selection, one of the basic problems in pattern recognition and machine learning, identifies subsets of data that are relevant to the parameters used and is normally called Maximum Relevance. These subsets often contain material which is relevant but redundant and mRMR attempts to address this problem by removing those redundant subsets. mRMR has a variety of applications in many areas such as cancer diagnosis and speech recognition.\nFeatures can be selected in many different ways. One scheme is to select features that correlate strongest to the classification variable. This has been called maximum-relevance selection. Many heuristic algorithms can be used, such as the sequential forward, backward, or floating selections.\nOn the other hand features can be selected to be mutually far away from each other while still having \"high\" correlation to the classification variable. This scheme, termed as Minimum Redundancy Maximum Relevance (mRMR) selection has been found to be more powerful than the maximum relevance selection.\nAs a special case, the \"correlation\" can be replaced by the statistical dependency between variables. Mutual information can be used to quantify the dependency. In this case, it is shown that mRMR is an approximation to maximizing the dependency between the joint distribution of the selected features and the classification variable.\nStudies have tried different measures for redundancy and relevance measures. A recent study compared several measures within the context of biomedical images.", "links": ["Artificial intelligence", "Feature selection", "Gene", "Phenotype"], "categories": ["All stub articles", "Artificial intelligence stubs", "Machine learning algorithms", "Robotics stubs"], "title": "Minimum redundancy feature selection"}
{"summary": "Multiple kernel learning refers to a set of machine learning methods that use a predefined set of kernels and learn an optimal linear or non-linear combination of kernels as part of the algorithm. Reasons to use multiple kernel learning include a) the ability to select for an optimal kernel and parameters from a larger set of kernels, reducing bias due to kernel selection while allowing for more automated machine learning methods, and b) combining data from different sources (e.g. sound and images from a video) that have different notions of similarity and thus require different kernels. Instead of creating a new kernel, multiple kernel algorithms can be used to combine kernels already established for each individual data source.\nMultiple kernel learning approaches have been used in many applications, such as event recognition in video.., object recognition in images, and biomedical data fusion.", "links": ["Anomaly detection", "Artificial neural network", "Association rule learning", "Autoencoder", "BIRCH", "Bayesian network", "Bias-variance dilemma", "Boosting (machine learning)", "Bootstrap aggregating", "Canonical correlation analysis", "Cluster analysis", "Computational learning theory", "Conditional random field", "Convolutional neural network", "DBSCAN", "Data mining", "Decision tree learning", "Deep learning", "Dimensionality reduction", "Elastic net regularization", "Empirical risk minimization", "Ensemble learning", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Gibbs sampling", "Grammar induction", "Graphical model", "Hidden Markov model", "Hierarchical clustering", "Independent component analysis", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Kernel method", "Kullback-Leibler divergence", "Learning to rank", "Linear discriminant analysis", "Linear regression", "Local outlier factor", "Logistic regression", "MATLAB", "Machine learning", "Mean-shift", "Multilayer perceptron", "Multinomial probit", "Naive Bayes classifier", "Non-negative matrix factorization", "OPTICS algorithm", "Online machine learning", "Perceptron", "Principal component analysis", "Probably approximately correct learning", "Proximal gradient methods for learning", "Random forest", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Reproducing kernel Hilbert spaces", "Restricted Boltzmann machine", "Self-organizing map", "Semi-supervised learning", "Semisupervised learning", "Statistical classification", "Statistical learning theory", "Structural risk minimization", "Structured prediction", "Supervised learning", "Support vector machine", "T-distributed stochastic neighbor embedding", "Tikhonov regularization", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory"], "categories": ["All orphaned articles", "Data mining", "Machine learning algorithms", "Orphaned articles from January 2015"], "title": "Multiple kernel learning"}
{"summary": "NMF redirects here. For the bridge convention, see new minor forcing.\nNon-negative matrix factorization (NMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.\nNMF finds applications in such fields as computer vision, document clustering, chemometrics, audio signal processing and recommender systems.", "links": ["80s", "Acta Neuropathologica", "Active set", "Algorithm", "Amnon Shashua", "Andrzej Cichocki", "ArXiv", "Association for Computing Machinery", "Atmospheric Environment (journal)", "Audio signal processing", "Bayesian network", "Bioinformatics", "Bioinformatics (journal)", "Chemometrics", "Chemometrics and Intelligent Laboratory Systems", "Chinese Science Bulletin", "Cluster analysis", "Cluster indicator", "Collaborative filtering", "Computational Intelligence and Neuroscience", "Computational and Mathematical Organization Theory", "Computer vision", "Conference on Neural Information Processing Systems", "Contract bridge", "Data clustering", "Digital object identifier", "Dna methylation", "Document-term matrix", "Edward A. Sylvestre", "Enron", "Environmetrics", "Frobenius norm", "Gaussian noise", "Gene expression", "Generative model", "Gradient descent", "Haesun Park", "IEEE Journal on Selected Areas in Communications", "IEEE Signal Processing Magazine", "IEEE Trans Med Imaging", "IEEE Trans Nucl Sci", "IEEE Transactions on Network and Service Management", "Inderjit S. Dhillon", "International Standard Book Number", "Inverse matrix", "JSTOR", "Journal of Computational and Graphical Statistics", "K-means clustering", "Kullback\u2013Leibler divergence", "Latent class model", "Least squares", "Linear algebra", "Loss function", "MIT Press", "Matrix (mathematics)", "Matrix decomposition", "Maximum likelihood", "Monomial matrix", "Multilinear algebra", "Multilinear subspace learning", "Multivariate analysis", "NP-complete", "NQP", "Nature (journal)", "Neural Computation", "NeuroImage", "Neurocomputing (journal)", "New minor forcing", "Non-negative least squares", "Non-negative matrix", "Nonnegative rank (linear algebra)", "PARAFAC", "PLoS Computational Biology", "Pattern Recognition Letters", "Pentti Paatero", "Permutation", "Pia Anttila", "Positron emission tomography", "Principal component analysis", "Probabilistic latent semantic analysis", "PubMed", "PubMed Central", "PubMed Identifier", "Quadratic programming", "Recommendation systems", "Recommender system", "Regularization (mathematics)", "SIAM Journal on Matrix Analysis and Applications", "SIAM Journal on Scientific Computing", "SPECT", "Scientific journal", "Sebastian Seung", "Shun-ichi Amari", "Sparse coding", "Streaming", "Support vector machine", "Suvrit Sra", "Technometrics", "Tensor", "Tensor decomposition", "Tensor software", "Text mining", "Tikhnov regularization", "Total variation norm", "Vector quantization", "Wiener filter", "Wikimania", "Wikipedia", "William H. Lawton"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from April 2015", "Linear algebra", "Machine learning algorithms", "Matrix theory", "Multivariate statistics", "Vague or ambiguous time from April 2011"], "title": "Non-negative matrix factorization"}
{"summary": "Online machine learning is used when data becomes available in a sequential order to determine a mapping from data set corresponding labels. The difference between online learning and batch learning (or \"offline\" learning) techniques, is that in online learning the mapping is updated after the arrival of every new data point in a scale fashion, whereas batch techniques are used when one has access to the entire training data set at once. Online learning could be used in the case of a process occurring in time, for example the value of a stock given its history and other external factors, in which case the mapping updates as time goes on and we get more and more samples.\nIdeally in online learning, the memory needed to store the function remains constant even with added data points, since the solution computed at one step is updated when a new data point becomes available, after which that data point can then be discarded. For many formulations, for example nonlinear kernel methods, true online learning is not possible, though a form of hybrid online learning with recursive algorithms can be used. In this case, the space requirements are no longer guaranteed to be constant since it requires storing all previous data points, but the solution may take less time to compute with the addition of a new data point, as compared to batch learning techniques.\nAs in all machine learning problems, the goal of the algorithm is to minimize some performance criteria using a loss function. For example, with stock market prediction the algorithm may attempt to minimize the mean squared error between the predicted and true value of a stock. Another popular performance criterion is to minimize the number of mistakes when dealing with classification problems. In addition to applications of a sequential nature, online learning algorithms are also relevant in applications with huge amounts of data such that traditional learning approaches that use the entire data set in aggregate are computationally infeasible.", "links": ["Anomaly detection", "Artificial neural network", "Association rule learning", "Autoencoder", "BIRCH", "Bayesian network", "Bias-variance dilemma", "Boosting (machine learning)", "Bootstrap aggregating", "Canonical correlation analysis", "Cluster analysis", "Computational learning theory", "Conditional random field", "Convolutional neural network", "DBSCAN", "Data mining", "Decision tree learning", "Deep learning", "Dimensionality reduction", "Empirical risk minimization", "Ensemble learning", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Grammar induction", "Graphical model", "Hidden Markov model", "Hierarchical clustering", "Hierarchical temporal memory", "I.i.d.", "Independent component analysis", "International Standard Book Number", "K-means clustering", "K-nearest neighbor algorithm", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Kernel methods", "Lazy learning", "Learning Vector Quantization", "Learning to rank", "Least squares", "Linear discriminant analysis", "Linear regression", "Local outlier factor", "Logistic regression", "Loss function", "L\u00e9on Bottou", "Machine learning", "Mean-shift", "Mean squared error", "Multilayer perceptron", "Naive Bayes classifier", "Nicol\u00f2 Cesa-Bianchi", "Non-negative matrix factorization", "OPTICS algorithm", "Offline learning", "Online algorithm", "Perceptron", "Principal component analysis", "Probably approximately correct learning", "Random forest", "Recurrent neural network", "Recursive least squares", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Self-organizing map", "Semi-supervised learning", "Statistical classification", "Statistical learning theory", "Stochastic gradient descent", "Streaming Algorithm", "Structured prediction", "Supervised learning", "Support vector machine", "Support vector machines", "T-distributed stochastic neighbor embedding", "Tikhonov regularization", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory"], "categories": ["All articles covered by WikiProject Wikify", "All articles needing additional references", "All pages needing cleanup", "Articles covered by WikiProject Wikify from March 2014", "Articles needing additional references from November 2008", "Articles with inconsistent citation formats", "Articles with weasel words from November 2012", "Machine learning algorithms", "Wikipedia introduction cleanup from March 2014"], "title": "Online machine learning"}
{"summary": "Prefrontal cortex basal ganglia working memory (PBWM) is an algorithm that models working memory in the prefrontal cortex and the basal ganglia. It can be compared to long short-term memory (LSTM) in functionality, but is more biologically explainable.\nIt uses the primary value learned value model to train prefrontal cortex working-memory updating system, based on the biology of the prefrontal cortex and basal ganglia.\nIt is used as part of the Leabra framework and was implemented in Emergent.", "links": ["1-2-AX working memory task", "Algorithm", "Backpropagation", "Basal ganglia", "Computer simulation", "Dopamine", "Dopaminergic", "Emergent (software)", "Leabra", "Long short term memory", "PVLV", "Pars reticulata", "Prefrontal cortex", "Reinforcement learning", "Striatum", "Substantia nigra", "Substantia nigra pars compacta", "Thalamus", "Ventral tegmental area", "Winner-take-all (computing)", "Working memory"], "categories": ["All articles lacking reliable references", "All articles needing additional references", "All articles with unsourced statements", "Articles lacking reliable references from April 2015", "Articles needing additional references from September 2015", "Articles with unsourced statements from September 2015", "Machine learning algorithms", "Neuroscience", "Vague or ambiguous time from September 2015"], "title": "Prefrontal cortex basal ganglia working memory"}
{"summary": "The primary value learned value (PVLV) model is a possible explanation for the reward-predictive firing properties of dopamine (DA) neurons. It simulates behavioral and neural data on Pavlovian conditioning and the midbrain dopaminergic neurons that fire in proportion to unexpected rewards. It is an alternative to the temporal-differences (TD) algorithm.\nIt is used as part of Leabra.", "links": ["Classical conditioning", "Computer simulation", "Dopamine", "Leabra", "Midbrain", "Neuroscience", "Temporal difference learning"], "categories": ["All stub articles", "Machine learning algorithms", "Neuroscience", "Neuroscience stubs"], "title": "PVLV"}
{"summary": "Quadratic unconstrained binary optimization (QUBO) is a pattern matching technique, common in machine learning applications. QUBO is an NP hard problem.\nQUBO problems may sometimes be well-suited to algorithms aided by quantum annealing.\nQUBO is given by the formula:", "links": ["Artificial intelligence", "Digital object identifier", "Machine learning", "NP hard", "Pattern matching", "Quantum annealing"], "categories": ["All articles needing additional references", "All stub articles", "Articles needing additional references from September 2014", "Artificial intelligence stubs", "Machine learning algorithms"], "title": "Quadratic unconstrained binary optimization"}
{"summary": "A query level feature or QLF is a ranking feature utilized in a machine-learned ranking algorithm.\nExample QLFs:\nHow many times has this query been run in the last month?\nHow many words are in the query?\nWhat is the sum/average/min/max/median of the BM25F values for the query?", "links": ["Artificial intelligence", "Machine-learned ranking", "Probabilistic relevance model (BM25)"], "categories": ["All stub articles", "Artificial intelligence stubs", "Machine learning", "Machine learning algorithms"], "title": "Query level feature"}
{"summary": "Quickprop is an iterative method for determining the minimum of the loss function of an artificial neural network, following an algorithm inspired by the Newton's method. Sometimes, the algorithm is classified to the group of the second order learning methods. It follows a quadratic approximation of the previous gradient step and the current gradient, which is expected to be closed to the minimum of the loss function, under the assumption that the loss function is locally approximately square, trying to describe it by means of an upwardly open parabola. The minimum is sought in the vertex of the parabola. The procedure requires only local information of the artificial neuron to which it is applied. The k-th approximation step is given by:\n\nBeing  the neuron j weight of its i input and E is the loss function.\nThe Quickprop algorithm is an implementation of the error backpropagation algorithm, but the network can behave chaotically during the learning phase due to large step sizes.", "links": ["Artificial neural network", "Artificial neuron", "Backpropagation", "Gradient", "Loss function", "Newton's method", "Parabola"], "categories": ["Artificial neural networks", "Computational neuroscience", "Machine learning algorithms"], "title": "Quickprop"}
{"summary": "The randomized weighted majority algorithm is an algorithm in machine learning theory. It improves the mistake bound of the weighted majority algorithm.\nImagine that every morning before the stock market opens, we get a prediction from each of our \"experts\" about whether the stock market will go up or down. Our goal is to somehow combine this set of predictions into a single prediction that we then use to make a buy or sell decision for the day. The RWMA gives us a way to do this combination such that our prediction record will be nearly as good as that of the single best expert in hindsight.", "links": ["Digital object identifier", "Game theory", "Machine learning", "Mistake bound", "Stock market", "Weighted majority algorithm"], "categories": ["Machine learning algorithms"], "title": "Randomized weighted majority algorithm"}
{"summary": "Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics, and genetic algorithms. In the operations research and control literature, the field where reinforcement learning methods are studied is called approximate dynamic programming. The problem has been studied in the theory of optimal control, though most studies are concerned with the existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.\nIn machine learning, the environment is typically formulated as a Markov decision process (MDP) as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical techniques and reinforcement learning algorithms is that the latter do not need knowledge about the MDP and they target large MDPs where exact methods become infeasible.\nReinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs.", "links": ["Abhijit Gosavi", "Action selection", "Actor critic", "Andrew G. Barto", "Andrew W. Moore", "Anomaly detection", "Artificial neural network", "Association rule learning", "Autoencoder", "BIRCH", "Backgammon", "Bart De Schutter", "Barto, Andrew G.", "Bayesian network", "Behaviorism", "Bellman equations", "Bias-variance dilemma", "Boosting (machine learning)", "Bootstrap aggregating", "Bounded rationality", "Brute-force search", "Canonical correlation analysis", "Checkers", "Christopher J.C.H. Watkins", "Cluster analysis", "Computational learning theory", "Conditional random field", "Control theory", "Convolutional neural network", "Cross-entropy method", "Csaba Szepesvari", "DBSCAN", "Damien Ernst", "Data mining", "Decision tree learning", "Deep learning", "Delft University of Technology", "Digital object identifier", "Dimensionality reduction", "Dimitri P. Bertsekas", "Direct policy search", "Distributed artificial intelligence", "Dopamine", "Dynamic programming", "Dynamic treatment regimes", "Economics", "Empirical risk minimization", "Ensemble learning", "Error-driven learning", "Evolutionary computation", "Expectation-maximization algorithm", "Exploitation", "Exploration", "Factor analysis", "Feature engineering", "Feature learning", "Fictitious play", "Game theory", "Genetic algorithm", "Gerhard Neumann", "Gradient descent", "Grammar induction", "Graphical model", "G\u00fcnther Palm", "Hidden Markov model", "Hierarchical clustering", "IROS", "Independent component analysis", "Information theory", "International Conference on Robotics and Automation", "International Standard Book Number", "Istvan Szita", "Jan Peters", "Jan Peters (researcher)", "John Tsitsiklis", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Lazy evaluation", "Learning classifier system", "Learning to rank", "Least-squares temporal difference method", "Leslie P. Kaelbling", "Likelihood ratio method", "Linear discriminant analysis", "Linear regression", "Local outlier factor", "Local search (optimization)", "Logistic regression", "Lucian Busoniu", "Machine learning", "Marc Peter Deisenroth", "Markov decision process", "Mean-shift", "Michael L. Littman", "Michel Tokic", "Monte-Carlo tree search", "Monte Carlo sampling", "Multi-agent system", "Multi-armed bandit", "Multilayer perceptron", "Naive Bayes classifier", "Non-negative matrix factorization", "Nonparametric statistics", "OPTICS algorithm", "Online machine learning", "Operant conditioning", "Operations research", "Optimal control", "Optimal control theory", "Orange (software)", "Partially observable Markov decision process", "Perceptron", "Peter Auer", "Policy iteration", "Predictive State Representation", "Principal component analysis", "Probably approximately correct learning", "Q-Learning", "Q-learning", "Random forest", "Recurrent neural network", "Regression analysis", "Reinforcement", "Relevance vector machine", "Restricted Boltzmann machine", "Richard S. Sutton", "Robert Babuska", "Robot control", "Ronald J. Williams", "Ronald Ortner", "Self-organizing map", "Semi-supervised learning", "Sethu Vijayakumar", "Simulated annealing", "Simulation-based optimization", "Software agent", "State-Action-Reward-State-Action", "Statistical classification", "Statistical learning theory", "Statistics", "Stefan Schaal", "Steven J. Bradtke", "Stochastic", "Stochastic optimization", "Structured prediction", "Supervised learning", "Support vector machine", "Swarm intelligence", "T-distributed stochastic neighbor embedding", "Telecommunications", "Temporal difference", "Temporal difference learning", "Thomas Jaksch", "Unsupervised learning", "Value function approaches", "Value iteration", "Vapnik\u2013Chervonenkis theory"], "categories": ["Belief revision", "CS1 errors: missing author or editor", "Machine learning algorithms", "Markov models", "Pages containing cite templates with deprecated parameters"], "title": "Reinforcement learning"}
{"summary": "Rprop, short for resilient backpropagation, is a learning heuristic for supervised learning in feedforward artificial neural networks. This is a first-order optimization algorithm. This algorithm was created by Martin Riedmiller and Heinrich Braun in 1992.\nSimilarly to the Manhattan update rule, Rprop takes into account only the sign of the partial derivative over all patterns (not the magnitude), and acts independently on each \"weight\". For each weight, if there was a sign change of the partial derivative of the total error function compared to the last iteration, the update value for that weight is multiplied by a factor \u03b7\u2212, where \u03b7\u2212 < 1. If the last iteration produced the same sign, the update value is multiplied by a factor of \u03b7+, where \u03b7+ > 1. The update values are calculated for each weight in the above manner, and finally each weight is changed by its own update value, in the opposite direction of that weight's partial derivative, so as to minimise the total error function. \u03b7+ is empirically set to 1.2 and \u03b7\u2212 to 0.5.\nNext to the cascade correlation algorithm and the Levenberg\u2013Marquardt algorithm, Rprop is one of the fastest weight update mechanisms.\nRPROP is a batch update algorithm.", "links": ["Algorithm", "Artificial neural network", "Backpropagation", "Cascade correlation algorithm", "Feedforward neural network", "First-order approximation", "Heuristics", "Levenberg\u2013Marquardt algorithm", "Manhattan update rule", "Optimization (mathematics)", "Partial derivative", "Sign (mathematics)", "Supervised learning"], "categories": ["Artificial neural networks", "Machine learning algorithms"], "title": "Rprop"}
{"summary": "State-Action-Reward-State-Action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was introduced in a technical note  where the alternative name SARSA was only mentioned as a footnote.\nThis name simply reflects the fact that the main function for updating the Q-value depends on the current state of the agent \"S1\", the action the agent chooses \"A1\", the reward \"R\" the agent gets for choosing this action, the state \"S2\" that the agent will now be in after taking that action, and finally the next action \"A2\" the agent will choose in its new state. Taking every letter in the quintuple (st, at, rt, st+1, at+1) yields the word SARSA.", "links": ["Algorithm", "Digital object identifier", "Machine learning", "Markov decision process", "PubMed Identifier", "Q-learning", "Reinforcement learning", "Sarsa (disambiguation)", "Temporal difference learning"], "categories": ["Machine learning algorithms"], "title": "State-Action-Reward-State-Action"}
{"summary": "t-distributed stochastic neighbor embedding (t-SNE) is a machine learning algorithm for dimensionality reduction developed by Laurens van der Maaten and Geoffrey Hinton. It is a nonlinear dimensionality reduction technique that is particularly well suited for embedding high-dimensional data into a space of two or three dimensions, which can then be visualized in a scatter plot. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points.\nThe t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, whilst dissimilar points have an infinitesimal probability of being picked. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback\u2013Leibler divergence between the two distributions with respect to the locations of the points in the map. Note that whilst the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this should be changed as appropriate.\nt-SNE has been used in a wide range of applications, including computer security research, music analysis, cancer research, and bioinformatics.", "links": ["Anomaly detection", "Artificial neural network", "Association rule learning", "Autoencoder", "BIRCH", "Bayesian network", "Bias-variance dilemma", "Binary search", "Bioinformatics", "Boosting (machine learning)", "Bootstrap aggregating", "Cancer research", "Canonical correlation analysis", "Cluster analysis", "Computational learning theory", "Computer security", "Conditional random field", "Convolutional neural network", "DBSCAN", "Data mining", "Decision tree learning", "Deep learning", "Density", "Digital object identifier", "Dimensionality reduction", "Empirical risk minimization", "Ensemble learning", "Euclidean distance", "Expectation-maximization algorithm", "Factor analysis", "Feature engineering", "Feature learning", "Gaussian kernel", "Geoffrey Hinton", "Gradient descent", "Grammar induction", "Graphical model", "Hidden Markov model", "Hierarchical clustering", "Independent component analysis", "Infinitesimal", "K-means clustering", "K-nearest neighbors algorithm", "K-nearest neighbors classification", "Kullback\u2013Leibler divergence", "Laurens van der Maaten", "Learning to rank", "Linear discriminant analysis", "Linear regression", "Local outlier factor", "Logistic regression", "Machine learning", "Mean-shift", "Multilayer perceptron", "Music analysis", "Naive Bayes classifier", "Non-negative matrix factorization", "Nonlinear dimensionality reduction", "OPTICS algorithm", "Online machine learning", "Perceptron", "Perplexity", "Principal component analysis", "Probability distribution", "Probably approximately correct learning", "Random forest", "Recurrent neural network", "Regression analysis", "Reinforcement learning", "Relevance vector machine", "Restricted Boltzmann machine", "Self-organizing map", "Semi-supervised learning", "Statistical classification", "Statistical learning theory", "Structured prediction", "Student-t distribution", "Supervised learning", "Support vector machine", "Unsupervised learning", "Vapnik\u2013Chervonenkis theory"], "categories": ["Machine learning algorithms"], "title": "T-distributed stochastic neighbor embedding"}
{"summary": "Temporal difference (TD) learning is a prediction-based machine learning method. It has primarily been used for the reinforcement learning problem, and is said to be \"a combination of Monte Carlo ideas and dynamic programming (DP) ideas.\" TD resembles a Monte Carlo method because it learns by sampling the environment according to some policy, and is related to dynamic programming techniques as it approximates its current estimate based on previously learned estimates (a process known as bootstrapping). The TD learning algorithm is related to the temporal difference model of animal learning.\nAs a prediction method, TD learning takes into account the fact that subsequent predictions are often correlated in some sense. In standard supervised predictive learning, one learns only from actually observed values: A prediction is made, and when the observation is available, the prediction is adjusted to better match the observation. As elucidated by Richard Sutton, the core idea of TD learning is that we adjust predictions to match other, more accurate, predictions about the future. This procedure is a form of bootstrapping, as illustrated with the following example:\nSuppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday - and thus be able to change, say, Monday's model before Saturday arrives.\nMathematically speaking, both in a standard and a TD approach, we would try to optimize some cost function, related to the error in our predictions of the expectation of some random variable, E[z]. However, while in the standard approach we in some sense assume E[z] = z (the actual observed value), in the TD approach we use a model. For the particular case of reinforcement learning, which is the major application of TD methods, z is the total return and E[z] is given by the Bellman equation of the return.", "links": ["Algorithm", "Arthur Samuel", "Backgammon", "Bellman equation", "Bootstrapping (machine learning)", "Digital object identifier", "Dopamine", "Dynamic programming", "Gerald Tesauro", "International Standard Book Number", "Machine learning", "Monte Carlo method", "Neurons", "Neuroscience", "PVLV", "PubMed Identifier", "Q-learning", "Reinforcement learning", "Rescorla-Wagner model", "Reward system", "Richard S. Sutton", "Sampling (statistics)", "Schizophrenia", "State-Action-Reward-State-Action", "Substantia nigra", "TD-Gammon", "Ventral tegmental area"], "categories": ["Computational neuroscience", "Machine learning algorithms"], "title": "Temporal difference learning"}
{"summary": "The wake-sleep algorithm is an unsupervised learning algorithm for a multilayer neural network (e.g. sigmoid belief network). It is one of the suggested ways of training the Helmholtz Machine. The algorithm consists of two learning phases \u2013 \u201cwake\u201d and \u201csleep\u201d that are performed alternately. It was first designed as a model for brain functioning using Variational Bayesian Learning. After that, the algorithm was adapted to machine learning.", "links": ["Artificial neural network", "Convergence of random variables", "Digital object identifier", "Helmholtz machine", "Machine learning", "Probability", "Restricted Boltzmann machine", "Unsupervised learning", "Variational Bayesian methods"], "categories": ["Machine learning algorithms"], "title": "Wake-sleep algorithm"}
{"summary": "In machine learning, Weighted Majority Algorithm (WMA) is a meta-learning algorithm used to construct a compound algorithm from a pool of prediction algorithms, which could be any type of learning algorithms, classifiers, or even real human experts. The algorithm assumes that we have no prior knowledge about the accuracy of the algorithms in the pool, but there are sufficient reasons to believe that one or more will perform well.\nAssume that the problem is a binary decision problem. To construct the compound algorithm, a positive weight is given to each of the algorithms in the pool. The compound algorithm then collects weighted votes from all the algorithms in the pool, and gives the prediction that has a higher vote. If the compound algorithm makes a mistake, the algorithms in the pool that contributed to the wrong predicting will be discounted by a certain ratio \u03b2 where 0<\u03b2<1.\nIt can be shown that the upper bounds on the number of mistakes made in a given sequence of predictions from a pool of algorithms  is\n\nif one algorithm in  makes at most  mistakes.\nThere are many variations of the Weighted Majority Algorithm to handle different situations, like shifting targets, infinite pools, or randomized predictions. The core mechanism remain similar, with the final performances of the compound algorithm bounded by a function of the performance of the specialist (best performing algorithm) in the pool.", "links": ["Digital object identifier", "Machine learning", "Randomized weighted majority algorithm"], "categories": ["Machine learning algorithms"], "title": "Weighted Majority Algorithm"}
{"summary": "Because matrix multiplication is such a central operation in many numerical algorithms, much work has been invested in making matrix multiplication algorithms efficient. Applications of matrix multiplication in computational problems are found in many fields including scientific computing and pattern recognition and in seemingly unrelated problems such counting the paths through a graph. Many different algorithms have been designed for multiplying matrices on different types of hardware, including parallel and distributed systems, where the computational work is spread over multiple processors (perhaps over a network).\nDirectly applying the mathematical definition of matrix multiplication gives an algorithm that takes time on the order of n3 to multiply two n \u00d7 n matrices (\u0398(n3) in big O notation). Better asymptotic bounds on the time required to multiply matrices have been known since the work of Strassen in the 1960s, but it is still unknown what the optimal time is (i.e., what the complexity of the problem is).", "links": ["Abelian group", "Analysis of algorithms", "ArXiv", "Asymptotic notation", "Bal\u00e1zs Szegedy", "Basic Linear Algebra Subprograms", "Big O notation", "Block matrix", "CPU cache", "CYK algorithm", "Cache-oblivious algorithm", "Cache-oblivious matrix multiplication", "Cambridge University Press", "Cannon's algorithm", "Charles E. Leiserson", "Chris Umans", "CiteSeer", "Clifford Stein", "Comparison of linear algebra libraries", "Comparison of numerical analysis software", "Computational complexity of mathematical operations", "Computational complexity theory", "Coppersmith\u2013Winograd algorithm", "Critical path length", "Digital object identifier", "Distributed computing", "Divide and conquer", "Don Coppersmith", "Floating point", "Fork\u2013join model", "Freivalds' algorithm", "Graph (graph theory)", "Group theory", "Harald Prokop", "International Standard Book Number", "Introduction to Algorithms", "Jack Dongarra", "List of unsolved problems in computer science", "Locality of reference", "Loop tiling", "Loop unrolling", "MapReduce", "Master theorem", "Mathematical Reviews", "Matrix chain multiplication", "Matrix decomposition", "Matrix multiplication", "Mesh networking", "Multiprocessing", "Multiprogramming", "Noga Alon", "Numerical Recipes", "Numerical algorithm", "Numerical linear algebra", "Numerical stability", "Parallel algorithm", "Parallel computing", "Pattern recognition", "Pseudocode", "Ran Raz", "Recursion", "Robert Kleinberg", "Ron Rivest", "Row-major order", "SIMD", "Saul Teukolsky", "Scalar multiplication", "Scientific computing", "Shared-memory multiprocessor", "Shmuel Winograd", "Sparse matrix", "Sparse matrix-vector multiplication", "Speedup", "Steven Skiena", "Strassen algorithm", "Sunflower conjecture", "System of linear equations", "Thomas H. Cormen", "Time complexity", "Translation lookaside buffer", "Triple product property", "Volker Strassen", "Wreath product"], "categories": ["All articles containing potentially dated statements", "Articles containing potentially dated statements from 2010", "Matrix multiplication algorithms", "Unsolved problems in computer science"], "title": "Matrix multiplication algorithm"}
{"summary": "In linear algebra, the Coppersmith\u2013Winograd algorithm, named after Don Coppersmith and Shmuel Winograd, was the asymptotically fastest known algorithm for square matrix multiplication until 2010. It can multiply two  matrices in  time  (see Big O notation). This is an improvement over the na\u00efve  time algorithm and the  time Strassen algorithm. Algorithms with better asymptotic running time than the Strassen algorithm are rarely used in practice, because the large constant factors in their running times make them impractical. It is possible to improve the exponent further; however, the exponent must be at least 2 (because an  matrix has  values, and all of them have to be read at least once to calculate the exact result).\nIn 2010, Andrew Stothers gave an improvement to the algorithm,  In 2011, Virginia Williams combined a mathematical short-cut from Stothers' paper with her own insights and automated optimization on computers, improving the bound to  In 2014, Fran\u00e7ois Le Gall simplified the methods of Williams and obtained an improved bound of \nThe Coppersmith\u2013Winograd algorithm is frequently used as a building block in other algorithms to prove theoretical time bounds. However, unlike the Strassen algorithm, it is not used in practice because it only provides an advantage for matrices so large that they cannot be processed by modern hardware.\nHenry Cohn, Robert Kleinberg, Bal\u00e1zs Szegedy and Chris Umans have re-derived the Coppersmith\u2013Winograd algorithm using a group-theoretic construction. They also showed that either of two different conjectures would imply that the optimal exponent of matrix multiplication is 2, as has long been suspected. However, they were not able to formulate a specific solution leading to a better running-time than Coppersmith-Winograd at the time.", "links": ["Algorithm", "ArXiv", "Bal\u00e1zs Szegedy", "Basic Linear Algebra Subprograms", "Big O notation", "CPU cache", "Cache-oblivious algorithm", "Chris Umans", "Comparison of linear algebra libraries", "Comparison of numerical analysis software", "Computational complexity of mathematical operations", "Digital object identifier", "Don Coppersmith", "Floating point", "Gauss\u2013Jordan elimination", "Group theory", "Henry Cohn", "ISSAC", "International Standard Book Number", "Linear algebra", "Matrix decomposition", "Matrix multiplication", "Matrix multiplication algorithm", "Multiprocessing", "Numerical linear algebra", "Numerical stability", "Robert Kleinberg", "SIMD", "Shmuel Winograd", "Sparse matrix", "Strassen algorithm", "System of linear equations", "Translation lookaside buffer"], "categories": ["Matrix multiplication algorithms", "Matrix theory", "Numerical linear algebra", "Use dmy dates from July 2013"], "title": "Coppersmith\u2013Winograd algorithm"}
{"summary": "Freivalds' algorithm (named after Rusins Freivalds) is a probabilistic randomized algorithm used to verify matrix multiplication. Given three n \u00d7 n matrices A, B, and C, a general problem is to verify whether A \u00d7 B = C. A na\u00efve algorithm would compute the product A \u00d7 B explicitly and compare term by term whether this product equals C. However, the best known matrix multiplication algorithm runs in O(n2.3729) time. Freivalds' algorithm utilizes randomization in order to reduce this time bound to O(n2)  with high probability. In O(kn2) time the algorithm can verify a matrix product with probability of failure less than .", "links": ["Algorithm", "Basic Linear Algebra Subprograms", "Bayes' Theorem", "Big O notation", "CPU cache", "Cache-oblivious algorithm", "Comparison of linear algebra libraries", "Comparison of numerical analysis software", "Coppersmith\u2013Winograd algorithm", "Deterministic algorithm", "Digital object identifier", "Error bound", "Floating point", "Matrix (mathematics)", "Matrix decomposition", "Matrix multiplication", "Matrix multiplication algorithm", "Multiprocessing", "Numerical linear algebra", "Numerical stability", "One-sided error", "Probabilistic algorithm", "Probability", "Random", "Randomization", "Randomized algorithm", "Randomized algorithms", "SIMD", "Schwartz\u2013Zippel lemma", "Sparse matrix", "System of linear equations", "Translation lookaside buffer", "Vector (geometric)", "With high probability"], "categories": ["Articles containing proofs", "Matrix multiplication algorithms", "Matrix theory", "Randomized algorithms"], "title": "Freivalds' algorithm"}
{"summary": "In linear algebra, the Strassen algorithm, named after Volker Strassen, is an algorithm used for matrix multiplication. It is faster than the standard matrix multiplication algorithm and is useful in practice for large matrices, but would be slower than the fastest known algorithms for extremely large matrices.", "links": ["Abuse of notation", "Algorithm", "Basic Linear Algebra Subprograms", "Bilinear map", "Block matrix", "CPU cache", "Cache-oblivious algorithm", "Charles E. Leiserson", "Clifford Stein", "Comparison of linear algebra libraries", "Comparison of numerical analysis software", "Computational complexity of mathematical operations", "Coppersmith-Winograd algorithm", "Coppersmith\u2013Winograd algorithm", "Dot product", "Dual space", "Eric W. Weisstein", "Floating point", "Gauss\u2013Jordan elimination", "Hadamard product (matrices)", "International Standard Book Number", "Introduction to Algorithms", "Karatsuba algorithm", "Linear algebra", "MathWorld", "Matrix decomposition", "Matrix inversion", "Matrix multiplication", "Matrix multiplication algorithm", "Multiplication algorithm", "Multiprocessing", "Numerical linear algebra", "Numerical stability", "Ring (mathematics)", "Ronald L. Rivest", "SIMD", "Sch\u00f6nhage\u2013Strassen algorithm", "Sparse matrix", "Springer-Verlag", "Square matrix", "Submatrices", "System of linear equations", "Tensor product", "Thomas H. Cormen", "Translation lookaside buffer", "Volker Strassen", "Z-order (curve)"], "categories": ["All articles needing additional references", "Articles needing additional references from January 2015", "Matrix multiplication algorithms"], "title": "Strassen algorithm"}
{"summary": "Adaptive Replacement Cache (ARC) is a page replacement algorithm with better performance than LRU (Least Recently Used) developed at the IBM Almaden Research Center. This is accomplished by keeping track of both Frequently Used and Recently Used pages plus a recent eviction history for both. In 2006, IBM was granted a patent for the adaptive replacement cache policy.", "links": ["Almaden Research Center", "Cache algorithms", "Clock with Adaptive Replacement", "Dharmendra Modha", "Nimrod Megiddo", "Page replacement algorithm", "PostgreSQL", "Solaris (operating system)", "Sun Microsystems", "ZFS"], "categories": ["Memory management algorithms", "Use dmy dates from August 2012", "Virtual memory"], "title": "Adaptive replacement cache"}
{"summary": "The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as possible. This system makes use of splitting memory into halves to try to give a best-fit. According to Donald Knuth, the buddy system was invented in 1963 by Harry Markowitz, who won the 1990 Nobel Memorial Prize in Economics, and was first described by Kenneth C. Knowlton (published 1965). Buddy memory allocation is relatively easy to implement. It supports limited but efficient splitting and coalescing of memory blocks.", "links": ["Binary tree", "Coalescing (computer science)", "Communications of the ACM", "Data compaction", "Donald Knuth", "Dynamic allocation", "Exclusive OR", "Fragmentation (computer)", "Harry Markowitz", "International Standard Book Number", "Ken Knowlton", "Linux kernel", "Memory allocation", "Memory pool", "Nobel Memorial Prize in Economics", "Programmer", "Slab allocation", "Stack-based memory allocation", "The Art of Computer Programming", "Wrox Press"], "categories": ["Memory management algorithms", "Use dmy dates from August 2012"], "title": "Buddy memory allocation"}
{"summary": "In computing, cache algorithms (also frequently called cache replacement algorithms or cache replacement policies) are optimizing instructions\u2014\u200bor algorithms\u2014\u200bthat a computer program or a hardware-maintained structure can follow in order to manage a cache of information stored on the computer. When the cache is full, the algorithm must choose which items to discard to make room for the new ones.", "links": ["ARM architecture", "Adaptive Replacement Cache", "Algorithm", "Benchmark (computing)", "CPU cache", "CPU caches", "Cache-oblivious algorithm", "Cache (computing)", "Cache coherency", "Cache pollution", "Clock with Adaptive Replacement", "Computer (magazine)", "Computer program", "Computing", "Digital object identifier", "Distributed cache", "International Standard Book Number", "LIRS caching algorithm", "Least-Frequently Used", "Locality of reference", "L\u00e1szl\u00f3 B\u00e9l\u00e1dy", "Multi Queue (MQ) caching algorithm", "Nimrod Megiddo", "Optimization (computer science)", "Page replacement algorithm", "Pseudo-LRU", "VLDB", "Yuanyuan Zhou"], "categories": ["Cache (computing)", "Memory management algorithms", "Use dmy dates from August 2012"], "title": "Cache algorithms"}
{"summary": "Least Frequently Used (LFU) is a type of cache algorithm used to manage memory within a computer. The standard characteristics of this method involve the system keeping track of the number of times a block is referenced in memory. When the cache is full and requires more room the system will purge the item with the lowest reference frequency.\nLFU is sometimes combined with a Least Recently Used algorithm and called LRFU.\n\n", "links": ["Cache (computing)", "Cache algorithm", "Least Recently Used", "Page (computer memory)", "Page replacement algorithm", "Paging", "Reference (computer science)"], "categories": ["Memory management algorithms", "Online algorithms", "Use dmy dates from August 2012", "Virtual memory"], "title": "Least frequently used"}
{"summary": "LIRS (Low Inter-reference Recency Set) is a page replacement algorithm with an improved performance over LRU (Least Recently Used) and many other newer replacement algorithms. This is achieved by using reuse distance as a metric for dynamically ranking accessed pages to make a replacement decision. The algorithm was developed by Song Jiang and Xiaodong Zhang.", "links": ["Cache algorithms", "Clock with Adaptive Replacement", "Infinispan", "Locality of reference", "MySQL", "NetBSD", "Page replacement algorithm"], "categories": ["Memory management algorithms", "Online algorithms", "Virtual memory"], "title": "LIRS caching algorithm"}
{"summary": "With multiple processes competing for frames, page-replacement algorithms can be classified into two broad categories: global replacement and local replacement. Global replacement allows a process to select a replacement frame from the set of all frames, even if that frame is currently allocated to some other process; that is; one process can take a frame from another. Local replacement requires that each process select from only its own set of allocated frames.", "links": ["Page fault", "Page replacement algorithm", "Process (computing)", "Strategy", "Throughput"], "categories": ["All articles covered by WikiProject Wikify", "All articles with too few wikilinks", "Articles covered by WikiProject Wikify from October 2012", "Articles with too few wikilinks from October 2012", "Memory management algorithms", "Use dmy dates from August 2012", "Virtual memory"], "title": "Local replacement algorithm"}
{"summary": "In computer science, a mark-compact algorithm is a type of garbage collection algorithm used to reclaim unreachable memory. Mark-compact algorithms can be regarded as a combination of the mark-sweep algorithm and Cheney's copying algorithm. First, reachable objects are marked, then a compacting step relocates the reachable (marked) objects towards the beginning of the heap area. Compacting garbage collection is used by Microsoft's Common Language Runtime and by the Glasgow Haskell Compiler.", "links": ["Algorithm", "Automatic variable", "Big O notation", "Boehm garbage collector", "Buffer over-read", "Buffer overflow", "C dynamic memory allocation", "Cheney's algorithm", "Common Language Runtime", "Comparison sort", "Computer science", "Dangling pointer", "Delete (C++)", "Demand paging", "Finalizer", "Fragmentation (computer)", "Fragmentation (computing)", "Garbage (computer science)", "Garbage collection (computer science)", "Glasgow Haskell Compiler", "International Symposium on Memory Management", "LISP2", "Manual memory management", "Mark-sweep algorithm", "Memory leak", "Memory management", "Memory management (operating systems)", "Memory management unit", "Memory safety", "Memory segmentation", "Microsoft", "New (C++)", "Page table", "Paging", "Protected mode", "Real mode", "Reference counting", "Region-based memory management", "Stack overflow", "Static memory allocation", "Strong reference", "Translation lookaside buffer", "Unreachable memory", "Virtual 8086 mode", "Virtual memory", "Virtual memory compression", "Weak reference", "X86 memory segmentation"], "categories": ["Automatic memory management", "Memory management algorithms", "Use dmy dates from August 2012"], "title": "Mark-compact algorithm"}
{"summary": "In a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which memory pages to page out (swap out, write to disk) when a page of memory needs to be allocated. Paging happens when a page fault occurs and a free page cannot be used to satisfy the allocation, either because there are none, or because the number of free pages is lower than some threshold.\nWhen the page that was selected for replacement and paged out is referenced again it has to be paged in (read in from disk), and this involves waiting for I/O completion. This determines the quality of the page replacement algorithm: the less time waiting for page-ins, the better the algorithm. A page replacement algorithm looks at the limited information about accesses to the pages provided by hardware, and tries to guess which pages should be replaced to minimize the total number of page misses, while balancing this with the costs (primary storage and processor time) of the algorithm itself.\nThe page replacing problem is a typical online problem from the competitive analysis perspective in the sense that the optimal deterministic algorithm is known.", "links": ["ARM architecture", "Adaptive Replacement Cache", "Adaptive replacement cache", "Amortized analysis", "B\u00e9l\u00e1dy's anomaly", "Cache algorithms", "Clairvoyance", "Computer", "Demand paging", "Digital object identifier", "Dirty bit", "FreeBSD", "Full duplex", "Garbage collection (computer science)", "Hash table", "Intel i860", "Journaling file system", "Kernel (computer science)", "Linux", "Linux kernel", "Locality of reference", "L\u00e1szl\u00f3 B\u00e9l\u00e1dy", "Memory management", "Memory management (operating systems)", "OS/390", "Object-oriented programming", "Online algorithm", "Online problem", "OpenVMS", "Operating system", "Page fault", "Page replacement algorithm", "Paging", "Prefetch input queue", "Scheduling (computing)", "Solaris (operating system)", "The LIRS caching algorithm", "Tree data structure", "VAX", "VAX/VMS", "Virtual memory", "Working set"], "categories": ["All Wikipedia articles needing clarification", "All articles with unsourced statements", "Articles with unsourced statements from June 2008", "Memory management algorithms", "Online algorithms", "Use dmy dates from August 2012", "Virtual memory", "Wikipedia articles needing clarification from August 2011", "Wikipedia articles needing clarification from January 2014"], "title": "Page replacement algorithm"}
{"summary": "Pseudo-LRU is a cache algorithm created to improve the Least Recently Used (LRU) algorithm.\nPLRU usually refers to two cache replacement algorithms: tree-PLRU and bit-PLRU.", "links": ["Algorithm", "Apple Computer", "Binary search tree", "CPU cache", "Cache algorithms", "Computer science", "Freescale", "Intel 486", "PowerPC", "PowerPC G4", "Power Architecture"], "categories": ["All stub articles", "Computer science stubs", "Memory management algorithms", "Use dmy dates from August 2012"], "title": "Pseudo-LRU"}
{"summary": "The SLOB (Simple List Of Blocks) allocator is one of three available memory allocators in the Linux kernel. (The other two are SLAB and SLUB.) The SLOB allocator is designed to require little memory for the implementation and housekeeping, for use in small systems such as embedded systems. Unfortunately, a major limitation of the SLOB allocator is that it suffers greatly from internal fragmentation.\nSLOB currently uses a first-fit algorithm, which uses the first available space for memory. In 2008, a reply from Linus Torvalds on a Linux mailing list was made where he suggested the use of a best-fit algorithm, which tries to find a memory block which suits needs best. Best fit finds the smallest space which fits the required amount available, avoiding loss of performance, both by fragmentation and consolidation of memory.\nBy default, Linux kernel used a SLAB Allocation system until version 2.6.23, when SLUB allocation became the default. When the CONFIG_SLAB flag is disabled, the kernel falls back to using the \"SLOB\" allocator. The SLOB allocator was used in DSLinux on Nintendo DS handheld console.", "links": ["Best fit", "DSLinux", "Dynamic memory allocation", "First fit algorithm", "Handheld console", "Linus Torvalds", "Linux", "Linux kernel", "Nintendo DS", "SLUB (software)", "Slab allocation"], "categories": ["All articles lacking in-text citations", "All articles needing cleanup", "All stub articles", "Articles lacking in-text citations from August 2008", "Articles needing cleanup from August 2008", "Cleanup tagged articles without a reason field from August 2008", "Linux kernel", "Linux stubs", "Memory management algorithms", "Use dmy dates from August 2012", "Wikipedia pages needing cleanup from August 2008"], "title": "SLOB"}
{"summary": "In queueing theory, a discipline within the mathematical theory of probability, the backpressure routing algorithm is a method for directing traffic around a queueing network that achieves maximum network throughput, which is established using concepts of Lyapunov drift. Backpressure routing considers the situation where each job can visit multiple service nodes in the network. It is an extension of max-weight scheduling where rather each job visits only a single service node.", "links": ["Ad hoc On-Demand Distance Vector Routing", "Back pressure", "Drift plus penalty", "ExOR (wireless network protocol)", "FIFO (computing and electronics)", "Geographic routing", "LIFO (computing)", "List of ad hoc routing protocols", "Lyapunov function", "Lyapunov optimization", "Max-weight scheduling", "Mobile ad hoc network", "Probability theory", "Queueing theory", "Wireless sensor network"], "categories": ["Networking algorithms", "Queueing theory", "Routing algorithms"], "title": "Backpressure routing"}
{"summary": "Chung Kwei is a spam filtering algorithm based on the TEIRESIAS Algorithm for finding coding genes within bulk DNA.", "links": ["Algorithm", "CAN-SPAM Act of 2003", "DNA", "DNSBL", "Data structure", "E-mail spam", "Gene", "Mail filter", "SpamAssassin", "Spam (electronic)", "TEIRESIAS Algorithm"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Anti-spam", "Computer science stubs", "Networking algorithms"], "title": "Chung Kwei (algorithm)"}
{"summary": "Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, in order to gradually find an acceptable rate.", "links": ["Algorithm", "Carrier sense multiple access with collision avoidance", "Carrier sense multiple access with collision detection", "Computer networks", "Control theory", "Copyright status of work by the U.S. government", "Data", "Data frame", "Ethernet", "Expected value", "Exponential growth", "Feedback", "General Services Administration", "IEEE 802.3", "Interval (mathematics)", "Media access control", "Network congestion avoidance", "Retransmission (data networks)", "Slot time", "Time", "Triangular number", "Uniform distribution (discrete)"], "categories": ["Ethernet", "Networking algorithms", "Scheduling algorithms", "Use dmy dates from July 2013", "Wikipedia articles incorporating text from the Federal Standard 1037C"], "title": "Exponential backoff"}
{"summary": "The generic cell rate algorithm (GCRA) is a leaky bucket-type scheduling algorithm for the network scheduler that is used in Asynchronous Transfer Mode (ATM) networks. It is used to measure the timing of cells on virtual channels (VCs) and or Virtual Paths (VPs) against bandwidth and jitter limits contained in a traffic contract for the VC or VP to which the cells belong. Cells that do not conform to the limits given by the traffic contract may then be re-timed (delayed) in traffic shaping, or may be dropped (discarded) or reduced in priority (demoted) in traffic policing. Nonconforming cells that are reduced in priority may then be dropped, in preference to higher priority cells, by downstream components in the network that are experiencing congestion. Alternatively they may reach their destination (VC or VP termination) if there is enough capacity for them, despite them being excess cells as far as the contract is concerned: see priority control.\nThe GCRA is given as the reference for checking the traffic on connections in the network, i.e. usage/network parameter control (UPC/NPC) at user\u2013network interfaces (UNI) or inter-network interfaces or network-network interfaces (INI/NNI) . It is also given as the reference for the timing of cells transmitted (ATM PDU Data_Requests) onto an ATM network by a network interface card (NIC) in a host, i.e. on the user side of the UNI . This ensures that cells are not then discarded by UPC/NCP in the network, i.e. on the network side of the UNI. However, as the GCRA is only given as a reference, the network providers and users may use any other algorithm that gives the same result.", "links": ["ATM Forum", "Asynchronous Transfer Mode", "Bandwidth (signal processing)", "Connection admission control", "FPGA", "ITU-T", "Jitter", "Leaky bucket", "Network-to-network interface", "Network interface card", "Network scheduler", "Priority control", "Scheduling algorithm", "Token bucket", "Traffic Contract", "Traffic contract", "Traffic policing (communications)", "Traffic shaping", "UPC and NPC", "User\u2013network interface", "Virtual Paths", "Virtual circuit"], "categories": ["Network scheduling algorithms", "Networking algorithms", "Teletraffic"], "title": "Generic cell rate algorithm"}
{"summary": "Karn's algorithm addresses the problem of getting accurate estimates of the round-trip time for messages when using the Transmission Control Protocol (TCP) in computer networking. The algorithm was proposed by Phil Karn in 1987.\nAccurate round trip estimates in TCP can be difficult to calculate because of an ambiguity created by retransmitted segments. The round trip time is estimated as the difference between the time that a segment was sent and the time that its acknowledgment was returned to the sender, but when packets are re-transmitted there is an ambiguity: the acknowledgment may be a response to the first transmission of the segment or to a subsequent re-transmission.\nKarn's Algorithm ignores retransmitted segments when updating the round trip time estimate. Round trip time estimation is based only on unambiguous acknowledgments, which are acknowledgments for segments that were sent only once.\nThis simplistic implementation of Karn's algorithm can lead to problems as well. Consider what happens when TCP sends a segment after a sharp increase in delay. Using the prior round trip time estimate, TCP computes a timeout and retransmits a segment. If TCP ignores the round trip time of all retransmitted packets, the round trip estimate will never be updated, and TCP will continue retransmitting every segment, never adjusting to the increased delay.\nA solution to this problem is to incorporate transmission timeouts with a timer backoff strategy. The timer backoff strategy computes an initial timeout. If the timer expires and causes a retransmission, TCP increases the timeout generally by a factor of 2. This algorithm has proven to be extremely effective in networks with high packet loss.", "links": ["Computer network", "Douglas E. Comer", "Phil Karn", "PostScript", "Round-trip time", "Transmission Control Protocol"], "categories": ["Networking algorithms", "Transmission Control Protocol", "Wikipedia articles needing page number citations from March 2015"], "title": "Karn's algorithm"}
{"summary": "The Lule\u00e5 algorithm of computer science, designed by Degermark et al. (1997), is a technique for storing and searching internet routing tables efficiently. It is named after the Lule\u00e5 University of Technology, the home institute of the technique's authors. The name of the algorithm does not appear in the original paper describing it, but was used in a message from Craig Partridge to the Internet Engineering Task Force describing that paper prior to its publication.\nThe key task to be performed in internet routing is to match a given IPv4 address (viewed as a sequence of 32 bits) to the longest prefix of the address for which routing information is available. This prefix matching problem may be solved by a trie, but trie structures use a significant amount of space (a node for each bit of each address) and searching them requires traversing a sequence of nodes with length proportional to the number of bits in the address. The Lule\u00e5 algorithm shortcuts this process by storing only the nodes at three levels of the trie structure, rather than storing the entire trie.\nThe main advantage of the Lule\u00e5 algorithm for the routing task is that it uses very little memory, averaging 4\u20135 bytes per entry for large routing tables. This small memory footprint often allows the entire data structure to fit into the routing processor's cache, speeding operations. However, it has the disadvantage that it cannot be modified easily: small changes to the routing table may require most or all of the data structure to be reconstructed.\nThe Lule\u00e5 algorithm is patented in the United States (Degermark et al. 2001).", "links": ["Binary search", "Bit vector", "Computer science", "Craig Partridge", "Data", "Digital object identifier", "IP address", "IPv4", "International Standard Book Number", "Internet", "Internet Engineering Task Force", "Lule\u00e5 University of Technology", "Node (computer science)", "Prefix", "Routing table", "Sequential search", "Software patent", "Trie", "Word (data type)"], "categories": ["Internet architecture", "Networking algorithms", "Routing algorithms", "Routing software"], "title": "Lule\u00e5 algorithm"}
{"summary": "Nagle's algorithm, named after John Nagle, is a means of improving the efficiency of TCP/IP networks by reducing the number of packets that need to be sent over the network.\nNagle's document, Congestion Control in IP/TCP Internetworks (RFC 896) describes what he called the \"small packet problem\", where an application repeatedly emits data in small chunks, frequently only 1 byte in size. Since TCP packets have a 40 byte header (20 bytes for TCP, 20 bytes for IPv4), this results in a 41 byte packet for 1 byte of useful information, a huge overhead. This situation often occurs in Telnet sessions, where most keypresses generate a single byte of data that is transmitted immediately. Worse, over slow links, many such packets can be in transit at the same time, potentially leading to congestion collapse.\nNagle's algorithm works by combining a number of small outgoing messages, and sending them all at once. Specifically, as long as there is a sent packet for which the sender has received no acknowledgment, the sender should keep buffering its output until it has a full packet's worth of output, so that output can be sent all at once.", "links": ["ACK (TCP)", "Bandwidth (computing)", "Byte", "Congestion collapse", "IPv4", "International Standard Book Number", "Larry L. Peterson", "Latency (engineering)", "Maximum segment size", "TCP/IP", "TCP delayed acknowledgment", "Telnet", "Transmission Control Protocol", "User Datagram Protocol"], "categories": ["All articles needing additional references", "Articles needing additional references from June 2014", "Networking algorithms", "Transmission Control Protocol"], "title": "Nagle's algorithm"}
{"summary": "In Internet routers, active queue management (AQM) is the intelligent drop of network packets inside a buffer associated with a network interface controller (NIC), when that buffer becomes full or gets close to becoming full, often with the larger goal of reducing network congestion. This task is performed by the network scheduler, which for this purpose uses various algorithms such as random early detection (RED), Explicit Congestion Notification (ECN), or controlled delay (CoDel). RFC 7567 recommends active queue management as a best practice.", "links": ["Blue (queue management algorithm)", "Bufferbloat", "CoDel", "Controlled Delay", "Denial-of-Service", "Digital object identifier", "Explicit Congestion Notification", "Internet", "NS-2", "Network interface controller", "Network latency", "Network scheduler", "PI controller", "RED with Preferential Dropping", "RRED", "Random Exponential Marking", "Random early detection", "Robust random early detection", "Router (computing)", "TCP global synchronization"], "categories": ["All articles needing additional references", "Articles needing additional references from December 2009", "Network performance", "Network scheduling algorithms", "Packets (information technology)"], "title": "Active queue management"}
{"summary": "Class-based queuing (CBQ) is a queuing discipline for the network scheduler that allows traffic to share bandwidth equally, after being grouped by classes. The classes can be based upon a variety of parameters, such as priority, interface, or originating program.\nCBQ is a traffic management algorithm developed by the Network Research Group at Lawrence Berkeley National Laboratory as an alternative to traditional router-based technology. Now in the public domain as an open technology, CBQ is deployed by companies at the boundary of their WANs.\nCBQ divides user traffic into a hierarchy of classes based on any combination of IP addresses, protocols and application types. A company's accounting department, for example, may not need the same Internet access privileges as the engineering department. Because every company is organized differently and has different policies and business requirements, it is vital for traffic management technology to provide flexibility and granularity in classifying traffic flows.\nCBQ lets network managers classify traffic in a multilevel hierarchy. For instance, some companies may first identify the overall needs of each department or business group, and then define the requirements of each application or group of applications within each department. For performance and architectural reasons, traditional router-based queuing schemes are limited to a small number of classes and only allow one-dimensional classification.\nBecause it operates at the IP network layer, CBQ provides the same benefits across any Layer 2 technology and is equally effective with any IP protocol, such as Transmission Control Protocol (TCP) and User Datagram Protocol (UDP). It also operates with any client or server TCP/IP stack variation, since it takes advantage of standard TCP/IP flow control mechanisms to control end-to-end traffic.\nAn implementation is available under the GNU General Public License for the Linux kernel.", "links": ["Bandwidth (computing)", "Data link layer", "GNU General Public License", "Internet Protocol", "Kernel.org", "Linux kernel", "Network scheduler", "Queuing discipline", "Transmission Control Protocol", "User Datagram Protocol"], "categories": ["All stub articles", "Computer network stubs", "Computer networking", "Network scheduling algorithms"], "title": "Class-based queueing"}
{"summary": "Deficit Round Robin (DRR), also Deficit Weighted Round Robin (DWRR), is a scheduling algorithm for the network scheduler. DRR is, like weighted fair queuing (WFQ), a packet-based implementation of the ideal Generalized Processor Sharing (GPS) policy. It was proposed by M. Shreedhar and G. Varghese in 1995 as an efficient (with O(1) complexity) and fair algorithm.", "links": ["DWRR", "DWRR-FM", "Digital object identifier", "Fair Queuing", "Fairness measure", "Flow (computer networking)", "GNU General Public License", "Generalized Processor Sharing", "Generalized processor sharing", "George Varghese", "International Standard Book Number", "International Standard Serial Number", "Kernel.org", "Linux kernel", "Network scheduler", "Scheduling algorithm", "Weighted Fair Queuing", "Weighted fair queuing", "Weighted round robin"], "categories": ["Network scheduling algorithms"], "title": "Deficit round robin"}
{"summary": "In computer networking, delay-gradient congestion control refers to a class of congestion control algorithms, which react to the differences in round-trip delay time (RTT), as opposed to classical congestion control methods, which react to packet loss or an RTT threshold being exceeded. Such algorithms include CAIA Delay-Gradient (CDG) and TIMELY.", "links": ["Computer network", "Congestion control algorithm", "LWN.net", "Packet loss", "Punctuality", "Round-trip delay time", "SIGCOMM", "TCP congestion-avoidance algorithm", "TIMELY", "Taxonomy of congestion control"], "categories": ["All stub articles", "Computer network stubs", "Network performance", "Network scheduling algorithms"], "title": "Delay-gradient congestion control"}
{"summary": "Fair queuing is a family of scheduling algorithms used in some process and network schedulers. The concept implies a separate data packet queue (or job queue) for each traffic flow (or for each program process) as opposed to the traditional approach with one FIFO queue for all packet flows (or for all process jobs). The purpose is to achieve fairness when a limited resource is shared, for example to avoid that flows with large packets (or processes that generate small jobs) achieve more throughput (or CPU time) than other flows (or processes).\nFair queuing is implemented in some advanced packet switches and routers.", "links": ["Active queue management", "Bit rate", "Buffer (computer science)", "Bufferbloat", "Deficit round robin", "Digital object identifier", "FIFO (computing and electronics)", "Fairness measure", "Flow (computer networking)", "Generalized processor sharing", "IEEE Transactions on Communications", "IETF", "International Standard Book Number", "Internet", "Local area network", "Max-min fairness", "Net neutrality", "Network scheduler", "Packet (information technology)", "Packet switch", "Process scheduler", "Quality of service", "Request for Comments", "Round-robin scheduling", "Router (computing)", "Scheduling algorithm", "Statistical multiplexer", "Statistical multiplexing", "Traffic flow (computer networking)", "Traffic shaping", "Weighted fair queuing", "Weighted round robin"], "categories": ["Network scheduling algorithms"], "title": "Fair queuing"}
{"summary": "The hierarchical fair-service curve (HFSC) is a network scheduling algorithm for a network scheduler proposed by Ion Stoica, Hui Zhang and T. S. Eugene from Carnegie Mellon University at SIGCOMM 1997\nIt is based on a QoS and CBQ. An implementation of HFSC is available in all operating systems based on the Linux kernel, such as e.g. OpenWrt, and also in DD-WRT, NetBSD 5.0, FreeBSD 8.0 and OpenBSD 4.6.", "links": ["Algorithm", "Carnegie Mellon University", "Class-based queueing", "Computer networking", "DD-WRT", "FreeBSD", "Kernel.org", "Linux kernel", "NetBSD", "Network scheduler", "OpenBSD", "OpenWrt", "Quality of service", "SIGCOMM", "Software"], "categories": ["All stub articles", "Network performance", "Network scheduling algorithms", "Network software stubs"], "title": "Hierarchical fair-service curve"}
{"summary": "Interleaved Polling with Adaptive Cycle Time (IPACT) is an algorithm designed by Glen Kramer, Biswanath Mukherjee and Gerry Pesavento Advanced Technology Lab.at the University of California, Davis. IPACT is a dynamic bandwidth allocation algorithm for use in Ethernet passive optical networks (EPONs).\nIPACT uses the Gate and Report messages provided by the EPON Multi-Point Control Protocol (MPCP) to allocate bandwidth to Optical Network Units (ONUs). If the optical line terminal grants bandwidth to an ONU and waits until it has received that particular ONU's transmission before granting bandwidth to another ONU, then time equivalent to a whole messaging round-trip is wasted during which the upstream may remain idle. IPACT eliminates this idle time by sending downstream grant messages to succeeding ONUs while receiving transmissions from previously granted ONUs. It accomplishes this by calculating the time at which a transmission grant allocated to a previous ONU ends.", "links": ["Algorithm", "Bandwidth (signal processing)", "Downstream (computer science)", "Dynamic bandwidth allocation", "Ethernet", "Optical line terminal", "Passive optical network", "Transmission (telecommunications)", "University of California, Davis"], "categories": ["All articles lacking sources", "All stub articles", "Articles lacking sources from June 2007", "Computer network stubs", "Network scheduling algorithms"], "title": "Interleaved polling with adaptive cycle time"}
{"summary": "The leaky bucket is an algorithm used in packet switched computer networks and telecommunications networks. It can be used to check that data transmissions, in the form of packets, conform to defined limits on bandwidth and burstiness (a measure of the unevenness or variations in the traffic flow). It can also be used as a scheduling algorithm to determine the timing of transmissions that will comply with the limits set for the bandwidth and burstiness: see network scheduler. The leaky bucket algorithm is also used in leaky bucket counters, e.g. to detect when the average or peak rate of random or stochastic events or stochastic processes exceed defined limits.\nA version of the leaky bucket, the Generic Cell Rate Algorithm, is recommended for Asynchronous Transfer Mode (ATM) networks in Usage/Network Parameter Control at User\u2013Network Interfaces or Inter-Network Interfaces or Network-Network Interfaces to protect a network from excessive traffic levels on connections routed through it. The Generic Cell Rate Algorithm, or an equivalent, may also be used to shape transmissions by a Network Interface Card onto an ATM network (i.e. on the user side of the User-Network Interface), e.g. to levels below the levels set for Usage/Network Parameter Control in the network to prevent it taking action to further limit that connection.", "links": ["ATM Forum", "Algorithm", "Analogy", "Andrew S. Tanenbaum", "Asynchronous Transfer Mode", "Bandwidth (computing)", "Bandwidth management", "Bit rate", "Bucket", "Burst transmission", "Computer network", "Congestion avoidance", "Data Link Layer", "Data transmission", "Ethernet II framing", "FIFO (computing and electronics)", "Fluid queue", "Frame synchronization", "GCRA", "Generic Cell Rate Algorithm", "ITU-T", "International Organization for Standardization", "Jitter", "Jonathan S. Turner", "Leaky bucket (economics)", "Network-to-network interface", "Network Interface Card", "Network Layer", "Network packet", "Network scheduler", "Network traffic measurement", "OSI model", "Packet-switching", "Protocol data unit", "Queueing theory", "Random", "Scheduling (computing)", "Scheduling algorithm", "Segmentation and Reassembly", "Sic", "Stochastic", "Stochastic processes", "Telecommunication", "Token bucket", "Traffic contract", "Traffic policing (communications)", "Traffic shaping", "UPC and NPC", "User\u2013network interface", "Variable bitrate", "Volume"], "categories": ["Network scheduling algorithms"], "title": "Leaky bucket"}
{"summary": "In communication networks, multiplexing and the division of scarce resources, max-min fairness is said to be achieved by an allocation if and only if the allocation is feasible and an attempt to increase the allocation of any participant necessarily results in the decrease in the allocation of some other participant with an equal or smaller allocation.\nIn best-effort statistical multiplexing, a first-come first-served (FCFS) scheduling policy is often used. The advantage with max-min fairness over FCFS is that it results in traffic shaping, meaning that an ill-behaved flow, consisting of large data packets or bursts of many packets, will only punish itself and not other flows. Network congestion is consequently to some extent avoided.\nFair queuing is an example of a max-min fair packet scheduling algorithm for statistical multiplexing and best effort packet-switched networks, since it gives scheduling priority to users that have achieved lowest data rate since they became active. In case of equally sized data packets, round-robin scheduling is max-min fair.", "links": ["Best-effort", "Best effort", "Bottleneck (network)", "Communication network", "Data flow", "Fair queuing", "Fairness measure", "First-come first-served", "Load balancing (computing)", "Maximum throughput resource management", "Multiplexing", "Network congestion", "Packet-switched network", "Proportional fairness", "Resource starvation", "Round-robin scheduling", "Scheduling (computing)", "Statistical multiplexing", "System spectral efficiency", "Traffic shaping"], "categories": ["Network scheduling algorithms", "Routing algorithms", "Wikipedia articles needing clarification from November 2012"], "title": "Max-min fairness"}
{"summary": "On a node in packet switching communication network, a network scheduler, also called packet scheduler, is an arbiter program that manages the sequence of network packets in the transmit and receive queues of the network interface controller, which is a circular data buffer. There are several network schedulers available for the different operating system kernels, that implement many of the existing network scheduling algorithms.\nThe network scheduler logic decides, in a way similar to statistical multiplexers, which network packet to forward next from the buffer. The buffer works as a queuing system, storing the network packets temporarily until they are transmitted. The buffer space may be divided into different queues, with each of them holding the packets of one flow according to configured packet classification rules; for example, packets can be divided into flows by their source and destination IP addresses. Network scheduling algorithms and their associated settings determine how the network scheduler manages the buffer.\nAlso, network schedulers are enabling accomplishment of the active queue management and traffic shaping.", "links": ["ALTQ", "Acknowledgement (data networks)", "Active queue management", "Adaptive virtual queue", "Algorithm", "Arbiter (electronics)", "Bandwidth management", "Berkeley Software Distribution", "Blue (queue management algorithm)", "Buffer (telecommunication)", "Bufferbloat", "Circular buffer", "Class-based queueing", "CoDel", "Credit-based fair queuing", "Data buffer", "Data link layer", "Deficit round robin", "Differentiated services", "Ethernet frame", "FIFO (computing and electronics)", "Fair queuing", "Free and open-source software", "GNU General Public License", "Generalized random early detection", "Generic cell rate algorithm", "Heavy-hitter filter", "Hierarchical fair-service curve", "Ifconfig", "Integrated services", "Iproute2", "Kernel.org", "Kernel (computing)", "Latency (engineering)", "Linux kernel", "Linux kernel module", "Netfilter", "Network congestion", "Network interface controller", "Network latency", "Network packet", "Nftables", "OSI model", "OpenWrt", "Operating system", "Packet-switched network", "Packet delay variation", "Packet switching", "Process scheduler", "Proportional integral controller enhanced", "Quality of service", "Queue (abstract data type)", "Queueing theory", "Quick fair queueing", "Random early detection", "Ring buffer", "Robust random early detection", "Round-robin scheduling", "Router (computing)", "Statistical multiplexer", "Statistical time division multiplexing", "Stochastic fairness queuing", "Systemd", "Tail drop", "Throughput", "Token Bucket", "Token bucket filter", "Traffic classification", "Traffic shaping", "Transmission Control Protocol", "Trivial link equalizer", "Type of service", "Voice over IP", "Weighted fair queuing", "Weighted random early detection", "Weighted round robin"], "categories": ["All articles with unsourced statements", "All pages needing cleanup", "Articles needing cleanup from September 2014", "Articles with sections that need to be turned into prose from September 2014", "Articles with unsourced statements from September 2014", "Linux kernel features", "Network performance", "Network scheduling algorithms", "Network theory", "Wikipedia articles needing clarification from June 2014"], "title": "Network scheduler"}
{"summary": "Proportional fair is a compromise-based scheduling algorithm. It is based upon maintaining a balance between two competing interests: Trying to maximize total [wired/wireless network] throughput while at the same time allowing all users at least a minimal level of service. This is done by assigning each data flow a data rate or a scheduling priority (depending on the implementation) that is inversely proportional to its anticipated resource consumption.", "links": ["CDMA", "Digital object identifier", "Dynamic Channel Allocation", "EVDO", "Link adaptation", "Proportional division", "Round-robin scheduling", "Scheduling (computing)", "Scheduling algorithm", "Signal-to-noise ratio", "Transmit power control", "Weighted fair queuing"], "categories": ["Mobile telecommunications", "Network scheduling algorithms", "Pages containing cite templates with deprecated parameters", "Pages using citations with accessdate and no URL", "Radio resource management", "Use dmy dates from September 2010", "Use dmy dates from September 2013", "Wireless"], "title": "Proportionally fair"}
{"summary": "Random early detection (RED), also known as random early discard or random early drop is a queueing discipline for a network scheduler suited for congestion avoidance.\nIn the conventional tail drop algorithm, a router or other network component buffers as many packets as it can, and simply drops the ones it cannot buffer. If buffers are constantly full, the network is congested. Tail drop distributes buffer space unfairly among traffic flows. Tail drop can also lead to TCP global synchronization as all TCP connections \"hold back\" simultaneously, and then step forward simultaneously. Networks become under-utilized and flooded by turns. RED addresses these issues.", "links": ["Active Queue Management", "Active queue management", "Blue (queue management algorithm)", "Computer networking device", "Differentiated Services Code Point", "Digital object identifier", "Explicit Congestion Notification", "International Standard Book Number", "Network congestion", "Network congestion avoidance", "Network scheduler", "Probability", "Quality of service", "Robust random early detection", "Router (computing)", "TCP global synchronization", "Tail drop", "Transmission Control Protocol", "Type of Service", "Van Jacobson", "Weighted random early detection"], "categories": ["Network performance", "Network scheduling algorithms"], "title": "Random early detection"}
{"summary": "Round-robin (RR) is one of the algorithms employed by process and network schedulers in computing. As the term is generally used, time slices are assigned to each process in equal portions and in circular order, handling all processes without priority (also known as cyclic executive). Round-robin scheduling is simple, easy to implement, and starvation-free. Round-robin scheduling can also be applied to other scheduling problems, such as data packet scheduling in computer networks. It is an Operating System concept.\nThe name of the algorithm comes from the round-robin principle known from other fields, where each person takes an equal share of something in turn.", "links": ["Abraham Silberschatz", "Adversarial queueing network", "Arrival theorem", "BCMP network", "Balance equation", "Bene\u0161 method", "Best-effort", "Bulk queue", "Burke's theorem", "Buzen's algorithm", "CPU", "Channel access", "Computing", "Continuous-time Markov chain", "Cyclic executive", "D/M/1 queue", "Data buffer", "Decomposition method (queueing theory)", "Deficit round robin", "Erlang (unit)", "Erlang distribution", "FIFO (computing and electronics)", "Fair queuing", "First-come first-served", "Flow-equivalent server method", "Flow control (data)", "Fluid limit", "Fluid queue", "Fork\u2013join queue", "G-network", "G/G/1 queue", "G/M/1 queue", "Gordon\u2013Newell theorem", "Heavy traffic approximation", "Information system", "International Standard Book Number", "Jackson network", "John Wiley & Sons", "Kelly network", "Kendall's notation", "Kingman's formula", "LIFO (computing)", "Layered queueing network", "Lindley equation", "Link adaptation", "Little's law", "Loss network", "M/D/1 queue", "M/D/c queue", "M/G/1 queue", "M/G/k queue", "M/M/1 queue", "M/M/c queue", "M/M/\u221e queue", "Markovian arrival process", "Matrix analytic method", "Max-min fairness", "Maximum throughput scheduling", "Mean field theory", "Mean value analysis", "Message queue", "Multilevel queue", "Multiple access", "Network congestion", "Network scheduler", "Operating System Concepts", "Operating system", "Packet switching", "Pipeline (software)", "Poisson process", "Pollaczek\u2013Khinchine formula", "Polling (computer science)", "Polling system", "Preemption (computing)", "Process scheduler", "Processor sharing", "Product-form solution", "Proportionally fair", "Quality of service", "Quasireversibility", "Queueing theory", "Rational arrival process", "Reflected Brownian motion", "Resource starvation", "Retrial queue", "Round-robin (disambiguation)", "Scheduling (computing)", "Scheduling starvation", "Shortest job first", "Shortest remaining time", "Statistical multiplexing", "System spectrum efficiency", "Teletraffic engineering", "Time-sharing", "Token passing", "Token ring", "Traffic equations", "Weighted fair queuing", "Weighted round robin", "Work-conserving"], "categories": ["All articles needing additional references", "Articles needing additional references from April 2015", "CS1 errors: external links", "Network scheduling algorithms", "Processor scheduling algorithms"], "title": "Round-robin scheduling"}
{"summary": "The token bucket is an algorithm used in packet switched computer networks and telecommunications networks. It can be used to check that data transmissions, in the form of packets, conform to defined limits on bandwidth and burstiness (a measure of the unevenness or variations in the traffic flow). It can also be used as a scheduling algorithm to determine the timing of transmissions that will comply with the limits set for the bandwidth and burstiness: see network scheduler.", "links": ["Algorithm", "Analogy", "Bandwidth (computing)", "Bandwidth management", "Bucket", "Burst transmission", "Class-based queueing", "Computer network", "Congestion avoidance", "Data transmission", "Download", "Host (network)", "International Standard Book Number", "Leaky bucket", "Linux", "Network interface controller", "Network packet", "Network scheduler", "Network traffic measurement", "Packet-switching", "Protocol data unit", "Queuing discipline", "Rate limiting", "Scheduling algorithm", "Telecommunication", "Traffic policing (communications)", "Traffic shaping", "Type\u2013token distinction", "Upload"], "categories": ["All articles to be expanded", "Articles to be expanded from June 2008", "Network performance", "Network scheduling algorithms"], "title": "Token bucket"}
{"summary": "DUAL, the Diffusing Update ALgorithm, is the algorithm used by Cisco's EIGRP routing protocol to ensure that a given route is recalculated globally whenever it might cause a routing loop. It was developed by J.J. Garcia-Luna-Aceves at SRI International. According to Cisco, the full name of the algorithm is DUAL finite-state machine (DUAL FSM). EIGRP is responsible for the routing within an autonomous system and DUAL responds to changes in the routing topology and dynamically adjusts the routing tables of the router automatically.\nEIGRP uses a feasibility condition to ensure that only loop-free routes are ever selected. The feasibility condition is conservative: when the condition is true, no loops can occur, but the condition might under some circumstances reject all routes to a destination although some are loop-free.\nWhen no feasible route to a destination is available, the DUAL algorithm  invokes a Diffusing Computation  to ensure that all traces of the problematic route are eliminated from the network. At which point the normal Bellman\u2013Ford algorithm is used to recover a new route.", "links": ["Algorithm", "Bellman\u2013Ford algorithm", "Cisco", "EIGRP", "Finite-state machine", "J.J. Garcia-Luna-Aceves", "SRI International"], "categories": ["Routing algorithms", "Routing protocols", "SRI International software"], "title": "Diffusing update algorithm"}
{"summary": "In computer communication theory relating to packet-switched networks, a distance-vector routing protocol is one of the two major classes of intra domain routing protocols, the other major class being the link-state protocol. Distance-vector routing protocols use the Bellman\u2013Ford algorithm, Ford\u2013Fulkerson algorithm, or DUAL FSM (in the case of Cisco Systems's protocols) to calculate paths.\nA distance-vector routing protocol requires that a router inform its neighbors of topology changes periodically. Compared to link-state protocols, which require a router to inform all the nodes in a network of topology changes, distance-vector routing protocols have less computational complexity and message overhead.\nThe term distance vector refers to the fact that the protocol manipulates vectors (arrays) of distances to other nodes in the network. The vector distance algorithm was the original ARPANET routing algorithm and was also used in the internet under the name of RIP (Routing Information Protocol).\nExamples of distance-vector routing protocols include RIPv1 and RIPv2 and IGRP.", "links": ["Analysis of algorithms", "Array data structure", "Babel (protocol)", "Bellman\u2013Ford algorithm", "Cisco Systems", "Computer communication", "DSDV", "DUAL FSM", "EIGRP", "Ford\u2013Fulkerson algorithm", "Interior Gateway Routing Protocol", "Link-state protocol", "Link-state routing protocol", "Message overhead", "OSPF", "Packet-switched network", "Route poisoning", "Routing Information Protocol", "Routing loop", "Routing protocol", "Split horizon route advertisement"], "categories": ["All articles lacking in-text citations", "All articles needing expert attention", "All articles that are too technical", "All articles with unsourced statements", "Articles lacking in-text citations from September 2010", "Articles needing expert attention from November 2013", "Articles with unsourced statements from January 2009", "Articles with unsourced statements from October 2015", "Routing algorithms", "Routing protocols", "Wikipedia articles that are too technical from November 2013"], "title": "Distance-vector routing protocol"}
{"summary": "Edge disjoint shortest pair algorithm is an algorithm in computer network routing. The algorithm is used for generating the shortest pair of edge disjoint paths between a given pair of vertices as follows:\nRun the shortest path algorithm for the given pair of vertices\nReplace each edge of the shortest path (equivalent to two oppositely directed arcs) by a single arc directed towards the source vertex\nMake the length of each of the above arcs negative\nRun the shortest path algorithm (Note: the algorithm should accept negative costs)\nErase the overlapping edges of the two paths found, and reverse the direction of the remaining arcs on the first shortest path such that each arc on it is directed towards the sink vertex now. The desired pair of paths results.\nSuurballe's algorithm solves the same problem more quickly by reweighting the edges of the graph to avoid negative costs, allowing Dijkstra's algorithm to be used for both shortest path steps.", "links": ["Algorithm", "Computer network", "Dijkstra's algorithm", "International Standard Book Number", "Routing", "Suurballe's algorithm", "Vertex (graph theory)"], "categories": ["All articles needing additional references", "All stub articles", "Articles needing additional references from January 2010", "Computer network stubs", "Routing algorithms"], "title": "Edge disjoint shortest pair algorithm"}
{"summary": "The ETX metric, or expected transmission count, is a measure of the quality of a path between two nodes in a wireless packet data network. It is used extensively in mesh networking algorithms.", "links": ["Algorithm", "Meraki", "Mesh networking", "Optimized Link State Routing Protocol", "RoofNet"], "categories": ["Routing algorithms", "Wireless networking"], "title": "Expected transmission count"}
{"summary": "Flooding is a simple computer network routing algorithm in which every incoming packet is sent through every outgoing link except the one it arrived on.\nFlooding is used in bridging and in systems such as Usenet and peer-to-peer file sharing and as part of some routing protocols, including OSPF, DVMRP, and those used in ad-hoc wireless networks.", "links": ["Ad-hoc wireless network", "Bridging (networking)", "Broadcasting (networking)", "Computer network", "DVMRP", "Denial of service attack", "Flood search routing", "Flooding algorithm", "International Standard Book Number", "Multicast", "Network topology", "Open Shortest Path First", "Packet (information technology)", "Peer-to-peer file sharing", "Ping flood", "Routing", "Routing protocol", "Time to live", "Usenet"], "categories": ["All articles needing additional references", "All articles with unsourced statements", "Articles needing additional references from October 2015", "Articles with unsourced statements from January 2012", "Routing algorithms"], "title": "Flooding (computer networking)"}
{"summary": "Geographic routing (also called georouting or position-based routing) is a routing principle that relies on geographic position information. It is mainly proposed for wireless networks and based on the idea that the source sends a message to the geographic location of the destination instead of using the network address. The idea of using position information for routing was first proposed in the 1980s in the area of packet radio networks  and interconnection networks. Geographic routing requires that each node can determine its own location and that the source is aware of the location of the destination. With this information a message can be routed to the destination without knowledge of the network topology or a prior route discovery.\nThere are various approaches, such as single-path, multi-path and flooding-based strategies (see  for a survey). Most single-path strategies rely on two techniques: greedy forwarding and face routing. Greedy forwarding tries to bring the message closer to the destination in each step using only local information. Thus, each node forwards the message to the neighbor that is most suitable from a local point of view. The most suitable neighbor can be the one who minimizes the distance to the destination in each step (Greedy). Alternatively, one can consider another notion of progress, namely the projected distance on the source-destination-line (MFR, NFP), or the minimum angle between neighbor and destination (Compass Routing). Not all of these strategies are loop-free, i.e. a message can circulate among nodes in a certain constellation. It is known that the basic greedy strategy and MFR are loop free, while NFP and Compass Routing are not .\n\nGreedy forwarding can lead into a dead end, where there is no neighbor closer to the destination. Then, face routing helps to recover from that situation and find a path to another node, where greedy forwarding can be resumed. A recovery strategy such as face routing is necessary to assure that a message can be delivered to the destination. The combination of greedy forwarding and face routing was first proposed in 1999 under the name GFG (Greedy-Face-Greedy). It guarantees delivery in the so-called unit disk graph network model. Various variants, which were proposed later  , also for non-unit disk graphs, are based on the principles of GFG .\nAlthough originally developed as a routing scheme that uses the physical positions of each node, geographic routing algorithms have also been applied to networks in which each node is associated with a point in a virtual space, unrelated to its physical position. The process of finding a set of virtual positions for the nodes of a network such that geographic routing using these positions is guaranteed to succeed is called greedy embedding.", "links": ["Backpressure Routing", "Christos Papadimitriou", "Digital object identifier", "Flooding algorithm", "Greedy embedding", "Ion Stoica", "Jit Bose", "Jorge Urrutia Galicia", "List of ad-hoc routing protocols", "Node (networking)", "PDF", "Routing", "Scott Shenker", "Wireless network"], "categories": ["Pages containing cite templates with deprecated parameters", "Pages with citations having bare URLs", "Pages with citations lacking titles", "Routing algorithms", "Routing protocols", "Wireless networking"], "title": "Geographic routing"}
{"summary": "Link-state routing protocols are one of the two main classes of routing protocols used in packet switching networks for computer communications, the other being distance-vector routing protocols. Examples of link-state routing protocols include open shortest path first (OSPF) and intermediate system to intermediate system (IS-IS).\nThe link-state protocol is performed by every switching node in the network (i.e., nodes that are prepared to forward packets; in the Internet, these are called routers). The basic concept of link-state routing is that every node constructs a map of the connectivity to the network, in the form of a graph, showing which nodes are connected to which other nodes. Each node then independently calculates the next best logical path from it to every possible destination in the network. The collection of best paths will then form the node's routing table.\nThis contrasts with distance-vector routing protocols, which work by having each node share its routing table with its neighbours. In a link-state protocol the only information passed between nodes is connectivity related.\nLink-state algorithms are sometimes characterized informally as each router 'telling the world about its neighbours\"", "links": ["Ad hoc routing protocol list", "Algorithm", "BBN Technologies", "Bolt, Beranek and Newman", "Cisco Systems", "Computer communication", "Dijkstra's algorithm", "Distance-vector routing protocol", "Enhanced interior gateway routing protocol", "Ethernet", "Graph theory", "Greedy Algorithm", "IEEE", "IEEE 802.1aq", "IS-IS", "Internet", "Internet Engineering Task Force", "John M. McQuillan", "Layer 2", "Link-state advertisement", "Mobile ad hoc network", "Multipoint relay", "Open shortest path first", "Optimized Link State Routing Protocol", "Optimized link state routing protocol", "Packet switching", "Radia Perlman", "Router (computing)", "Routing Bridge", "Routing protocol", "Routing table", "Shortest Path Bridging", "Shortest path problem", "TRILL (Computer Networking)", "Tree data structure", "Wireless ad hoc network", "Wireless mesh network"], "categories": ["All articles lacking in-text citations", "All articles with unsourced statements", "Articles lacking in-text citations from September 2010", "Articles with unsourced statements from February 2013", "Routing algorithms", "Routing protocols"], "title": "Link-state routing protocol"}
{"summary": "The MENTOR routing algorithm is an algorithm for use in routing of mesh networks, specifically pertaining to their initial topology. It was developed in 1991 by Aaron Kershenbaum, Parviz Kermani, and George A. Grove and was published by the IEEE.", "links": ["Algorithm", "Dijkstra's algorithm", "Heuristic", "Mesh networks", "Minimum spanning tree", "Network Topology", "Prim's algorithm", "Quadratic function", "Routing"], "categories": ["All orphaned articles", "Orphaned articles from February 2009", "Routing algorithms"], "title": "MENTOR routing algorithm"}
{"summary": "In network science started in, the optimization mechanism is a network growth algorithm, which randomly places new nodes in the system, and connects them to the existing nodes based on a cost-benefit analysis. Depending on the parameters used in the optimization mechanism, the algorithm can build three types of networks: a star network, a random network, and a scale-free network. Optimization mechanism is thought to be the underlying mechanism in several real networks, such as transportation networks, power grid, router networks, the network of highways, etc.", "links": ["Algorithm", "Node (networking)", "Randomness", "Star network"], "categories": ["All articles needing additional references", "All orphaned articles", "Articles needing additional references from August 2014", "Networks", "Orphaned articles from August 2014", "Routing algorithms"], "title": "Optimization mechanism"}
{"summary": "The Temporally Ordered Routing Algorithm (TORA) is an algorithm for routing data across Wireless Mesh Networks or Mobile ad hoc networks.\nIt was developed by Vincent Park and Scott Corson at the University of Maryland and the Naval Research Laboratory. Park has patented his work, and it was licensed by Nova Engineering, who are marketing a wireless router product based on Park's algorithm.", "links": ["Algorithm", "Directed acyclic graph", "Information", "Mobile ad hoc network", "Naval Research Laboratory", "Nova Engineering", "Router (computing)", "Routing", "Scalability", "Shortest path problem", "University of Maryland, College Park", "Vincent Park", "Wireless", "Wireless mesh network"], "categories": ["Ad hoc routing protocols", "All articles lacking sources", "Articles lacking sources from July 2013", "Routing algorithms", "Wireless networking"], "title": "Temporally ordered routing algorithm"}
{"summary": "In mathematics, the Odlyzko\u2013Sch\u00f6nhage algorithm is a fast algorithm for evaluating the Riemann zeta function at many points, introduced by (Odlyzko & Sch\u00f6nhage 1988). The main point is the use of the fast Fourier transform to speed up the evaluation of a finite Dirichlet series of length N at O(N) equally spaced values from O(N2) to O(N1+\u03b5) steps (at the cost of storing O(N1+\u03b5) intermediate values). The Riemann\u2013Siegel formula used for calculating the Riemann zeta function with imaginary part T uses a finite Dirichlet series with about N = T1/2 terms, so when finding about N values of the Riemann zeta function it is sped up by a factor of about T1/2. This reduces the time to find the zeros of the zeta function with imaginary part at most T from about T3/2+\u03b5 steps to about T1+\u03b5 steps.\nThe algorithm can be used not just for the Riemann zeta function, but also for many other functions given by Dirichlet series.\nThe algorithm was used by Gourdon (2004) to verify the Riemann hypothesis for the first 1013 zeros of the zeta function.", "links": ["Algorithm", "Andrew Odlyzko", "Arnold Sch\u00f6nhage", "Data structure", "Digital object identifier", "Dirichlet series", "Fast Fourier transform", "JSTOR", "Mathematical Reviews", "Odlyzko", "Riemann hypothesis", "Riemann zeta function", "Riemann\u2013Siegel formula", "Sch\u00f6nhage"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Analytic number theory", "Computational number theory", "Computer science stubs", "Zeta and L-functions"], "title": "Odlyzko\u2013Sch\u00f6nhage algorithm"}
{"summary": "In mathematics, ancient Egyptian multiplication (also known as Egyptian multiplication, Ethiopian multiplication, Russian multiplication, or peasant multiplication), one of two multiplication methods used by scribes, was a systematic method for multiplying two numbers that does not require the multiplication table, only the ability to multiply and divide by 2, and to add. It decomposes one of the multiplicands (generally the larger) into a sum of powers of two and creates a table of doublings of the second multiplicand. This method may be called mediation and duplation, where mediation means halving one number and duplation means doubling the other number. It is still used in some areas.\nThe second Egyptian multiplication and division technique was known from the hieratic Moscow and Rhind Mathematical Papyri written in the seventeenth century B.C. by the scribe Ahmes.\nAlthough in ancient Egypt the concept of base 2 did not exist, the algorithm is essentially the same algorithm as long multiplication after the multiplier and multiplicand are converted to binary. The method as interpreted by conversion to binary is therefore still in wide use today as implemented by binary multiplier circuits in modern computer processors.", "links": ["AKS primality test", "Addition", "Adleman\u2013Pomerance\u2013Rumely primality test", "Ahmes", "Akhmim Wooden Tablet", "Algorithm", "Aliquot part", "Ancient Egypt", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Battiscombe Gunn", "Bharati Krishna Tirtha's Vedic mathematics", "Binary GCD algorithm", "Binary multiplier", "Binary numeral system", "Chakravala method", "Cipolla's algorithm", "Continued fraction factorization", "Cornacchia's algorithm", "Cut-the-knot", "Discrete logarithm", "Division by 2", "Division by two", "Dixon's factorization method", "Dover Publications", "Egyptian Mathematical Leather Roll", "Egyptian mathematics", "Egyptian multiplication and division", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "False position", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Hieratic", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "James Joseph Sylvester", "Kahun Papyrus", "Karatsuba algorithm", "Least common multiple", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Mathematics", "Miller\u2013Rabin primality test", "Modular exponentiation", "Moscow Mathematical Papyrus", "Multiplicand", "Multiplication", "Multiplication algorithm", "Multiplication table", "Number theory", "Otto E. Neugebauer", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Powers of two", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Quotient", "RMP 2/n table", "Rational sieve", "Reisner Papyrus", "Remainder", "Rhind Mathematical Papyrus", "Rhind Papyrus", "Rhind papyrus", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Stan Wagon", "The Daily WTF", "The Wolfram Demonstrations Project", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Victor Klee", "Wheel factorization", "Wilbur Knorr", "Williams' p + 1 algorithm"], "categories": ["All articles lacking in-text citations", "All articles needing cleanup", "Ancient Egyptian literature", "Articles lacking in-text citations from February 2011", "Articles needing cleanup from February 2011", "Cleanup tagged articles without a reason field from February 2011", "Egyptian fractions", "Egyptian mathematics", "Mathematics manuscripts", "Multiplication", "Number theoretic algorithms", "Wikipedia pages needing cleanup from February 2011"], "title": "Ancient Egyptian multiplication"}
{"summary": "In group theory, a branch of mathematics, the baby-step giant-step is a meet-in-the-middle algorithm computing the discrete logarithm. The discrete log problem is of fundamental importance to the area of public key cryptography. Many of the most commonly used cryptography systems are based on the assumption that the discrete log is extremely difficult to compute; the more difficult it is, the more security it provides a data transfer. One way to increase the difficulty of the discrete log problem is to base the cryptosystem on a larger group.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baillie\u2013PSW primality test", "Big O notation", "Binary GCD algorithm", "Chakravala method", "Cipolla's algorithm", "Continued fraction factorization", "Cornacchia's algorithm", "Cyclic group", "Daniel Shanks", "Deterministic algorithm", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "GNU MP", "General number field sieve", "Generating primes", "Generating set of a group", "Greatest common divisor", "Group theory", "Hash table", "Index calculus algorithm", "Integer factorization", "Integer relation algorithm", "Integer square root", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Meet-in-the-middle attack", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "Public key cryptography", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Space-time tradeoff", "Special number field sieve", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Articles with example C code", "Group theory", "Number theoretic algorithms"], "title": "Baby-step giant-step"}
{"summary": "The binary GCD algorithm, also known as Stein's algorithm, is an algorithm that computes the greatest common divisor of two nonnegative integers. Stein's algorithm uses simpler arithmetic operations than the conventional Euclidean algorithm; it replaces division with arithmetic shifts, comparisons, and subtraction. Although the algorithm was first published by the Israeli physicist and programmer Josef Stein in 1967, it may have been known in 1st-century China.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Alexander Stepanov", "Algorithm", "Ancient Egyptian multiplication", "Arbitrary-precision arithmetic", "Arithmetic shift", "Asymptotic notation", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Big-O notation", "Bitwise operator", "Brigitte Vall\u00e9e", "C (programming language)", "Chakravala method", "Charles E. Leiserson", "Cipolla's algorithm", "CiteSeerX", "Clifford Stein", "Continued fraction factorization", "Cornacchia's algorithm", "Cut-the-knot", "Digital object identifier", "Discrete logarithm", "Dixon's factorization method", "Donald Knuth", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Exclusive or", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "International Standard Serial Number", "Introduction to Algorithms", "Josef Stein", "Karatsuba algorithm", "Least common multiple", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "MIX", "Mathematical Reviews", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic function", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Recursion (computer science)", "Richard Brent (scientist)", "Ronald L. Rivest", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Tail recursive", "The Art of Computer Programming", "The Nine Chapters on the Mathematical Art", "Thomas H. Cormen", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["All articles with unsourced statements", "Articles with example C code", "Articles with unsourced statements from March 2014", "Number theoretic algorithms"], "title": "Binary GCD algorithm"}
{"summary": "The chakravala method (Sanskrit: \u091a\u0915\u094d\u0930\u0935\u093e\u0932 \u0935\u093f\u0927\u093f) is a cyclic algorithm to solve indeterminate quadratic equations, including Pell's equation. It is commonly attributed to Bh\u0101skara II, (c. 1114 \u2013 1185 CE) although some attribute it to Jayadeva (c. 950 ~ 1000 CE). Jayadeva pointed out that Brahmagupta's approach to solving equations of this type could be generalized, and he then described this general method, which was later refined by Bh\u0101skara II in his Bijaganita treatise. He called it the Chakravala method: chakra meaning \"wheel\" in Sanskrit, a reference to the cyclic nature of the algorithm. E. O. Selenius held that no European performances at the time of Bh\u0101skara, nor much later, exceeded its marvellous height of mathematical complexity.\nThis method is also known as the cyclic method and contains traces of mathematical induction.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algebra", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Bhaskara's lemma", "Bh\u0101skara II", "Bijaganita", "Binary GCD algorithm", "Brahmagupta", "Brahmagupta's identity", "Cipolla's algorithm", "Continued fraction", "Continued fraction factorization", "Cornacchia's algorithm", "Discrete logarithm", "Dixon's factorization method", "Edmund F. Robertson", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Europe", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Florian Cajori", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Hermann Hankel", "Indeterminate equation", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Jayadeva (mathematician)", "John J. O'Connor (mathematician)", "John Stillwell", "Karatsuba algorithm", "Lagrange", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "MacTutor History of Mathematics archive", "Mathematical induction", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pell's equation", "Pierre de Fermat", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic equation", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Sanskrit", "Sanskrit language", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Springer Science+Business Media", "Square root", "The American Mathematical Monthly", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "University of St Andrews", "Wheel factorization", "William Brouncker, 2nd Viscount Brouncker", "Williams' p + 1 algorithm"], "categories": ["Articles containing Sanskrit-language text", "Brahmagupta", "Diophantine equations", "Indian mathematics", "Number theoretic algorithms"], "title": "Chakravala method"}
{"summary": "In computational number theory, Cipolla's algorithm is a technique for solving a congruence of the form\n\nwhere , so n is the square of x, and where  is an odd prime. Here  denotes the finite field with  elements; . The algorithm is named after Michele Cipolla, an Italian mathematician who discovered it in 1907.", "links": ["AKS primality test", "Addition", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Associativity", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Binary numeral system", "Chakravala method", "Characteristic (algebra)", "Commutativity", "Complex number", "Computational number theory", "Congruence relation", "Continued fraction factorization", "Cornacchia's algorithm", "Discrete logarithm", "Distributivity", "Dixon's factorization method", "Element (mathematics)", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's criterion", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat's little theorem", "Fermat primality test", "Field (mathematics)", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Identity element", "Imaginary unit", "Index calculus algorithm", "Integer factorization", "Integer polynomial", "Integer square root", "Inverse element", "Italy", "Karatsuba algorithm", "Lagrange's theorem (number theory)", "Legendre symbol", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Mathematics", "Michele Cipolla", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication", "Multiplication algorithm", "Number theory", "Numerical digit", "Parity (mathematics)", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Prime number", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Square root", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial and error", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Articles containing proofs", "Modular arithmetic", "Number theoretic algorithms"], "title": "Cipolla's algorithm"}
{"summary": "In computational number theory, Cornacchia's algorithm is an algorithm for solving the Diophantine equation , where  and d and m are coprime. The algorithm was described in 1908 by Giuseppe Cornacchia.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Chakravala method", "Cipolla's algorithm", "Computational number theory", "Continued fraction factorization", "Coprime", "Diophantine equation", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer square root", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic residues", "Quadratic sieve", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Square-free integer", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Number theoretic algorithms"], "title": "Cornacchia's algorithm"}
{"summary": "In mathematics, the Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two numbers, the largest number that divides both of them without leaving a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in Euclid's Elements (c. 300 BC). It is an example of an algorithm, a step-by-step procedure for performing a calculation according to well-defined rules, and is one of the oldest numerical algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.\nThe Euclidean algorithm is based on the principle that the greatest common divisor of two numbers does not change if the larger number is replaced by its difference with the smaller number. For example, 21 is the GCD of 252 and 105 (252 = 21 \u00d7 12 and 105 = 21 \u00d7 5), and the same number 21 is also the GCD of 105 and 147 = 252 \u2212 105. Since this replacement reduces the larger of the two numbers, repeating this process gives successively smaller pairs of numbers until one of the two numbers reaches zero. When that occurs, the other number (the one that is not zero) is the GCD of the original two numbers. By reversing the steps, the GCD can be expressed as a sum of the two original numbers each multiplied by a positive or negative integer, e.g., 21 = 5 \u00d7 105 + (\u22122) \u00d7 252. The fact that the GCD can always be expressed in this way is known as B\u00e9zout's identity.\nThe version of the Euclidean algorithm described above (and by Euclid) can take many subtraction steps to find the GCD when one of the given numbers is much bigger than the other. A more efficient version of the algorithm shortcuts these steps, instead replacing the larger of the two numbers by its remainder when divided by the smaller of the two. With this improvement, the algorithm never requires more steps than five times the number of digits (base 10) of the smaller integer. This was proven by Gabriel Lam\u00e9 in 1844, and marks the beginning of computational complexity theory. Additional methods for improving the algorithm's efficiency were developed in the 20th century.\nThe Euclidean algorithm has many theoretical and practical applications. It is used for reducing fractions to their simplest form and for performing division in modular arithmetic. Computations using this algorithm form part of the cryptographic protocols that are used to secure internet communications, and in methods for breaking these cryptosystems by factoring large composite numbers. The Euclidean algorithm may be used to solve Diophantine equations, such as finding numbers that satisfy multiple congruences according to the Chinese remainder theorem, to construct continued fractions, and to find accurate rational approximations to real numbers. Finally, it is a basic tool for proving theorems in number theory such as Lagrange's four-square theorem and the uniqueness of prime factorizations. The original algorithm was described only for natural numbers and geometric lengths (real numbers), but the algorithm was generalized in the 19th century to other types of numbers, such as Gaussian integers and polynomials of one variable. This led to modern abstract algebraic notions such as Euclidean domains.", "links": ["AKS primality test", "Absolute value", "Abstract algebra", "Adleman\u2013Pomerance\u2013Rumely primality test", "Alfred Aho", "Algebraic integer", "Algorithm", "Almost all", "Ancient Egyptian multiplication", "Andr\u00e9 Weil", "Arnold Sch\u00f6nhage", "Aryabhata", "Associative property", "Associativity", "BCH code", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Bartel Leendert van der Waerden", "Berlekamp\u2013Massey algorithm", "Bibcode", "Big-O notation", "Big O notation", "Binary GCD algorithm", "Binary numeral system", "Binary operation", "Binary search tree", "B\u00e9zout's identity", "C. Stanley Ogilvy", "Calkin\u2013Wilf tree", "Cambridge University Press", "Carl Friedrich Gauss", "Carl Pomerance", "Chakravala method", "Chinese remainder theorem", "Cipolla's algorithm", "Clark Kimberling", "Claude Gaspard Bachet de M\u00e9ziriac", "Commensurability (mathematics)", "Commutative property", "Commutative ring", "Commutativity", "Compass (drawing tool)", "Complex number", "Computational complexity theory", "Computer programming", "Constant time", "Continued fraction", "Continued fraction factorization", "Control theory", "Coordinate vector", "Coprime integers", "Cornacchia's algorithm", "Cryptographic protocol", "Cut-the-knot", "Cyclotomic field", "David Fowler (mathematician)", "David J. Darling", "Degree of a polynomial", "Derivative", "Derrick Henry Lehmer", "Determinant", "Digital object identifier", "Diophantine approximation", "Diophantine equation", "Diophantus", "Discrete logarithm", "Disquisitiones Arithmeticae", "Distributive property", "Distributivity", "Division (mathematics)", "Dixon's factorization method", "Domain (ring theory)", "Donald Knuth", "Eisenstein integer", "Electronic commerce", "Elliptic curve primality", "Elliptic curve primality proving", "Empty product", "Eric Bach", "Eric W. Weisstein", "Ernst Kummer", "Error-correcting code", "Euclid", "Euclid's Elements", "Euclid's lemma", "Euclidean (disambiguation)", "Euclidean division", "Euclidean domain", "Euclidean geometry", "Euclidean rhythm", "Eudoxus of Cnidus", "Euler's factorization method", "Euler's totient function", "Euler\u2013Mascheroni constant", "Extended Euclidean algorithm", "Fermat's Last Theorem", "Fermat's factorization method", "Fermat's theorem on sums of two squares", "Fermat primality test", "Fibonacci number", "Field norm", "Finite field", "Florian Cajori", "Fraction (mathematics)", "Function field sieve", "Fundamental theorem of arithmetic", "F\u00fcrer's algorithm", "GCD domain", "Gabriel Lam\u00e9", "Gal's accurate tables", "Gaussian integer", "General number field sieve", "Generalized Riemann hypothesis", "Generating primes", "Golden ratio", "Greatest common divisor", "Group (mathematics)", "Hans Heilbronn", "Harold Edwards (mathematician)", "Harold Stark", "Helaman Ferguson", "Hendrik Lenstra", "Henri Cohen (number theorist)", "Hurwitz quaternion", "I. N. Herstein", "IEEE Computer Society Press", "Ideal (ring theory)", "Ideal number", "Imaginary unit", "Index calculus algorithm", "Infinitesimal", "Integer", "Integer factorization", "Integer relation algorithm", "Integer square root", "Integral domain", "International Standard Book Number", "Internet", "Interval (mathematics)", "Invertible matrix", "Irrational number", "Irreducible element", "Irreducible fraction", "Irreducible polynomial", "Ivan Matveyevich Vinogradov", "Ivor Grattan-Guinness", "JSTOR", "Jacques Charles Fran\u00e7ois Sturm", "Jeffrey Shallit", "Jeffrey Ullman", "John Hopcroft", "John Stillwell", "Joseph Liouville", "Karatsuba algorithm", "Lagrange's four-square theorem", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Leopold Kronecker", "Library of Congress Control Number", "Linear combination", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "L\u00e1szl\u00f3 Lov\u00e1sz", "MIT Press", "Manfred R. Schroeder", "MathWorld", "Mathematical Association of America", "Mathematical Reviews", "Mathematical Treatise in Nine Sections", "Mathematical induction", "Mathematician", "Mathematics", "Matrix (mathematics)", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Modular multiplicative inverse", "Modulo operation", "Monoid", "Multiplication algorithm", "Natural number", "Nicholas Saunderson", "Nicomachus", "Norm-Euclidean field", "Norm (mathematics)", "Number theory", "OCLC", "Oren Patashnik", "Oxford University Press", "Peter Gustav Lejeune Dirichlet", "Peter Shor", "PlanetMath", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Polynomial", "Polynomial greatest common divisor", "Polynomial long division", "Primality test", "Prime number", "Principal ideal", "Principal ideal domain", "Proth's theorem", "Pseudocode", "Pythagoras", "Pythagorean triple", "P\u00e9pin's test", "Qin Jiushao", "Quadratic Frobenius test", "Quadratic integer", "Quadratic residue", "Quadratic sieve", "Quotient", "RSA algorithm", "Rational number", "Rational sieve", "Real number", "Recurrence relation", "Reed\u2013Solomon code", "Remainder", "Richard Dedekind", "Riemann zeta function", "Ring (mathematics)", "Ring theory", "Roger Cotes", "Ronald Graham", "Root of unity", "Ross Honsberger", "Routh\u2013Hurwitz stability criterion", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Serge Lang", "Sergei Tabachnikov", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Society for Industrial and Applied Mathematics", "Solovay\u2013Strassen primality test", "Special number field sieve", "Square root of 2", "Stan Wagon", "Stern\u2013Brocot tree", "Sturm's theorem", "Sturm chain", "Sun Tzu (mathematician)", "System of linear equations", "T. L. Heath", "Telescoping series", "Temporary variable", "The Art of Computer Programming", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Underdetermined system", "Uniform cost model", "Unique factorization domain", "Von Mangoldt function", "Vorlesungen \u00fcber Zahlentheorie", "Well-order", "Wheel factorization", "William J. LeVeque", "Williams' p + 1 algorithm", "Zentralblatt MATH", "Zero of a function", "\u00c9variste Galois", "\u00d8ystein Ore"], "categories": ["Articles containing proofs", "Articles with example pseudocode", "CS1 German-language sources (de)", "Commons category with local link same as on Wikidata", "Euclid", "Featured articles", "Number theoretic algorithms"], "title": "Euclidean algorithm"}
{"summary": "In arithmetic and computer programming, the extended Euclidean algorithm is an extension to the Euclidean algorithm, which computes, besides the greatest common divisor of integers a and b, the coefficients of B\u00e9zout's identity, that is integers x and y such that\n\nIt allows one to compute also, with almost no extra cost, the quotients of a and b by their greatest common divisor.\nExtended Euclidean algorithm also refers to a very similar algorithm for computing the polynomial greatest common divisor and the coefficients of B\u00e9zout's identity of two univariate polynomials.\nThe extended Euclidean algorithm is particularly useful when a and b are coprime, since x is the modular multiplicative inverse of a modulo b, and y is the modular multiplicative inverse of b modulo a. Similarly, the polynomial extended Euclidean algorithm allows one to compute the multiplicative inverse in algebraic field extensions and, in particular in finite fields of non prime order. It follows that both extended Euclidean algorithms are widely used in cryptography. In particular, the computation of the modular multiplicative inverse is an essential step in RSA public-key encryption method.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algebraic field extension", "Algorithm", "Ancient Egyptian multiplication", "Arithmetic", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Bijective", "Binary GCD algorithm", "B\u00e9zout's identity", "Chakravala method", "Charles E. Leiserson", "Cipolla's algorithm", "Clifford Stein", "Coding theory", "Computer algebra", "Computer programming", "Content (algebra)", "Continued fraction factorization", "Coprime", "Cornacchia's algorithm", "Cryptography", "Discrete logarithm", "Dixon's factorization method", "Donald Knuth", "Elliptic curve primality", "Elliptic curve primality proving", "Euclid's lemma", "Euclidean algorithm", "Euclidean division", "Euclidean division of polynomials", "Euclidean domain", "Euler's factorization method", "Fermat's factorization method", "Fermat primality test", "Field (mathematics)", "Finite field", "Finite field arithmetic", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer square root", "Introduction to Algorithms", "Irreducible polynomial", "Karatsuba algorithm", "Leading coefficient", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Linear congruence theorem", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Modular multiplicative inverse", "Monic polynomial", "Multiplication algorithm", "Multiplicative inverse", "Number theory", "Parallel assignment", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Polynomial greatest common divisor", "Primality test", "Prime field", "Prime number", "Primitive polynomial (ring theory)", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Quotient ring", "RSA (algorithm)", "Rational sieve", "Resultant", "Ronald L. Rivest", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Simple extension", "Solovay\u2013Strassen primality test", "Special number field sieve", "Subresultant", "The Art of Computer Programming", "Thomas H. Cormen", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Unit (ring theory)", "Univariate polynomial", "Wheel factorization", "Williams' p + 1 algorithm", "Z/nZ"], "categories": ["Articles with example pseudocode", "Euclid", "Number theoretic algorithms"], "title": "Extended Euclidean algorithm"}
{"summary": "An integer relation between a set of real numbers x1, x2, ..., xn is a set of integers a1, a2, ..., an, not all 0, such that\n\nAn integer relation algorithm is an algorithm for finding integer relations. Specifically, given a set of real numbers known to a given precision, an integer relation algorithm will either find an integer relation between them, or will determine that no integer relation exists with coefficients whose magnitudes are less than a certain upper bound.\n\n", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algebraic number", "Algorithm", "Almost integer", "Ancient Egyptian multiplication", "Arbitrary precision arithmetic", "Arjen Lenstra", "Baby-step giant-step", "Bailey-Borwein-Plouffe formula", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Chakravala method", "Cipolla's algorithm", "Closed-form expression", "Continued fraction", "Continued fraction factorization", "Cornacchia's algorithm", "David H. Bailey", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Eric W. Weisstein", "Euclidean algorithm", "Euler's factorization method", "Experimental mathematics", "Extended Euclidean algorithm", "Factorization of polynomials", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Helaman Ferguson", "Hendrik Lenstra", "Index calculus algorithm", "Infinite product", "Integer factorization", "Integer square root", "Integral", "Inverse Symbolic Calculator", "Jack Dongarra", "Jonathan Borwein", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Logistic map", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "L\u00e1szl\u00f3 Lov\u00e1sz", "MathWorld", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiple zeta function", "Multiplication algorithm", "Number theory", "Numerical method", "Pi", "Plouffe's Inverter", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Quantum field theory", "R.W. Forcade", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Series (mathematics)", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Simon Plouffe", "Solovay\u2013Strassen primality test", "Special number field sieve", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Upper bound", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Number theoretic algorithms"], "title": "Integer relation algorithm"}
{"summary": "Lehmer's GCD algorithm, named after Derrick Henry Lehmer, is a fast GCD algorithm, an improvement on the simpler but slower Euclidean algorithm. It is mainly used for big integers that have a representation as a string of digits relative to some chosen numeral system base, say \u03b2 = 1000 or \u03b2 = 232.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Chakravala method", "Cipolla's algorithm", "Continued fraction factorization", "Cornacchia's algorithm", "Derrick Henry Lehmer", "Discrete logarithm", "Dixon's factorization method", "Donald Knuth", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Identity matrix", "Index calculus algorithm", "Integer factorization", "Integer square root", "Karatsuba algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Matrix (mathematics)", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Quotient", "Radix", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "The Art of Computer Programming", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Number theoretic algorithms"], "title": "Lehmer's GCD algorithm"}
{"summary": "Pocklington's algorithm is a technique for solving a congruence of the form\n\nwhere x and a are integers and a is a quadratic residue.\nThe algorithm is one of the first efficient methods to solve such a congruence. It was described by H.C. Pocklington in 1917.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Chakravala method", "Cipolla's algorithm", "Continued fraction factorization", "Cornacchia's algorithm", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Henry Cabourn Pocklington", "Index calculus algorithm", "Integer factorization", "Integer square root", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Prime number", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Modular arithmetic", "Number theoretic algorithms"], "title": "Pocklington's algorithm"}
{"summary": "In number theory, the Pohlig\u2013Hellman algorithm sometimes credited as the Silver\u2013Pohlig\u2013Hellman algorithm is a special-purpose algorithm for computing discrete logarithms in a multiplicative group whose order is a smooth integer.\nThe algorithm was discovered by Roland Silver, but first published by Stephen Pohlig and Martin Hellman (independent of Silver).\nWe will explain the algorithm as it applies to the group Z*p consisting of all the elements of Zp which are coprime to p, and leave it to the advanced reader to extend the algorithm to other groups by using Lagrange's theorem.\nInput Integers p, g, e.\nOutput An Integer x, such that e \u2261 gx (mod p) (if one exists).\n\nDetermine the prime factorization of the order of the group  :\n(All the pi are considered small since the group order is smooth.)\nFrom the Chinese remainder theorem it will be sufficient to determine the values of x modulo each prime power dividing the group order. Suppose for illustration that p1 divides this order but p12 does not. Then we need to determine x mod p1, that is, we need to know the ending coefficient b1 in the base-p1 expansion of x, i.e. in the expansion x = a1 p1 + b1. We can find the value of b1 by examining all the possible values between 0 and p1-1. (We may also use a faster algorithm such as baby-step giant-step when the order of the group is prime.) The key behind the examination is that:\n\n(using Euler's theorem). With everything else now known, we may try each value of b1 to see which makes the equation be true. If , then there is exactly one b1, and that b1 is the value of x modulo p1. (An exception arises if  since then the order of g is less than \u03c6(p). The conclusion in this case depends on the value of  on the left: if this quantity is not 1, then no solution x exists; if instead this quantity is also equal to 1, there will be more than one solution for x less than \u03c6(p), but since we are attempting to return only one solution x, we may use b1=0.)\nThe same operation is now performed for p2 through pn.\nA minor modification is needed where a prime number is repeated. Suppose we are seeing pi for the (k + 1)st time. Then we already know ci in the equation x = ai pik+1 + bi pik + ci, and we find either bi or ci the same way as before, depending on whether .\nWith all the bi known, we have enough simultaneous congruences to determine x using the Chinese remainder theorem.", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Alfred Menezes", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "CRC Press", "Chakravala method", "Chinese remainder theorem", "Cipolla's algorithm", "Congruence relation", "Continued fraction factorization", "Coprime", "Cornacchia's algorithm", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Euler's theorem", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "IEEE", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Karatsuba algorithm", "Lagrange's theorem (group theory)", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Martin Hellman", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Multiplicative group", "Number theory", "Paul van Oorschot", "Pocklington's algorithm", "Pocklington primality test", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Scott Vanstone", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Smooth integer", "Solovay\u2013Strassen primality test", "Special number field sieve", "Stephen Pohlig", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Number theoretic algorithms"], "title": "Pohlig\u2013Hellman algorithm"}
{"summary": "Pollard's rho algorithm for logarithms is an algorithm introduced by John Pollard in 1978 to solve the discrete logarithm problem, analogous to Pollard's rho algorithm to solve the integer factorization problem.\nThe goal is to compute  such that , where  belongs to a cyclic group  generated by . The algorithm computes integers , , , and  such that . Assuming, for simplicity, that the underlying group is cyclic of order , we can calculate  as a solution of the equation .\nTo find the needed , , , and  the algorithm uses Floyd's cycle-finding algorithm to find a cycle in the sequence , where the function  is assumed to be random-looking and thus is likely to enter into a loop after approximately  steps. One way to define such a function is to use the following rules: Divide  into three disjoint subsets of approximately equal size: , , and . If  is in  then double both  and ; if  then increment , if  then increment .", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "C++", "Chakravala method", "Cipolla's algorithm", "Continued fraction factorization", "Cornacchia's algorithm", "Cyclic group", "Digital object identifier", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Euclidean algorithm", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Floyd's cycle-finding algorithm", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generating primes", "Greatest common divisor", "Index calculus algorithm", "Integer factorization", "Integer square root", "John Pollard (mathematician)", "Karatsuba algorithm", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Mathematics of Computation", "Miller\u2013Rabin primality test", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Primality test", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Tonelli\u2013Shanks algorithm", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Logarithms", "Number theoretic algorithms"], "title": "Pollard's rho algorithm for logarithms"}
{"summary": "The Tonelli\u2013Shanks algorithm (referred to by Shanks as the RESSOL algorithm) is used within modular arithmetic to solve a congruence of the form\n\nwhere n is a quadratic residue (mod p), and p is an odd prime.\nTonelli\u2013Shanks cannot be used for composite moduli; finding square roots modulo composite numbers is a computational problem equivalent to integer factorization.\nAn equivalent, but slightly more redundant version of this algorithm was developed by Alberto Tonelli in 1891. The version discussed here was developed independently by Daniel Shanks in 1973, who explained:\n\n\"My tardiness in learning of these historical references was because I had lent Volume 1 of Dickson's History to a friend and it was never returned.\"", "links": ["AKS primality test", "Adleman\u2013Pomerance\u2013Rumely primality test", "Algorithm", "Ancient Egyptian multiplication", "Baby-step giant-step", "Baillie\u2013PSW primality test", "Binary GCD algorithm", "Chakravala method", "Cipolla's algorithm", "Continued fraction factorization", "Cornacchia's algorithm", "Daniel Shanks", "Digital object identifier", "Discrete logarithm", "Dixon's factorization method", "Elliptic curve primality", "Elliptic curve primality proving", "Elliptic curves", "Euclidean algorithm", "Euler's criterion", "Euler's factorization method", "Extended Euclidean algorithm", "Fermat's factorization method", "Fermat primality test", "Finite field", "Function field sieve", "F\u00fcrer's algorithm", "General number field sieve", "Generalized Riemann hypothesis", "Generating primes", "Greatest common divisor", "Herbert Zuckerman", "History of the Theory of Numbers", "Hugh L. Montgomery", "Index calculus algorithm", "Integer factorization", "Integer square root", "International Standard Book Number", "Ivan Niven", "JSTOR", "Karatsuba algorithm", "Legendre symbol", "Lehmer's GCD algorithm", "Lenstra elliptic curve factorization", "Lenstra\u2013Lenstra\u2013Lov\u00e1sz lattice basis reduction algorithm", "Leonard Eugene Dickson", "Long multiplication", "Lucas primality test", "Lucas\u2013Lehmer primality test", "Lucas\u2013Lehmer\u2013Riesel test", "Miller\u2013Rabin primality test", "Modular arithmetic", "Modular exponentiation", "Multiplication algorithm", "Number theory", "Order (group theory)", "Pocklington's algorithm", "Pocklington primality test", "Pohlig\u2013Hellman algorithm", "Pollard's kangaroo algorithm", "Pollard's p \u2212 1 algorithm", "Pollard's rho algorithm", "Pollard's rho algorithm for logarithms", "Polynomial time", "Primality test", "Prime number", "Proth's theorem", "P\u00e9pin's test", "Quadratic Frobenius test", "Quadratic residue", "Quadratic sieve", "Rabin cryptosystem", "Rational sieve", "Schoof's algorithm", "Sch\u00f6nhage\u2013Strassen algorithm", "Shanks' square forms factorization", "Shor's algorithm", "Sieve of Atkin", "Sieve of Eratosthenes", "Sieve of Sundaram", "Solovay\u2013Strassen primality test", "Special number field sieve", "Toom\u2013Cook multiplication", "Trial division", "Wheel factorization", "Williams' p + 1 algorithm"], "categories": ["Articles containing proofs", "Modular arithmetic", "Number theoretic algorithms", "Pages containing cite templates with deprecated parameters"], "title": "Tonelli\u2013Shanks algorithm"}
{"summary": "In mathematics, an interval contractor (or contractor for short)  associated to a set X is an operator C which associates to a box [x] in Rn another box C([x]) of Rn such that the two following properties are always satisfied\n (contractance property)\n (completeness property)\nA contractor associated to a constraint (such as an equation or an inequality) is a contractor associated to the set X of all x which satisfy the constraint. Contractors make it possible to improve the efficiency of branch-and-bound algorithms classically used in interval analysis.", "links": ["Branch and bound", "Digital object identifier", "International Standard Book Number", "Interval arithmetic"], "categories": ["Arithmetic", "Computer arithmetic", "Mathematical optimization", "Numerical analysis", "Optimization algorithms and methods"], "title": "Interval contractor"}
{"summary": "The Jenkins\u2013Traub algorithm for polynomial zeros is a fast globally convergent iterative method published in 1970 by Michael A. Jenkins and Joseph F. Traub. They gave two variants, one for general polynomials with complex coefficients, commonly known as the \"CPOLY\" algorithm, and a more complicated variant for the special case of polynomials with real coefficients, commonly known as the \"RPOLY\" algorithm. The latter is \"practically a standard in black-box polynomial root-finders\".\nThis article describes the complex variant. Given a polynomial P,\n\nwith complex coefficients it computes approximations to the n zeros  of P(z), one at a time in roughly increasing order of magnitude. After each root is computed, its linear factor is removed from the polynomial. Using this deflation guarantees that each root is computed only once and that all roots are found.\nThe real variant follows the same pattern, but computes two roots at a time, either two real roots or a pair of conjugate complex roots. By avoiding complex arithmetic, the real variant can be faster (by a factor of 4) than the complex variant. The Jenkins\u2013Traub algorithm has stimulated considerable research on theory and software for methods of this type.", "links": ["Companion matrix", "Golden ratio", "Horner scheme", "Inverse power iteration", "James H. Wilkinson", "Joseph F. Traub", "Joseph F Traub", "Michael A. Jenkins", "Newton's method", "Newton\u2013Raphson iteration", "Philip Rabinowitz (mathematician)", "Polynomial", "Rate of convergence", "Rational functions", "Ruffini rule"], "categories": ["Numerical analysis", "Root-finding algorithms"], "title": "Jenkins\u2013Traub algorithm"}
{"summary": "A minimax approximation algorithm (or L\u221e approximation or uniform approximation) is a method to find an approximation of a mathematical function that minimizes maximum error.\nFor example, given a function  defined on the interval  and a degree bound , a minimax polynomial approximation algorithm will find a polynomial  of degree at most  to minimize", "links": ["Algorithm", "Chebyshev series", "Data structure", "Digital object identifier", "International Standard Book Number", "Mathematical function", "Michael J. D. Powell", "Remez algorithm", "Taylor series", "Weierstrass approximation theorem"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Numerical analysis"], "title": "Minimax approximation algorithm"}
{"summary": "The Frank\u2013Wolfe algorithm is an iterative first-order optimization algorithm for constrained convex optimization. Also known as the conditional gradient method, reduced gradient algorithm and the convex combination algorithm, the method was originally proposed by Marguerite Frank and Philip Wolfe in 1956. In each iteration, the Frank\u2013Wolfe algorithm considers a linear approximation of the objective function, and moves slightly towards a minimizer of this linear function (taken over the same domain).\n\n", "links": ["Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Compact space", "Comparison of optimization software", "Constrained optimization", "Convex function", "Convex minimization", "Convex optimization", "Convex set", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Differentiable function", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Duality gap", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "First-order approximation", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "International Standard Book Number", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear approximation", "Linear programming", "Lipschitz continuity", "Local convergence", "Local search (optimization)", "Machine learning", "Marguerite Frank", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization algorithm", "Optimization problem", "Penalty method", "Philip Wolfe (mathematician)", "Powell's method", "Projection (mathematics)", "Proximal gradient methods", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Real-valued function", "Revised simplex algorithm", "Sequential quadratic programming", "Signal processing", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Tangent", "Taylor series", "Transport network", "Truncated Newton method", "Trust region", "Vector space", "Wolfe conditions"], "categories": ["First order methods", "Gradient methods", "Iterative methods", "Optimization algorithms and methods"], "title": "Frank\u2013Wolfe algorithm"}
{"summary": "Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.\nGradient descent is also known as steepest descent, or the method of steepest descent. However, gradient descent should not be confused with the method of steepest descent for approximating integrals.", "links": ["Accuracy", "Algorithm", "Artificial neural network", "Augustin Cauchy", "BFGS method", "Backpropagation", "Big O notation", "Bowl (vessel)", "Concentric circles", "Conjugate gradient", "Conjugate gradient method", "Constraint (mathematics)", "Contour line", "Convex function", "Convex programming", "Coursera", "Curvature", "Defined and undefined", "Delta rule", "Differentiable function", "Digital object identifier", "Eigenvalues", "Euclidean norm", "Euler's method", "First-order method", "Forward\u2013backward algorithm (convex programming)", "Function space", "Geoffrey Hinton", "Gradient", "Gradient descent", "Gradient flow", "G\u00e2teaux derivative", "Hessian matrix", "International Standard Serial Number", "Isosurface", "Jacobian matrix", "Limited-memory BFGS", "Line search", "Linear least squares (mathematics)", "Lipschitz continuity", "Local maximum", "Local minimum", "Mathematical Reviews", "Mathematical analysis", "Mathematical optimization", "Method of steepest descent", "Nelder\u2013Mead method", "Neural Networks (journal)", "Newton's method in optimization", "Ordinary differential equations", "Orthogonal", "Preconditioner", "Preconditioning", "Projection (linear algebra)", "Python (programming language)", "Regina S. Burachik", "Rosenbrock function", "Rprop", "Stochastic gradient descent", "Subgradient method", "Variational inequality", "Willamette University", "Wolfe conditions", "YouTube", "Yurii Nesterov"], "categories": ["Articles with example Python code", "First order methods", "Gradient methods", "Optimization algorithms and methods"], "title": "Gradient descent"}
{"summary": "In optimization, gradient method is an algorithm to solve problems of the form\n\nwith the search directions defined by the gradient of the function at the current point. Examples of gradient method are the gradient descent and the conjugate gradient.", "links": ["Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Biconjugate gradient method", "Biconjugate gradient stabilized method", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Conjugate gradient", "Conjugate gradient method", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Derivation of the conjugate gradient method", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Gradient descent method", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "International Standard Book Number", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization (mathematics)", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["First order methods", "Gradient methods", "Numerical linear algebra", "Optimization algorithms and methods"], "title": "Gradient method"}
{"summary": "The adaptive projected subgradient method (APSM) is an algorithm, the goal of which is to minimize iteratively a sequence of cost functions.\nThis algorithmic \"tool\" is general and has been used in several tasks, such as: online/adaptive parameter estimation, online classification,  adaptive distributed learning, just to name a few. The algorithm can be used in both linear and non-linear scenarios (using kernels).\n^ Yamada, I.; Ogura, N. (2003). \"Adaptive projected subgradient method and its applications to set theoretic adaptive filtering\". The Thirty-seventh Asilomar Conference on Signals, Systems & Computers, 2003. p. 600. doi:10.1109/ACSSC.2003.1291982. ISBN 0-7803-8104-1. \n^ Yamada, I.; Ogura, N. (2005). \"Adaptive Projected Subgradient Method for Asymptotic Minimization of Sequence of Nonnegative Convex Functions\". Numerical Functional Analysis and Optimization 25 (7\u20138): 593. doi:10.1081/NFA-200045806. \n^ Slavakis, Konstantinos; Yamada, Isao (5 August 2011). \"The adaptive projected subgradient method constrained by families of quasi-nonexpansive mappings and its application to online learning\". Ithaca, New York: Cornell University.\n^ Slavakis, Konstantinos, Sergios Theodoridis, and Isao Yamada. \"Online kernel-based classification using adaptive projection algorithms.\" Signal Processing, IEEE Transactions on 56.7 (2008): 2781-2796.\n^ O\u011eUZ, \u00d6ZKAN. PERFORMANCE OF A NON-LINEAR ADAPTIVE BEAMFORMER ALGORITHM FOR SIGNAL-OF-INTEREST EXTRACTION. Diss. MIDDLE EAST TECHNICAL UNIVERSITY, 2015.\n^ Chouvardas, Symeon, Konstantinos Slavakis, and Sergios Theodoridis. \"Adaptive robust distributed learning in diffusion sensor networks.\" Signal Processing, IEEE Transactions on 59.10 (2011): 4692-4707.\n^ Slavakis, K.; Bouboulis, P.; Theodoridis, S. (2012-02-01). \"Adaptive Multiregression in Reproducing Kernel Hilbert Spaces: The Multiaccess MIMO Channel Case\". IEEE Transactions on Neural Networks and Learning Systems 23 (2): 260\u2013276. doi:10.1109/TNNLS.2011.2178321. ISSN 2162-237X. \n^ Bouboulis, P.; Slavakis, K.; Theodoridis, S. (2012-03-01). \"Adaptive Learning in Complex Reproducing Kernel Hilbert Spaces Employing Wirtinger's Subgradients\". IEEE Transactions on Neural Networks and Learning Systems 23 (3): 425\u2013438. doi:10.1109/TNNLS.2011.2179810. ISSN 2162-237X. \n^ Theodoridis, Sergios (2013). Academic Press Library in Signal Processing. CHAPTER 17 Online Learning in Reproducing Kernel Hilbert Spaces- Konstantinos Slavakis, Pantelis Bouboulis and Sergios Theodoridis: ACADEMIC PRESS. ISBN 978-0-12-397226-2. \n^ \"Source code for Kernel APSM\".", "links": ["Algorithm", "Applied mathematics", "Cornell University", "Data structure", "Digital object identifier", "Estimation theory", "International Standard Book Number", "International Standard Serial Number", "Ithaca, New York", "Loss function", "Maxima and minima", "Statistical classification", "Statistics"], "categories": ["Algorithms and data structures stubs", "All orphaned articles", "All stub articles", "Applied mathematics stubs", "Computer science stubs", "Estimation theory", "Gradient methods", "Machine learning", "Orphaned articles from June 2013", "Statistics stubs"], "title": "Adaptive projected subgradient method"}
{"summary": "In numerical optimization, the nonlinear conjugate gradient method generalizes the conjugate gradient method to nonlinear optimization. For a quadratic function :\n\nThe minimum of  is obtained when the gradient is 0:\n\n.\n\nWhereas linear conjugate gradient seeks a solution to the linear equation , the nonlinear conjugate gradient method is generally used to find the local minimum of a nonlinear function using its gradient  alone. It works when the function is approximately quadratic near the minimum, which is the case when the function is twice differentiable at the minimum.\nGiven a function  of  variables to minimize, its gradient  indicates the direction of maximum increase. One simply starts in the opposite (steepest descent) direction:\n\nwith an adjustable step length  and performs a line search in this direction until it reaches the minimum of :\n\n,\n\nAfter this first iteration in the steepest direction , the following steps constitute one iteration of moving along a subsequent conjugate direction , where :\nCalculate the steepest direction: ,\nCompute  according to one of the formulas below,\nUpdate the conjugate direction: \nPerform a line search: optimize ,\nUpdate the position: ,\nWith a pure quadratic function the minimum is reached within N iterations (excepting roundoff error), but a non-quadratic function will make slower progress. Subsequent search directions lose conjugacy requiring the search direction to be reset to the steepest descent direction at least every N iterations, or sooner if progress stops. However, resetting every iteration turns the method into steepest descent. The algorithm stops when it finds the minimum, determined when no progress is made after a direction reset (i.e. in the steepest descent direction), or when some tolerance criterion is reached.\nWithin a linear approximation, the parameters  and  are the same as in the linear conjugate gradient method but have been obtained with line searches. The conjugate gradient method can follow narrow (ill-conditioned) valleys where the steepest descent method slows down and follows a criss-cross pattern.\nFour of the best known formulas for  are named after their developers and are given by the following formulas:\nFletcher\u2013Reeves:\n\nPolak\u2013Ribi\u00e8re:\n\nHestenes-Stiefel:\n\nDai\u2013Yuan:\n\n.\n\nThese formulas are equivalent for a quadratic function, but for nonlinear optimization the preferred formula is a matter of heuristics or taste. A popular choice is  which provides a direction reset automatically.\nNewton based methods - Newton-Raphson Algorithm, Quasi-Newton methods (e.g., BFGS method) - tend to converge in fewer iterations, although each iteration typically requires more computation than a conjugate gradient iteration as Newton-like methods require computing the Hessian (matrix of second derivatives) in addition to the gradient. Quasi-Newton methods also require more memory to operate (see also the limited memory L-BFGS method).", "links": ["Approximation algorithm", "Augmented Lagrangian method", "BFGS method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Conjugate gradient method", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Ill-conditioned", "Integer programming", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "L-BFGS", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Maxima and minima", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Newton-Raphson Algorithm", "Nonlinear optimization", "Nonlinear programming", "Numerical optimization", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Quasi-Newton methods", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Steepest descent", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["Gradient methods", "Optimization algorithms and methods"], "title": "Nonlinear conjugate gradient method"}
{"summary": "Adaptive coordinate descent is an extension of the coordinate descent algorithm to non-separable optimization. The adaptive coordinate descent approach gradually builds a transformation of the coordinate system such that the new coordinates are as decorrelated as possible with respect to the objective function. The adaptive coordinate descent was shown to be competitive to the state-of-the-art evolutionary algorithms and has the following invariance properties:\nInvariance with respect to monotonous transformations of the function (scaling)\nInvariance with respect to orthogonal transformations of the search space (rotation).\nCMA-like Adaptive Encoding Update (b) mostly based on principal component analysis (a) is used to extend the coordinate descent method (c) to the optimization of non-separable problems (d).\n\nThe adaptation of an appropriate coordinate system allows adaptive coordinate descent to outperform coordinate descent on non-separable functions. The following figure illustrates the convergence of both algorithms on 2-dimensional Rosenbrock function up to a target function value , starting from the initial point .\n\nThe adaptive coordinate descent method reaches the target value after only 325 function evaluations (about 70 times faster than coordinate descent), that is comparable to gradient-based methods. The algorithm has linear time complexity if update coordinate system every D iterations, it is also suitable for large-scale (D>>100) non-linear optimization.", "links": ["Algorithm", "CMA-ES", "Coordinate descent", "Evolutionary algorithms", "Gradient descent", "Mathematical optimization", "Orthogonal transform", "Principal component analysis", "Rosenbrock methods"], "categories": ["Optimization algorithms and methods"], "title": "Adaptive coordinate descent"}
{"summary": "The term \"auction algorithm\"  applies to several variations of a combinatorial optimization algorithm which solves assignment problems, and network optimization problems with linear and convex/nonlinear cost. An auction algorithm has been used in a business setting to determine the best prices on a set of products offered to multiple buyers. It is an iterative procedure, so the name \"auction algorithm\" is related to a sales auction, where multiple bids are compared to determine the best offer, with the final sales going to the highest bidders.\nThe original form of the auction algorithm is an iterative method to find the optimal prices and an assignment that maximizes the net benefit in a bipartite graph, the maximum weight matching problem (MWM).  This algorithm was first proposed by Dimitri Bertsekas in 1979. Detailed analysis and extensions to more general network optimization problems (\u03b5-relaxation in 1986, and network auction in 1992) are provided in his network optimization books Linear Network Optimization 1991, and Network Optimization: Continuous and Discrete Models 1998. The auction algorithm has excellent computational complexity, as given in these books, and is reputed to be among the fastest for solving single commodity network optimization problems. In addition, the original version of this algorithm is known to possess a distributed nature particularly suitable for distributed systems, since its basic computational primitives (bidding and auctioning) are localized rather than relying on queries of global information. However, the original version that is intrinsically distributable has a pseudo-polynomial time complexity, which means that the running time depends on the input data pattern. Later versions have improved the time complexity to the state-of-the-art level by using techniques such as \u03b5-scaling (also discussed in the original 1979 paper) but at the sacrifice of undermining its distributed characteristics. In order to retain the distributed nature and also attain a polynomial time complexity, recently some researchers from the multi-agent community have been trying to improve the earlier version of the auction algorithm by switching to a different economic model, namely, from the selfish bidders' perspective to a merchant\u2019s point of view, where the merchant of a market adjusts the article prices in order to quickly clear the inventory.\nThe ideas of the auction algorithm and \u03b5-scaling  are also central in preflow-push algorithms for single commodity linear network flow problems. In fact the preflow-push algorithm for max-flow can be derived by applying the original 1979 auction algorithm to the max flow problem after reformulation as an assignment problem; see the 1998 Network Optimization book, by Bertsekas, Section 7.3.3. Moreover the preflow-push algorithm for the linear minimum cost flow problem is mathematically equivalent to the \u03b5-relaxation method, which is obtained by applying the original auction algorithm after the problem is reformulated as an equivalent assignment problem.\nA later variation of the auction algorithm that solves shortest path problems was introduced by Bertsekas in 1991. It is a simple algorithm for finding shortest paths in a directed graph. In the single origin/single destination case, the auction algorithm maintains a single path starting at the origin, which is then extended or contracted by a single node at each iteration. Simultaneously, at most one dual variable will be adjusted at each iteration, in order to either improve or maintain the value of a dual function. In the case of multiple origins, the auction algorithm is well-suited for parallel computation. The algorithm is closely related to auction algorithms for other network flow problems. According to computational experiments, the auction algorithm is generally inferior to other state-of-the-art algorithms for the all destinations shortest path problem, but is very fast for problems with few destinations (substantially more than one and substantially less than the total number of nodes); see the article by Bertsekas, Pallottino, and Scutella, Polynomial Auction Algorithms for Shortest Paths.\nAuction algorithms for shortest hyperpath problems have been defined by De Leone and Pretolani in 1998. This is also a parallel auction algorithm for weighted bipartite matching, described by E. Jason Riedy in 2004.", "links": ["Algorithm", "Assignment problem", "Auction", "Bipartite graph", "Dimitri Bertsekas", "Dimitri P. Bertsekas", "Directed graph", "Hungarian algorithm", "International Standard Serial Number", "Maximum weight matching", "Monotonic function", "Optimization (mathematics)", "Shortest path problem"], "categories": ["Optimization algorithms and methods"], "title": "Auction algorithm"}
{"summary": "Augmented Lagrangian methods are a certain class of algorithms for solving constrained optimization problems. They have similarities to penalty methods in that they replace a constrained optimization problem by a series of unconstrained problems and add a penalty term to the objective; the difference is that the augmented Lagrangian method adds yet another term, designed to mimic a Lagrange multiplier. The augmented Lagrangian is not the same as the method of Lagrange multipliers.\nViewed differently, the unconstrained objective is the Lagrangian of the constrained problem, with an additional penalty term (the augmentation).\nThe method was originally known as the method of multipliers, and was studied much in the 1970 and 1980s as a good alternative to penalty methods. It was first discussed by Magnus Hestenes in 1969 and by Powell in 1969. The method was studied by R. Tyrrell Rockafellar in relation to Fenchel duality, particularly in relation to proximal-point methods, Moreau\u2013Yosida regularization, and maximal monotone operators: These methods were used in structural optimization. The method was also studied by Dimitri Bertsekas, notably in his 1982 book, together with extensions involving nonquadratic regularization functions, such as entropic regularization, which gives rise to the \"exponential method of multipliers,\" a method that handles inequality constraints with a twice differentiable augmented Lagrangian function.\nSince the 1970s, sequential quadratic programming (SQP) and interior point methods (IPM) have had increasing attention, in part because they more easily use sparse matrix subroutines from numerical software libraries, and in part because IPMs have proven complexity results via the theory of self-concordant functions. The augmented Lagrangian method was rejuvenated by the optimization systems LANCELOT and AMPL, which allowed sparse matrix techniques to be used on seemingly dense but \"partially separable\" problems. The method is still useful for some problems. Around 2007, there was a resurgence of augmented Lagrangian methods in fields such as total-variation denoising and compressed sensing. In particular, a variant of the standard augmented Lagrangian method that uses partial updates (similar to the Gauss-Seidel method for solving linear equations) known as the alternating direction method of multipliers or ADMM gained some attention.", "links": ["AMPL", "Algorithm", "Approximation algorithm", "Athena Scientific", "Augmented Lagrangian method", "Barrier function", "Barrier method (mathematics)", "Basis pursuit", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Bregman divergence", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Compressed sensing", "Constraint (mathematics)", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dimitri Bertsekas", "Dinic's algorithm", "Douglas-Rachford splitting algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Fenchel duality", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Galahad library", "Gauss-Seidel method", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Interior point method", "International Standard Book Number", "Iterative method", "Jacobi method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lagrange multiplier", "Lagrange multipliers", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "MINOS (optimization software)", "Magnus Hestenes", "Mathematical optimization", "Matroid", "Metaheuristic", "Michael J. D. Powell", "Minimum spanning tree", "Monotone operator", "Moreau\u2013Yosida regularization", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Numerical linear algebra", "Numerical software", "Objective function", "Optimization (mathematics)", "Optimization algorithm", "PENOPT", "Penalty method", "Powell's method", "Proximal point algorithm", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "R. Tyrrell Rockafellar", "REASON", "Revised simplex algorithm", "Self-concordant function", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Sparse matrix", "Springer-Verlag", "Structural engineering", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Total variation denoising", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["Optimization algorithms and methods"], "title": "Augmented Lagrangian method"}
{"summary": "The Berndt\u2013Hall\u2013Hall\u2013Hausman (BHHH) algorithm is a numerical optimization algorithm similar to the Gauss\u2013Newton algorithm. It is named after the four originators: Ernst R. Berndt, B. Hall, Robert Hall, and Jerry Hausman.", "links": ["Algorithm", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Coefficient", "Data", "Davidon\u2013Fletcher\u2013Powell algorithm", "Ernst R. Berndt", "Gauss\u2013Newton algorithm", "International Standard Book Number", "Jerry Hausman", "Newton\u2013Raphson", "Nonlinear", "Numerical optimization", "Optimization (mathematics)", "Robert Hall (economist)"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from March 2013", "Econometrics", "Optimization algorithms and methods"], "title": "Berndt\u2013Hall\u2013Hall\u2013Hausman algorithm"}
{"summary": "In operations research, the Big M method is a method of solving linear programming problems using the simplex algorithm. The Big M method extends the power of the simplex algorithm to problems that contain \"greater-than\" constraints. It does so by associating the constraints with large negative constants which would not be part of any optimal solution, if it exists.", "links": ["Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Dijkstra's algorithm", "Dinic's algorithm", "Dublin City University", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "International Standard Book Number", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Karush\u2013Kuhn\u2013Tucker conditions", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Non-Linear Optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Operations research", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Row reductions", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Two phase method (linear programming)", "Wolfe conditions"], "categories": ["All articles needing expert attention", "Articles needing expert attention from March 2011", "Articles needing expert attention with no reason or talk parameter", "Articles needing unspecified expert attention", "Linear algebra", "Operations research", "Optimization algorithms and methods"], "title": "Big M method"}
{"summary": "In the bin packing problem, objects of different volumes must be packed into a finite number of bins or containers each of volume V in a way that minimizes the number of bins used. In computational complexity theory, it is a combinatorial NP-hard problem.\nThere are many variations of this problem, such as 2D packing, linear packing, packing by weight, packing by cost, and so on. They have many applications, such as filling up containers, loading trucks with weight capacity constraints, creating file backups in media and technology mapping in Field-programmable gate array semiconductor chip design.\nThe bin packing problem can also be seen as a special case of the cutting stock problem. When the number of bins is restricted to 1 and each item is characterised by both a volume and a value, the problem of maximising the value of items that can fit in the bin is known as the knapsack problem.\nDespite the fact that the bin packing problem has an NP-hard computational complexity, optimal solutions to very large instances of the problem can be produced with sophisticated algorithms. In addition, many heuristics have been developed: for example, the first fit algorithm provides a fast but often non-optimal solution, involving placing each item into the first bin in which it will fit. It requires \u0398(n log n) time, where n is the number of elements to be packed. The algorithm can be made much more effective by first sorting the list of elements into decreasing order (sometimes known as the first-fit decreasing algorithm), although this still does not guarantee an optimal solution, and for longer lists may increase the running time of the algorithm. It is known, however, that there always exists at least one ordering of items that allows first-fit to produce an optimal solution.\nA variant of bin packing that occurs in practice is when items can share space when packed into a bin. Specifically, a set of items could occupy less space when packed together than the sum of their individual sizes. This variant is known as VM packing since when virtual machines (VMs) are packed in a server, their total memory requirement could decrease due to pages shared by the VMs that need only be stored once. If items can share space in arbitrary ways, the bin packing problem is hard to even approximate. However, if the space sharing fits into a hierarchy, as is the case with memory sharing in virtual machines, the bin packing problem can be efficiently approximated.", "links": ["APX", "Apollonian gasket", "Apollonian sphere packing", "Approximation algorithm", "Asymptotically tight bound", "Backup", "Big O notation", "Circle packing", "Circle packing in a circle", "Circle packing in a square", "Circle packing in an equilateral triangle", "Circle packing in an isosceles right triangle", "Circle packing theorem", "Close-packing of equal spheres", "Combinatorics", "Computational complexity theory", "Conway puzzle", "Cutting stock problem", "David S. Johnson", "Digital object identifier", "Field-programmable gate array", "Greedy algorithm", "Guillotine problem", "Hamming bound", "Heuristic (computer science)", "International Standard Book Number", "International Standard Serial Number", "Kissing number problem", "Knapsack problem", "Memory management", "Michael R. Garey", "Multiprocessor scheduling", "NP-hard", "Online algorithm", "OpenOpt", "OptaPlanner", "Packing problem", "Packing problems", "Page (computer memory)", "Partition of a set", "Partition problem", "Polynomial-time approximation scheme", "Ramesh Sitaraman", "Semiconductor chip", "Set packing", "Slothouber\u2013Graatsma puzzle", "Sorting", "Sphere packing", "Sphere packing in a sphere", "Subset sum problem", "Sun Ultra series", "Tammes problem", "Tetrahedron packing", "VPSolver", "Vijay Vazirani", "Virtual machines"], "categories": ["All articles needing additional references", "Articles needing additional references from July 2015", "CS1 errors: chapter ignored", "Optimization algorithms and methods", "Packing problems", "Strongly NP-complete problems"], "title": "Bin packing problem"}
{"summary": "BOBYQA (Bound Optimization BY Quadratic Approximation) is a numerical optimization algorithm by Michael J. D. Powell. It is also the name of Powell's Fortran 77 implementation of the algorithm.\nBOBYQA solves bound constrained optimization problems without using derivatives of the objective function, which makes it a derivative-free algorithm. The algorithm solves the problem using a trust region method that forms quadratic models by interpolation. One new point is computed on each iteration, usually by solving a trust region subproblem subject to the bound constraints, or alternatively, by choosing a point to replace an interpolation point so as to promote good linear independence in the interpolation conditions.\nThe same as NEWUOA, BOBYQA constructs the quadratic models by the least Frobenius norm updating  technique.\nBOBYQA software was released on January 5, 2009.\nIn the comment of the software's source code, it is said that the name BOBYQA denotes \"Bound Approximation BY Quadratic Approximation\", which seems to be a typo of \"Bound Optimization BY Quadratic Approximation\".\nThe BOBYQA software is distributed under The GNU Lesser General Public License (LGPL).", "links": ["Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "COBYLA", "Combinatorial optimization", "Comment (computer programming)", "Comparison of optimization software", "Constrained optimization", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Derivative", "Derivative-free optimization", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Fortran", "Frank\u2013Wolfe algorithm", "Frobenius norm", "Function (mathematics)", "GNU Lesser General Public License", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Interpolation", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "LINCOA", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Michael J. D. Powell", "Minimum spanning tree", "NEWUOA", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Numerical analysis", "Optimization (mathematics)", "Optimization algorithm", "Optimization problem", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Software", "Source code", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "TOLMIN (optimization software)", "Tabu search", "Truncated Newton method", "Trust region", "UOBYQA", "Wolfe conditions"], "categories": ["All articles with links needing disambiguation", "Articles with links needing disambiguation from April 2014", "Optimization algorithms and methods"], "title": "BOBYQA"}
{"summary": "In numerical optimization, the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) algorithm is an iterative method for solving unconstrained nonlinear optimization problems.\nThe BFGS method approximates Newton's method, a class of hill-climbing optimization techniques that seeks a stationary point of a (preferably twice continuously differentiable) function. For such problems, a necessary condition for optimality is that the gradient be zero. Newton's method and the BFGS methods are not guaranteed to converge unless the function has a quadratic Taylor expansion near an optimum. These methods use both the first and second derivatives of the function. However, BFGS has proven to have good performance even for non-smooth optimizations.\nIn quasi-Newton methods, the Hessian matrix of second derivatives doesn't need to be evaluated directly. Instead, the Hessian matrix is approximated using rank-one updates specified by gradient evaluations (or approximate gradient evaluations). Quasi-Newton methods are generalizations of the secant method to find the root of the first derivative for multidimensional problems. In multi-dimensional problems, the secant equation does not specify a unique solution, and quasi-Newton methods differ in how they constrain the solution. The BFGS method is one of the most popular members of this class. Also in common use is L-BFGS, which is a limited-memory version of BFGS that is particularly suited to problems with very large numbers of variables (e.g., >1000). The BFGS-B variant handles simple box constraints.", "links": ["Approximation algorithm", "Approximation theory", "Augmented Lagrangian method", "BHHH algorithm", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden's method", "Charles George Broyden", "Claude Lemar\u00e9chal", "Combinatorial optimization", "Comparison of optimization software", "Confidence interval", "Convex minimization", "Convex optimization", "Credible interval", "Criss-cross algorithm", "Cutting-plane method", "David G. Luenberger", "Davidon\u2013Fletcher\u2013Powell formula", "Derivative", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Donald Goldfarb", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Eigen (C++ library)", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "GNU Scientific Library", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Gretl", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "International Standard Book Number", "Iterative method", "John Wiley & Sons", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Kuhn\u2013Tucker conditions", "L-BFGS", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local optimum", "Local search (optimization)", "MIT License", "Mathematical Reviews", "Mathematical optimization", "Matrix inverse", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear optimization", "Nonlinear programming", "Numerical analysis", "Optimization (mathematics)", "Optimization Toolbox", "Optimization algorithm", "Pattern search (optimization)", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Quasi-Newton methods", "Revised simplex algorithm", "SciPy", "Secant method", "Sequential quadratic programming", "Sherman\u2013Morrison formula", "Simplex algorithm", "Simulated annealing", "Springer-Verlag", "Stationary point", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Taylor expansion", "Truncated Newton method", "Trust region", "Wolfe conditions", "Yinyu Ye"], "categories": ["Optimization algorithms and methods"], "title": "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm"}
{"summary": "Constrained optimization by linear approximation (COBYLA) is a numerical optimization method for constrained problems where the derivative of the objective function is not known, invented by Michael J. D. Powell. That is, COBYLA can find the vector  with  that has the minimal (or maximal)  without knowing the gradient of . COBYLA is also the name of Powell's software implementation of the algorithm in Fortran.\nPowell invented COBYLA while working for Westland Helicopters.\nIt works by iteratively approximating the actual constrained optimization problem with linear programming problems. During an iteration, an approximating linear programming problem is solved to obtain a candidate for the optimal solution. The candidate solution is evaluated using the original objective and constraint functions, yielding a new data point in the optimization space. This information is used to improve the approximating linear programming problem used for the next iteration of the algorithm. When the solution cannot be improved anymore, the step size is reduced, refining the search. When the step size becomes sufficiently small, the algorithm finishes.\nThe COBYLA software is distributed under The GNU Lesser General Public License (LGPL).", "links": ["Applied mathematics", "Approximation algorithm", "Augmented Lagrangian method", "BOBYQA", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Constrained optimization", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Derivative", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Fortran", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "GNU Lesser General Public License", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "LINCOA", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Michael J. D. Powell", "Minimum spanning tree", "NEWUOA", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "TOLMIN (optimization software)", "Tabu search", "Truncated Newton method", "Trust region", "UOBYQA", "Westland Helicopters", "Wolfe conditions"], "categories": ["All stub articles", "Applied mathematics stubs", "Optimization algorithms and methods"], "title": "COBYLA"}
{"summary": "Column generation or delayed column generation is an efficient algorithm for solving larger linear programs.\nThe overarching idea is that many linear programs are too large to consider all the variables explicitly. Since most of the variables will be non-basic and assume a value of zero in the optimal solution, only a subset of variables need to be considered in theory when solving the problem. Column generation leverages this idea to generate only the variables which have the potential to improve the objective function\u2014that is, to find variables with negative reduced cost (assuming without loss of generality that the problem is a minimization problem).\nThe problem being solved is split into two problems: the master problem and the subproblem. The master problem is the original problem with only a subset of variables being considered. The subproblem is a new problem created to identify a new variable. The objective function of the subproblem is the reduced cost of the new variable with respect to the current dual variables, and the constraints require that the variable obey the naturally occurring constraints.\nThe process works as follows. The master problem is solved\u2014from this solution, we are able to obtain dual prices for each of the constraints in the master problem. This information is then utilized in the objective function of the subproblem. The subproblem is solved. If the objective value of the subproblem is negative, a variable with negative reduced cost has been identified. This variable is then added to the master problem, and the master problem is re-solved. Re-solving the master problem will generate a new set of dual values, and the process is repeated until no negative reduced cost variables are identified. The subproblem returns a solution with non-negative reduced cost, we can conclude that the solution to the master problem is optimal.\nIn many cases, this allows large linear programs that had been previously considered intractable to be solved. The classical example of a problem where this is successfully used is the cutting stock problem. One particular technique in linear programming which uses this kind of approach is the Dantzig\u2013Wolfe decomposition algorithm. Additionally, column generation has been applied to many problems such as crew scheduling, vehicle routing, and the capacitated p-median problem.", "links": ["Applied mathematics", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Capacitated p-median problem", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Crew scheduling", "Criss-cross algorithm", "Cutting-plane method", "Cutting stock problem", "Dantzig\u2013Wolfe decomposition", "Davidon\u2013Fletcher\u2013Powell formula", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization (mathematics)", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Reduced cost", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Vehicle routing", "Without loss of generality", "Wolfe conditions"], "categories": ["All stub articles", "Applied mathematics stubs", "Optimization algorithms and methods"], "title": "Column generation"}
{"summary": "Dykstra's algorithm is a method that computes a point in the intersection of convex sets, and is a variant of the alternating projection method (also called the projections onto convex sets method). In its simplest form, the method finds a point in the intersection of two convex sets by iteratively projecting onto each of the convex set; it differs from the alternating projection method in that there are intermediate steps. A parallel version of the algorithm was developed by Gaffke and Mathar.\nThe method is named after R. L. Dykstra who proposed it in the 1980s.\nA key difference between Dykstra's algorithm and the standard alternating projection method occurs when there is more than one point in the intersection of the two sets. In this case, the alternating projection method gives some arbitrary point in this intersection, whereas Dykstra's algorithm gives a specific point: the projection of r onto the intersection, where r is the initial point used in the algorithm,", "links": ["Alternating projection", "Convex set", "Digital object identifier", "Dijkstra's algorithm", "John von Neumann", "Projection (mathematics)", "Projections onto convex sets", "Regina S. Burachik"], "categories": ["Convex geometry", "Optimization algorithms and methods"], "title": "Dykstra's projection algorithm"}
{"summary": "In mathematics, management science, economics, computer science, and bioinformatics, dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions - ideally, using a memory-based data structure. The next time the same subproblem occurs, instead of recomputing its solution, one simply looks up the previously computed solution, thereby saving computation time at the expense of a (hopefully) modest expenditure in storage space. (Each of the subproblem solutions is indexed in some way, typically based on the values of its input parameters, so as to facilitate its lookup.) The act of storing solutions to subproblems is called \"memoization\". In contrast, a more naive method would not recognize that a particular subproblem has already been solved previously, and would repeatedly solve the same subproblem many times.\nThis approach is especially useful when the number of repeating subproblems grows exponentially as a function of the size of the input.\nDynamic programming is applicable to problems exhibiting the properties of overlapping subproblems and optimal substructure (described below). When applicable, the method takes far less time than other methods that don't take advantage of the subproblem overlap (like depth-first search).\nDynamic programming algorithms are used for optimization (for example, finding the shortest path between two points, or the fastest way to multiply many matrices). A dynamic programming algorithm will examine the previously solved subproblems and will combine their solutions to give the best solution for the given problem. The alternatives are many, such as using a greedy algorithm, which picks the locally optimal choice at each branch in the road. The locally optimal choice may be a poor choice for the overall solution. While a greedy algorithm does not guarantee an optimal solution, it is often faster to calculate. Fortunately, some greedy algorithms (such as minimum spanning trees) are proven to lead to the optimal solution.\nFor example, let's say that you have to get from point A to point B as fast as possible, in a given city, during rush hour. A dynamic programming algorithm will look at finding the shortest paths to points close to A, and use those solutions to eventually find the shortest path to B. On the other hand, a greedy algorithm will start you driving immediately and will pick the road that looks the fastest at every intersection. As you can imagine, this strategy might not lead to the fastest arrival time, since you might take some \"easy\" streets and then find yourself hopelessly stuck in a traffic jam.\nSometimes, applying memoization to a naive basic recursive solution already results in a dynamic programming solution with asymptotically optimal time complexity; however, the optimal solution to some problems requires more sophisticated dynamic programming algorithms. Some of these may be recursive as well but parametrized differently from the naive solution. Others can be more complicated and cannot be implemented as a recursive function with memoization. Examples of these are the two solutions to the Egg Dropping puzzle below.", "links": ["Alexander Zasedatelev", "Approximation algorithm", "Artificial neural networks", "Associative array", "Augmented Lagrangian method", "Backtracking", "Backward induction", "Barrier function", "Beat (music)", "Bellman equation", "Bellman\u2013Ford algorithm", "Big-O notation", "Bioinformatics", "Bitonic tour", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Brute-force search", "Bulletin of the American Mathematical Society", "CYK algorithm", "Call-by-name", "Call-by-need", "Capital (economics)", "Chain matrix multiplication", "Charles DeLisi", "Charles E. Leiserson", "Charles Erwin Wilson", "Chart parser", "Checkerboard", "Clifford Stein", "Clique-width", "Combinatorial optimization", "Common Lisp", "Comparison of optimization software", "Computational complexity of mathematical operations", "Computer chess", "Computer science", "Context-free grammar", "Convex minimization", "Convex optimization", "Convexity in economics", "Correspondence problem", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "De Boor algorithm", "Depth-first search", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Discounting", "Discrete-time", "Divide and conquer algorithm", "Dover Publications", "Duckworth\u2013Lewis method", "Dynamic algorithm", "Dynamic problem", "Dynamic programming", "Dynamic programming language", "Dynamic time warping", "Earley algorithm", "Economics", "Edit distance", "Edmonds\u2013Karp algorithm", "Edward Prescott", "Ellipsoid method", "Engineering", "Evolutionary algorithm", "Exchange algorithm", "Exponential growth", "Exponential time", "Fibonacci sequence", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Genetics", "Georgii Gurskii", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hanoi", "Hessian matrix", "Heuristic algorithm", "Hidden Markov model", "Hill climbing", "IBM System R", "IEEE", "Integer programming", "Integrated Authority File", "International Standard Book Number", "Interval scheduling", "Introduction to Algorithms", "Iterative method", "J (programming language)", "Jacques Philippe Marie Binet", "Johnson's algorithm", "Karmarkar's algorithm", "Knapsack problem", "Kruskal's algorithm", "Lattice models", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Levenshtein distance", "Limited-memory BFGS", "Line search", "Linear programming", "Linear search problem", "Local convergence", "Local search (optimization)", "Longest common subsequence problem", "Longest common substring problem", "Longest increasing subsequence problem", "Management science", "Markov decision process", "Mathematica", "Mathematical Reviews", "Mathematical game", "Mathematical optimization", "Mathematics", "Matrix chain multiplication", "Matroid", "Maximum subarray problem", "Memoization", "Memoize", "Mergesort", "Metaheuristic", "Method of undetermined coefficients", "Minimum spanning tree", "Music information retrieval", "Nancy Stokey", "National Diet Library", "Needleman\u2013Wunsch algorithm", "Nelder\u2013Mead method", "Newton's method in optimization", "Non-convexity (economics)", "Nonlinear conjugate gradient method", "Nonlinear programming", "Nucleosome", "On-Line Encyclopedia of Integer Sequences", "Optimal substructure", "Optimization algorithm", "Overlapping subproblem", "Partition problem", "Patricia Selinger", "Penalty method", "Perl", "Photoshop", "Powell's method", "Principle of Optimality", "Programming language", "Prolog", "Pseudo-polynomial time", "Push\u2013relabel maximum flow algorithm", "Puzzle", "Quadratic programming", "Quasi-Newton method", "Quicksort", "RNA structure", "Recursion", "Recursion (computer science)", "Recursive least squares", "Referential transparency (computer science)", "Refutation table", "Revised simplex algorithm", "Richard Bellman", "Robert E. Lucas", "Ronald L. Rivest", "Scheme (programming language)", "Seam carving", "Sequence alignment", "Sequential quadratic programming", "Shortest path problem", "Simplex algorithm", "Simulated annealing", "Smith\u2013Waterman algorithm", "State variable", "Stochastic programming", "String (computer science)", "Structural alignment", "Subgradient method", "Subroutine", "Subset sum problem", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Systems analysis", "Tabu search", "Taylor & Francis", "The Mathematical Association of America", "Thomas H. Cormen", "Time-invariant system", "Top-down", "Top-down and bottom-up design", "Tower of Hanoi", "Transcription factor", "Transposition table", "Travelling salesman problem", "Tree decomposition", "Tree structure", "Treewidth", "Truncated Newton method", "Trust region", "U.S. English", "Undirected graph", "Utility", "Viterbi algorithm", "Wolfe conditions", "Word wrap"], "categories": ["All articles needing additional references", "All articles with unsourced statements", "Articles needing additional references from April 2014", "Articles needing additional references from May 2013", "Articles with unsourced statements from May 2009", "Dynamic programming", "Equations", "Mathematical optimization", "Operations research", "Optimal control", "Optimization algorithms and methods", "Systems engineering", "Wikipedia articles with GND identifiers"], "title": "Dynamic programming"}
{"summary": "Fernandez's method (FB) is a method which is used in the multiprocessing scheduling algorithm. It is actually used to improve the quality of the lower bounding schemes which are adopted by branch and bound algorithms for solving multiprocessor scheduling problem. Fernandez's problem derives a better lower bound than HF,and propose a quadratic-time algorithm from calculating the bound. It is known that a straightforward calculation of FB takes O time, since it must examine O combinations each of which takes O time in the worst case.", "links": ["Branch and bound", "HF", "Multiprocessing scheduling algorithm"], "categories": ["All Wikipedia articles needing context", "All articles lacking sources", "All articles with links needing disambiguation", "All orphaned articles", "All pages needing cleanup", "Articles lacking sources from April 2012", "Articles with links needing disambiguation from December 2013", "Optimization algorithms and methods", "Orphaned articles from April 2012", "Wikipedia articles needing context from April 2012", "Wikipedia introduction cleanup from April 2012"], "title": "Fernandez\u2019s method"}
{"summary": "Fourier\u2013Motzkin elimination, also known as the FME method, is a mathematical algorithm for eliminating variables from a system of linear inequalities. It can output real solutions.\nThe algorithm is named after Joseph Fourier and Theodore Motzkin.", "links": ["Aarhus University", "Alexander Schrijver", "Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "CiteSeer", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "International Standard Book Number", "Iterative method", "Johnson's algorithm", "Joseph Fourier", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Real closed field", "Real number", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "System of linear inequalities", "Tabu search", "Theodore Motzkin", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["All articles lacking in-text citations", "All articles to be expanded", "Articles lacking in-text citations from November 2014", "Articles needing translation from German Wikipedia", "Articles to be expanded from September 2013", "Optimization algorithms and methods", "Pages using web citations with no URL", "Pages with no translate target", "Real algebraic geometry"], "title": "Fourier\u2013Motzkin elimination"}
{"summary": "The Gauss\u2013Newton algorithm is used to solve non-linear least squares problems. It is a modification of Newton's method for finding a minimum of a function. Unlike Newton's method, the Gauss\u2013Newton algorithm can only be used to minimize a sum of squared function values, but it has the advantage that second derivatives, which can be challenging to compute, are not required.\nNon-linear least squares problems arise for instance in non-linear regression, where parameters in a model are sought such that the model is in good agreement with available observations.\nThe method is named after the mathematicians Carl Friedrich Gauss and Isaac Newton.", "links": ["Abraham de Moivre", "Absolute time and space", "An Historical Account of Two Notable Corruptions of Scripture", "Analysis of covariance", "Analysis of variance", "Approximation algorithm", "Approximation theory", "Arithmetica Universalis", "Augmented Lagrangian method", "BFGS method", "Barrier function", "Bayesian experimental design", "Bellman\u2013Ford algorithm", "Benjamin Pulleyn", "Binomial regression", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Bucket argument", "Calculus", "Calibration curve", "Carl Friedrich Gauss", "Catherine Barton", "Chebyshev nodes", "Chebyshev polynomials", "Cholesky decomposition", "Classical mechanics", "Column vectors", "Combinatorial optimization", "Comparison of optimization software", "Computational statistics", "Confounding", "Conjugate gradient", "Convex minimization", "Convex optimization", "Copernican Revolution", "Corpuscular theory of light", "Correlation and dependence", "Cranbury Park", "Criss-cross algorithm", "Curve fitting", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "De analysi per aequationes numero terminorum infinitas", "De motu corporum in gyrum", "Descent direction", "Design of experiments", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Direction (geometry, geography)", "Dynamic programming", "Dynamics (mechanics)", "Early life of Isaac Newton", "Edmonds\u2013Karp algorithm", "Elements of the Philosophy of Newton", "Ellipsoid method", "Errors and residuals in statistics", "Evolutionary algorithm", "Exchange algorithm", "Finite difference", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gaussian quadrature", "Gauss\u2013Markov theorem", "General Scholium", "General linear model", "Generalized Gauss\u2013Newton method", "Generalized least squares", "Generalized linear model", "Golden section search", "Goodness of fit", "Gradient", "Gradient descent", "Graph algorithm", "Gravitational constant", "Greedy algorithm", "Growth curve (statistics)", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Hypotheses non fingo", "Ill-conditioned", "Impact depth", "Inertia", "Integer programming", "International Standard Book Number", "Isaac Barrow", "Isaac Newton", "Isaac Newton's occult studies", "Isaac Newton S/O Philipose", "Isaac Newton in popular culture", "Isotonic regression", "Iterative method", "Iteratively reweighted least squares", "Jacobian matrix", "John Conduitt", "John Keill", "John Wiley & Sons", "Johnson's algorithm", "Karmarkar's algorithm", "Kendall tau rank correlation coefficient", "Kepler's laws of planetary motion", "Kissing number problem", "Kruskal's algorithm", "Later life of Isaac Newton", "Least squares", "Leibniz\u2013Newton calculus controversy", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear approximation", "Linear least squares (mathematics)", "Linear programming", "Linear regression", "List of statistics articles", "List of things named after Isaac Newton", "Local convergence", "Local regression", "Local search (optimization)", "Logistic regression", "Mallows's Cp", "Mathematical optimization", "Matrix transpose", "Matroid", "Maxima and minima", "Mean and predicted response", "Metaheuristic", "Method of Fluxions", "Minimum mean-square error", "Minimum spanning tree", "Model selection", "Moving least squares", "Multivariate analysis of variance", "Nelder\u2013Mead method", "Newton's cannonball", "Newton's cradle", "Newton's identities", "Newton's inequalities", "Newton's law of cooling", "Newton's law of universal gravitation", "Newton's laws of motion", "Newton's metal", "Newton's method", "Newton's method in optimization", "Newton's reflector", "Newton's rings", "Newton's theorem about ovals", "Newton's theorem of revolving orbits", "Newton (Blake)", "Newton (unit)", "Newton disc", "Newton fractal", "Newton polygon", "Newton polynomial", "Newton scale", "Newtonian dynamics", "Newtonian fluid", "Newtonian potential", "Newtonian telescope", "Newtonianism", "Newton\u2013Cartan theory", "Newton\u2013Cotes formulas", "Newton\u2013Euler equations", "Newton\u2013Okounkov body", "Newton\u2013Pepys problem", "Non-linear least squares", "Non-linear regression", "Nonlinear conjugate gradient method", "Nonlinear programming", "Nonlinear regression", "Nonparametric regression", "Notes on the Jewish Temple", "Numerical analysis", "Numerical integration", "Numerical smoothing and differentiation", "Opticks", "Optimal design", "Optimization algorithm", "Ordinary least squares", "Orthogonal polynomials", "Outline of statistics", "Parameterized post-Newtonian formalism", "Partial correlation", "Partial least squares", "Partition of sums of squares", "Pearson product-moment correlation coefficient", "Penalty method", "Philosophi\u00e6 Naturalis Principia Mathematica", "Poisson regression", "Polynomial regression", "Post-Newtonian expansion", "Powell's method", "Power number", "Problem of Apollonius", "Push\u2013relabel maximum flow algorithm", "QR factorization", "Quadratic programming", "Quaestiones quaedam philosophicae", "Quantile regression", "Quasi-Newton method", "Rank correlation", "Rate of convergence", "Regression analysis", "Regression model validation", "Religious views of Isaac Newton", "Residual (statistics)", "Response surface methodology", "Revised simplex algorithm", "Ridge regression", "Robust regression", "Rotating spheres", "Schr\u00f6dinger\u2013Newton equation", "Scientific revolution", "Segmented regression", "Semiparametric regression", "Sequential quadratic programming", "Sextant", "Simple linear regression", "Simplex algorithm", "Simulated annealing", "Solar mass", "Spearman's rank correlation coefficient", "Standing on the shoulders of giants", "Stationary point", "Statistical model", "Steepest descent", "Stepwise regression", "Structural coloration", "Studentized residual", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "System identification", "Table of Newtonian series", "Tabu search", "Taylor's theorem", "The Chronology of Ancient Kingdoms Amended", "The Mysteryes of Nature and Art", "The Queries", "Total least squares", "Truncated Newton method", "Trust region", "Weighted least squares", "William Clarke (apothecary)", "William Jones (mathematician)", "William Stukeley", "Wolfe conditions", "Woolsthorpe Manor", "Writing of Principia Mathematica"], "categories": ["Least squares", "Optimization algorithms and methods", "Statistical algorithms"], "title": "Gauss\u2013Newton algorithm"}
{"summary": "In statistics, generalized iterative scaling (GIS) and improved iterative scaling (IIS) are two early algorithms used to fit log-linear models, notably multinomial logistic regression (MaxEnt) classifiers and extensions of it such as MaxEnt Markov models and conditional random fields. These algorithms have been largely surpassed by gradient-based methods such as L-BFGS and coordinate descent algorithms.", "links": ["Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Conditional random field", "Convex minimization", "Convex optimization", "Coordinate descent", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Expectation\u2013maximization algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "L-BFGS", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Log-linear model", "Mathematical optimization", "Matroid", "Maximum-entropy Markov model", "Metaheuristic", "Minimum spanning tree", "Multinomial logistic regression", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Statistical classification", "Statistics", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["All stub articles", "CS1 errors: missing author or editor", "Log-linear models", "Optimization algorithms and methods", "Statistics stubs"], "title": "Generalized iterative scaling"}
{"summary": "The glowworm swarm optimization (GSO) is a swarm intelligence optimization algorithm developed based on the behaviour of glowworms (also known as fireflies or lightning bugs). The behaviour pattern of glowworms which is used for this algorithm is the apparent capability of the glowworms to change the intensity of the luciferin emission and thus appear to glow at different intensities.\n1.The GSO algorithm makes the agents glow at intensities approximately proportional to the function value being optimized. It is assumed that glowworms of brighter intensities attract glowworms that have lower intensity.\n2.The second significant part of the algorithm incorporates a dynamic decision range by which the effect of distant glowworms are discounted when a glowworm has sufficient number of neighbours or the range goes beyond the range of perception of the glowworms.\nThe part 2 of the algorithm makes it different from Firefly algorithm(FA). In the Firefly algorithm, fireflies can automatically subdivide into subgroups and thus can find multiple global solutions simultaneously, and thus FA is very suitable for multimodal problems. However, in GSO, there is no \"sufficient number or neighbours\" limit and there is no perception limit based on distance, but it can have still have \"cognitive limits\" which allows swarms of glowworms to split into sub-groups and converge to high function value points. This property of the algorithm allows it to be used to identify multiple peaks of a multi-modal function and makes it part of Evolutionary multi-modal optimization algorithms family.\nThe GSO algorithm was developed and introduced by K.N. Krishnanand and Debasish Ghose in 2005 at the Guidance, Control, and Decision Systems Laboratory in the Department of Aerospace Engineering at the Indian Institute of Science, Bangalore, India. Subsequently, it has been used in various applications and several papers have appeared in the literature using the GSO algorithm.", "links": ["Active matter", "Agent-based model", "Agent-based model in biology", "Algorithm", "Allee effect", "Altitudinal migration", "Animal migration", "Animal migration tracking", "Animal navigation", "Ant colony optimization algorithms", "Ant robotics", "Approximation algorithm", "Artificial Ants", "Augmented Lagrangian method", "Bait ball", "Bangalore", "Barrier function", "Bat algorithm", "Bees algorithm", "Bellman\u2013Ford algorithm", "Bird migration", "Boids", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Cell migration", "Clustering of self-propelled particles", "Coded wire tag", "Collective animal behavior", "Collective intelligence", "Collective motion", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Crowd simulation", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Debasish Ghose", "Decentralised system", "Diel vertical migration", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Eusociality", "Evolutionary algorithm", "Evolutionary multi-modal optimization", "Exchange algorithm", "Feeding frenzy", "Firefly algorithm", "Fish migration", "Flock (birds)", "Flocking (behavior)", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Glowworm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Group size measures", "Herd", "Herd behavior", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Homing (biology)", "India", "Indian Institute of Science", "Insect migration", "Integer programming", "Intelligent Small World Autonomous Robots for Micro-manipulation", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Lepidoptera migration", "Lessepsian migration", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Microbial intelligence", "Microbotics", "Minimum spanning tree", "Mixed-species foraging flock", "Mobbing (animal behavior)", "Monarch butterfly migration", "Mutualism (biology)", "Natal homing", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization (mathematics)", "Optimization algorithm", "Pack (canine)", "Pack hunter", "Particle swarm optimization", "Patterns of self-organization in ants", "Penalty method", "Philopatry", "Powell's method", "Predator satiation", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Quorum sensing", "Reverse migration (birds)", "Revised simplex algorithm", "Salmon run", "Sardine run", "Sea turtle migration", "Self-propelled particles", "Sequential quadratic programming", "Shoaling and schooling", "Simplex algorithm", "Simulated annealing", "Sort sol (bird flock)", "Spatial organization", "Stigmergy", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Swarm", "Swarm (simulation)", "Swarm behaviour", "Swarm intelligence", "Swarm robotics", "Swarming (honey bee)", "Swarming (military)", "Swarming motility", "Symbrion", "Symmetric rank-one", "Symmetry breaking of escaping ants", "Tabu search", "Task allocation and partitioning of social insects", "Truncated Newton method", "Trust region", "Vicsek model", "Wolfe conditions"], "categories": ["Optimization algorithms and methods"], "title": "Glowworm swarm optimization"}
{"summary": "The golden section search is a technique for finding the extremum (minimum or maximum) of a strictly unimodal function by successively narrowing the range of values inside which the extremum is known to exist. The technique derives its name from the fact that the algorithm maintains the function values for triples of points whose distances form a golden ratio. The algorithm is the limit of Fibonacci search (also described below) for a large number of function evaluations. Fibonacci search and Golden section search were discovered by Kiefer (1953). (see also Avriel and Wilde (1966)).", "links": ["Absolute value", "Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Binary search", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Brent's method", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Extremum", "Fibonacci Quarterly", "Fibonacci number", "Fibonacci search", "Fibonacci search technique", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden angle", "Golden ratio", "Golden ratio base", "Golden rectangle", "Golden rhombus", "Golden spiral", "Golden triangle (mathematics)", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "International Standard Book Number", "Iterative method", "JSTOR", "Jack Kiefer (mathematician)", "Jack Kiefer (statistician)", "Johnson's algorithm", "Karmarkar's algorithm", "Kepler triangle", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical Reviews", "Mathematical optimization", "Matroid", "Metaheuristic", "Metallic mean", "Minimax", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization algorithm", "Pell number", "Penalty method", "Powell's method", "Proceedings of the American Mathematical Society", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequence", "Sequential quadratic programming", "Silver ratio", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Ternary search", "Truncated Newton method", "Trust region", "Unimodal function", "Wolfe conditions"], "categories": ["Articles with example Java code", "Fibonacci numbers", "Golden ratio", "Optimization algorithms and methods"], "title": "Golden section search"}
{"summary": "The Great Deluge algorithm (GD) is a generic algorithm applied to optimization problems. It is similar in many ways to the hill-climbing and simulated annealing algorithms.\nThe name comes from the analogy that in a great deluge a person climbing a hill will try to move in any direction that does not get his/her feet wet in the hope of finding a way up as the water level rises.\nIn a typical implementation of the GD, the algorithm starts with a poor approximation, S, of the optimum solution. A numerical value called the badness is computed based on S and it measures how undesirable the initial approximation is. The higher the value of badness the more undesirable is the approximate solution. Another numerical value called the tolerance is calculated based on a number of factors, often including the initial badness.\nA new approximate solution S' , called a neighbour of S, is calculated based on S. The badness of S' , b' , is computed and compared with the tolerance. If b' is better than tolerance, then the algorithm is recursively restarted with S : = S' , and tolerance := decay(tolerance) where decay is a function that lowers the tolerance (representing a rise in water levels). If b' is worse than tolerance, a different neighbour S* of S is chosen and the process repeated. If all the neighbours of S produce approximate solutions beyond tolerance, then the algorithm is terminated and S is put forward as the best approximate solution obtained.", "links": ["Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill-climbing", "Hill climbing", "Integer programming", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization (mathematics)", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["Optimization algorithms and methods"], "title": "Great Deluge algorithm"}
{"summary": "Guided Local Search is a metaheuristic search method. A meta-heuristic method is a method that sits on top of a local search algorithm to change its behaviour.\nGuided Local Search builds up penalties during a search. It uses penalties to help local search algorithms escape from local minima and plateaus. When the given local search algorithm settles in a local optimum, GLS modifies the objective function using a specific scheme (explained below). Then the local search will operate using an augmented objective function, which is designed to bring the search out of the local optimum. The key is in the way that the objective function is modified.", "links": ["Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Breakout method", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Constraint satisfaction", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Maxima and minima", "Metaheuristic", "Minimum spanning tree", "Multi-objective optimization", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic assignment problem", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["All articles needing additional references", "All articles with unsourced statements", "Articles needing additional references from December 2009", "Articles with unsourced statements from July 2014", "Heuristics", "Optimization algorithms and methods"], "title": "Guided Local Search"}
{"summary": "In computer science, hill climbing is a mathematical optimization technique which belongs to the family of local search. It is an iterative algorithm that starts with an arbitrary solution to a problem, then attempts to find a better solution by incrementally changing a single element of the solution. If the change produces a better solution, an incremental change is made to the new solution, repeating until no further improvements can be found.\nFor example, hill climbing can be applied to the travelling salesman problem. It is easy to find an initial solution that visits all the cities but will be very poor compared to the optimal solution. The algorithm starts with such a solution and makes small improvements to it, such as switching the order in which two cities are visited. Eventually, a much shorter route is likely to be obtained.\nHill climbing is good for finding a local optimum (a solution that cannot be improved by considering a neighbouring configuration) but it is not necessarily guaranteed to find the best possible solution (the global optimum) out of all possible solutions (the search space). In convex problems, hill-climbing is optimal. Examples of algorithms that solve convex problems by hill-climbing include the simplex algorithm for linear programming and binary search.\nThe characteristic that only local optima are guaranteed can be cured by using restarts (repeated local search), or more complex schemes based on iterations, like iterated local search, on memory, like reactive search optimization and tabu search, or memory-less stochastic modifications, like simulated annealing.\nThe relative simplicity of the algorithm makes it a popular first choice amongst optimizing algorithms. It is used widely in artificial intelligence, for reaching a goal state from a starting node. Choice of next node and starting node can be varied to give a list of related algorithms. Although more advanced algorithms such as simulated annealing or tabu search may give better results, in some situations hill climbing works just as well. Hill climbing can often produce a better result than other algorithms when the amount of time available to perform a search is limited, such as with real-time systems. It is an anytime algorithm: it can return a valid solution even if it's interrupted at any time before it ends.", "links": ["A* search algorithm", "Alpha\u2013beta pruning", "Anytime algorithm", "Approximation algorithm", "Artificial intelligence", "Augmented Lagrangian method", "B*", "Backtracking", "Barrier function", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Binary search", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Breadth-first search", "British Museum algorithm", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Candidate solution", "Combinatorial optimization", "Comparison of optimization software", "Computer science", "Conjugate gradient method", "Convex minimization", "Convex optimization", "Coordinate descent", "Criss-cross algorithm", "Cutting-plane method", "D*", "Davidon\u2013Fletcher\u2013Powell formula", "Depth-first search", "Depth-limited search", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds' algorithm", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Free On-line Dictionary of Computing", "Fringe search", "Function (mathematics)", "GNU Free Documentation License", "Gauss\u2013Newton algorithm", "Genetic algorithm", "Global optimum", "Golden section search", "Gradient", "Gradient descent", "Graph (mathematics)", "Graph algorithm", "Graph traversal", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hillclimbing (disambiguation)", "Incremental heuristic search", "Integer programming", "International Standard Book Number", "Iterated local search", "Iterative deepening A*", "Iterative deepening depth-first search", "Iterative method", "Johnson's algorithm", "Jump point search", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Lexicographic breadth-first search", "Limited-memory BFGS", "Line search", "Linear programming", "List of algorithms", "Local convergence", "Local maximum", "Local minimum", "Local optimum", "Local search (optimization)", "Mathematical optimization", "Matroid", "Maxima and minima", "Mean-shift", "Meta-algorithm", "Metaheuristic", "Minimum spanning tree", "Motorsport", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization (mathematics)", "Optimization algorithm", "Penalty method", "Peter Norvig", "Powell's method", "Prim's algorithm", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Random optimization", "Random walk", "Reactive search optimization", "Revised simplex algorithm", "SMA*", "Search game", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Springer Science+Business Media", "Steven Skiena", "Stochastic hill climbing", "Stuart J. Russell", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Travelling salesman problem", "Tree traversal", "Truncated Newton method", "Trust region", "Vertex (graph theory)", "Walrasian auction", "Wolfe conditions"], "categories": ["All articles that may contain original research", "Articles that may contain original research from September 2007", "Optimization algorithms and methods", "Search algorithms"], "title": "Hill climbing"}
{"summary": "In computer science, Imperialist Competitive Algorithm (ICA) is a computational method that is used to solve optimization problems of different types. Like most of the methods in the area of evolutionary computation, ICA does not need the gradient of the function in its optimization process.\nFrom a specific point of view, ICA can be thought of as the social counterpart of genetic algorithms (GAs). ICA is the mathematical model and the computer simulation of human social evolution, while GAs are based on the biological evolution of species.", "links": ["Biological evolution", "Computer science", "Digital object identifier", "Evolutionary computation", "Genetic algorithms", "Loss function", "Optimization problem", "Particle Swarm Optimization", "Pseudocode", "Social evolution"], "categories": ["Optimization algorithms and methods", "Pages with reference errors"], "title": "Imperialist competitive algorithm"}
{"summary": "Intelligent Water Drops algorithm, or the IWD algorithm, is a swarm-based nature-inspired optimization algorithm. This algorithm contains a few essential elements of natural water drops and actions and reactions that occur between river's bed and the water drops that flow within. The IWD algorithm may fall into the category of Swarm intelligence and Metaheuristic. Intrinsically, the IWD algorithm can be used for Combinatorial optimization. However, it may be adapted for continuous optimization too. The IWD was first introduced for the traveling salesman problem in 2007. Since then, multitude of researchers have focused on improving the algorithm for different problems.", "links": ["Clique problem", "Combinatorial optimization", "Digital object identifier", "Job shop scheduling", "Knapsack problem", "Metaheuristic", "Steiner tree problem", "Swarm Intelligence", "Swarm intelligence", "Traveling salesman problem", "Travelling salesman problem", "Vehicle routing problem"], "categories": ["Artificial intelligence", "Collective intelligence", "Optimization algorithms and methods"], "title": "Intelligent Water Drops algorithm"}
{"summary": "Interior point methods (also referred to as barrier methods) are a certain class of algorithms that solves linear and nonlinear convex optimization problems.\n\nJohn von Neumann suggested an interior point method of linear programming which was neither a polynomial time method nor an efficient method in practice. In fact, it turned out to be slower in practice compared to simplex method which is not a polynomial time method. In 1984, Narendra Karmarkar developed a method for linear programming called Karmarkar's algorithm which runs in provably polynomial time and is also very efficient in practice. It enabled solutions of linear programming problems which were beyond the capabilities of simplex method. Contrary to the simplex method, it reaches a best solution by traversing the interior of the feasible region. The method can be generalized to convex programming based on a self-concordant barrier function used to encode the convex set.\nAny convex optimization problem can be transformed into minimizing (or maximizing) a linear function over a convex set by converting to the epigraph form. The idea of encoding the feasible set using a barrier and designing barrier methods was studied by Anthony V. Fiacco, Garth P. McCormick, and others in the early 1960s. These ideas were mainly developed for general nonlinear programming, but they were later abandoned due to the presence of more competitive methods for this class of problems (e.g. sequential quadratic programming).\nYurii Nesterov and Arkadi Nemirovski came up with a special class of such barriers that can be used to encode any convex set. They guarantee that the number of iterations of the algorithm is bounded by a polynomial in the dimension and accuracy of the solution.\nKarmarkar's breakthrough revitalized the study of interior point methods and barrier problems, showing that it was possible to create an algorithm for linear programming characterized by polynomial complexity and, moreover, that was competitive with the simplex method. Already Khachiyan's ellipsoid method was a polynomial time algorithm; however, it was too slow to be of practical interest.\nThe class of primal-dual path-following interior point methods is considered the most successful. Mehrotra's predictor-corrector algorithm provides the basis for most implementations of this class of methods.", "links": ["Algorithm", "Approximation algorithm", "Arkadi Nemirovski", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Cambridge University Press", "Candidate solution", "Claude Lemar\u00e9chal", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Convex set", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Diagonal matrix", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Epigraph", "Evolutionary algorithm", "Exchange algorithm", "Feasible region", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "International Standard Book Number", "Iteration", "Iterative method", "Jacobian matrix and determinant", "John von Neumann", "Johnson's algorithm", "KKT conditions", "Karmarkar's algorithm", "Karush\u2013Kuhn\u2013Tucker conditions", "Kruskal's algorithm", "Lagrange multiplier", "Lemke's algorithm", "Leonid Khachiyan", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear function", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical Reviews", "Mathematical optimization", "Matroid", "Mehrotra predictor-corrector method", "Metaheuristic", "Minimum spanning tree", "Narendra Karmarkar", "Nelder\u2013Mead method", "Newton's method in optimization", "Newton method", "Nonlinear conjugate gradient method", "Nonlinear optimization", "Nonlinear programming", "Optimization algorithm", "Penalty method", "Polynomial time", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Self-concordant", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions", "Yurii Nesterov"], "categories": ["CS1 errors: chapter ignored", "Optimization algorithms and methods", "Use dmy dates from February 2011"], "title": "Interior point method"}
{"summary": "Karmarkar's algorithm is an algorithm introduced by Narendra Karmarkar in 1984 for solving linear programming problems. It was the first reasonably efficient algorithm that solves these problems in polynomial time. The ellipsoid method is also polynomial time but proved to be inefficient in practice.\nWhere  is the number of variables and  is the number of bits of input to the algorithm, Karmarkar's algorithm requires  operations on  digit numbers, as compared to  such operations for the ellipsoid algorithm. The runtime of Karmarkar's algorithm is thus\n\nusing FFT-based multiplication (see Big O notation).\nKarmarkar's algorithm falls within the class of interior point methods: the current guess for the solution does not follow the boundary of the feasible set as in the simplex method, but it moves through the interior of the feasible region, improving the approximation of the optimal solution by a definite fraction with every iteration, and converging to an optimal solution with rational data.\n\n", "links": ["AT&T", "Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Big O notation", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Combinatorica", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Feasible set", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Foundation for a Free Information Infrastructure", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Gilbert Strang", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Interior point method", "International Standard Serial Number", "Iterative method", "Johnson's algorithm", "KORBX", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear Programming", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical Reviews", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Narendra Karmarkar", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Numerical analysis", "Optimization algorithm", "Penalty method", "Philip Gill", "Polynomial time", "Powell's method", "Prior art", "Projected Newton barrier method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "RSA (algorithm)", "Revised simplex algorithm", "Robert J. Vanderbei", "Ronald Rivest", "Sch\u00f6nhage\u2013Strassen algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simplex method", "Simulated annealing", "Software patent", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "The Mathematical Intelligencer", "The New York Times", "The Pentagon", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["Articles with example pseudocode", "Optimization algorithms and methods", "Pages containing cite templates with deprecated parameters", "Software patent law"], "title": "Karmarkar's algorithm"}
{"summary": "In competitive two-player games, the killer heuristic is a technique for improving the efficiency of alpha-beta pruning, which in turn improves the efficiency of the minimax algorithm. This algorithm has an exponential search time to find the optimal next move, so general methods for speeding it up are very useful.\nAlpha-beta pruning works best when the best moves are considered first. This is because the best moves are the ones most likely to produce a cutoff, a condition where the game playing program knows that the position it is considering could not possibly have resulted from best play by both sides and so need not be considered further. I.e. the game playing program will always make its best available move for each position. It only needs to consider the other player's possible responses to that best move, and can skip evaluation of responses to (worse) moves it will not make.\nThe killer heuristic attempts to produce a cutoff by assuming that a move that produced a cutoff in another branch of the game tree at the same depth is likely to produce a cutoff in the present position, that is to say that a move that was a very good move from a different (but possibly similar) position might also be a good move in the present position. By trying the killer move before other moves, a game playing program can often produce an early cutoff, saving itself the effort of considering or even generating all legal moves from a position.\nIn practical implementation, game playing programs frequently keep track of two killer moves for each depth of the game tree (greater than depth of 1) and see if either of these moves, if legal, produces a cutoff before the program generates and considers the rest of the possible moves. If a non-killer move produces a cutoff, it replaces one of the two killer moves at its depth. This idea can be generalized into a set of refutation tables.\nA generalization of the killer heuristic is the history heuristic. The history heuristic can be implemented as a table that is indexed by some characteristic of the move, for example \"from\" and \"to\" squares or piece moving and the \"to\" square. When there is a cutoff, the appropriate entry in the table is incremented, such as by adding d\u00b2 or 2d where d is the current search depth. This information is used when ordering moves.", "links": ["Alpha-beta pruning", "Exponential time", "Game tree", "Minimax algorithm", "Refutation table"], "categories": ["All articles lacking sources", "Articles lacking sources from October 2007", "Game artificial intelligence", "Heuristics", "Optimization algorithms and methods"], "title": "Killer heuristic"}
{"summary": "In mathematical optimization, Lemke's algorithm is a procedure for solving linear complementarity problems, and more generally mixed linear complementarity problems.\nLemke's algorithm is of pivoting or basis-exchange type. Similar algorithms can compute Nash equilibria for two-person matrix and bimatrix games.", "links": ["Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Data structure", "Davidon\u2013Fletcher\u2013Powell formula", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "George Dantzig", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "International Standard Book Number", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear complementarity problem", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical Reviews", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Mixed complementarity problem", "Mixed linear complementarity problem", "Nash equilibrium", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear complementarity problem", "Nonlinear conjugate gradient method", "Nonlinear programming", "Normal-form game", "Optimization (mathematics)", "Optimization algorithm", "Penalty method", "Pivot element", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Siconos", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Mathematical optimization", "Optimization algorithms and methods"], "title": "Lemke's algorithm"}
{"summary": "In mathematics and computing, the Levenberg\u2013Marquardt algorithm (LMA), also known as the damped least-squares (DLS) method, is used to solve non-linear least squares problems. These minimization problems arise especially in least squares curve fitting.\nThe LMA is used in many software applications for solving generic curve-fitting problems. However, as for many fitting algorithms, the LMA finds only a local minimum, which is not necessarily the global minimum. The LMA interpolates between the Gauss\u2013Newton algorithm (GNA) and the method of gradient descent. The LMA is more robust than the GNA, which means that in many cases it finds a solution even if it starts very far off the final minimum. For well-behaved functions and reasonable starting parameters, the LMA tends to be a bit slower than the GNA. LMA can also be viewed as Gauss\u2013Newton using a trust region approach.\nThe algorithm was first published in 1944 by Kenneth Levenberg, while working at the Frankford Army Arsenal. It was rediscovered in 1963 by Donald Marquardt who worked as a statistician at DuPont and independently by Girard, Wynn and Morrison.", "links": [".NET Framework", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "C++", "C (programming language)", "C programming language", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Curve fitting", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Donald Marquardt", "DuPont", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Estimation theory", "Evolutionary algorithm", "Exchange algorithm", "Fityk", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Fortran", "Frankford Arsenal", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "GNU General Public License", "GNU Octave", "GNU Scientific Library", "Gauss\u2013Newton algorithm", "Global minimum", "Gnuplot", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Haskell (programming language)", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "IDL (programming language)", "IGOR Pro", "Ill-posed problems", "Integer programming", "International Standard Book Number", "Iteration", "Iterative method", "Jacobian matrix and determinant", "Java (programming language)", "Johnson's algorithm", "Karmarkar's algorithm", "Kenneth Levenberg", "Kruskal's algorithm", "LabVIEW", "Least squares", "Lemke's algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local minimum", "Local search (optimization)", "MATLAB", "MEX file", "MINPACK", "Mathematica", "Mathematical optimization", "Mathematics", "Matlab", "Matroid", "Metaheuristic", "Minimum spanning tree", "NMath", "Nelder\u2013Mead method", "NeuroSolutions", "Newton's method in optimization", "Non-linear least squares", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization algorithm", "Origin (software)", "Penalty method", "Perl", "Perl Data Language", "Powell's method", "Public domain", "Push\u2013relabel maximum flow algorithm", "Python (programming language)", "Quadratic programming", "Quasi-Newton method", "R (programming language)", "Revised simplex algorithm", "Ridge regression", "Robustness (computer science)", "SAS (software)", "SIAM Journal on Numerical Analysis", "SciPy", "Scipy", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Sparse matrix", "Statistician", "Statistics", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Tikhonov regularization", "Truncated Newton method", "Trust region", "Walter Murray", "Wolfe conditions"], "categories": ["All articles with specifically marked weasel-worded phrases", "Articles with specifically marked weasel-worded phrases from July 2015", "Least squares", "Optimization algorithms and methods", "Pages with citations lacking titles", "Statistical algorithms"], "title": "Levenberg\u2013Marquardt algorithm"}
{"summary": "Limited-memory BFGS (L-BFGS or LM-BFGS) is an optimization algorithm in the family of quasi-Newton methods that approximates the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) algorithm using a limited amount of computer memory. It is a popular algorithm for parameter estimation in machine learning.\nLike the original BFGS, L-BFGS uses an estimation to the inverse Hessian matrix to steer its search through variable space, but where BFGS stores a dense n\u00d7n approximation to the inverse Hessian (n being the number of variables in the problem), L-BFGS stores only a few vectors that represent the approximation implicitly. Due to its resulting linear memory requirement, the L-BFGS method is particularly well suited for optimization problems with a large number of variables. Instead of the inverse Hessian Hk, L-BFGS maintains a history of the past m updates of the position x and gradient \u2207f(x), where generally the history size m can be small (often m<10). These updates are used to implicitly do operations requiring the Hk-vector product.", "links": ["Active set", "Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "BFGS method", "Backtracking line search", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "C++11", "C (programming language)", "Combinatorial optimization", "Comparison of optimization software", "Computer memory", "Conditional random field", "Constraint (mathematics)", "Convex function", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Differentiable", "Differentiable function", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Eigen (C++ library)", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "F2c", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Fortran", "Fortran 77", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "International Standard Book Number", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Loss function", "Machine learning", "Mathematical optimization", "Matlab", "Matroid", "Metaheuristic", "Minimum spanning tree", "Multinomial logit", "Nelder\u2013Mead method", "Netlib", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Online machine learning", "Optimization (mathematics)", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "R (programming language)", "Regularization (mathematics)", "Revised simplex algorithm", "SIAM Journal on Scientific Computing", "SciPy", "Sequential quadratic programming", "Shar", "Sign (mathematics)", "Simplex algorithm", "Simulated annealing", "Smooth function", "Sparse matrix", "Stochastic gradient descent", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Taxicab geometry", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["Optimization algorithms and methods"], "title": "Limited-memory BFGS"}
{"summary": "LINCOA (LINearly Constrained Optimization Algorithm) is a numerical optimization algorithm by Michael J. D. Powell. It is also the name of Powell's Fortran 77 implementation of the algorithm.\nLINCOA solves linearly constrained optimization problems without using derivatives of the objective function, which makes it a derivative-free algorithm. The algorithm solves the problem using a trust region method that forms quadratic models by interpolation. One new point is computed on each iteration, usually by solving a trust region subproblem subject to the linear constraints, or alternatively, by choosing a point to replace an interpolation point that may be too far away for reliability. In the second case, the new point may not satisfy the linear constraints.\nThe same as NEWUOA, LINCOA constructs the quadratic models by the least Frobenius norm updating  technique. A model function is determined by interpolating the objective function at  (an integer between  and ) points; the remaining freedom, if any, is taken up by minimizing the Frobenius norm of the change to the model's Hessian (with respect to the last iteration).\nLINCOA software was released on December 6, 2013. In the comment of the source code, it is said that LINCOA is not suitable for very large numbers of variables (which is typically true for algorithms not using derivatives), but \"a few calculations with 1000 variables, however, have been run successfully overnight, and the performance of LINCOA is satisfactory usually for small numbers of variables.\" It is also pointed out that the author's typical choices of  are  and , the latter \"being recommended for a start\", and \"larger values tend to be highly inefficent when the number of variables is substantial, due to the amount of work and extra difficulty of adjusting more points.\"\nThe trust region subproblem is solved by the truncated conjugate gradient method described in Powell's report, but Powell did not write a report on the other details of LINCOA.\nThe LINCOA software is distributed under The GNU Lesser General Public License (LGPL).", "links": ["Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "BOBYQA", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "COBYLA", "Combinatorial optimization", "Comment (computer programming)", "Comparison of optimization software", "Constrained optimization", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Derivative", "Derivative-free optimization", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Fortran", "Frank\u2013Wolfe algorithm", "Frobenius norm", "Function (mathematics)", "GNU Lesser General Public License", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Interpolation", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Michael J. D. Powell", "Minimum spanning tree", "NEWUOA", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Numerical analysis", "Optimization (mathematics)", "Optimization algorithm", "Optimization problem", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Software", "Source code", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "TOLMIN (optimization software)", "Tabu search", "Truncated Newton method", "Trust region", "UOBYQA", "Wolfe conditions"], "categories": ["All articles with links needing disambiguation", "Articles with links needing disambiguation from April 2014", "Optimization algorithms and methods"], "title": "LINCOA"}
{"summary": "In computer science, local search is a metaheuristic method for solving computationally hard optimization problems. Local search can be used on problems that can be formulated as finding a solution maximizing a criterion among a number of candidate solutions. Local search algorithms move from solution to solution in the space of candidate solutions (the search space) by applying local changes, until a solution deemed optimal is found or a time bound is elapsed.\nLocal search algorithms are widely applied to numerous hard computational problems, including problems from computer science (particularly artificial intelligence), mathematics, operations research, engineering, and bioinformatics. Examples of local search algorithms are WalkSAT and the 2-opt algorithm for the Traveling Salesman Problem.", "links": ["2-opt", "Anytime algorithm", "Approximation algorithm", "Artificial intelligence", "Bioinformatics", "Boolean satisfiability problem", "Candidate solution", "Clause (logic)", "Combinatorial optimization", "Computer science", "Constraint satisfaction", "Convex programming", "Cycle (graph theory)", "Engineering", "Facility location", "Graph (mathematics)", "Hill climbing", "Holger H. Hoos", "Hypersphere", "Incomplete algorithm", "Infinite-dimensional optimization", "Integer programming", "International Standard Book Number", "Iterated local search", "Iterative method", "Juraj Hromkovi\u010d", "K-medoid", "Local optimum", "Luus\u2013Jaakola", "Machine learning", "Mathematical optimization", "Mathematics", "Metaheuristic", "Multiobjective optimization", "Neighborhood relation", "Neighbourhood (mathematics)", "Nonlinear programming", "Normal distribution", "Nurse scheduling problem", "Operations research", "Optimization (mathematics)", "Pattern search (optimization)", "Quadratic programming", "Random optimization", "Random search", "Reactive search optimization", "Real number", "Robust optimization", "Shift work", "Simulated annealing", "Springer Verlag", "Stochastic optimization", "Stochastic programming", "Tabu search", "Travelling salesman problem", "Uniform distribution (continuous)", "Vertex cover", "Vertex cover problem", "Very large-scale neighborhood search", "WalkSAT"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from May 2015", "Optimization algorithms and methods"], "title": "Local search (optimization)"}
{"summary": "In computer science, the maximum subarray problem is the task of finding the contiguous subarray within a one-dimensional array of numbers (containing at least one positive number) which has the largest sum. For example, for the sequence of values \u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4; the contiguous subarray with the largest sum is 4, \u22121, 2, 1, with sum 6.\nThe problem was first posed by Ulf Grenander of Brown University in 1977, as a simplified model for maximum likelihood estimation of patterns in digitized images. A linear time algorithm was found soon afterwards by Jay Kadane of Carnegie-Mellon University (Bentley 1984).", "links": ["Algorithm", "Array data structure", "Brown University", "Carnegie-Mellon University", "Communications of the ACM", "Computer science", "Digital object identifier", "Dynamic programming", "Empty sum", "Jay Kadane", "Jon Bentley", "Linear time", "Maximum likelihood", "Python (programming language)", "Subset sum problem", "Ulf Grenander"], "categories": ["Articles with example Python code", "Dynamic programming", "Optimization algorithms and methods"], "title": "Maximum subarray problem"}
{"summary": "Multilevel Coordinate Search (MCS) is an algorithm for bound constrained global optimization using function values only.\nTo do so, the n-dimensional search space is represented by a set of non-intersecting hypercubes (boxes). The boxes are then iteratively split along an axis plane according to the value of the function at a representative point of the box and the box's size. These two splitting criteria combine to form a global search by splitting large boxes and a local search by splitting areas for which the function value is good.\nAdditionally a local search combining a quadratic interpolant of the function and line searches can be used to augment performance of the algorithm.", "links": ["Algorithm", "Applied mathematics", "Candidate solution", "Function (mathematics)", "Global optimization", "Line search"], "categories": ["All Wikipedia articles needing context", "All articles lacking sources", "All pages needing cleanup", "All stub articles", "Applied mathematics stubs", "Articles lacking sources from July 2012", "Optimization algorithms and methods", "Wikipedia articles needing context from October 2009", "Wikipedia introduction cleanup from October 2009"], "title": "MCS algorithm"}
{"summary": "Mehrotra's predictor\u2013corrector method in optimization is an implementation of interior point methods. It was proposed in 1989 by Sanjay Mehrotra.\nThe method is based on the fact that at each iteration of an interior point algorithm it is necessary to compute the Cholesky decomposition (factorization) of a large matrix to find the search direction. The factorization step is the most computationally expensive step in the algorithm. Therefore it makes sense to use the same decomposition more than once before recomputing it.\nAt each iteration of the algorithm, Mehrotra's predictor\u2013corrector method uses the same Cholesky decomposition to find two different directions: a predictor and a corrector.\nThe idea is to first compute an optimizing search direction based on a first order term (predictor). The step size that can be taken in this direction is used to evaluate how much centrality correction is needed. Then, a corrector term is computed: this contains both a centrality term and a second order term.\nThe complete search direction is the sum of the predictor direction and the corrector direction.\nAlthough there is no theoretical complexity bound on it yet, Mehrotra's predictor\u2013corrector method is widely used in practice. Its corrector step uses the same Cholesky decomposition found during the predictor step in an effective way, and thus it is only marginally more expensive than a standard interior point algorithm. However, the additional overhead per iteration is usually paid off by a reduction in the number of iterations needed to reach an optimal solution. It also appears to converge very fast when close to the optimum.", "links": ["Applied mathematics", "Cholesky decomposition", "Digital object identifier", "Interior point method", "Iteration", "Optimization (mathematics)", "Sanjay Mehrotra"], "categories": ["All stub articles", "Applied mathematics stubs", "Optimization algorithms and methods"], "title": "Mehrotra predictor\u2013corrector method"}
{"summary": "The MM algorithm is an iterative optimization method which exploits the convexity of a function in order to find their maxima or minima. The MM stands for \u201cMajorize-Minimization\u201d or \u201cMinorize-Maximization\u201d, depending on whether you're doing maximization or minimization. MM itself is not an algorithm, but a description of how to construct an optimization algorithm.\nThe EM algorithm can be treated as a special case of the MM algorithm. However, in the EM algorithm complex conditional expectation and extensive analytical skills are usually involved, while in the MM algorithm convexity and inequalities are our major focus, and it is relatively easier to understand and apply in most of the cases.\n\n", "links": ["Cauchy\u2013Schwarz inequality", "Conditional expectation", "Convex function", "Convexity inequality", "Digital object identifier", "EM algorithm", "Engineering", "Inequality of arithmetic and geometric means", "Jensen's inequality", "Journal of Computational and Graphical Statistics", "Line search", "Machine learning", "Mathematics", "New York: Academic", "Optimization", "Optimization algorithm", "Optimum", "Statistics"], "categories": ["Optimization algorithms and methods"], "title": "MM algorithm"}
{"summary": "Negamax search is a variant form of minimax search that relies on the zero-sum property of a two-player game.\nThis algorithm relies on the fact that max(a, b) = \u2212min(\u2212a, \u2212b) to simplify the implementation of the minimax algorithm. More precisely, the value of a position to player A in such a game is the negation of the value to player B. Thus, the player on move looks for a move that maximizes the negation of the value of the position resulting from the move: this successor position must by definition have been valued by the opponent. The reasoning of the previous sentence works regardless of whether A or B is on move. This means that a single procedure can be used to value both positions. This is a coding simplification over minimax, which requires that A select the move with the maximum-valued successor while B selects the move with the minimum-valued successor.\nIt should not be confused with negascout, an algorithm to compute the minimax or negamax value quickly by clever use of alpha-beta pruning discovered in the 1980s. Note that alpha-beta pruning is itself a way to compute the minimax or negamax value of a position quickly by avoiding the search of certain uninteresting positions.\nMost adversarial search engines are coded using some form of negamax search.", "links": ["Adversarial search", "Alpha-beta pruning", "Foreach loop", "International Standard Book Number", "MTD-f", "Minimax", "Negascout", "Oreilly Media", "Pseudocode", "Transposition table", "Two-player game", "UMI Research Press", "Zero-sum (Game theory)"], "categories": ["Articles with example pseudocode", "Game artificial intelligence", "Optimization algorithms and methods"], "title": "Negamax"}
{"summary": "See simplex algorithm for Dantzig's algorithm for the problem of linear optimization.\nThe Nelder\u2013Mead method or downhill simplex method or amoeba method is a commonly applied numerical method used to find the minimum or maximum of an objective function in a many-dimensional space. It is applied to nonlinear optimization problems for which derivatives may not be known. However, the Nelder\u2013Mead technique is a heuristic search method that can converge to non-stationary points on problems that can be solved by alternative methods.\nThe Nelder\u2013Mead technique was proposed by John Nelder & Roger Mead (1965).", "links": ["Approximation algorithm", "Augmented Lagrangian method", "BFGS method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "CMA-ES", "COBYLA", "Centroid", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Derivative-free optimization", "Differential evolution", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "George B. Dantzig", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic", "Heuristic algorithm", "Hill climbing", "Himmelblau's function", "Integer programming", "International Standard Book Number", "Iterative method", "John Nelder", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "LINCOA", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Michael J. D. Powell", "Minimum spanning tree", "NEWUOA", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Numerical method", "Objective function", "Optimization (mathematics)", "Optimization algorithm", "Pattern search (optimization)", "Penalty method", "Polytope", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Rosenbrock function", "Sequential quadratic programming", "Simplex", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Tetrahedron", "Truncated Newton method", "Trust region", "Unimodal", "Wolfe conditions"], "categories": ["Operations research", "Optimization algorithms and methods"], "title": "Nelder\u2013Mead method"}
{"summary": "In numerical analysis, Newton's method (also known as the Newton\u2013Raphson method), named after Isaac Newton and Joseph Raphson, is a method for finding successively better approximations to the roots (or zeroes) of a real-valued function.\n\nThe Newton\u2013Raphson method in one variable is implemented as follows:\nGiven a function \u0192 defined over the reals x, and its derivative \u0192', we begin with a first guess x0 for a root of the function f. Provided the function satisfies all the assumptions made in the derivation of the formula, a better approximation x1 is\n\nGeometrically, (x1, 0) is the intersection with the x-axis of the tangent to the graph of f at (x0, f (x0)).\nThe process is repeated as\n\nuntil a sufficiently accurate value is reached.\nThis algorithm is first in the class of Householder's methods, succeeded by Halley's method. The method can also be extended to complex functions and to systems of equations.", "links": ["Abraham de Moivre", "Absolute time and space", "Aitken's delta-squared process", "Almost all", "An Historical Account of Two Notable Corruptions of Scripture", "Approximation algorithm", "Arithmetica Universalis", "Arthur Cayley", "Augmented Lagrangian method", "Babylonian method", "Banach space", "Barrier function", "Basin of attraction", "Bellman\u2013Ford algorithm", "Benjamin Pulleyn", "Bisection method", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Bucket argument", "Calculus", "Catherine Barton", "Chaos theory", "Classical mechanics", "Claude Lemar\u00e9chal", "Combinatorial optimization", "Comparison of optimization software", "Complex analysis", "Convex minimization", "Convex optimization", "Copernican Revolution", "Corpuscular theory of light", "Cranbury Park", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "De analysi per aequationes numero terminorum infinitas", "De motu corporum in gyrum", "Derivative", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Division algorithm", "Division by zero", "Dynamic programming", "Dynamics (mechanics)", "Early life of Isaac Newton", "Edmonds\u2013Karp algorithm", "Elements of the Philosophy of Newton", "Ellipsoid method", "Encyclopedia of Mathematics", "Endre S\u00fcli", "Eric W. Weisstein", "Euler method", "Evolutionary algorithm", "Exchange algorithm", "Fast inverse square root", "Finite difference", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Fractal", "Franciscus Vieta", "Frank\u2013Wolfe algorithm", "Fr\u00e9chet derivative", "Function (mathematics)", "Functional (mathematics)", "Gauss\u2013Newton algorithm", "General Scholium", "Generalized Gauss\u2013Newton method", "Generalized inverse", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Graph of a function", "Gravitational constant", "Greedy algorithm", "Halley's method", "Hensel's lemma", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Householder's method", "Hypotheses non fingo", "Impact depth", "Inertia", "Integer programming", "Integer square root", "Intermediate value theorem", "International Standard Book Number", "Interval (mathematics)", "Isaac Barrow", "Isaac Newton", "Isaac Newton's occult studies", "Isaac Newton S/O Philipose", "Isaac Newton in popular culture", "Iterative method", "Jacobian matrix", "Jacobian matrix and determinant", "Jamsh\u012bd al-K\u0101sh\u012b", "John Colson", "John Conduitt", "John Keill", "John Wallis", "Johnson's algorithm", "Joseph Fourier", "Joseph Raphson", "Kantorovich theorem", "Karmarkar's algorithm", "Kepler's laws of planetary motion", "Kissing number problem", "Kruskal's algorithm", "Lagrange remainder", "Laguerre's method", "Later life of Isaac Newton", "Leibniz\u2013Newton calculus controversy", "Lemke's algorithm", "Leonid Kantorovich", "Levenberg\u2013Marquardt algorithm", "Limit of a sequence", "Limited-memory BFGS", "Line search", "Linear programming", "List of things named after Isaac Newton", "Local convergence", "Local search (optimization)", "MathWorld", "Mathematical Gazette", "Mathematical Reviews", "Mathematical optimization", "Mathematics in medieval Islam", "Matroid", "Metaheuristic", "Method of Fluxions", "Methods of computing square roots", "Minimum spanning tree", "Multiplicative inverse", "Multiplicity (mathematics)", "Neighbourhood (mathematics)", "Nelder\u2013Mead method", "Newton's cannonball", "Newton's cradle", "Newton's identities", "Newton's inequalities", "Newton's law of cooling", "Newton's law of universal gravitation", "Newton's laws of motion", "Newton's metal", "Newton's method in optimization", "Newton's reflector", "Newton's rings", "Newton's theorem about ovals", "Newton's theorem of revolving orbits", "Newton (Blake)", "Newton (unit)", "Newton disc", "Newton fractal", "Newton polygon", "Newton polynomial", "Newton scale", "Newtonian dynamics", "Newtonian fluid", "Newtonian potential", "Newtonian telescope", "Newtonianism", "Newton\u2013Cartan theory", "Newton\u2013Cotes formulas", "Newton\u2013Euler equations", "Newton\u2013Okounkov body", "Newton\u2013Pepys problem", "Non-linear least squares", "Nonlinear conjugate gradient method", "Nonlinear programming", "Notes on the Jewish Temple", "Numerical analysis", "Opticks", "Optimization algorithm", "Parameterized post-Newtonian formalism", "Penalty method", "Philosophi\u00e6 Naturalis Principia Mathematica", "Post-Newtonian expansion", "Powell's method", "Power number", "Power series", "Problem of Apollonius", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quaestiones quaedam philosophicae", "Quasi-Newton method", "Rate of convergence", "Real number", "Religious views of Isaac Newton", "Revised simplex algorithm", "Richardson extrapolation", "Root-finding algorithm", "Root of a function", "Rotating spheres", "Schr\u00f6dinger\u2013Newton equation", "Scientific revolution", "Scoring algorithm", "Secant method", "Second derivative", "Seki K\u014dwa", "Sequence", "Sequential quadratic programming", "Sextant", "Sharaf al-Din al-Tusi", "Simplex algorithm", "Simulated annealing", "Solar mass", "Springer Science+Business Media", "Standing on the shoulders of giants", "Stationary point", "Steffensen's method", "Structural coloration", "Subgradient method", "Subroutine", "Successive linear programming", "Successive over-relaxation", "Successive parabolic interpolation", "Supremum", "Symmetric rank-one", "System of linear equations", "Table of Newtonian series", "Tabu search", "Tangent", "Tangent line", "Taylor's theorem", "The Chronology of Ancient Kingdoms Amended", "The College Mathematics Journal", "The Mysteryes of Nature and Art", "The Queries", "Thomas Simpson", "Topological neighborhood", "Transcendental equation", "Transcendental function", "Truncated Newton method", "Trust region", "Wikibooks", "William Clarke (apothecary)", "William Jones (mathematician)", "William Stukeley", "Wolfe conditions", "Woolsthorpe Manor", "Writing of Principia Mathematica"], "categories": ["All articles lacking in-text citations", "All articles needing additional references", "Articles lacking in-text citations from February 2014", "Articles needing additional references from February 2014", "Articles needing additional references from November 2013", "Articles with inconsistent citation formats", "Commons category with local link same as on Wikidata", "Optimization algorithms and methods", "Root-finding algorithms", "Use dmy dates from January 2012"], "title": "Newton's method"}
{"summary": "NEWUOA is a numerical optimization algorithm by Michael J. D. Powell. It is also the name of Powell's Fortran 77 implementation of the algorithm.\nNEWUOA solves unconstrained optimization problems without using derivatives, which makes it a derivative-free algorithm. The algorithm is iterative, and exploits trust region technique. On each iteration, the algorithm establishes a model function  by quadratic interpolation, and then minimizes  within a trust region.\nOne important feature of NEWUOA algorithm is the least Frobenius norm updating  technique. Suppose that the objective function  has  variables, and one wants to uniquely determine the quadratic model  by purely interpolating the function values of , then it is necessary to evaluate  at  points, as a quadratic polynomial of  variables has this amount of independent coefficients. But this is impractical when  is large, because the function values are supposed to be expensive in derivative-free optimization. In NEWUOA, the model  interpolates only  (an integer between  and , typically of order ) function values of , and the remaining degree of freedom is taken up by minimizing the Frobenius norm of . This technique mimics the least change secant updates  for Quasi-Newton methods, and can be considered as the derivative-free version of PSB update (Powell's Symmetric Broyden update).\nTo construct the models, NEWUOA maintains a set of interpolation points throughout the iterations. The update of this set is another feature of NEWUOA.\nNEWUOA algorithm was developed from UOBYQA (Unconstrained Optimization BY Quadratic Approximation). A major difference between them is that UOBYQA constructs quadratic models by interpolating the objective function at  points.\nNEWUOA software was released on December 16, 2004. It can solve unconstrained optimization problems of a few hundreds variables to high precision without using derivatives. In the software,  is set to  by default.\nOther derivative-free optimization algorithms by Powell include COBYLA, UOBYQA, BOBYQA, and LINCOA. BOBYQA and LINCOA are extensions of NEWUOA to bound constrained and linearly constrained optimization respectively.\nPowell did not explain how he coined the name \"NEWUOA\" either in the introducing report  or in the software, although COBYLA, UOBYQA, BOBYQA and LINCOA are all named by acronyms. Probably \"NEWUOA\" means \"NEW Unconstrained Optimization Algorithm\".\nThe NEWUOA software is distributed under The GNU Lesser General Public License (LGPL).", "links": ["Acronyms", "Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "BOBYQA", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "COBYLA", "Charles George Broyden", "Combinatorial optimization", "Comparison of optimization software", "Constrained optimization", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Derivative", "Derivative-free optimization", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Fortran", "Frank\u2013Wolfe algorithm", "Frobenius norm", "Function (mathematics)", "GNU Lesser General Public License", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Interpolation", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "LINCOA", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Michael J. D. Powell", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Numerical analysis", "Optimization (mathematics)", "Optimization algorithm", "Optimization problem", "Orders of approximation", "PSB update", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic interpolation", "Quadratic programming", "Quasi-Newton method", "Quasi-Newton methods", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "TOLMIN (optimization software)", "Tabu search", "Truncated Newton method", "Trust region", "UOBYQA", "Wolfe conditions"], "categories": ["Optimization algorithms and methods"], "title": "NEWUOA"}
{"summary": "Pattern search (PS) is a family of numerical optimization methods that do not require the gradient of the problem to be optimized. Hence PS can be used on functions that are not continuous or differentiable. Such optimization methods are also known as direct-search, derivative-free, or black-box methods.\nThe name, pattern search, was coined by Hooke and Jeeves. An early and simple PS variant is attributed to Fermi and Metropolis when they worked at the Los Alamos National Laboratory as described by Davidon  who summarized the algorithm as follows:\n\nThey varied one theoretical parameter at a time by steps of the same magnitude, and when no such increase or decrease in any one parameter further improved the fit to the experimental data, they halved the step size and repeated the process until the steps were deemed sufficiently small.", "links": ["Combinatorial optimization", "Constraint satisfaction", "Continuous function", "Convex programming", "Differentiable", "Digital object identifier", "Enrico Fermi", "Golden section search", "Gradient", "Heuristic", "Hypersphere", "Infinite-dimensional optimization", "Integer programming", "Iterative method", "Los Alamos National Laboratory", "Luus\u2013Jaakola", "Metaheuristic", "Michael J. D. Powell", "Multiobjective optimization", "Nelder\u2013Mead method", "Nicholas Metropolis", "Nonlinear programming", "Normal distribution", "Optimization (mathematics)", "Pattern search (disambiguation)", "Positive basis", "Quadratic programming", "Random optimization", "Random search", "Robust optimization", "Stochastic programming", "Uniform distribution (continuous)"], "categories": ["Mathematical optimization", "Optimization algorithms and methods"], "title": "Pattern search (optimization)"}
{"summary": "Penalty methods are a certain class of algorithms for solving constrained optimization problems.\nA penalty method replaces a constrained optimization problem by a series of unconstrained problems whose solutions ideally converge to the solution of the original constrained problem. The unconstrained problems are formed by adding a term, called a penalty function, to the objective function that consists of a penalty parameter multiplied by a measure of violation of the constraints. The measure of violation is nonzero when the constraints are violated and is zero in the region where constraints are not violated.", "links": ["Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Barrier method (mathematics)", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Constraint (mathematics)", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Image compression", "Integer programming", "Interior point method", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Objective function", "Optimization (mathematics)", "Optimization algorithm", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["Optimization algorithms and methods"], "title": "Penalty method"}
{"summary": "Powell's method, strictly Powell's conjugate direction method, is an algorithm proposed by Michael J. D. Powell for finding a local minimum of a function. The function need not be differentiable, and no derivatives are taken.\nThe function must be a real-valued function of a fixed number of real-valued inputs. The caller passes in the initial point. The caller also passes in a set of initial search vectors. Typically N search vectors are passed in which are simply the normals aligned to each axis.\nThe method minimises the function by a bi-directional search along each search vector, in turn. The new position can then be expressed as a linear combination of the search vectors. The new displacement vector becomes a new search vector, and is added to the end of the search vector list. Meanwhile, the search vector which contributed most to the new direction, i.e. the one which was most successful, is deleted from the search vector list. The algorithm iterates an arbitrary number of times until no significant improvement is made.\nThe method is useful for calculating the local minimum of a continuous but complex function, especially one without an underlying mathematical definition, because it is not necessary to take derivatives. The basic algorithm is simple; the complexity is in the linear searches along the search vectors, which can be achieved via Brent's method.", "links": ["Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Brent's method", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "International Standard Book Number", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Maxima and minima", "Metaheuristic", "Michael J. D. Powell", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization algorithm", "Penalty method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["All articles needing additional references", "Articles needing additional references from August 2009", "Optimization algorithms and methods"], "title": "Powell's method"}
{"summary": "pSeven is a design space exploration software platform developed by DATADVANCE LLC, extending design, simulation and analysis capabilities and assisting in smarter and faster design decisions. It provides a seamless integration with third party CAD and CAE software tools, powerful multi-objective and robust optimization algorithms, data mining and uncertainty management tools. pSeven comes under the notion of PIDO (Process Integration and Design Optimization) software. Design space exploration functionality is based on the mathematical algorithms of Macros software library, also developed by DATADVANCE. pSeven is used for the needs of predicting system behavior, analyzing the existing experimental data, estimating model uncertainties or performing multidisciplinary design optimization. pSeven implements DATADVANCE strategy to:\nEnable engineers to easily use mathematical algorithms thanks to \u201cSmart Selection\u201d technique, which automatically and adaptively selects the most suitable algorithm for a given problem from a pool of available methods;\nAddress complex industrial challenges: architecture trade-off, optimization using expensive simulations.\n\n^ Trade Delegation of the Russian Federation in the United Kingdom\n^ https://www.datadvance.net/product/technology/", "links": ["ASCON", "Abaqus", "Airbus Group", "Ansys", "Approximation", "Box-Behnken design", "CAD", "CATIA", "CD-adapco", "C (programming language)", "Central processing unit", "Computer-aided engineering", "Computer Simulation Technology", "Constraint satisfaction problem", "Data analysis", "Data fusion", "Data mining", "Dependence analysis", "Design of Experiments", "Design of experiments", "Design space exploration", "Engineering optimization", "Experimental data", "Factorial experiment", "GNU Octave", "Gaussian process", "Halton sequence", "High Performance Computing", "Kolmogorov-Smirnov test", "Kriging", "LS-DYNA", "LSF", "Latin hypercube sampling", "Linear Regression", "Linear programming", "MATLAB", "MSC ADAMS", "Mathematical optimization", "Mentor Graphics", "Monte-Carlo method", "Multi-objective optimization", "Multidisciplinary design optimization", "Nastran", "Nonlinear programming", "OpenFOAM", "Operating system", "Optimal design", "PIDO", "PTC Creo", "Parallel coordinates", "Pearson product-moment correlation coefficient", "Quadratic programming", "Random sequence", "Response surface methodology", "Robust optimization", "Russian Academy of Sciences", "SLURM", "Scatter plot", "Scilab", "Sensitivity analysis", "Siemens NX", "Simulink", "Sobol sequence", "Software developer", "Software license", "Software release life cycle", "SolidWorks", "Spearman's rank correlation coefficient", "Spline", "Statistical model", "Surrogate model", "TORQUE", "Taguchi methods", "Uncertainty quantification"], "categories": ["All articles needing style editing", "Computer system optimization software", "Mathematical optimization software", "Optimization algorithms and methods", "Wikipedia articles needing style editing from March 2015"], "title": "PSeven"}
{"summary": "Reactive search optimization (RSO) defines local-search heuristics based on machine learning, a family of optimization algorithms based on the local search techniques. It refers to a class of heuristics that automatically adjust their working parameters during the optimization phase. RSO methods are at the basis of the Learning and Intelligent Optimization (LION) approach combining machine learning and optimization .", "links": ["Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Business analytics", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Data mining", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Genetic algorithm", "Global optimization", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Heuristics", "Hill climbing", "Integer programming", "Interactive visualization", "International Standard Book Number", "Iterated local search", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "LIONsolver", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local optimum", "Local search (optimization)", "Machine learning", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Online machine learning", "Optimization (mathematics)", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Reactive business intelligence", "Revised simplex algorithm", "Satisfiability problem", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Springer Verlag", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Travelling salesman problem", "Truncated Newton method", "Trust region", "Vitruvian Man", "Wolfe conditions"], "categories": ["All articles with peacock terms", "All articles with topics of unclear notability", "All articles with unsourced statements", "Articles with peacock terms from December 2014", "Articles with topics of unclear notability from December 2014", "Articles with unsourced statements from July 2013", "Heuristics", "Optimization algorithms and methods", "Wikipedia articles with possible conflicts of interest from December 2014"], "title": "Reactive search optimization"}
{"summary": "Sequential minimal optimization (SMO) is an algorithm for solving the quadratic programming (QP) problem that arises during the training of support vector machines. It was invented by John Platt in 1998 at Microsoft Research. SMO is widely used for training support vector machines and is implemented by the popular LIBSVM tool. The publication of the SMO algorithm in 1998 has generated a lot of excitement in the SVM community, as previously available methods for SVM training were much more complex and required expensive third-party QP solvers.", "links": ["Bernhard E Boser", "Best, worst and average case", "Binary classification", "Bregman method", "CiteSeer", "Digital object identifier", "Dual problem", "E. Osuna", "F. Girosi", "International Standard Book Number", "Isabelle M Guyon", "John Platt (Principal Researcher)", "Karush\u2013Kuhn\u2013Tucker conditions", "Kernel function", "Kernel perceptron", "LIBSVM", "Lagrange multiplier", "Microsoft Research", "Optimization algorithm", "Quadratic programming", "R. Freund", "Support vector machine", "Vladimir Vapnik"], "categories": ["Optimization algorithms and methods", "Support vector machines"], "title": "Sequential minimal optimization"}
{"summary": "The shuffled frog leaping algorithm (SFLA) is an optimization algorithm used in artificial intelligence (AI). It is like a genetic algorithm.", "links": ["Artificial intelligence", "Genetic algorithm"], "categories": ["All stub articles", "Artificial intelligence stubs", "Mathematical optimization", "Optimization algorithms and methods", "Search algorithms"], "title": "Shuffled frog leaping algorithm"}
{"summary": "Simultaneous perturbation stochastic approximation (SPSA) is an algorithmic method for optimizing systems with multiple unknown parameters. It is a type of stochastic approximation algorithm. As an optimization method, it is appropriately suited to large-scale population models, adaptive modeling, simulation optimization, and atmospheric modeling. Many examples are presented at the SPSA website http://www.jhuapl.edu/SPSA. A comprehensive recent book on the subject is Bhatnagar et al. (2013). An early paper on the subject is Spall (1987) and the foundational paper providing the key theory and justification is Spall (1992).\nSPSA is a descent method capable of finding global minima, sharing this property with other methods as simulated annealing. Its main feature is the gradient approximation that requires only two measurements of the objective function, regardless of the dimension of the optimization problem. Recall that we want to find the optimal control  with loss function :\n\nBoth Finite Differences Stochastic Approximation (FDSA) and SPSA use the same iterative process:\n\nwhere  represents the  iterate,  is the estimate of the gradient of the objective function  evaluated at , and  is a positive number sequence converging to 0. If  is a p-dimensional vector, the  component of the symmetric finite difference gradient estimator is:\nFD: \n1 \u2264i \u2264p, where  is the unit vector with a 1 in the  place, and is a small positive number that decreases with n. With this method, 2p evaluations of J for each  are needed. Clearly, when p is large, this estimator loses efficiency.\nLet now  be a random perturbation vector. The  component of the stochastic perturbation gradient estimator is:\nSP: \nRemark that FD perturbs only one direction at a time, while the SP estimator disturbs all directions at the same time (the numerator is identical in all p components). The number of loss function measurements needed in the SPSA method for each  is always 2, independent of the dimension p. Thus, SPSA uses p times fewer function evaluations than FDSA, which makes it a lot more efficient.\nSimple experiments with p=2 showed that SPSA converges in the same number of iterations as FDSA. The latter follows approximately the steepest descent direction, behaving like the gradient method. On the other hand, SPSA, with the random search direction, does not follow exactly the gradient path. In average though, it tracks it nearly because the gradient approximation is an almost unbiased estimator of the gradient, as shown in the following lemma.", "links": ["Algorithmic", "Approximation", "Atmospheric model", "Convergence (mathematics)", "Differentiable", "Dimension", "Finite Differences Stochastic Approximation", "Hypothesis", "Idea", "Optimization", "Parameters", "Probability", "Rademacher distribution", "Steepest", "Stochastic approximation", "Symmetric", "Unbiased"], "categories": ["Numerical climate and weather models", "Optimization algorithms and methods", "Stochastic algorithms"], "title": "Simultaneous perturbation stochastic approximation"}
{"summary": "In discrete optimization, a special ordered set (SOS) is an ordered set of variables, used as an additional way to specify integrality conditions in an optimization model. Special order sets are basically a device or tool used in branch and bound methods for branching on sets of variables, rather than individual variables, as in ordinary mixed integer programming. Knowing that a variable is part of a set and that it is ordered gives the branch and bound algorithm a more intelligent way to face the optimization problem, helping to speed up the search procedure. The members of a special ordered set individually may be continuous or discrete variables in any combination. However, even when all the members are themselves continuous, a model containing one or more special ordered sets becomes a discrete optimization problem requiring a mixed integer optimizer for its solution.\nThe \u2018only\u2019 bene\ufb01t of using Special Ordered Sets compared with using only constraints, is that the search procedure will generally be noticeably faster.\nAs per J.A. Tomlin, Special Order Sets provide a powerful means of modeling nonconvex functions and discrete requirements, though there has been a tendency to think of them only in terms of multiple-choice zero-one programming.", "links": ["Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Dijkstra's algorithm", "Dinic's algorithm", "Discrete optimization", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Set (mathematics)", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["Mathematical optimization", "Optimization algorithms and methods"], "title": "Special ordered set"}
{"summary": "Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.\nSI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of \"intelligent\" global behavior, unknown to the individual agents. Examples in natural systems of SI include ant colonies, bird flocking, animal herding, bacterial growth, fish schooling and microbial intelligence.\nThe application of swarm principles to robots is called swarm robotics, while 'swarm intelligence' refers to the more general set of algorithms. 'Swarm prediction' has been used in the context of forecasting problems.", "links": ["Active matter", "Agent-based model", "Agent-based model in biology", "Algorithm", "Allee effect", "Altitudinal migration", "Animal cognition", "Animal communication", "Animal consciousness", "Animal echolocation", "Animal language", "Animal migration", "Animal migration tracking", "Animal navigation", "Ant-based routing", "Ant colony", "Ant colony optimization", "Ant colony optimization algorithms", "Ant robotics", "Approximation algorithm", "ArXiv", "Artificial Ants", "Artificial bee colony algorithm", "Artificial immune systems", "Artificial intelligence", "Atlantis", "Augmented Lagrangian method", "Bacteria", "Bait ball", "Barrier function", "Bat algorithm", "Batman Returns", "Bees algorithm", "Bellman\u2013Ford algorithm", "Bibcode", "Bird intelligence", "Bird migration", "Boids", "Bor\u016fvka's algorithm", "Brain-to-body mass ratio", "Brain size", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Bruce Sterling", "Cat intelligence", "Cell migration", "Cellular automaton", "Cephalopod intelligence", "Cetacean intelligence", "CiteSeer", "Clustering of self-propelled particles", "Coded wire tag", "Cognitive bias in animals", "Cognitive ethology", "Collective animal behavior", "Collective behavior", "Collective intelligence", "Collective motion", "Combinatorial optimization", "Comparative cognition", "Comparison of optimization software", "Complex systems", "Convex minimization", "Convex optimization", "Craig Reynolds (computer graphics)", "Criss-cross algorithm", "Crowd simulation", "Cutting-plane method", "Daniel Suarez (author)", "Davidon\u2013Fletcher\u2013Powell formula", "Decentralised system", "Decentralization", "Decipher (novel)", "Deleuze", "Diel vertical migration", "Differential evolution", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dinosaur intelligence", "Dog intelligence", "Douglas A. Lawson", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Elephant cognition", "Ellipsoid method", "Emergence", "Emergent behaviour", "Emotion in animals", "Encephalization quotient", "Ender's Game", "Endo-exo", "European Space Agency", "Eusociality", "Evolution of human intelligence", "Evolutionary algorithm", "Evolutionary computation", "Exchange algorithm", "Face Dancer", "Feeding frenzy", "Firefly algorithm", "Fish intelligence", "Fish migration", "Fitness (biology)", "Flock (birds)", "Flocking (behavior)", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Formics", "Frank Sch\u00e4tzing", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "George A. Bekey", "Gerardo Beni", "Geth", "Global brain", "Global optimization", "Glowworm", "Glowworm swarm optimization", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Group mind (science fiction)", "Group size measures", "Hallucination (short story)", "Halo (series)", "Harmony search", "Herd", "Herd behavior", "Herding", "Hessian matrix", "Heuristic algorithm", "Hewlett Packard", "Hill climbing", "Homing (biology)", "Honey bees", "Hybrid algorithm", "Hypothesis", "Insect migration", "Integer programming", "Intelligent Small World Autonomous Robots for Micro-manipulation", "Intelligent agent", "International Standard Book Number", "Isaac Asimov", "Iterative method", "J. Phys. A", "James Kennedy (social psychologist)", "Jean-Baptiste Waldner", "Johnson's algorithm", "Karmarkar's algorithm", "Khrone", "Kill Decision", "Kruskal's algorithm", "Last and First Men", "Law of gravity", "Legends of Dune", "Lemke's algorithm", "Lepidoptera migration", "Leptothorax acervorum", "Lessepsian migration", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "List of animals by number of neurons", "Local convergence", "Local minima", "Local search (optimization)", "Louis B. Rosenberg", "Luca Maria Gambardella", "Luciferin", "Luminescence", "M. Anthony Lewis (roboticist)", "Marco Dorigo", "Mars", "Mass Effect", "Massive (software)", "Mathematical optimization", "Matroid", "Metaheuristic", "Michael Crichton", "Michael Ende", "Microbat", "Microbial intelligence", "Microbotics", "Minimum spanning tree", "Mirror test", "Mixed-species foraging flock", "Mobbing (animal behavior)", "Monarch butterfly migration", "Multi-agent system", "Multi-swarm optimization", "Mutualism (biology)", "Myrmecology", "NASA", "Nanobots", "Natal homing", "National Geographic Magazine", "Nelder\u2013Mead method", "Neuroethology", "Neuroscience and intelligence", "Newton's method in optimization", "Nikolaus Correll", "Nonlinear conjugate gradient method", "Nonlinear programming", "Observational learning", "Olaf Stapledon", "Omnius", "Optimization (mathematics)", "Optimization algorithm", "Otto-von-Guericke University of Magdeburg", "Pack (canine)", "Pack hunter", "Pain in animals", "Pain in crustaceans", "Pain in fish", "Pain in invertebrates", "Parameter space", "Particle swarm optimization", "Patterns of self-organization in ants", "Penalty method", "Penumbra: Black Plague", "Pheromone", "Philopatry", "Physica A", "Physical Review Letters", "Powell's method", "Predator satiation", "Prey (novel)", "Primate cognition", "Probabilistic algorithm", "Promise theory", "PubMed Identifier", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quarians", "Quasi-Newton method", "Quorum sensing", "Radio waves", "Random selection", "Reinforcement learning", "Reverse migration (birds)", "Revised simplex algorithm", "Robot", "Rudy Rucker", "Rule 110", "Russell C. Eberhart", "Salmon run", "Sandworms of Dune", "Sardine run", "Science fiction", "Sea turtle migration", "Self-organization", "Self-organized criticality", "Self-propelled particles", "Sequential quadratic programming", "Shoaling and schooling", "Simplex algorithm", "Simulated annealing", "Sort sol (bird flock)", "Southwest Airlines", "Spatial organization", "Stanis\u0142aw Lem", "Stanley and Stella in: Breaking the Ice", "Star Maker", "Starcraft (series)", "Stel Pavlou", "Stigmergy", "Stochastic Diffusion Search", "Stochastic diffusion search", "Stochastic optimization", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Swarm (novelette)", "Swarm (simulation)", "Swarm Development Group", "Swarm Wall", "Swarm behaviour", "Swarm robotics", "Swarming (honey bee)", "Swarming (military)", "Swarming motility", "SwisTrack", "Symbrion", "Symmetric rank-one", "Symmetry breaking of escaping ants", "Tabu search", "Talking bird", "Tamas Vicsek", "Task allocation and partitioning of social insects", "Telecommunications network", "The Andromeda Strain", "The Hacker and the Ants", "The Invincible", "The Lord of the Rings (film series)", "The Neverending Story", "The New York Times", "The Swarm (novel)", "The Wisdom of Crowds", "Tool use by animals", "Truncated Newton method", "Trust region", "Unmanned combat air vehicle", "Velocity", "Vicsek model", "Vocal learning", "Waggle dance", "Weaver ant", "Wisdom of the crowd", "Wolfe conditions", "Ygramul", "Yuhui Shi", "Zerg"], "categories": ["All articles needing additional references", "All articles with unsourced statements", "Articles needing additional references from August 2013", "Articles with unsourced statements from September 2015", "Behavioral and social facets of systemic risk", "Collective intelligence", "Intelligence by type", "Multi-agent systems", "Optimization algorithms and methods"], "title": "Swarm intelligence"}
{"summary": "Tabu search, created by Fred W. Glover in 1986 and formalized in 1989, is a metaheuristic search method employing local search methods used for mathematical optimization.\nLocal (neighborhood) searches take a potential solution to a problem and check its immediate neighbors (that is, solutions that are similar except for one or two minor details) in the hope of finding an improved solution. Local search methods have a tendency to become stuck in suboptimal regions or on plateaus where many solutions are equally fit.\nTabu search enhances the performance of local search by relaxing its basic rule. First, at each step worsening moves can be accepted if no improving move is available (like when the search is stuck at a strict local mimimum). In addition, prohibitions (henceforth the term tabu) are introduced to discourage the search from coming back to previously-visited solutions.\nThe implementation of tabu search uses memory structures that describe the visited solutions or user-provided sets of rules. If a potential solution has been previously visited within a certain short-term period or if it has violated a rule, it is marked as \"tabu\" (forbidden) so that the algorithm does not consider that possibility repeatedly.", "links": ["Algorithm", "Ant colony optimization algorithms", "Approximation algorithm", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Fred W. Glover", "Function (mathematics)", "Gauss\u2013Newton algorithm", "Genetic algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Greedy randomized adaptive search procedure", "Guided Local Search", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local optima", "Local search (optimization)", "Mathematical optimization", "Matroid", "Maxima and minima", "Metaheuristic", "Minimum spanning tree", "NP-hard", "Nearest neighbor algorithm", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Optimization (mathematics)", "Optimization algorithm", "Pattern classification", "Penalty method", "Polynesia", "Powell's method", "Pseudocode", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Reactive search optimization", "Resource planning", "Revised simplex algorithm", "Satisficing", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Taboo", "Tonga", "Traveling salesman problem", "Truncated Newton method", "Trust region", "VLSI design", "Wolfe conditions"], "categories": ["1989 introductions", "Optimization algorithms and methods"], "title": "Tabu search"}
{"summary": "TOLMIN  is a numerical optimization algorithm by Michael J. D. Powell. It is also the name of Powell's Fortran 77 implementation of the algorithm.\nTOLMIN seeks the minimum of a differentiable nonlinear function subject to linear constraints (equality and/or inequality) and simple bounds on variables. Each search direction is calculated so that it does not intersect the boundary of any inequality constraint that is satisfied and that has a \"small\" residual at the beginning of the line search. The meaning of \"small\" depends on a parameter called TOL which is automatically adjusted, and which gives the name of the software.\nFeatures of the software include: quadratic approximations of the objective function whose second derivative matrices are updated by means of the BFGS formula, active sets technique, primal-dual quadratic programming procedure for calculation of the search direction.\nThe TOLMIN software is distributed under The GNU Lesser General Public License (LGPL).", "links": ["Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "BFGS", "BOBYQA", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "COBYLA", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Fortran", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "GNU Lesser General Public License", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "LINCOA", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Michael J. D. Powell", "Minimum spanning tree", "NEWUOA", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Numerical analysis", "Optimization (mathematics)", "Optimization algorithm", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Tabu search", "Tolmin (disambiguation)", "Truncated Newton method", "Trust region", "UOBYQA", "Wolfe conditions"], "categories": ["Optimization algorithms and methods"], "title": "TOLMIN (optimization software)"}
{"summary": "Tree rearrangements are used in heuristic algorithms devoted to searching for an optimal tree structure. They can be applied to any set of data that are naturally arranged into a tree, but have most applications in computational phylogenetics, especially in maximum parsimony and maximum likelihood searches of phylogenetic trees, which seek to identify one among many possible trees that best explains the evolutionary history of a particular gene or species.", "links": ["Algorithm", "Computational phylogenetics", "Evolution", "Gene", "Genetic algorithm", "Heuristic", "Maximum likelihood", "Maximum parsimony", "Objective function", "Optimization (mathematics)", "Phylogenetic tree", "Species", "Tree structure"], "categories": ["Optimization algorithms and methods", "Phylogenetics", "Trees (data structures)"], "title": "Tree rearrangement"}
{"summary": "Truncated Newton methods, also known as Hessian-free optimization, are a family of optimization algorithms designed for optimizing non-linear function with large numbers of variables. A truncated Newton method consists of repeated application of an iterative optimization algorithm to approximately solve Newton's equations, to determine an update to the function's parameters. The inner solver is truncated, i.e., run for only a limited number of iterations. It follows that, for truncated Newton methods to work, the inner solver needs to produce a good approximation in a few number of iterations; conjugate gradient has been suggested and evaluated as a candidate inner loop. Another prerequisite is good preconditioning for the inner algorithm.", "links": ["Abraham de Moivre", "Absolute time and space", "An Historical Account of Two Notable Corruptions of Scripture", "Approximation algorithm", "Arithmetica Universalis", "Augmented Lagrangian method", "Barrier function", "Bellman\u2013Ford algorithm", "Benjamin Pulleyn", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "Bucket argument", "Calculus", "Catherine Barton", "CiteSeer", "Classical mechanics", "Combinatorial optimization", "Comparison of optimization software", "Conjugate gradient", "Convex minimization", "Convex optimization", "Copernican Revolution", "Corpuscular theory of light", "Cranbury Park", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "De analysi per aequationes numero terminorum infinitas", "De motu corporum in gyrum", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Dynamics (mechanics)", "Early life of Isaac Newton", "Edmonds\u2013Karp algorithm", "Elements of the Philosophy of Newton", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Finite difference", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "Gauss\u2013Newton algorithm", "General Scholium", "Generalized Gauss\u2013Newton method", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Gravitational constant", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Hypotheses non fingo", "Impact depth", "Inertia", "Integer programming", "International Conference on Machine Learning", "Isaac Barrow", "Isaac Newton", "Isaac Newton's occult studies", "Isaac Newton S/O Philipose", "Isaac Newton in popular culture", "Iterative method", "John Conduitt", "John Keill", "Johnson's algorithm", "Karmarkar's algorithm", "Kepler's laws of planetary motion", "Kissing number problem", "Kruskal's algorithm", "Later life of Isaac Newton", "Leibniz\u2013Newton calculus controversy", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "List of things named after Isaac Newton", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Method of Fluxions", "Minimum spanning tree", "Nelder\u2013Mead method", "Newton's cannonball", "Newton's cradle", "Newton's identities", "Newton's inequalities", "Newton's law of cooling", "Newton's law of universal gravitation", "Newton's laws of motion", "Newton's metal", "Newton's method", "Newton's method in optimization", "Newton's reflector", "Newton's rings", "Newton's theorem about ovals", "Newton's theorem of revolving orbits", "Newton (Blake)", "Newton (unit)", "Newton disc", "Newton fractal", "Newton polygon", "Newton polynomial", "Newton scale", "Newtonian dynamics", "Newtonian fluid", "Newtonian potential", "Newtonian telescope", "Newtonianism", "Newton\u2013Cartan theory", "Newton\u2013Cotes formulas", "Newton\u2013Euler equations", "Newton\u2013Okounkov body", "Newton\u2013Pepys problem", "Nonlinear conjugate gradient method", "Nonlinear programming", "Notes on the Jewish Temple", "Opticks", "Optimization algorithm", "Parameterized post-Newtonian formalism", "Penalty method", "Philosophi\u00e6 Naturalis Principia Mathematica", "Post-Newtonian expansion", "Powell's method", "Power number", "Preconditioning", "Problem of Apollonius", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quaestiones quaedam philosophicae", "Quasi-Newton method", "Religious views of Isaac Newton", "Revised simplex algorithm", "Rotating spheres", "Schr\u00f6dinger\u2013Newton equation", "Scientific revolution", "Sequential quadratic programming", "Sextant", "Simplex algorithm", "Simulated annealing", "Solar mass", "Standing on the shoulders of giants", "Structural coloration", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "Table of Newtonian series", "Tabu search", "The Chronology of Ancient Kingdoms Amended", "The Mysteryes of Nature and Art", "The Queries", "Trust region", "William Clarke (apothecary)", "William Jones (mathematician)", "William Stukeley", "Wolfe conditions", "Woolsthorpe Manor", "Writing of Principia Mathematica"], "categories": ["All stub articles", "Mathematics stubs", "Optimization algorithms and methods"], "title": "Truncated Newton method"}
{"summary": "UOBYQA (Unconstrained Optimization BY Quadratic Approximation) is a numerical optimization algorithm by Michael J. D. Powell. It is also the name of Powell's Fortran 77 implementation of the algorithm.\nUOBYQA solves unconstrained optimization problems without using derivatives, which makes it a derivative-free algorithm. The algorithm is iterative, and exploits trust region technique. On each iteration, the algorithm establishes a quadratic model  by interpolating the objective function at  points, and then minimizes  within a trust region.\nAfter UOBYQA, Powell developed NEWUOA, which also solves unconstrained optimization problems without using derivatives. In general, NEWUOA is much more efficient than UOBYQA and is capable of solving much larger problems (with up to several hundreds of variables). A major difference between them is that NEWUOA constructs quadratic models by interpolating the objective function at much less than  points ( by default). For general usage, NEWUOA is recommended to replace UOBYQA.\nThe UOBYQA software is distributed under The GNU Lesser General Public License (LGPL).", "links": ["Algorithm", "Approximation algorithm", "Augmented Lagrangian method", "BOBYQA", "Barrier function", "Bellman\u2013Ford algorithm", "Bor\u016fvka's algorithm", "Branch and bound", "Branch and cut", "Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno algorithm", "COBYLA", "Combinatorial optimization", "Comparison of optimization software", "Convex minimization", "Convex optimization", "Criss-cross algorithm", "Cutting-plane method", "Davidon\u2013Fletcher\u2013Powell formula", "Derivative", "Derivative-free optimization", "Digital object identifier", "Dijkstra's algorithm", "Dinic's algorithm", "Dynamic programming", "Edmonds\u2013Karp algorithm", "Ellipsoid method", "Evolutionary algorithm", "Exchange algorithm", "Flow network", "Floyd\u2013Warshall algorithm", "Ford\u2013Fulkerson algorithm", "Fortran", "Frank\u2013Wolfe algorithm", "Function (mathematics)", "GNU Lesser General Public License", "Gauss\u2013Newton algorithm", "Golden section search", "Gradient", "Gradient descent", "Graph algorithm", "Greedy algorithm", "Hessian matrix", "Heuristic algorithm", "Hill climbing", "Integer programming", "Interpolation", "Iterative method", "Johnson's algorithm", "Karmarkar's algorithm", "Kruskal's algorithm", "LINCOA", "Lemke's algorithm", "Levenberg\u2013Marquardt algorithm", "Limited-memory BFGS", "Line search", "Linear programming", "Local convergence", "Local search (optimization)", "Mathematical optimization", "Matroid", "Metaheuristic", "Michael J. D. Powell", "Minimum spanning tree", "NEWUOA", "Nelder\u2013Mead method", "Newton's method in optimization", "Nonlinear conjugate gradient method", "Nonlinear programming", "Numerical analysis", "Optimization (mathematics)", "Optimization algorithm", "Optimization problem", "Penalty method", "Powell's method", "Push\u2013relabel maximum flow algorithm", "Quadratic programming", "Quasi-Newton method", "Revised simplex algorithm", "Sequential quadratic programming", "Simplex algorithm", "Simulated annealing", "Subgradient method", "Subroutine", "Successive linear programming", "Successive parabolic interpolation", "Symmetric rank-one", "TOLMIN (optimization software)", "Tabu search", "Truncated Newton method", "Trust region", "Wolfe conditions"], "categories": ["Optimization algorithms and methods"], "title": "UOBYQA"}
{"summary": "In mathematical optimization, Neighborhood Search is a technique that tries to find good or near-optimal solutions to a mathematical optimisation problem by repeatedly trying to improve the current solution by looking for a better solution which is in the neighbourhood of the current solution. In that sense, the neighborhood of the current solution includes a possibly large number of solutions which are near to the current solution. Obviously, there is a degree of looseness in that definition in that the neighborhood might include just those solutions that require a single change from the current solution, or it might include the larger set of solutions that differ in two or more values from the current solution. A very large-scale neighborhood search is a local search algorithm which makes use of a neighborhood definition, which is large and possibly exponentially sized.\nThe resulting algorithms are often far superior to algorithms using small neighborhoods because the local improvements are larger. If the neighbourhood searched is limited to just one or a very small number of changes from the current solution, then it is often very difficult to escape from local minima and additional meta-heuristic techniques may need to be used such as Simulated Annealing or Tabu search to allow the search process to escape from a local minimum. In large neighborhood search techniques, the possible changes from one solution to its neighbor may allow tens or hundreds of values to change, and this means that the size of the neighborhood may itself be sufficient to allow the search process to avoid or escape local minima. As a result, it is often unnecessary to introduce additional meta-heuristic techniques.", "links": ["Digital object identifier", "James B. Orlin", "Local search (optimization)", "Mathematical optimization", "Neighbourhood (mathematics)", "Ravindra K. Ahuja", "Simulated Annealing", "Tabu search"], "categories": ["All Wikipedia articles needing context", "All articles covered by WikiProject Wikify", "All articles with too few wikilinks", "All pages needing cleanup", "Articles covered by WikiProject Wikify from October 2012", "Articles with too few wikilinks from October 2012", "Optimization algorithms and methods", "Wikipedia articles needing context from February 2009", "Wikipedia introduction cleanup from February 2009"], "title": "Very large-scale neighborhood search"}
{"summary": "The odds-algorithm is a mathematical method for computing optimal strategies for a class of problems that belong to the domain of optimal stopping problems. Their solution follows from the odds-strategy, and the importance of the odds-strategy lies in its optimality, as explained below.\nThe odds-algorithm applies to a class of problems called last-success-problems. Formally, the objective in these problems is to maximize the probability of identifying in a sequence of sequentially observed independent events the last event satisfying a specific criterion (a \"specific event\"). This identification must be done at the time of observation. No revisiting of preceding observations is permitted. Usually, a specific event is defined by the decision maker as an event that is of true interest in the view of \"stopping\" to take a well-defined action. Such problems are encountered in several situations.", "links": ["Annals of Probability", "Clinical trial", "Compassionate use", "European Mathematical Society", "Expanded access", "F. Thomas Bruss", "House selling problem", "Journal of Applied Probability", "Odds", "Optimal stopping", "Parking problem", "Poisson process", "Portfolio (finance)", "Sciences et Technologies de l'automation", "Secretary problem", "Secretary problems", "Sequential estimate"], "categories": ["Mathematical optimization", "Optimal decisions", "Statistical algorithms"], "title": "Odds algorithm"}
{"summary": "A ternary search algorithm is a technique in computer science for finding the minimum or maximum of a unimodal function. A ternary search determines either that the minimum or maximum cannot be in the first third of the domain or that it cannot be in the last third of the domain, then repeats on the remaining two-thirds. A ternary search is an example of a divide and conquer algorithm (see search algorithm).", "links": ["Binary search algorithm", "Computer science", "Divide and conquer algorithm", "Exponential search", "Golden section search", "Interpolation search", "Linear search", "Maxima and minima", "Newton's method in optimization", "Search algorithm", "Unimodal", "Unimodality"], "categories": ["All articles lacking sources", "Articles lacking sources from May 2007", "Mathematical optimization", "Search algorithms"], "title": "Ternary search"}
{"summary": "Iterated filtering algorithms are a tool for maximum likelihood inference on partially observed dynamical systems. Stochastic perturbations to the unknown parameters are used to explore the parameter space. Applying sequential Monte Carlo (the particle filter) to this extended model results in the selection of the parameter values that are more consistent with the data. Appropriately constructed procedures, iterating with successively diminished perturbations, converge to the maximum likelihood estimate. Iterated filtering methods have so far been used most extensively to study infectious disease transmission dynamics. Case studies include cholera, Ebola virus, influenza, malaria, HIV, pertussis, poliovirus and measles. Other areas which have been proposed to be suitable for these methods include ecological dynamics and finance.\nThe perturbations to the parameter space play several different roles. Firstly, they smooth out the likelihood surface, enabling the algorithm to overcome small-scale features of the likelihood during early stages of the global search. Secondly, Monte Carlo variation allows the search to escape from local minima. Thirdly, the iterated filtering update uses the perturbed parameter values to construct an approximation to the derivative of the log likelihood even though this quantity is not typically available in closed form. Fourthly, the parameter perturbations help to overcome numerical difficulties that can arise during sequential Monte Carlo.", "links": ["ArXiv", "Cholera", "Digital object identifier", "Dynamical system", "Ebola virus", "Finance", "HIV", "Influenza", "Malaria", "Markov process", "Maximum likelihood", "Measles", "Normal distribution", "Parameter", "Particle filter", "Perturbation theory", "Pertussis", "Poliovirus", "PubMed Central", "PubMed Identifier", "Stochastic"], "categories": ["Dynamical systems", "Monte Carlo methods", "Nonlinear filters", "Pages containing cite templates with deprecated parameters", "Statistical algorithms"], "title": "Iterated filtering"}
{"summary": "In statistics and in statistical physics, the Metropolis\u2013Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult. This sequence can be used to approximate the distribution (i.e., to generate a histogram), or to compute an integral (such as an expected value). Metropolis\u2013Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. For single-dimensional distributions, other methods are usually available (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and are free from the problem of auto-correlated samples that is inherent in MCMC methods.", "links": ["Adaptive rejection sampling", "American Statistician", "Ann. Appl. Probab.", "Arianna W. Rosenbluth", "Augusta H. Teller", "Autocorrelation", "Bayesian statistics", "Bernd A. Berg", "Bibcode", "Biometrika", "Canonical ensemble", "Detailed balance", "Digital object identifier", "Edward Teller", "Emilio Segr\u00e8", "Enrico Fermi", "Equation of State Calculations by Fast Computing Machines", "Equations of State Calculations by Fast Computing Machines", "Expected value", "Gaussian distribution", "Genetic algorithm", "Gibbs sampling", "Hierarchical Bayesian model", "Histogram", "International Standard Book Number", "JSTOR", "John Wiley & Sons", "Journal of Chemical Physics", "Markov Chain", "Markov chain", "Markov chain Monte Carlo", "Markov process", "Marshall N. Rosenbluth", "Mean field particle methods", "Metropolis light transport", "Monte Carlo integration", "Multiple-try Metropolis", "Multivariate distribution", "Nicholas Metropolis", "Parallel tempering", "Particle filter", "Perseus Publishing", "Physics of Plasmas", "Posterior probability", "Probability distribution", "Pseudo-random number sampling", "Random variable", "Random walk", "Rejection sampling", "Rosenbrock function", "Roy Glauber", "Sample (statistics)", "Simulated annealing", "Slice sampling", "Stan Ulam", "Statistical physics", "Statistics", "W. K. Hastings", "World Scientific", "Zentralblatt MATH"], "categories": ["Markov chain Monte Carlo", "Monte Carlo methods", "Statistical algorithms"], "title": "Metropolis\u2013Hastings algorithm"}
{"summary": "In statistics and physics, multicanonical ensemble (also called multicanonical sampling or flat histogram) is a Markov chain Monte Carlo sampling technique that uses the Metropolis\u2013Hastings algorithm to compute integrals where the integrand has a rough landscape with multiple local minima. It samples states according to the inverse of the density of states, which has to be known a priori or be computed using other techniques like the Wang and Landau algorithm. Multicanonical sampling is an important technique for spin systems like the Ising model or spin glasses.", "links": ["ArXiv", "Bibcode", "Boltzmann factor", "Density of states", "Detailed balance", "Digital object identifier", "Dirac delta function", "Equation of State Calculations by Fast Computing Machines", "Estimator", "Hamiltonian (quantum mechanics)", "Importance sampling", "Integral", "International Standard Book Number", "Ising model", "Law of large numbers", "Local minimum", "Markov chain Monte Carlo", "Metropolis algorithm", "Metropolis\u2013Hastings algorithm", "Monte Carlo integration", "Phase transition", "Physics", "Potts model", "PubMed Identifier", "Spin (physics)", "Spin glass", "Statistics", "Stochastic drift", "Wang and Landau algorithm", "Wolff algorithm"], "categories": ["Computational physics", "Monte Carlo methods", "Statistical algorithms"], "title": "Multicanonical ensemble"}
{"summary": "The VEGAS algorithm, due to G. P. Lepage, is a method for reducing error in Monte Carlo simulations by using a known or approximate probability distribution function to concentrate the search in those areas of the integrand that make the greatest contribution to the final integral.\nThe VEGAS algorithm is based on importance sampling. It samples points from the probability distribution described by the function , so that the points are concentrated in the regions that make the largest contribution to the integral.\nIn general, if the Monte Carlo integral of  is sampled with points distributed according to a probability distribution described by the function , we obtain an estimate ,\n.\nThe variance of the new estimate is then\n\nwhere  is the variance of the original estimate, .\nIf the probability distribution is chosen as  then it can be shown that the variance  vanishes, and the error in the estimate will be zero. In practice it is not possible to sample from the exact distribution g for an arbitrary function, so importance sampling algorithms aim to produce efficient approximations to the desired distribution.\nThe VEGAS algorithm approximates the exact distribution by making a number of passes over the integration region while histogramming the function f. Each histogram is used to define a sampling distribution for the next pass. Asymptotically this procedure converges to the desired distribution. In order to avoid the number of histogram bins growing like  with dimension d the probability distribution is approximated by a separable function:  so that the number of bins required is only Kd. This is equivalent to locating the peaks of the function from the projections of the integrand onto the coordinate axes. The efficiency of VEGAS depends on the validity of this assumption. It is most efficient when the peaks of the integrand are well-localized. If an integrand can be rewritten in a form which is approximately separable this will increase the efficiency of integration with VEGAS.", "links": ["Digital object identifier", "Histogram", "Importance sampling", "Integral", "Integrand", "Las Vegas algorithm", "Monte Carlo integration", "Monte Carlo simulation", "Probability distribution", "Projection (mathematics)", "Variance", "Variance reduction"], "categories": ["Computational physics", "Monte Carlo methods", "Statistical algorithms", "Variance reduction"], "title": "VEGAS algorithm"}
{"summary": "The Wang and Landau algorithm, proposed by Fugao Wang and David P. Landau, is a Monte Carlo method designed to calculate the density of states of a system. The method performs a non-markovian random walk to build the density of states by quickly visiting all the available energy spectrum. The Wang and Landau algorithm is an important method to obtain the density of states required to perform a multicanonical simulation.\nThe Wang\u2013Landau algorithm can be applied to any system which is characterized by a cost (or energy) function. For instance, it has been applied to the solution of numerical integrals and the folding of proteins. The Wang-Landau Sampling is related to the Metadynamics algorithm.", "links": ["ArXiv", "Bibcode", "David P. Landau", "Density of states", "Digital object identifier", "Harmonic oscillator", "Metadynamics", "Metropolis-Hastings algorithm", "Metropolis\u2013Hastings algorithm", "Monte Carlo method", "Multicanonical ensemble", "PubMed Central", "PubMed Identifier", "Python (programming language)", "Stochastic process"], "categories": ["Articles with example Python code", "Computational physics", "Markov chain Monte Carlo", "Statistical algorithms"], "title": "Wang and Landau algorithm"}
{"summary": "The ziggurat algorithm is an algorithm for pseudo-random number sampling. Belonging to the class of rejection sampling algorithms, it relies on an underlying source of uniformly-distributed random numbers, typically from a pseudo-random number generator, as well as precomputed tables. The algorithm is used to generate values from a monotone decreasing probability distribution. It can also be applied to symmetric unimodal distributions, such as the normal distribution, by choosing a value from one half of the distribution and then randomly choosing which half the value is considered to have been drawn from. It was developed by George Marsaglia and others in the 1960s.\nA typical value produced by the algorithm only requires the generation of one random floating-point value and one random table index, followed by one table lookup, one multiply operation and one comparison. Sometimes (2.5% of the time, in the case of a normal or exponential distribution when using typical table sizes) more computations are required. Nevertheless, the algorithm is computationally much faster than the two most commonly used methods of generating normally distributed random numbers, the Marsaglia polar method and the Box\u2013Muller transform, which require at least one logarithm and one square root calculation for each pair of generated values. However, since the ziggurat algorithm is more complex to implement it is best used when large quantities of random numbers are required.\nThe term ziggurat algorithm dates from Marsaglia's paper with Wai Wan Tsang in 2000; it is so named because it is conceptually based on covering the probability distribution with rectangular segments stacked in decreasing order of size, resulting in a figure that resembles a ziggurat.", "links": ["Algorithm", "ArXiv", "Bisection method", "Box\u2013Muller transform", "Digital object identifier", "Error function", "Gaussian distribution", "George Marsaglia", "IEEE 754", "Inline function", "International Standard Serial Number", "MATLAB", "Marsaglia polar method", "Monotonic function", "Normal distribution", "Normalizing constant", "Numerical integration", "Probability distribution", "Pseudo-random number generator", "Pseudo-random number sampling", "Recursion", "Rejection sampling", "Root-finding algorithm", "Round-off error", "Sanity test", "Symmetric function", "The Journal of Business", "Unimodal distribution", "Ziggurat"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from September 2011", "Non-uniform random numbers", "Pages using citations with format and no URL", "Pseudorandom number generators", "Statistical algorithms"], "title": "Ziggurat algorithm"}
{"summary": "Pantelides algorithm gives a systematic method for reducing high-index systems of differential-algebraic equations to lower index, by selectively adding differentiated forms of the equations already present in the system. It is possible for the algorithm to fail in some instances.\nPantelides algorithm is implemented in several significant equation-based simulation programs such as gPROMS, Modelica and EMSO.", "links": ["Algorithm", "Data structure", "Differential algebraic equation", "Digital object identifier", "PHP"], "categories": ["Algorithms and data structures stubs", "All Wikipedia articles needing context", "All pages needing cleanup", "All stub articles", "Computer science stubs", "Numerical differential equations", "Wikipedia articles needing context from August 2010", "Wikipedia introduction cleanup from August 2010"], "title": "Pantelides algorithm"}
{"summary": "In computer science, the block Lanczos algorithm is an algorithm for finding the nullspace of a matrix over a finite field, using only multiplication of the matrix by long, thin matrices. Such matrices are considered as vectors of tuples of finite-field entries, and so tend to be called 'vectors' in descriptions of the algorithm.\nThe block Lanczos algorithm is amongst the most efficient methods known for finding nullspaces, which is the final stage in integer factorization algorithms such as the quadratic sieve and number field sieve, and its development has been entirely driven by this application.", "links": ["Algorithm", "Block Wiedemann algorithm", "Computer science", "Data structure", "Eigenvalue", "Finite field", "Integer factorization", "Lanczos algorithm", "Linear algebra", "Matrix (mathematics)", "Nullspace", "Number field sieve", "Peter Montgomery (mathematician)", "Quadratic sieve", "Tuple"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Linear algebra stubs", "Numerical linear algebra"], "title": "Block Lanczos algorithm"}
{"summary": "LU reduction is an algorithm related to LU decomposition. This term is usually used in the context of super computing and highly parallel computing. In this context it is used as a benchmarking algorithm, i.e. to provide a comparative measurement of speed for different computers. LU reduction is a special parallelized version of an LU decomposition algorithm, an example can be found in (Guitart 2001). The parallelized version usually distributes the work for a matrix row to a single processor and synchronizes the result with the whole matrix (Escribano 2000).", "links": ["Algorithm", "Applied mathematics", "Basic Linear Algebra Subprograms", "Benchmarking", "CPU cache", "Cache-oblivious algorithm", "Comparison of linear algebra libraries", "Comparison of numerical analysis software", "Data structure", "Floating point", "LU decomposition", "Matrix decomposition", "Matrix multiplication", "Matrix multiplication algorithm", "Multiprocessing", "Numerical linear algebra", "Numerical stability", "Parallel computing", "SIMD", "Sparse matrix", "Super computing", "System of linear equations", "Translation lookaside buffer"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Applied mathematics stubs", "Computer science stubs", "Numerical linear algebra", "Supercomputers"], "title": "LU reduction"}
{"summary": "An RRQR factorization or rank-revealing QR factorization is a matrix decomposition algorithm based on the QR factorization which can be used to determine the rank of a matrix. The SVD can be used to generate an RRQR, but it is not an efficient method to do so. A RRQR implementation is available for MATLAB.", "links": ["Algorithm", "Basic Linear Algebra Subprograms", "CPU cache", "Cache-oblivious algorithm", "Comparison of linear algebra libraries", "Comparison of numerical analysis software", "Data structure", "Digital object identifier", "Floating point", "Linear algebra", "Matrix decomposition", "Matrix multiplication", "Matrix multiplication algorithm", "Multiprocessing", "Numerical linear algebra", "Numerical stability", "QR decomposition", "Rank (linear algebra)", "SIMD", "Sparse matrix", "System of linear equations", "Translation lookaside buffer"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Linear algebra stubs", "Matrix decompositions", "Numerical linear algebra", "Use dmy dates from April 2011"], "title": "RRQR factorization"}
{"summary": "In the mathematical subfield of matrix theory, the Cuthill\u2013McKee algorithm (CM), named for Elizabeth Cuthill and J. McKee , is an algorithm to permute a sparse matrix that has a symmetric sparsity pattern into a band matrix form with a small bandwidth. The reverse Cuthill\u2013McKee algorithm (RCM) due to Alan George is the same algorithm but with the resulting index numbers reversed. In practice this generally results in less fill-in than the CM ordering when Gaussian elimination is applied.\nThe Cuthill McKee algorithm is a variant of the standard breadth-first search algorithm used in graph algorithms. It starts with a peripheral node and then generates levels  for  until all nodes are exhausted. The set  is created from set  by listing all vertices adjacent to all nodes in . These nodes are listed in increasing degree. This last detail is the only difference with the breadth-first search algorithm.", "links": ["Adjacency matrix", "Algorithm", "Association for Computing Machinery", "Band matrix", "Bandwidth (matrix theory)", "Boost C++ Libraries", "Breadth-first search", "Degree (graph theory)", "Graph (mathematics)", "Graph bandwidth", "Level structure", "Mathematics", "Matrix (mathematics)", "N-tuple", "Peripheral vertex", "Sparse matrix", "Symmetric matrix", "Vertex (graph theory)"], "categories": ["Graph algorithms", "Matrix theory", "Sparse matrices"], "title": "Cuthill\u2013McKee algorithm"}
{"summary": "A polylogarithmic function in n is a polynomial in the logarithm of n,\n\nIn computer science, polylogarithmic functions occur as the order of memory used by some algorithms (e.g., \"it has polylogarithmic order\").\nAll polylogarithmic functions are\n\nfor every exponent \u03b5 > 0 (for the meaning of this symbol, see small o notation), that is, a polylogarithmic function grows more slowly than any positive exponent. This observation is the basis for the soft O notation \u00d5(n).", "links": ["Algorithm", "Big O notation", "Computer science", "Logarithm", "Mathematical analysis", "Polylogarithm", "Polynomial", "Small o notation", "Soft O notation", "Space complexity"], "categories": ["All stub articles", "Analysis of algorithms", "Computer science stubs", "Mathematical analysis", "Mathematical analysis stubs", "Polynomials"], "title": "Polylogarithmic function"}
{"summary": "A root-finding algorithm is a numerical method, or algorithm, for finding a value x such that f(x) = 0, for a given function f. Such an x is called a root of the function f.\nThis article is concerned with finding scalar, real or complex roots, approximated as floating point numbers. Finding integer roots or exact algebraic roots are separate problems, whose algorithms have little in common with those discussed here. (See: Diophantine equation for integer roots)\nFinding a root of f(x) \u2212 g(x) = 0 is the same as solving the equation f(x) = g(x). Here, x is called the unknown in the equation. Conversely, any equation can take the canonical form f(x) = 0, so equation solving is the same thing as computing (or finding) a root of a function.\nNumerical root-finding methods use iteration, producing a sequence of numbers that hopefully converge towards a limit, which is a root. The first values of this series are initial guesses. Many methods computes subsequent values by evaluating an auxiliary function on the preceding values. The limit is thus a fixed point of the auxiliary function, which is chosen for having the roots of the original equation as fixed points.\nThe behaviour of root-finding algorithms is studied in numerical analysis. Algorithms perform best when they take advantage of known characteristics of the given function. Thus an algorithm to find isolated real roots of a low-degree polynomial in one variable may bear little resemblance to an algorithm for complex roots of a \"black-box\" function which is not even known to be differentiable. Questions include ability to separate close roots, robustness against failures of continuity and differentiability, reliability despite inevitable numerical errors, and rate of convergence.", "links": ["Abel\u2013Ruffini theorem", "Aberth method", "Algorithm", "Bairstow's method", "Bernoulli's method", "Birge\u2013Vieta's method", "Bisection method", "Bit", "Brent's method", "Broyden's method", "Budan's theorem", "Canonical form", "Companion matrix", "Complex number", "Computational complexity theory", "Continuous function", "Cryptographically secure pseudorandom number generator", "Degree of a polynomial", "Derivative", "Descartes' rule of signs", "Differentiability", "Digital object identifier", "Diophantine equation", "Durand\u2013Kerner method", "Eigenvalue algorithm", "Equation", "Equation solving", "False position method", "Fast Fourier transform", "Finite difference", "Fixed point (mathematics)", "Floating point", "Function (mathematics)", "GNU Scientific Library", "Graeffe's method", "Halley's method", "Horner's method", "Householder's method", "Ill-conditioned", "Integer", "International Standard Book Number", "Interval arithmetic", "Inverse function", "Inverse power method", "Inverse quadratic interpolation", "Iteration", "Jenkins\u2013Traub algorithm", "Jenkins\u2013Traub method", "Laguerre's method", "Lehmer\u2013Schur algorithm", "Limit of a sequence", "Linear interpolation", "MPSolve", "Maple (software)", "Mathematica", "Muller's method", "Multiplicity (mathematics)", "Newton's method", "Nikolai Ivanovich Lobachevsky", "Nth root algorithm", "Numerical analysis", "Polynomial", "Polynomial greatest common divisor", "Polynomial interpolation", "Polynomial transformations", "Power method", "Precision (arithmetic)", "Quadratic equation", "Quadratic formula", "Rational number", "Real number", "Ridders' method", "Root-finding algorithm", "Root of a function", "Ruffini rule", "Sage (mathematics software)", "Scalar (mathematics)", "Secant method", "Sequence", "Sidi's method", "Splitting circle method", "Square-free factorization", "Sturm's theorem", "SymPy", "System of polynomial equations", "Vincent's theorem", "Wilkinson's polynomial", "Xcas"], "categories": ["Root-finding algorithms", "Vague or ambiguous time from February 2014"], "title": "Root-finding algorithm"}
{"summary": "The Aberth method, or Aberth\u2013Ehrlich method, named after Oliver Aberth and Louis W. Ehrlich, is a root-finding algorithm for simultaneous approximation of all the roots of a univariate polynomial.\nThe fundamental theorem of algebra states that for each polynomial with complex coefficients there are as many roots as the degree of the polynomial. This method converges cubically, an improvement over the Weierstrass\u2013(Durand\u2013Kerner) method, another numerical algorithm that approximates all roots at once, which converges quadratically. (However, both algorithms converge linearly at multiple zeros.)", "links": ["Digital object identifier", "Durand\u2013Kerner method", "Electrostatic", "Factorisation", "Fundamental theorem of algebra", "Gauss\u2013Seidel method", "JSTOR", "Jacobi method", "MPSolve", "Multiplicity (mathematics)", "Newton's method", "Point particle", "Polynomial", "Properties of polynomial roots", "Root-finding algorithm", "Thomas Joannes Stieltjes", "Univariate"], "categories": ["Root-finding algorithms"], "title": "Aberth method"}
{"summary": "In numerical analysis, Bairstow's method is an efficient algorithm for finding the roots of a real polynomial of arbitrary degree. The algorithm first appeared in the appendix of the 1920 book \"Applied Aerodynamics\" by Leonard Bairstow. The algorithm finds the roots in complex conjugate pairs using only real arithmetic.\nSee root-finding algorithm for other algorithms.", "links": ["Algorithm", "Complex conjugate", "Leonard Bairstow", "Newton's method", "Numerical analysis", "Polynomial", "Polynomial long division", "Quadratic function", "Root-finding algorithm", "Root of a function"], "categories": ["Root-finding algorithms"], "title": "Bairstow's method"}
{"summary": "In numerical analysis, Brent's method is a complicated but popular root-finding algorithm combining the bisection method, the secant method and inverse quadratic interpolation. It has the reliability of bisection but it can be as quick as some of the less reliable methods. The algorithm tries to use the potentially fast-converging secant method or inverse quadratic interpolation if possible, but it falls back to the more robust bisection method if necessary. Brent's method is due to Richard Brent and builds on an earlier algorithm by Theodorus Dekker. Consequently, the method is also known as Brent-Dekker.", "links": ["Algol 60", "Apache Commons", "Bisection method", "Boost (C++ libraries)", "C++", "Intermediate value theorem", "International Standard Book Number", "Inverse quadratic interpolation", "Java (programming language)", "Linear interpolation", "Modelica", "Netlib", "Numerical Recipes", "Numerical analysis", "PARI/GP", "Python (programming language)", "R (software)", "Rate of convergence", "Richard Brent (scientist)", "Root-finding algorithm", "Scipy", "Secant method", "Theodorus Dekker"], "categories": ["Root-finding algorithms"], "title": "Brent's method"}
{"summary": "In numerical analysis, the Durand\u2013Kerner method, established 1960\u201366 and named after E. Durand and Immo Kerner, also called the method of Weierstrass, established 1859\u201391 and named after Karl Weierstrass, is a root-finding algorithm for solving polynomial equations. In other words, the method can be used to solve numerically the equation\n\u0192(x) = 0\nwhere \u0192 is a given polynomial, which can be taken to be scaled so that the leading coefficient is 1.", "links": ["Ada programming language", "Coefficient", "Companion matrix", "Complex number", "Contraction mapping", "Degree of a polynomial", "Digital object identifier", "Eigenvalue", "Elementary symmetric polynomial", "Endomorphism", "Equation (mathematics)", "Fixed point (mathematics)", "Gauss\u2013Seidel method", "Gershgorin circle theorem", "Iteration", "Jacobi method", "Java applet", "Java programming language", "Karl Weierstrass", "Karl Weierstra\u00df", "Lagrange interpolation", "Newton's method", "Numerical analysis", "Open-Source", "Polynomial", "Quotient ring", "Real number", "Residue class", "Root-finding algorithm", "Root of unity", "Victor Pan"], "categories": ["CS1 maint: Explicit use of et al.", "Root-finding algorithms"], "title": "Durand\u2013Kerner method"}
{"summary": "Fast inverse square root (sometimes referred to as Fast InvSqrt() or by the hexadecimal constant 0x5f3759df) is a method of calculating x\u2212\u00bd, the reciprocal (or multiplicative inverse) of a square root for a 32-bit floating point number in IEEE 754 floating point format. The algorithm was probably developed at Silicon Graphics in the early 1990s, and an implementation appeared in 1999 in the Quake III Arena source code, but the method did not appear on public forums such as Usenet until 2002 or 2003.  (There is a discussion on Chinese developer forum CSDN back in 2000 ) At the time, the primary advantage of the algorithm came from avoiding computationally expensive floating point operations in favor of integer operations. Inverse square roots are used to compute angles of incidence and reflection for lighting and shading in computer graphics.\nThe algorithm accepts a 32-bit floating point number as the input and stores a halved value for later use. Then, treating the bits representing the floating point number as a 32-bit integer, a logical shift right of one bit is performed and the result subtracted from the magic number 0x5f3759df. This is the first approximation of the inverse square root of the input. Treating the bits again as floating point it runs one iteration of Newton's method to return a more precise approximation. This computes an approximation of the inverse square root of a floating point number approximately four times faster than floating point division.\nThe algorithm was originally attributed to John Carmack, but an investigation showed that the code had deeper roots in both the hardware and software side of computer graphics. Adjustments and alterations passed through both Silicon Graphics and 3dfx Interactive, with Gary Tarolli's implementation for the SGI Indigo as the earliest known use. It is not known how the constant was originally derived, though investigation has shed some light on possible methods.", "links": ["3D Realms", "3D graphics", "3dfx Interactive", "ARQuake", "Action Quake 2", "Activision", "Adrian Carmack", "American McGee", "Angle of incidence", "Angles of incidence", "Approximation error", "Approximation theory", "Ardent Computer", "BFG (weapon)", "Bisection method", "Blahbalicious", "Brute-force search", "C preprocessor", "Catacomb 3-D", "Challenge ProMode Arena", "Cleve Moler", "Commander Keen", "Comment out", "Computationally expensive", "Computer graphics", "Dangerous Dave", "Dave Taylor (game programmer)", "DeFRaG", "Dennis Fong", "Diary of a Camper", "Digital object identifier", "Doom (series)", "Doom engine", "Enemy Territory: Quake Wars", "Euclidean distance", "Euclidean norm", "Euclidean space", "Exponent bias", "Fatal1ty", "Field-programmable gate array", "First-person shooter", "Floating point", "Floating point arithmetic", "FormGen", "GT Interactive Software", "Graeme Devine", "GtkRadiant", "Heretic (video game)", "Hexadecimal", "Hexen: Beyond Heretic", "Hexen II", "Hovertank 3D", "IEEE 754-1985", "Id Software", "Id Tech", "Id Tech 3", "Id Tech 4", "Id Tech 5", "Id Tech 6", "International Federation for Information Processing", "International Standard Book Number", "Irvine, California", "Jake2", "Jay Wilbur (computer scientist)", "Jennell Jaquays", "Jim Blinn", "Johan Quick", "John Carmack", "John D. Carmack", "John Romero", "Juggernaut: The New Story", "Katherine Anna Kang", "Kevin Cloud", "Kludge", "Lambert's cosine law", "Lighting", "List of Institute of Electrical and Electronics Engineers publications", "Logical shift", "Lookup table", "MATLAB", "Maciej Krzykowski", "Magic number (programming)", "Malice (video game mod)", "Mark Rein (software executive)", "Marty Stratton", "Masters of Doom", "Matthew Costello", "Methods of computing square roots", "Michael Abrash", "Mike Wilson (executive)", "Multiplicative inverse", "Newton's method", "Nexuiz", "Normal number (computing)", "Normalized number", "O'Reilly Media", "OpenArena", "Operation Bayshield", "Orcs & Elves", "Paul Steed", "Qoole", "Quad God", "QuakeC", "QuakeCon", "QuakeNet", "QuakeWorld", "Quake (series)", "Quake (video game)", "Quake 3 Fortress", "Quake 4", "Quake Army Knife", "Quake II", "Quake III: Team Arena", "Quake III Arena", "Quake II engine", "Quake Live", "Quake done Quick", "Quake engine", "Rage (video game)", "Rate of convergence", "Reflection (computer graphics)", "Rescue Rover", "Rocket Arena", "SGI Indigo", "Sander Kaasjager", "Sandy Petersen", "Shading", "Shadow Knights", "Shane Hendrixson", "Shrak", "Silicon Graphics", "Single-precision floating-point format", "Single precision floating-point format", "Smokin' Guns", "Softdisk", "Square root", "Streaming SIMD Extensions", "Sujoy Roy", "Surface normal", "Table of logarithms", "Team Fortress", "The Seal of Nehahra", "Tim Willits", "Timothee Besset", "Todd Hollenshead", "Tom Hall", "Transform, clipping, and lighting", "Tremulous", "Trent Reznor", "Trial and error", "Uniform norm", "Unit vector", "Urban Terror", "Usenet", "Vector components", "Vertex shader", "Warsow (video game)", "Weapons Factory", "William Kahan", "Wolfenstein (series)", "Wolfenstein 3D engine", "World of Padman", "X-Men: The Ravages of Apocalypse", "Zaero", "ZeniMax Media"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from April 2012", "Good articles", "Quake (series)", "Root-finding algorithms", "Source code"], "title": "Fast inverse square root"}
{"summary": "In mathematics, Graeffe's method or Dandelin\u2013Graeffe method is an algorithm for finding all of the roots of a polynomial. It was developed independently by Germinal Pierre Dandelin in 1826 and Karl Heinrich Gr\u00e4ffe in 1837. Lobachevsky in 1834 also discovered the principal idea of the method. The method separates the roots of a polynomial by squaring them repeatedly. This squaring of the roots is done implicitly, that is, only working on the coefficients of the polynomial. Finally, Vi\u00e8te's formulas are used in order to approximate the roots.", "links": ["Alston Scott Householder", "Dual numbers", "Eric W. Weisstein", "Germinal Pierre Dandelin", "Karl Heinrich Gr\u00e4ffe", "Lobachevsky", "MathWorld", "Mathematics", "Newton's method", "Power series", "Root-finding algorithm", "Vieta relations", "Vi\u00e8te's formulas"], "categories": ["Root-finding algorithms"], "title": "Graeffe's method"}
{"summary": "In numerical analysis, Halley\u2019s method is a root-finding algorithm used for functions of one real variable with a continuous second derivative, i.e., C2 functions. It is named after its inventor Edmond Halley.\nThe algorithm is second in the class of Householder's methods, right after Newton's method. Like the latter, it produces iteratively a sequence of approximations to the root; their rate of convergence to the root is cubic. Multidimensional versions of this method exist.\nHalley's method can be viewed as exactly finding the roots of a linear-over-linear Pad\u00e9 approximation to the function, in contrast to Newton's method/Secant method (approximates/interpolates the function linearly) or Cauchy's method/Muller's method (approximates/interpolates the function quadratically).", "links": ["Bond (finance)", "Bond Exchange of South Africa", "Cauchy's method", "Differentiability class", "Digital object identifier", "Edmond Halley", "Eric W. Weisstein", "Householder's method", "Interpolation", "MathWorld", "Muller's method", "Newton's method", "Numerical analysis", "Pad\u00e9 approximant", "Rate of convergence", "Root-finding algorithm", "Secant method", "Second derivative", "Taylor's theorem", "Yield to maturity"], "categories": ["Root-finding algorithms", "Use dmy dates from July 2013"], "title": "Halley's method"}
{"summary": "In mathematics, and more specifically in numerical analysis, Householder's methods are a class of root-finding algorithms that are used for functions of one real variable with continuous derivatives up to some order d+1. Each of these methods is characterized by the number d, which is known as the order of the method. The algorithm is iterative and has a rate of convergence of d+1.\nThese methods are named after the American mathematician Alston Scott Householder.", "links": ["Alston Scott Householder", "Automatic differentiation", "Continuous function", "Derivative", "Euclid's algorithm", "Geometric series", "Halley's method", "Horner method", "K\u00f6nig's theorem (complex analysis)", "Library of Congress Control Number", "Mathematics", "Meromorphic function", "Newton's method", "Numerator", "Numerical analysis", "Pad\u00e9 approximation", "PostScript", "Rate of convergence", "Real number", "Root-finding algorithm", "Taylor expansion"], "categories": ["All articles needing additional references", "Articles needing additional references from November 2013", "Root-finding algorithms"], "title": "Householder's method"}
{"summary": "In numerical analysis, inverse quadratic interpolation is a root-finding algorithm, meaning that it is an algorithm for solving equations of the form f(x) = 0. The idea is to use quadratic interpolation to approximate the inverse of f. This algorithm is rarely used on its own, but it is important because it forms part of the popular Brent's method.", "links": ["Brent's method", "Inverse function", "James F. Epperson", "Lagrange polynomial", "Linear interpolation", "Muller's method", "Numerical analysis", "Polynomial interpolation", "Recurrence relation", "Root-finding algorithm", "Secant Method", "Secant method", "Successive parabolic interpolation"], "categories": ["Root-finding algorithms"], "title": "Inverse quadratic interpolation"}
{"summary": "In numerical analysis, Laguerre's method is a root-finding algorithm tailored to polynomials. In other words, Laguerre's method can be used to numerically solve the equation\n\nfor a given polynomial p. One of the most useful properties of this method is that it is, from extensive empirical study, very close to being a \"sure-fire\" method, meaning that it is almost guaranteed to always converge to some root of the polynomial, no matter what initial guess is chosen. This method is named in honour of Edmond Laguerre, a French mathematician.", "links": ["Digital object identifier", "Edmond Laguerre", "Forman S. Acton", "Fundamental theorem of algebra", "Graeffe's method", "Halley's method", "International Standard Book Number", "Jenkins\u2013Traub algorithm", "Loss of significance", "Multiple root", "Natural logarithm", "Newton's method", "Numerical analysis", "Polynomial", "Rate of convergence", "Root-finding algorithm"], "categories": ["Root-finding algorithms"], "title": "Laguerre's method"}
{"summary": "In mathematics, the Lehmer\u2013Schur algorithm (named after Derrick Henry Lehmer and Issai Schur) is a root-finding algorithm extending the idea of enclosing roots like in the one-dimensional bisection method to the complex plane. It uses the Schur\u2013Cohn test to test increasingly smaller disks for the presence or absence of roots. Alternative methods like Wilf's algorithm apply different tests to differently shaped areas but keep the idea of descent by subdivision.", "links": ["Analytic function", "Argument principle", "Bisection method", "Congruence (geometry)", "Derrick Henry Lehmer", "Digital object identifier", "Holomorphic function", "International Standard Book Number", "Issai Schur", "Jan van Leeuwen", "Mathematical Association of America", "Mathematics", "Multiplicity (mathematics)", "Newton's method", "Numerical Recipes", "Root-finding algorithm", "Rouch\u00e9's theorem", "Winding number"], "categories": ["All articles lacking reliable references", "Articles lacking reliable references from June 2011", "Root-finding algorithms"], "title": "Lehmer\u2013Schur algorithm"}
{"summary": "Muller's method is a root-finding algorithm, a numerical method for solving equations of the form f(x) = 0. It was first presented by David E. Muller in 1956.\nMuller's method is based on the secant method, which constructs at every iteration a line through two points on the graph of f. Instead, Muller's method uses three points, constructs the parabola through these three points, and takes the intersection of the x-axis with the parabola to be the next approximation.", "links": ["David E. Muller", "Degree of a polynomial", "Divided differences", "International Standard Book Number", "JSTOR", "Loss of significance", "Newton's method", "Newton polynomial", "Numerical analysis", "Parabola", "Polynomial", "Quadratic equation", "Rate of convergence", "Recurrence relation", "Root-finding algorithm", "Secant method", "Sidi's generalized secant method", "X-axis", "Zero of a function"], "categories": ["Root-finding algorithms"], "title": "Muller's method"}
{"summary": "The principal nth root  of a positive real number A, is the positive real solution of the equation\n\n(for integer n there are n distinct complex solutions to this equation if , but only one is positive and real).\nThere is a very fast-converging nth root algorithm for finding :\nMake an initial guess \nSet . In practice we do .\nRepeat step 2 until the desired precision is reached, i.e.  .\nA special case is the familiar square-root algorithm. By setting n = 2, the iteration rule in step 2 becomes the square root iteration rule:\n\nSeveral different derivations of this algorithm are possible. One derivation shows it is a special case of Newton's method (also called the Newton-Raphson method) for finding zeros of a function  beginning with an initial guess. Although Newton's method is iterative, meaning it approaches the solution through a series of increasingly accurate guesses, it converges very quickly. The rate of convergence is quadratic, meaning roughly that the number of bits of accuracy doubles on each iteration (so improving a guess from 1 bit to 64 bits of precision requires only 6 iterations). For this reason, this algorithm is often used in computers as a very fast method to calculate square roots.\nFor large n, the nth root algorithm is somewhat less efficient since it requires the computation of  at each step, but can be efficiently implemented with a good exponentiation algorithm.", "links": ["Complex number", "Exponentiation", "International Standard Book Number", "Limit of a sequence", "Methods of computing square roots", "Negative and positive numbers", "Newton's method", "Nth root", "Principal branch", "Real number", "Recurrence relation"], "categories": ["Root-finding algorithms"], "title": "Nth root algorithm"}
{"summary": "In numerical analysis, Ridders' method is a root-finding algorithm based on the false position method and the use of an exponential function to successively approximate a root of a function f. The method is due to C. Ridders.\nRidders' method is simpler than Muller's method or Brent's method but with similar performance. The formula below converges quadratically when the function is well-behaved, which implies that the number of additional significant digits found at each step approximately doubles; but the function has to be evaluated twice for each step, so the overall order of convergence of the method is \u221a2. If the function is not well-behaved, the root remains bracketed and the length of the bracketing interval at least halves on each iteration, so convergence is guaranteed. The algorithm also makes use of square roots, which are slower than basic floating point operations.", "links": ["Applied mathematics", "Brent's method", "Digital object identifier", "Exponential function", "False position method", "Function (mathematics)", "International Standard Book Number", "Muller's method", "Numerical Recipes", "Numerical analysis", "Order of convergence", "Root-finding algorithm", "Root of a function"], "categories": ["All stub articles", "Applied mathematics stubs", "Root-finding algorithms"], "title": "Ridders' method"}
{"summary": "In numerical analysis, the secant method is a root-finding algorithm that uses a succession of roots of secant lines to better approximate a root of a function f. The secant method can be thought of as a finite difference approximation of Newton's method. However, the method was developed independently of Newton's method, and predated the latter by over 3,000 years.", "links": ["Bisection method", "Broyden's method", "Eric W. Weisstein", "False position method", "Finite difference", "Function (mathematics)", "Golden ratio", "International Standard Book Number", "John Wiley & Sons", "MathWorld", "Matlab", "Newton's method", "Numerical analysis", "Order of convergence", "Quadratic convergence", "Quasi-Newton method", "Recurrence relation", "Root-finding algorithm", "Root of a function", "Secant line"], "categories": ["Root-finding algorithms"], "title": "Secant method"}
{"summary": "Sidi's generalized secant method is a root-finding algorithm, that is, a numerical method for solving equations of the form  . The method was published by Avram Sidi.\nThe method is a generalization of the secant method. Like the secant method, it is an iterative method which requires one evaluation of  in each iteration and no derivatives of . The method can converge much faster though, with an order which approaches 2 provided that  satisfies the regularity conditions described below.\n^ Sidi, Avram, \"Generalization Of The Secant Method For Nonlinear Equations\", Applied Mathematics E-notes 8 (2008), 115\u2013123, http://www.math.nthu.edu.tw/~amen/2008/070227-1.pdf", "links": ["Cubic function", "Degree of a polynomial", "Derivative", "Descartes's rule of signs", "Equations", "Iterative method", "Limit of a sequence", "Muller's method", "Newton's method", "Newton polynomial", "Numerical method", "Open interval", "Polynomial interpolation", "Properties of polynomial roots", "Rate of convergence", "Root-finding algorithm", "Secant method", "Sequence", "Smooth function"], "categories": ["Root-finding algorithms"], "title": "Sidi's generalized secant method"}
{"summary": "In mathematics, a quadratic equation is a polynomial equation of the second degree. The general form is\n\nwhere a \u2260 0.\nThe quadratic equation can be solved using the well-known quadratic formula, which can be derived by completing the square. That formula always gives the roots of the quadratic equation, but the solutions are expressed in a form that often involves a quadratic irrational number, which is an algebraic fraction that can be evaluated as a decimal fraction only by applying an additional root extraction algorithm.\nIf the roots are real, there is an alternative technique that obtains a rational approximation to one of the roots by manipulating the equation directly. The method works in many cases, and long ago it stimulated further development of the analytical theory of continued fractions.", "links": ["Absolute value", "Abstract algebra", "Algebraic fraction", "Algebraic number field", "Approximant (continued fraction)", "Coefficient", "Completing the square", "Complex analysis", "Continued fraction", "Convergence problem", "Convergent (continued fraction)", "Decimal", "Degree of a polynomial", "Discriminant", "Equation", "Fundamental recurrence formulas", "Fundamental theorem of algebra", "Generalized continued fraction", "Geometric progression", "Imaginary number", "Integral domain", "Leonhard Euler", "Lucas sequence", "Mathematics", "Methods of computing square roots", "Monic polynomial", "Pell's equation", "Pell number", "Polynomial", "Quadratic equation", "Quadratic irrational", "Real number", "Ring (mathematics)", "Root of a function", "Set (mathematics)", "Square number", "Unit (ring theory)"], "categories": ["Continued fractions", "Elementary algebra", "Equations", "Mathematical analysis", "Root-finding algorithms"], "title": "Solving quadratic equations with continued fractions"}
{"summary": "In mathematics, the splitting circle method is a numerical algorithm for the numerical factorization of a polynomial and, ultimately, for finding its complex roots. It was introduced by Arnold Sch\u00f6nhage in his 1982 paper The fundamental theorem of algebra in terms of computational complexity (Technical report, Mathematisches Institut der Universit\u00e4t T\u00fcbingen). A revised algorithm was presented by Victor Pan in 1998. An implementation was provided by Xavier Gourdon in 1996 for the Magma and PARI/GP computer algebra systems.", "links": ["Arnold Sch\u00f6nhage", "Complex analysis", "Complex number", "Descartes' rule of signs", "Digital object identifier", "Elementary symmetric polynomial", "Extended Euclidean algorithm", "Fast Fourier transform", "Formal power series", "Graeffe's method", "Magma computer algebra system", "Mathematics", "Newton's identities", "Newton's method", "Numerical analysis", "PARI/GP", "Pad\u00e9 approximant", "Polynomial", "Residue theorem", "Root of a function", "Rouch\u00e9 theorem", "Victor Pan", "Xavier Gourdon"], "categories": ["Root-finding algorithms"], "title": "Splitting circle method"}
{"summary": "In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i.e., in the order that the input is fed to the algorithm, without having the entire input available from the start.\nIn contrast, an offline algorithm is given the whole problem data from the beginning and is required to output an answer which solves the problem at hand.\nAs an example, consider the sorting algorithms selection sort and insertion sort: Selection sort repeatedly selects the minimum element from the unsorted remainder and places it at the front, which requires access to the entire input; it is thus an offline algorithm. On the other hand, insertion sort considers one input element per iteration and produces a partial solution without considering future elements. Thus insertion sort is an online algorithm.\nNote that insertion sort produces the optimum result, i.e., a correctly sorted list. For many problems, online algorithms cannot match the performance of offline algorithms. If the ratio between the performance of an online algorithm and an optimal offline algorithm is bounded, the online algorithm is called competitive.\nNot every online algorithm has an offline counterpart.", "links": ["Adversary (online algorithm)", "Algorithm", "Algorithms for calculating variance", "Allan Borodin", "Bandit problem", "Canadian Traveller Problem", "Competitive analysis (online algorithm)", "Computer science", "Dynamic algorithm", "Greedy algorithm", "Insertion sort", "International Standard Book Number", "Job shop scheduling", "K-server problem", "Linear search problem", "List update problem", "Metrical task systems", "Odds algorithm", "Offline learning", "Online machine learning", "PSPACE-complete", "Page replacement algorithm", "Perceptron", "Real-time computing", "Reservoir sampling", "Search games", "Secretary problem", "Selection sort", "Sequential algorithm", "Shortest path problem", "Ski rental problem", "Sorting algorithms", "Streaming algorithm", "Ukkonen's algorithm"], "categories": ["All articles needing additional references", "Articles needing additional references from June 2013", "Online algorithms"], "title": "Online algorithm"}
{"summary": "In computer science, an online algorithm measures its competitiveness against different adversary models. For deterministic algorithms, the adversary is the same as the adaptive offline adversary. For randomized online algorithms competitiveness can depend upon the adversary model used.", "links": ["Algorithm", "Allan Borodin", "Competitive analysis (online algorithm)", "Computer science", "Data structure", "Digital object identifier", "International Standard Book Number", "K-server problem", "Online algorithm", "Richard Karp"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Analysis of algorithms", "Computer science stubs", "Online algorithms"], "title": "Adversary model"}
{"summary": "Competitive analysis is a method invented for analyzing online algorithms, in which the performance of an online algorithm (which must satisfy an unpredictable sequence of requests, completing each request without being able to see the future) is compared to the performance of an optimal offline algorithm that can view the sequence of requests in advance. An algorithm is competitive if its competitive ratio\u2014the ratio between its performance and the offline algorithm's performance\u2014is bounded. Unlike traditional worst-case analysis, where the performance of an algorithm is measured only for \"hard\" inputs, competitive analysis requires that an algorithm perform well both on hard and easy inputs, where \"hard\" and \"easy\" are defined by the performance of the optimal offline algorithm.\nFor many algorithms, performance is dependent not only on the size of the inputs, but also on their values. One such example is the quicksort algorithm, which sorts an array of elements. Such data-dependent algorithms are analysed for average-case and worst-case data. Competitive analysis is a way of doing worst case analysis for on-line and randomized algorithms, which are typically data dependent.\nIn competitive analysis, one imagines an \"adversary\" that deliberately chooses difficult data, to maximize the ratio of the cost of the algorithm being studied and some optimal algorithm. Adversaries range in power from the oblivious adversary, which has no knowledge of the random choices made by the algorithm pitted against it, to the adaptive adversary that has full knowledge of how an algorithm works and its internal state at any point during its execution. Note that this distinction is only meaningful for randomized algorithms. For a deterministic algorithm, either adversary can simply compute what state that algorithm must have at any time in the future, and choose difficult data accordingly.\nFor example, the quicksort algorithm chooses one element, called the \"pivot\", that is, on average, not too far from the center value of the data being sorted. Quicksort then separates the data into two piles, one of which contains all elements with value less than the value of the pivot, and the other containing the rest of the elements. If quicksort chooses the pivot in some deterministic fashion (for instance, always choosing the first element in the list), then it is easy for an adversary to arrange the data beforehand so that quicksort will perform in worst-case time. If, however, quicksort chooses some random element to be the pivot, then an adversary without knowledge of what random numbers are coming up cannot arrange the data to guarantee worst-case execution time for quicksort.\nThe classic on-line problem first analysed with competitive analysis (Sleator & Tarjan 1985) is the list update problem: Given a list of items and a sequence of requests for the various items, minimize the cost of accessing the list where the elements closer to the front of the list cost less to access. (Typically, the cost of accessing an item is equal to its position in the list.) After an access, the list may be rearranged. Most rearrangements have a cost. The Move-To-Front algorithm simply moves the requested item to the front after the access, at no cost. The Transpose algorithm swaps the accessed item with the item immediately before it, also at no cost. Classical methods of analysis showed that Transpose is optimal in certain contexts. In practice, Move-To-Front performed much better. Competitive analysis was used to show that an adversary can make Transpose perform arbitrarily badly compared to an optimal algorithm, whereas Move-To-Front can never be made to incur more than twice the cost of an optimal algorithm.\nIn the case of online requests from a server, competitive algorithms are used to overcome uncertainties about the future. That is, the algorithm does not \"know\" the future, while the imaginary adversary (the \"competitor\") \"knows\". Similarly, competitive algorithms were developed for distributed systems, where the algorithm has to react to a request arriving at one location, without \"knowing\" what has just happened in a remote location. This setting was presented in (Awerbuch, Kutten & Peleg 1992).", "links": ["Adversary (online algorithm)", "Allan Borodin", "Amortized analysis", "Amos Fiat", "Best, worst and average case", "Daniel Sleator", "Digital object identifier", "Gerhard J. Woeginger", "International Standard Book Number", "K-server problem", "List update problem", "Online algorithm", "Quicksort", "Randomized algorithm", "Robert Tarjan"], "categories": ["Analysis of algorithms", "Online algorithms"], "title": "Competitive analysis (online algorithm)"}
{"summary": "The k-server problem is a problem of theoretical computer science in the category of online algorithms, one of two abstract problems on metric spaces that are central to the theory of competitive analysis (the other being metrical task systems). In this problem, an online algorithm must control the movement of a set of k servers, represented as points in a metric space, and handle requests that are also in the form of points in the space. As each request arrives, the algorithm must determine which server to move to the requested point. The goal of the algorithm is to keep the total distance all servers move small, relative to the total distance the servers could have moved by an optimal adversary who knows in advance the entire sequence of requests.\nThe problem was first posed by Mark Manasse, Lyle A. McGeoch and Daniel Sleator (1990). The most prominent open question concerning the k-server problem is the so-called k-server conjecture, also posed by Manasse et al. This conjecture states that there is an algorithm for solving the k-server problem in an arbitrary metric space and for any number k of servers that has competitive ratio at least k. Manasse et al. were able to prove their conjecture when k = 2, and for more general values of k when the metric space is restricted to have exactly k+1 points. Chrobak and Larmore (1991) proved the conjecture for tree metrics. The special case of metrics in which all distances are equal is called the paging problem because it models the problem of page replacement algorithms in memory caches, and was also already known to have a k-competitive algorithm (Sleator and Tarjan 1985). Fiat et al. (1990) first proved that there exists an algorithm with finite competitive ratio for any constant k and any metric space, and finally Koutsoupias and Papadimitriou (1995) proved that Work Function Algorithm (WFA) has competitive ratio 2k - 1. However, despite the efforts of many other researchers, reducing the competitive ratio to k or providing an improved lower bound remains open as of 2014. The most common believed scenario is that the Work Function Algorithm is k-competitive. To this direction,in 2000 Bartal and Koutsoupias showed that this is true for some special cases (if the metric space is a line, a weighted star or any metric of k+2 points).\nIn 2011, a randomized algorithm with competitive bound \u00d5(log2k log3n) was found.", "links": ["Christos Papadimitriou", "Competitive analysis (online algorithm)", "Daniel Sleator", "Digital object identifier", "Lawrence L. Larmore", "List of unsolved problems in computer science", "Marek Chrobak", "Mark Manasse", "Metric space", "Metrical task systems", "Online algorithm", "Page replacement algorithm", "Robert Tarjan", "Theoretical computer science"], "categories": ["All articles containing potentially dated statements", "Articles containing potentially dated statements from 2014", "Online algorithms", "Unsolved problems in computer science"], "title": "K-server problem"}
{"summary": "The List Update or the List Access problem is a simple model used in the study of competitive analysis of online algorithms. Given a set of items in a list where the cost of accessing an item is proportional to its distance from the head of the list, e.g. a Linked List, and a request sequence of accesses, the problem is to come up with a strategy of reordering the list so that the total cost of accesses is minimized. The reordering can be done at any time but incurs a cost. The standard model includes two reordering actions:\nA free transposition of the item being accessed anywhere ahead of its current position;\nA paid transposition of a unit cost for exchanging any two items in the list. Performance of algorithms depend on the construction of request sequences by adversaries under various Adversary models\nAn online algorithm for this problem has to reorder the elements and serve requests based only on the knowledge of previously requested items and hence its strategy may not have the optimum cost as compared to an offline algorithm that gets to see the entire request sequence and devise a complete strategy before serving the first request.", "links": ["Adversary model", "Allan Borodin", "Cache algorithms", "Christoph Amb\u00fchl", "Competitive analysis (online algorithm)", "Daniel Sleator", "Digital object identifier", "International Standard Book Number", "Linked List", "NP-hard", "Online algorithm", "Online algorithms", "Potential method", "Robert Tarjan"], "categories": ["Analysis of algorithms", "Online algorithms", "Randomized algorithms"], "title": "List update problem"}
{"summary": "Task systems are mathematical objects used to model the set of possible configuration of online algorithms. They were introduced by Borodin, Linial and Saks (1992) to model a variety of online problems. A task system determines a set of states and costs to change states. Task systems obtain as input a sequence of requests such that each request assigns processing times to the states. The objective of an online algorithm for task systems is to create a schedule that minimizes the overall cost incurred due to processing the tasks with respect to the states and due to the cost to change states.\nIf the cost function to change states is a metric, the task system is a metrical task system (MTS). This is the most common type of task systems. Metrical task systems generalize online problems such as paging, list accessing, and the k-server problem (in finite spaces).", "links": ["Adversary (online algorithm)", "Allan Borodin", "Assaf Naor", "B\u00e9la Bollob\u00e1s", "Competitive analysis (online algorithm)", "Digital object identifier", "K-server problem", "List accessing problem", "Metric (mathematics)", "Michael Saks (mathematician)", "Nati Linial", "Online algorithm", "Page replacement algorithm", "Real-time computing"], "categories": ["Online algorithms"], "title": "Metrical task system"}
{"summary": "Backtracking is a general algorithm for finding all (or some) solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons each partial candidate c (\"backtracks\") as soon as it determines that c cannot possibly be completed to a valid solution.\nThe classic textbook example of the use of backtracking is the eight queens puzzle, that asks for all arrangements of eight chess queens on a standard chessboard so that no queen attacks any other. In the common backtracking approach, the partial candidates are arrangements of k queens in the first k rows of the board, all in different rows and columns. Any partial solution that contains two mutually attacking queens can be abandoned.\nBacktracking can be applied only for problems which admit the concept of a \"partial candidate solution\" and a relatively quick test of whether it can possibly be completed to a valid solution. It is useless, for example, for locating a given value in an unordered table. When it is applicable, however, backtracking is often much faster than brute force enumeration of all complete candidates, since it can eliminate a large number of candidates with a single test.\nBacktracking is an important tool for solving constraint satisfaction problems, such as crosswords, verbal arithmetic, Sudoku, and many other puzzles. It is often the most convenient (if not the most efficient) technique for parsing, for the knapsack problem and other combinatorial optimization problems. It is also the basis of the so-called logic programming languages such as Icon, Planner and Prolog.\nBacktracking depends on user-given \"black box procedures\" that define the problem to be solved, the nature of the partial candidates, and how they are extended into complete candidates. It is therefore a metaheuristic rather than a specific algorithm \u2013 although, unlike many other meta-heuristics, it is guaranteed to find all solutions to a finite problem in a bounded amount of time.\nThe term \"backtrack\" was coined by American mathematician D. H. Lehmer in the 1950s. The pioneer string-processing language SNOBOL (1962) may have been the first to provide a built-in general backtracking facility.", "links": ["A* search algorithm", "Algorithm", "Algorithmics of sudoku", "Alpha\u2013beta pruning", "Amsterdam", "Ariadne's thread (logic)", "B*", "Backjumping", "Backtracking line search", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Boolean-valued function", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Brute force search", "Central processing unit", "Chess", "Chessboard", "Combinatorial optimization", "Computational problem", "Constraint propagation", "Constraint satisfaction problem", "Crosswords", "D*", "Depth-first search", "Depth-limited search", "Derrick Henry Lehmer", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Eight queens puzzle", "Elsevier", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph traversal", "Hill climbing", "Icon programming language", "International Standard Book Number", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Knapsack problem", "Kruskal's algorithm", "Lexicographic breadth-first search", "Line search", "List of algorithms", "Logic programming", "Logical conjunction", "Mathematical optimization", "Metaheuristic", "Parsing", "Peg solitaire", "Planner programming language", "Prim's algorithm", "Procedural parameter", "Prolog", "Puzzle", "Queen (chess)", "Recursion (computer science)", "SMA*", "SNOBOL", "Search game", "The Art of Computer Programming", "Timestamp", "Tree structure", "Tree traversal", "Variable trail", "Verbal arithmetic"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from January 2011", "Operations research", "Pages with URL errors", "Pattern matching", "Search algorithms"], "title": "Backtracking"}
{"summary": "The regular expression denial of service (ReDoS) is an algorithmic complexity attack that produces a denial-of-service by providing a regular expression that takes a very long time to evaluate. The attack exploits the fact that most regular expression implementations have exponential time worst case complexity: the time taken can grow exponentially in relation to input size. An attacker can thus cause a program to spend an effectively infinite amount of time processing by providing such a regular expression, either slowing down or becoming unresponsive.", "links": ["Algorithmic complexity attack", "Compiler", "Denial-of-service", "Deterministic finite automaton", "Digital object identifier", "E-mail scanner", "EXPTIME", "Exponential time", "Finite-state automaton", "International Standard Book Number", "Intrusion detection system", "Lazy evaluation", "MSDN Magazine", "Nondeterministic finite-state automaton", "OWASP", "Programming language", "Regular expression", "RiverStar Software", "Time complexity", "Worst case complexity"], "categories": ["Algorithmic complexity attacks", "Denial-of-service attacks", "Pattern matching", "Regular expressions"], "title": "ReDoS"}
{"summary": "ReteOO is an improved version of the Rete algorithm.\nRete supports only boolean, first order logic.\n\n^ http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5551128&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D5551128", "links": ["Algorithm", "Data structure", "Rete algorithm"], "categories": ["Algorithms and data structures stubs", "All orphaned articles", "All stub articles", "Computer science stubs", "Expert systems", "Orphaned articles from February 2013", "Pattern matching"], "title": "ReteOO"}
{"summary": "A phonetic algorithm is an algorithm for indexing of words by their pronunciation. Most phonetic algorithms were developed for use with the English language; consequently, applying the rules to words in other languages might not give a meaningful result.\nThey are necessarily complex algorithms with many rules and exceptions, because English spelling and pronunciation is complicated by historical changes in pronunciation and words borrowed from many languages.\nAmong the best-known phonetic algorithms are:\nSoundex, which was developed to encode surnames for use in censuses. Soundex codes are four-character strings composed of a single letter followed by three numbers.\nDaitch\u2013Mokotoff Soundex, which is a refinement of Soundex designed to better match surnames of Slavic and Germanic origin. Daitch\u2013Mokotoff Soundex codes are strings composed of six numeric digits.\nK\u00f6lner Phonetik: This is similar to Soundex, but more suitable for German words.\nMetaphone, Double Metaphone, and Metaphone 3 which are suitable for use with most English words, not just names. Metaphone algorithms are the basis for many popular spell checkers.\nNew York State Identification and Intelligence System (NYSIIS), which maps similar phonemes to the same letter. The result is a string that can be pronounced by the reader without decoding.\nMatch Rating Approach developed by Western Airlines in 1977 - this algorithm has an encoding and range comparison technique.\nCaverphone, created to assist in data matching between late 19th century and early 20th century electoral rolls, optimized for accents present in parts of New Zealand.", "links": ["Algorithm", "Approximate string matching", "Caverphone", "Clojure", "Daitch\u2013Mokotoff Soundex", "Damerau\u2013Levenshtein distance", "Data structure", "Dictionary of Algorithms and Data Structures", "Double Metaphone", "English language", "Hamming distance", "Index (publishing)", "Language", "Levenshtein distance", "Loanword", "Match Rating Approach", "Metaphone", "National Institute of Standards and Technology", "New York State Identification and Intelligence System", "Phonemes", "Phonetics", "Pronunciation", "R (programming language)", "Scala programming language", "Search engine technology", "Soundex", "Spell checkers", "Spelling", "Word"], "categories": ["Algorithms and data structures stubs", "All articles needing additional references", "All stub articles", "Articles needing additional references from August 2009", "Computer science stubs", "Phonetic algorithms", "Phonetics stubs", "Phonology"], "title": "Phonetic algorithm"}
{"summary": "The Caverphone phonetic matching algorithm was created by David Hood in the Caversham Project at the University of Otago in New Zealand in 2002, revised in 2004. It was created to assist in data matching between late 19th century and early 20th century electoral rolls, where the name only needed to be in a \"commonly recognisable form\". The algorithm was intended to apply to those names that could not easily be matched between electoral rolls, after the exact matches were removed from the pool of potential matches. The algorithm is optimised for accents present in the study area (southern part of the city of Dunedin, New Zealand).", "links": ["Caversham Project", "CiteSeer", "Dunedin", "International Standard Book Number", "Match rating approach", "Metaphone", "New York State Identification and Intelligence System", "New Zealand", "Phonetic matching algorithm", "Soundex", "University of Otago"], "categories": ["All Wikipedia articles needing context", "All articles with topics of unclear notability", "All pages needing cleanup", "Articles with topics of unclear notability from September 2008", "Phonetic algorithms", "Wikipedia articles needing context from October 2009", "Wikipedia introduction cleanup from October 2009"], "title": "Caverphone"}
{"summary": "Daitch\u2013Mokotoff Soundex (D\u2013M Soundex) is a phonetic algorithm invented in 1985 by Jewish genealogists Gary Mokotoff and Randy Daitch. It is a refinement of the Russell and American Soundex algorithms designed to allow greater accuracy in matching of Slavic and Yiddish surnames with similar pronunciation but differences in spelling.\nDaitch\u2013Mokotoff Soundex is sometimes referred to as \"Jewish Soundex\" and \"Eastern European Soundex\", although the authors discourage use of these nicknames for the algorithm because the algorithm itself is independent of the fact the motivation for creating the new system was the poor results of predecessor systems when dealing with Slavic and Yiddish surnames.", "links": ["Alexander Beider", "Avotaynu", "Gary Mokotoff", "Phonetic algorithm", "Pronunciation", "Randy Daitch", "Slavic (language)", "Soundex", "Stephen P. Morse", "Surname", "Where Once We Walked", "Yiddish"], "categories": ["Genealogy", "Phonetic algorithms"], "title": "Daitch\u2013Mokotoff Soundex"}
{"summary": "The match rating approach (MRA) is a phonetic algorithm developed by Western Airlines in 1977 for the indexation and comparison of homophonous names.\nThe algorithm itself has a simple set of encoding rules but a more lengthy set of comparison rules. The main mechanism being the similarity comparison which calculates the number of unmatched characters by comparing the strings from left to right and then from right to left and removing identical characters. This value is subtracted from 6 and then compared to a minimum threshold. The minimum threshold is defined by table A and is dependent upon the length of the strings.\nThe encoded name is known (perhaps incorrectly) as a personal numeric identifier (PNI). The PNI codex can never contain more than 6 alpha only characters.\nMatch rating approach performs well with names containing the letter \"y\" unlike the original flavour of the NYSIIS algorithm. For example, the surnames \"Smith\" and \"Smyth\" are successfully matched.\nMRA does not perform well with encoded names that differ in length by more than 2.", "links": ["Codex", "Homophonous", "NYSIIS", "Phonetic algorithm", "Western Airlines"], "categories": ["All Wikipedia articles needing context", "All pages needing cleanup", "Phonetic algorithms", "Wikipedia articles needing context from October 2009", "Wikipedia introduction cleanup from October 2009"], "title": "Match rating approach"}
{"summary": "Lawrence Philips redirects here. For the football player, see Lawrence Phillips.\nMetaphone is a phonetic algorithm, published by Lawrence Philips in 1990, for indexing words by their English pronunciation. It fundamentally improves on the Soundex algorithm by using information about variations and inconsistencies in English spelling and pronunciation to produce a more accurate encoding, which does a better job of matching words and names which sound similar. As with Soundex, similar sounding words should share the same keys. Metaphone is available as a built-in operator in a number of systems.\nThe original author later produced a new version of the algorithm, which he named Double Metaphone. Contrary to the original algorithm whose application is limited to English only, this version takes into account spelling peculiarities of a number of other languages. In 2009 Lawrence Philips released a third version, called Metaphone 3, which achieves an accuracy of approximately 99% for English words, non-English words familiar to Americans, and first names and family names commonly found in the United States, having been developed according to modern engineering standards against a test harness of prepared correct encodings.", "links": ["ASCII", "C/C++ Users Journal", "Caverphone", "Celtic languages", "Ch (digraph)", "Chinese language", "Consonant", "English language", "French language", "Germanic languages", "Greek language", "Italian language", "Lawrence Phillips", "Match Rating Approach", "New York State Identification and Intelligence System", "Phonetic algorithm", "Sh (digraph)", "Slavic languages", "Soundex", "Spanish language", "Th (digraph)", "Theta", "Vowels"], "categories": ["Phonetic algorithms"], "title": "Metaphone"}
{"summary": "The New York State Identification and Intelligence System Phonetic Code, commonly known as NYSIIS, is a phonetic algorithm devised in 1970 as part of the New York State Identification and Intelligence System (now a part of the New York State Division of Criminal Justice Services). It features an accuracy increase of 2.7% over the traditional Soundex algorithm.", "links": ["New York State", "Phonetic algorithm", "Scala (programming language)", "Soundex"], "categories": ["Phonetic algorithms"], "title": "New York State Identification and Intelligence System"}
{"summary": "Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English. The goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. Soundex is the most widely known of all phonetic algorithms (in part because it is a standard feature of popular database software such as DB2, PostgreSQL, MySQL, Ingres, MS SQL Server and Oracle) and is often used (incorrectly) as a synonym for \"phonetic algorithm\". Improvements to Soundex are the basis for many modern phonetic algorithms.", "links": ["Communications of the ACM", "Consonant", "Daitch\u2013Mokotoff Soundex", "Donald Knuth", "Double Metaphone", "Encoding", "Homophone", "Index (publishing)", "International Standard Book Number", "Journal of the ACM", "Labial consonant", "Lawrence Philips", "Letter (alphabet)", "Match Rating Approach", "Metaphone", "Metonym", "Microsoft SQL Server", "MySQL", "N-gram", "National Archives and Records Administration", "New York State Identification and Intelligence System", "Numerical digit", "OCLC", "Oracle Database", "Patent", "Phonetic algorithm", "Place of articulation", "PostgreSQL", "Pronunciation", "Spelling", "The Art of Computer Programming", "The SoundEx", "United States Census"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from August 2007", "Phonetic algorithms"], "title": "Soundex"}
{"summary": "In computer science, string searching algorithms, sometimes called string matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.\nLet \u03a3 be an alphabet (finite set). Formally, both the pattern and searched text are vectors of elements of \u03a3. The \u03a3 may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (\u03a3 = {0,1}) or DNA alphabet (\u03a3 = {A,C,G,T}) in bioinformatics.\nIn practice, how the string is encoded can affect the feasible string search algorithms. In particular if a variable width encoding is in use then it is slow (time proportional to N) to find the Nth character. This will significantly slow down many of the more advanced search algorithms. A possible solution is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it.", "links": ["Aho\u2013Corasick algorithm", "Aho\u2013Corasick string matching algorithm", "Algorithm", "Alphabet (computer science)", "Apostolico\u2013Giancarlo algorithm", "Approximate string matching", "Big O notation", "Bioinformatics", "Bitap algorithm", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "Charles E. Leiserson", "Clifford Stein", "Commentz-Walter algorithm", "Comparison of regular expression engines", "Compressed pattern matching", "Computer science", "Damerau\u2013Levenshtein distance", "Depth-first search", "Deterministic acyclic finite state automaton", "Deterministic finite automaton", "Digital object identifier", "Directed acyclic word graph", "Edit distance", "Finite-state machine", "Finite set", "Fuzzy string searching", "Generalized suffix tree", "Hamming distance", "Hirschberg's algorithm", "Introduction to Algorithms", "Jaro\u2013Winkler distance", "Knuth\u2013Morris\u2013Pratt algorithm", "Lee distance", "Levenshtein automaton", "Levenshtein distance", "List of regular expression software", "Longest common subsequence", "Longest common substring", "Needleman\u2013Wunsch algorithm", "Nondeterministic finite automaton", "Parsing", "Pattern", "Pattern matching", "Powerset construction", "Rabin\u2013Karp algorithm", "Rabin\u2013Karp string search algorithm", "Regular expression", "Regular grammar", "Regular tree grammar", "Ronald L. Rivest", "Rope (data structure)", "Sequence alignment", "Sequential pattern mining", "Smith\u2013Waterman algorithm", "String (computer science)", "String algorithm", "String metric", "Substring index", "Suffix array", "Suffix automaton", "Suffix tree", "Ternary search tree", "Thomas H. Cormen", "Thompson's construction", "Trie", "Trigram search", "Variable width encoding", "Wagner\u2013Fischer algorithm"], "categories": ["All articles needing additional references", "Articles needing additional references from July 2013", "String matching algorithms"], "title": "String searching algorithm"}
{"summary": "In computer science, the Aho\u2013Corasick algorithm is a string searching algorithm invented by Alfred V. Aho and Margaret J. Corasick. It is a kind of dictionary-matching algorithm that locates elements of a finite set of strings (the \"dictionary\") within an input text. It matches all patterns simultaneously. The complexity of the algorithm is linear in the length of the patterns plus the length of the searched text plus the number of output matches. Note that because all matches are found, there can be a quadratic number of matches if every substring matches (e.g. dictionary = a, aa, aaa, aaaa and input string is aaaa).\nInformally, the algorithm constructs a finite state machine that resembles a trie with additional links between the various internal nodes. These extra internal links allow fast transitions between failed pattern matches (e.g. a search for cat in a trie that does not contain cat, but contains cart, and thus would fail at the node prefixed by ca), to other branches of the trie that share a common prefix (e.g., in the previous case, a branch for attribute might be the best lateral transition). This allows the automaton to transition between pattern matches without the need for backtracking.\nWhen the pattern dictionary is known in advance (e.g. a computer virus database), the construction of the automaton can be performed once off-line and the compiled automaton stored for later use. In this case, its run time is linear in the length of the input plus the number of matched entries.\nThe Aho\u2013Corasick string matching algorithm formed the basis of the original Unix command fgrep.", "links": ["Alfred Aho", "Alfred V. Aho", "Apostolico\u2013Giancarlo algorithm", "Approximate string matching", "Bitap algorithm", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "Commentz-Walter algorithm", "Comparison of regular expression engines", "Compressed pattern matching", "Computational complexity theory", "Computer science", "Computer virus", "Damerau\u2013Levenshtein distance", "Deterministic acyclic finite state automaton", "Dictionary of Algorithms and Data Structures", "Digital object identifier", "Directed acyclic word graph", "Edit distance", "Finite state machine", "Generalized suffix tree", "Grep", "Hamming distance", "Hirschberg's algorithm", "Jaro\u2013Winkler distance", "Knuth\u2013Morris\u2013Pratt algorithm", "Lee distance", "Levenshtein automaton", "Levenshtein distance", "List of Unix programs", "List of regular expression software", "Longest common subsequence", "Longest common substring", "Needleman\u2013Wunsch algorithm", "Nondeterministic finite automaton", "Parsing", "Pattern matching", "Rabin\u2013Karp algorithm", "Rabin\u2013Karp string search algorithm", "Regular expression", "Regular tree grammar", "Rope (data structure)", "Sequence alignment", "Sequential pattern mining", "Smith\u2013Waterman algorithm", "String (computer science)", "String metric", "String searching algorithm", "Suffix array", "Suffix automaton", "Suffix tree", "Ternary search tree", "Thompson's construction", "Trie", "Wagner\u2013Fischer algorithm"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from February 2013", "String matching algorithms"], "title": "Aho\u2013Corasick algorithm"}
{"summary": "In computer science, the Apostolico\u2013Giancarlo algorithm is a variant of the Boyer\u2013Moore string search algorithm, the basic application of which is searching for occurrences of a pattern  in a text . As with other comparison-based string searches, this is done by aligning  to a certain index of  and checking whether a match occurs at that index.  is then shifted relative to  according to the rules of the Boyer-Moore algorithm, and the process repeats until the end of  has been reached. Application of the Boyer-Moore shift rules often results in large chunks of the text being skipped entirely.\nWith regard to the shift operation, Apostolico-Giancarlo is exactly equivalent in functionality to Boyer-Moore. The utility of Apostolico-Giancarlo is to speed up the match-checking operation at any index. With Boyer-Moore, finding an occurrence of  in  requires that all  characters of  be explicitly matched. For certain patterns and texts, this is very inefficient - a simple example is when both pattern and text consist of the same repeated character, in which case Boyer-Moore runs in  where  is the length in characters of . Apostolico-Giancarlo speeds this up by recording the number of characters matched at the alignments of  in a table, which is combined with data gathered during the pre-processing of  to avoid redundant equality checking for sequences of characters that are known to match.", "links": ["Aho\u2013Corasick algorithm", "Approximate string matching", "Bitap algorithm", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "Cambridge University Press", "Commentz-Walter algorithm", "Comparison of regular expression engines", "Compressed pattern matching", "Computer science", "Damerau\u2013Levenshtein distance", "Deterministic acyclic finite state automaton", "Directed acyclic word graph", "Edit distance", "Generalized suffix tree", "Hamming distance", "Hirschberg's algorithm", "Jaro\u2013Winkler distance", "Knuth\u2013Morris\u2013Pratt algorithm", "Lee distance", "Levenshtein automaton", "Levenshtein distance", "List of regular expression software", "Longest common subsequence", "Longest common substring", "Needleman\u2013Wunsch algorithm", "Nondeterministic finite automaton", "Orl\u00e9ans", "Oxford University Press", "Parsing", "Pattern matching", "Rabin\u2013Karp algorithm", "Rabin\u2013Karp string search algorithm", "Regular expression", "Regular tree grammar", "Rope (data structure)", "SIAM Journal on Computing", "Sequence alignment", "Sequential pattern mining", "Smith\u2013Waterman algorithm", "String (computer science)", "String metric", "String searching algorithm", "Suffix array", "Suffix automaton", "Suffix tree", "Ternary search tree", "Thompson's construction", "Trie", "Wagner\u2013Fischer algorithm", "Wojciech Rytter"], "categories": ["String matching algorithms"], "title": "Apostolico\u2013Giancarlo algorithm"}
{"summary": "The bitap algorithm (also known as the shift-or, shift-and or Baeza-Yates\u2013Gonnet algorithm) is an approximate string matching algorithm. The algorithm tells whether a given text contains a substring which is \"approximately equal\" to a given pattern, where approximate equality is defined in terms of Levenshtein distance \u2014 if the substring and pattern are within a given distance k of each other, then the algorithm considers them equal. The algorithm begins by precomputing a set of bitmasks containing one bit for each element of the pattern. Then it is able to do most of the work with bitwise operations, which are extremely fast.\nThe bitap algorithm is perhaps best known as one of the underlying algorithms of the Unix utility agrep, written by Udi Manber, Sun Wu, and Burra Gopal. Manber and Wu's original paper gives extensions of the algorithm to deal with fuzzy matching of general regular expressions.\nDue to the data structures required by the algorithm, it performs best on patterns less than a constant length (typically the word length of the machine in question), and also prefers inputs over a small alphabet. Once it has been implemented for a given alphabet and word length m, however, its running time is completely predictable \u2014 it runs in O(mn) operations, no matter the structure of the text or the pattern.\nThe bitap algorithm for exact string searching was invented by B\u00e1lint D\u00f6m\u00f6lki in 1964[1][2] and extended by R. K. Shyamasundar in 1977[3], before being reinvented in the context of fuzzy string searching by Manber and Wu in 1991[4][5] based on work done by Ricardo Baeza-Yates and Gaston Gonnet[6]. The algorithm was improved by Baeza-Yates and Navarro in 1996[7] and later by Gene Myers for long patterns in 1998[8].", "links": ["Agrep", "Aho\u2013Corasick algorithm", "Apostolico\u2013Giancarlo algorithm", "Approximate string matching", "Big O notation", "Bitmask", "Bitwise operation", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "Burra Gopal", "Commentz-Walter algorithm", "Communications of the ACM", "Comparison of regular expression engines", "Compressed pattern matching", "Damerau\u2013Levenshtein distance", "Deterministic acyclic finite state automaton", "Digital object identifier", "Directed acyclic word graph", "Edit distance", "Fuzzy matching", "Gaston Gonnet", "Gene Myers", "Generalized suffix tree", "Gonzalo Navarro", "Hamming distance", "Hirschberg's algorithm", "Inner loop", "Jaro\u2013Winkler distance", "Knuth\u2013Morris\u2013Pratt algorithm", "Lee distance", "Levenshtein automaton", "Levenshtein distance", "List of regular expression software", "Longest common subsequence", "Longest common substring", "Needleman\u2013Wunsch algorithm", "Nondeterministic finite automaton", "Parsing", "Pattern matching", "Programming tool", "Rabin\u2013Karp algorithm", "Rabin\u2013Karp string search algorithm", "Regular expression", "Regular tree grammar", "Ricardo Baeza-Yates", "Rope (data structure)", "Running time", "Sequence alignment", "Sequential pattern mining", "Smith\u2013Waterman algorithm", "String (computer science)", "String metric", "String searching algorithm", "Suffix array", "Suffix automaton", "Suffix tree", "Sun Wu (computer scientist)", "Ternary search tree", "Thompson's construction", "Trie", "Udi Manber", "University of Arizona", "Unix", "Wagner\u2013Fischer algorithm", "Word length"], "categories": ["Articles with example C code", "String matching algorithms"], "title": "Bitap algorithm"}
{"summary": "In computer science, the Boyer\u2013Moore string search algorithm is an efficient string searching algorithm that is the standard benchmark for practical string search literature. It was developed by Robert S. Boyer and J Strother Moore in 1977. The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text.", "links": ["Aho\u2013Corasick algorithm", "Algorithm", "Andrew Odlyzko", "Apostolico\u2013Giancarlo algorithm", "Approximate string matching", "Bitap algorithm", "Boost (C++ libraries)", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "Brute-force search", "C++", "Commentz-Walter algorithm", "Comparison of regular expression engines", "Compressed pattern matching", "Computer science", "D (programming language)", "Damerau\u2013Levenshtein distance", "Deterministic acyclic finite state automaton", "Digital object identifier", "Directed acyclic word graph", "Donald Knuth", "Edit distance", "Generalized suffix tree", "Go (programming language)", "Hamming distance", "Hirschberg's algorithm", "International Standard Book Number", "International Standard Serial Number", "J Strother Moore", "James H. Morris", "Jaro\u2013Winkler distance", "Knuth\u2013Morris\u2013Pratt algorithm", "Lee distance", "Leonidas J. Guibas", "Levenshtein automaton", "Levenshtein distance", "List of regular expression software", "Longest common subsequence", "Longest common substring", "Needleman\u2013Wunsch algorithm", "Nondeterministic finite automaton", "Nqthm", "Parsing", "Pattern matching", "Preprocessor", "Rabin\u2013Karp algorithm", "Rabin\u2013Karp string search algorithm", "Regular expression", "Regular tree grammar", "Robert S. Boyer", "Rope (data structure)", "Sequence alignment", "Sequential pattern mining", "Smith\u2013Waterman algorithm", "String (computer science)", "String metric", "String searching algorithm", "Substring", "Suffix array", "Suffix automaton", "Suffix tree", "Ternary search tree", "Thompson's construction", "Trie", "Vaughan Pratt", "Wagner\u2013Fischer algorithm", "Zvi Galil"], "categories": ["Algorithms on strings", "Articles with example C code", "Articles with example Java code", "Articles with example Python code", "String matching algorithms"], "title": "Boyer\u2013Moore string search algorithm"}
{"summary": "In computer science, the Boyer\u2013Moore\u2013Horspool algorithm or Horspool's algorithm is an algorithm for finding substrings in strings. It was published by Nigel Horspool in 1980.\nIt is a simplification of the Boyer\u2013Moore string search algorithm which is related to the Knuth\u2013Morris\u2013Pratt algorithm. The algorithm trades space for time in order to obtain an average-case complexity of O(N) on random text, although it has O(MN) in the worst case, where the length of the pattern is M and the length of the search string is N.", "links": ["Aho\u2013Corasick algorithm", "Algorithm", "Alphabet (formal languages)", "Apostolico\u2013Giancarlo algorithm", "Approximate string matching", "Average-case complexity", "Big O notation", "Bitap algorithm", "Boyer\u2013Moore string search algorithm", "CiteSeer", "Commentz-Walter algorithm", "Comparison of regular expression engines", "Compressed pattern matching", "Computer science", "Damerau\u2013Levenshtein distance", "Deterministic acyclic finite state automaton", "Digital object identifier", "Directed acyclic word graph", "Edit distance", "Generalized suffix tree", "Hamming distance", "Hirschberg's algorithm", "Jaro\u2013Winkler distance", "Knuth\u2013Morris\u2013Pratt algorithm", "Lee distance", "Levenshtein automaton", "Levenshtein distance", "List of regular expression software", "Longest common subsequence", "Longest common substring", "Needleman\u2013Wunsch algorithm", "Nigel Horspool", "Nondeterministic finite automaton", "Parsing", "Pattern matching", "Rabin\u2013Karp algorithm", "Rabin\u2013Karp string search algorithm", "Raita Algorithm", "Regular expression", "Regular tree grammar", "Rope (data structure)", "Sequence alignment", "Sequential pattern mining", "Smith\u2013Waterman algorithm", "String (computer science)", "String metric", "String searching algorithm", "Substring", "Suffix array", "Suffix automaton", "Suffix tree", "Ternary search tree", "Thompson's construction", "Trie", "Wagner\u2013Fischer algorithm", "Worst case"], "categories": ["All accuracy disputes", "All articles needing additional references", "All articles to be merged", "Articles needing additional references from October 2015", "Articles to be merged from March 2015", "Articles with disputed statements from June 2015", "Articles with example C code", "Pages using citations with accessdate and no URL", "String matching algorithms"], "title": "Boyer\u2013Moore\u2013Horspool algorithm"}
{"summary": "In computer science, the Commentz-Walter algorithm is a string searching algorithm invented by Beate Commentz-Walter. Like the Aho\u2013Corasick string matching algorithm, it can search for multiple patterns at once. It combines ideas from Aho\u2013Corasick with the fast matching of the Boyer\u2013Moore string search algorithm. For a text of length n and maximum pattern length of m, its worst-case running time is O(mn), though the average case is often much better.\nGNU grep implements a string matching algorithm very similar to Commentz-Walter.", "links": ["Aho\u2013Corasick algorithm", "Aho\u2013Corasick string matching algorithm", "Algorithm", "Apostolico\u2013Giancarlo algorithm", "Approximate string matching", "Beate Commentz-Walter", "Big-O notation", "Bitap algorithm", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "Comparison of regular expression engines", "Compressed pattern matching", "Computer science", "Damerau\u2013Levenshtein distance", "Data structure", "Deterministic acyclic finite state automaton", "Directed acyclic word graph", "Edit distance", "GNU", "Generalized suffix tree", "Grep", "Hamming distance", "Hirschberg's algorithm", "Jaro\u2013Winkler distance", "Knuth\u2013Morris\u2013Pratt algorithm", "Lee distance", "Levenshtein automaton", "Levenshtein distance", "List of regular expression software", "Longest common subsequence", "Longest common substring", "Needleman\u2013Wunsch algorithm", "Nondeterministic finite automaton", "Parsing", "Pattern matching", "Rabin\u2013Karp algorithm", "Rabin\u2013Karp string search algorithm", "Regular expression", "Regular tree grammar", "Rope (data structure)", "Sequence alignment", "Sequential pattern mining", "Smith\u2013Waterman algorithm", "String (computer science)", "String metric", "String searching algorithm", "Suffix array", "Suffix automaton", "Suffix tree", "Ternary search tree", "Thompson's construction", "Trie", "Wagner\u2013Fischer algorithm"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "String matching algorithms"], "title": "Commentz-Walter algorithm"}
{"summary": "In computer science, the Knuth\u2013Morris\u2013Pratt string searching algorithm (or KMP algorithm) searches for occurrences of a \"word\" W within a main \"text string\" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.\nThe algorithm was conceived in 1974 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. The three published it jointly in 1977.", "links": ["-yllion", "AMS Euler", "Aho\u2013Corasick algorithm", "Algorithm", "Apostolico\u2013Giancarlo algorithm", "Approximate string matching", "Big-O notation", "Bitap algorithm", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "CWEB", "Charles E. Leiserson", "Clifford Stein", "Commentz-Walter algorithm", "Comparison of regular expression engines", "Compressed pattern matching", "Computational complexity theory", "Computer Modern", "Computer science", "Computers and Typesetting", "Concrete Mathematics", "Concrete Roman", "Damerau\u2013Levenshtein distance", "Dancing Links", "David Eppstein", "Deterministic acyclic finite state automaton", "Digital object identifier", "Dijkstra's algorithm", "Directed acyclic word graph", "Donald Knuth", "Edit distance", "Fisher\u2013Yates shuffle", "Font", "GNU MIX Development Kit", "Generalized suffix tree", "Hamming distance", "Hirschberg's algorithm", "International Standard Book Number", "Introduction to Algorithms", "James H. Morris", "Jaro\u2013Winkler distance", "Knuth's Algorithm X", "Knuth's Simpath algorithm", "Knuth Prize", "Knuth reward check", "Knuth\u2013Bendix completion algorithm", "Lee distance", "Levenshtein automaton", "Levenshtein distance", "Lexicographically minimal string rotation", "Linear time", "List of regular expression software", "Literate programming", "Longest common subsequence", "Longest common substring", "METAFONT", "MIX", "MMIX", "Man or boy test", "Needleman\u2013Wunsch algorithm", "Nondeterministic finite automaton", "Parsing", "Pattern matching", "Potrzebie", "Pseudocode", "Quater-imaginary base", "Rabin\u2013Karp algorithm", "Rabin\u2013Karp string search algorithm", "Real-time computing", "Regular expression", "Regular tree grammar", "Robinson\u2013Schensted\u2013Knuth correspondence", "Ronald L. Rivest", "Rope (data structure)", "Selected papers series of Knuth", "Sequence alignment", "Sequential pattern mining", "Smith\u2013Waterman algorithm", "Software", "String (computer science)", "String metric", "String searching algorithm", "Substring", "Suffix array", "Suffix automaton", "Suffix tree", "Surreal Numbers (book)", "TeX", "Ternary search tree", "The Art of Computer Programming", "The Complexity of Songs", "Things a Computer Scientist Rarely Talks About", "Thomas H. Cormen", "Thompson's construction", "Trabb Pardo\u2013Knuth algorithm", "Trie", "Vaughan Pratt", "WEB", "Wagner\u2013Fischer algorithm", "Wojciech Rytter", "Zentralblatt MATH", "Zero-based numbering"], "categories": ["All articles needing additional references", "Articles needing additional references from October 2009", "Articles with example pseudocode", "Donald Knuth", "String matching algorithms"], "title": "Knuth\u2013Morris\u2013Pratt algorithm"}
{"summary": "In computer science, the Rabin\u2013Karp algorithm or Karp\u2013Rabin algorithm is a string searching algorithm created by Richard M. Karp and Michael O. Rabin (1987) that uses hashing to find any one of a set of pattern strings in a text. For text of length n and p patterns of combined length m, its average and best case running time is O(n+m) in space O(p), but its worst-case time is O(nm). In contrast, the Aho\u2013Corasick string matching algorithm has asymptotic worst-time complexity O(n+m) in space O(m).\nA practical application of the algorithm is detecting plagiarism. Given source material, the algorithm can rapidly search through a paper for instances of sentences from the source material, ignoring details such as case and punctuation. Because of the abundance of the sought strings, single-string searching algorithms are impractical.", "links": ["ASCII", "Aho\u2013Corasick algorithm", "Aho\u2013Corasick string matching algorithm", "Apostolico\u2013Giancarlo algorithm", "Approximate string matching", "Big-O notation", "Bitap algorithm", "Bloom filter", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "Cambridge, Massachusetts", "Charles E. Leiserson", "CiteSeer", "Clifford Stein", "Commentz-Walter algorithm", "Comparison of regular expression engines", "Compressed pattern matching", "Computer science", "Damerau\u2013Levenshtein distance", "Data type", "Deterministic acyclic finite state automaton", "Digital object identifier", "Directed acyclic word graph", "Edit distance", "Generalized suffix tree", "Hamming distance", "Hash collision", "Hash function", "Hash value", "Hirschberg's algorithm", "International Standard Book Number", "Introduction to Algorithms", "Jaro\u2013Winkler distance", "Knuth\u2013Morris\u2013Pratt algorithm", "Lee distance", "Levenshtein automaton", "Levenshtein distance", "List of regular expression software", "Longest common subsequence", "Longest common substring", "Michael O. Rabin", "Modular arithmetic", "Needleman\u2013Wunsch algorithm", "Nondeterministic finite automaton", "Parsing", "Pattern matching", "Plagiarism", "Prime number", "Rabin fingerprint", "Rabin\u2013Karp string search algorithm", "Regular expression", "Regular tree grammar", "Richard Karp", "Richard M. Karp", "Rolling hash", "Ronald L. Rivest", "Rope (data structure)", "Sequence alignment", "Sequential pattern mining", "Set data structure", "Smith\u2013Waterman algorithm", "String (computer science)", "String metric", "String searching algorithm", "Suffix array", "Suffix automaton", "Suffix tree", "Ternary search tree", "Thomas H. Cormen", "Thompson's construction", "Trie", "Wagner\u2013Fischer algorithm"], "categories": ["Hashing", "String matching algorithms"], "title": "Rabin\u2013Karp algorithm"}
{"summary": "In computer science, the Raita algorithm is a string searching algorithm which improves the performance of Boyer-Moore-Horspool algorithm. This algorithm preprocesses the string being searched for the pattern, which is similar to Boyer-Moore string search algorithm. The searching pattern of particular sub-string in a given string is different from Boyer-Moore-Horspool algorithm. This algorithm was published by Tim Raita in 1991.", "links": ["Boyer-Moore-Horspool algorithm", "Boyer-Moore string search algorithm", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "String searching algorithm"], "categories": ["All articles needing additional references", "All articles to be merged", "All articles with topics of unclear notability", "All orphaned articles", "Articles needing additional references from March 2015", "Articles to be merged from March 2015", "Articles with topics of unclear notability from March 2015", "Orphaned articles from May 2013", "String matching algorithms"], "title": "Raita Algorithm"}
{"summary": "In computer science, the Zhu\u2013Takaoka string matching algorithm is a variant of the Boyer\u2013Moore string search algorithm. It uses two consecutive text characters to compute the bad character shift. It is faster when the alphabet or pattern is small, but the skip table grows quickly, slowing the pre-processing phase.", "links": ["Boyer\u2013Moore string search algorithm", "Computer science", "Dictionary of Algorithms and Data Structures", "National Institute of Standards and Technology", "Pre-processing", "Skip table"], "categories": ["String matching algorithms"], "title": "Zhu\u2013Takaoka string matching algorithm"}
{"summary": "Featherstone's algorithm is a technique used for computing the effects of forces applied to a structure of joints and links (an \"open kinematic chain\") such as a skeleton used in ragdoll physics.\nThe Featherstone's algorithm uses a reduced coordinate representation. This is in contrast to the more popular Lagrange multiplier method, which uses maximal coordinates. Brian Mirtich's PhD Thesis has a very clear and detailed description of the algorithm. Baraff's paper \"Linear-time dynamics using Lagrange multipliers\" has a discussion and comparison of both algorithms.", "links": ["Algorithm", "Data structure", "International Standard Book Number", "Kinematic chain", "Lagrange multiplier method", "Ragdoll physics", "Skeleton"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computational physics", "Computer physics engines", "Computer science stubs", "Mechanics"], "title": "Featherstone's algorithm"}
{"summary": "In computational complexity theory, a numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input, but is exponential in the length of the input \u2013 the number of bits required to represent it.\nAn NP-complete problem with known pseudo-polynomial time algorithms is called weakly NP-complete. An NP-complete problem is called strongly NP-complete if it is proven that it cannot be solved by a pseudo-polynomial time algorithm unless P=NP. The strong/weak kinds of NP-hardness are defined analogously.", "links": ["AKS primality test", "Big O notation", "Computation time", "Computational complexity theory", "Computers and Intractability: A Guide to the Theory of NP-Completeness", "Knapsack problem", "NP-complete", "NP-hard", "P=NP", "Polynomial", "Polynomial function", "Primality test", "Problem size", "Quasi-polynomial time", "Strongly NP-complete", "Unary numeral system", "Weakly NP-complete"], "categories": ["Analysis of algorithms", "Complexity classes", "Computational complexity theory", "Pseudo-polynomial time algorithms"], "title": "Pseudo-polynomial time"}
{"summary": "In quantum computing, a quantum algorithm is an algorithm which runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation. A classical (or non-quantum) algorithm is a finite sequence of instructions, or a step-by-step procedure for solving a problem, where each step or instruction can be performed on a classical computer. Similarly, a quantum algorithm is a step-by-step procedure, where each of the steps can be performed on a quantum computer. Although all classical algorithms can also be performed on a quantum computer, the term quantum algorithm is usually used for those algorithms which seem inherently quantum, or use some essential feature of quantum computation such as quantum superposition or quantum entanglement.\nAll problems which can be solved on a quantum computer can be solved on a classical computer. In particular, problems which are undecidable using classical computers remain undecidable using quantum computers. What makes quantum algorithms interesting is that they might be able to solve some problems faster than classical algorithms.\nThe most well known algorithms are Shor's algorithm for factoring, and Grover's algorithm for searching an unstructured database or an unordered list. Shor's algorithms runs exponentially faster than the best known classical algorithm for factoring, the general number field sieve. Grover's algorithm runs quadratically faster than the best possible classical algorithm for the same task.", "links": ["Abelian group", "Adiabatic quantum computation", "Algorithm", "Algorithmic cooling", "Algorithmica", "Amplitude amplification", "Andris Ambainis", "ArXiv", "Association for Computing Machinery", "BQP", "Bibcode", "Black-box", "Black box group", "Boson", "Cambridge University Press", "Cavity quantum electrodynamics", "Charge qubit", "Chern-Simons", "Circuit quantum electrodynamics", "Classical capacity", "Clique (graph theory)", "Cluster state", "Communications in Mathematical Physics", "Commutativity", "Computer", "Cris Moore", "Deutsch\u2013Jozsa algorithm", "Digital object identifier", "Dihedral group", "Discrete Fourier transform", "Discrete logarithm", "EQP (complexity)", "Element distinctness problem", "Entanglement-assisted classical capacity", "Entanglement-assisted stabilizer formalism", "Entanglement distillation", "Exponential sum", "Flux qubit", "Fock state", "Gauss sum", "General number field sieve", "Graph isomorphism", "Grover's algorithm", "HOMFLY", "Hadamard transform", "Hamiltonian oracle model", "Hidden subgroup problem", "IEEE", "Igor Pak", "Integer factorization", "International Journal of Theoretical Physics", "International Standard Book Number", "Jones polynomial", "Kane quantum computer", "LMS Journal of Computation and Mathematics", "LOCC", "Lattice problems", "Linear optical quantum computing", "Loss\u2013DiVincenzo quantum computer", "Measurement", "NP-complete", "Nitrogen-vacancy center", "Nuclear magnetic resonance quantum computer", "One-way quantum computer", "Optical lattice", "Oracle machine", "P (complexity)", "Pell's equation", "Phase kick-back", "Phase qubit", "Physical Review A", "Physical Review Letters", "PostBQP", "Primality test", "Principal ideal", "Probability distribution", "Proceedings of the National Academy of Sciences of the United States of America", "PubMed Central", "PubMed Identifier", "QMA", "Quantum Fourier transform", "Quantum Information and Computation", "Quantum Turing machine", "Quantum annealing", "Quantum capacity", "Quantum channel", "Quantum circuit", "Quantum complexity theory", "Quantum computation", "Quantum computer", "Quantum computing", "Quantum convolutional code", "Quantum cryptography", "Quantum decoherence", "Quantum energy teleportation", "Quantum entanglement", "Quantum error correction", "Quantum gate", "Quantum information", "Quantum information science", "Quantum invariant", "Quantum key distribution", "Quantum network", "Quantum optics", "Quantum phase estimation algorithm", "Quantum programming", "Quantum sort", "Quantum superposition", "Quantum teleportation", "Quantum walk", "Qubit", "Random walk", "Reviews of Modern Physics", "Ring (mathematics)", "SIAM Journal on Computing", "SIAM Journal on Scientific and Statistical Computing", "Shor's Algorithm", "Shor's algorithm", "Simon's algorithm", "Simon's problem", "Spin (physics)", "Springer-Verlag", "Stabilizer code", "Superconducting quantum computing", "Superdense coding", "Symmetric group", "Symposium on Foundations of Computer Science", "Theory of Computing", "Timeline of quantum computing", "Topological quantum computer", "Topological quantum field theory", "Trapped ion quantum computer", "Triangle finding problem", "Turaev-Viro invariant", "Ultracold atom", "Undecidable problem", "Unitarity", "Universal quantum simulator", "Yaoyun Shi"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from December 2014", "Quantum algorithms", "Quantum computing", "Quantum information science", "Theoretical computer science", "Use dmy dates from September 2011"], "title": "Quantum algorithm"}
{"summary": "Amplitude amplification is a technique in quantum computing which generalizes the idea behind the Grover's search algorithm, and gives rise to a family of quantum algorithms. It was discovered by Gilles Brassard and Peter H\u00f8yer in 1997,  and independently rediscovered by Lov Grover in 1998. \nIn a quantum computer, amplitude amplification can be used to obtain a quadratic speedup over several classical algorithms.", "links": ["Adiabatic quantum computation", "Algorithmic cooling", "ArXiv", "BQP", "Bibcode", "Cavity quantum electrodynamics", "Charge qubit", "Circuit quantum electrodynamics", "Classical capacity", "Cluster state", "Deutsch\u2013Jozsa algorithm", "Digital object identifier", "EQP (complexity)", "Entanglement-assisted classical capacity", "Entanglement-assisted stabilizer formalism", "Entanglement distillation", "Flux qubit", "Gilles Brassard", "Grover's algorithm", "Hermitian", "Hilbert space", "Kane quantum computer", "LOCC", "Linear optical quantum computing", "Loss\u2013DiVincenzo quantum computer", "Lov K. Grover", "Mathematical formulation of quantum mechanics", "Nitrogen-vacancy center", "Nuclear magnetic resonance quantum computer", "One-way quantum computer", "Optical lattice", "Oracle machine", "Peter H\u00f8yer", "Phase qubit", "PostBQP", "Projection operator", "QMA", "Quantum Fourier transform", "Quantum Turing machine", "Quantum algorithm", "Quantum annealing", "Quantum capacity", "Quantum channel", "Quantum circuit", "Quantum complexity theory", "Quantum computer", "Quantum computing", "Quantum convolutional code", "Quantum cryptography", "Quantum decoherence", "Quantum energy teleportation", "Quantum error correction", "Quantum gate", "Quantum information", "Quantum information science", "Quantum key distribution", "Quantum network", "Quantum optics", "Quantum phase estimation algorithm", "Quantum programming", "Quantum teleportation", "Qubit", "Shor's algorithm", "Simon's problem", "Spin (physics)", "Stabilizer code", "Superconducting quantum computing", "Superdense coding", "Timeline of quantum computing", "Topological quantum computer", "Trapped ion quantum computer", "Ultracold atom", "Universal quantum simulator"], "categories": ["Quantum algorithms", "Search algorithms"], "title": "Amplitude amplification"}
{"summary": "The BHT algorithm is a quantum algorithm that solves the collision problem. In this problem, one is given n and an r-to-1 function  and needs to find two inputs that f maps to the same output. The BHT algorithm only makes  queries to f, which matches the lower bound of  in the black box model.\nThe algorithm was discovered by Brassard, Hoyer, and Tapp in 1997. It uses Grover's algorithm, which was discovered in the previous year.", "links": ["ArXiv", "Birthday paradox", "Black box", "Collision problem", "Digital object identifier", "Element distinctness problem", "Grover's algorithm", "Quantum algorithm", "Theory of Computing"], "categories": ["Quantum algorithms"], "title": "BHT algorithm"}
{"summary": "The Deutsch\u2013Jozsa algorithm is a quantum algorithm, proposed by David Deutsch and Richard Jozsa in 1992 with improvements by Richard Cleve, Artur Ekert, Chiara Macchiavello, and Michele Mosca in 1998. Although of little practical use, it is one of the first examples of a quantum algorithm that is exponentially faster than any possible deterministic classical algorithm. It is also a deterministic algorithm, meaning that it always produces an answer, and that answer is always correct.", "links": ["Adiabatic quantum computation", "Algorithmic cooling", "ArXiv", "Artur Ekert", "BPP (complexity)", "BQP", "Bibcode", "Cavity quantum electrodynamics", "Charge qubit", "Circuit quantum electrodynamics", "Classical capacity", "Cluster state", "Constant function", "Constructive interference", "Controlled NOT gate", "David Deutsch", "Destructive interference", "Deterministic", "Deterministic algorithm", "Digital object identifier", "EQP (complexity)", "Entanglement-assisted classical capacity", "Entanglement-assisted stabilizer formalism", "Entanglement distillation", "Flux qubit", "Function domain", "Grover's algorithm", "Hadamard transform", "Hadamard transformation", "Kane quantum computer", "LOCC", "Linear optical quantum computing", "Loss\u2013DiVincenzo quantum computer", "Michele Mosca", "Nitrogen-vacancy center", "Nuclear magnetic resonance quantum computer", "One-way quantum computer", "Optical lattice", "Oracle machine", "Phase qubit", "PostBQP", "Promise problem", "QMA", "Quantum Fourier transform", "Quantum Turing machine", "Quantum algorithm", "Quantum annealing", "Quantum capacity", "Quantum channel", "Quantum circuit", "Quantum complexity theory", "Quantum computer", "Quantum convolutional code", "Quantum cryptography", "Quantum decoherence", "Quantum energy teleportation", "Quantum error correction", "Quantum gate", "Quantum information", "Quantum information science", "Quantum key distribution", "Quantum network", "Quantum optics", "Quantum phase estimation algorithm", "Quantum programming", "Quantum teleportation", "Qubit", "Randomized algorithm", "Richard Cleve", "Richard Jozsa", "Shor's algorithm", "Simon's problem", "Spin (physics)", "Stabilizer code", "Superconducting quantum computing", "Superdense coding", "Symposium on Foundations of Computer Science", "Symposium on Theory of Computing", "Timeline of quantum computing", "Topological quantum computer", "Trapped ion quantum computer", "Ultracold atom", "Universal quantum simulator", "XOR gate"], "categories": ["Quantum algorithms"], "title": "Deutsch\u2013Jozsa algorithm"}
{"summary": "Grover's algorithm is a quantum algorithm that finds with high probability the unique input to a black box function that produces a particular output value, using just O(N1/2) evaluations of the function, where N is the size of the function's domain.\nThe analogous problem in classical computation cannot be solved in fewer than O(N) evaluations (because, in the worst case, the correct input might be the last one that is tried). At roughly the same time that Grover published his algorithm, Bennett, Bernstein, Brassard, and Vazirani published a proof that no quantum solution to the problem can evaluate the function fewer than O(N1/2) times, so Grover's algorithm is asymptotically optimal.\nUnlike other quantum algorithms, which may provide exponential speedup over their classical counterparts, Grover's algorithm provides only a quadratic speedup. However, even quadratic speedup is considerable when N is large. Grover's algorithm could brute force a 128-bit symmetric cryptographic key in roughly 264 iterations, or a 256-bit key in roughly 2128 iterations. As a result, it is sometimes suggested that symmetric key lengths be doubled to protect against future quantum attacks.\nLike many quantum algorithms, Grover's algorithm is probabilistic in the sense that it gives the correct answer with a probability of less than 1. Though there is technically no upper bound on the number of repetitions that might be needed before the correct answer is obtained, the expected number of repetitions is a constant factor that does not grow with N.\nGrover's original paper described the algorithm as a database search algorithm, and this description is still common. The database in this analogy is a table of all of the function's outputs, indexed by the corresponding input.", "links": ["Adiabatic quantum computation", "Algorithmic cooling", "Amplitude amplification", "ArXiv", "BQP", "Bibcode", "Black box", "Brute-force attack", "Cavity quantum electrodynamics", "Charge qubit", "Circuit quantum electrodynamics", "Classical capacity", "Cluster state", "Collision problem", "Deutsch\u2013Jozsa algorithm", "Digital object identifier", "Domain of a function", "EQP (complexity)", "Entanglement-assisted classical capacity", "Entanglement-assisted stabilizer formalism", "Entanglement distillation", "Flux qubit", "Jordan form", "Kane quantum computer", "LOCC", "Linear optical quantum computing", "Loss\u2013DiVincenzo quantum computer", "Mathematical formulation of quantum mechanics", "Mean", "Measurement in quantum mechanics", "Median", "NP (complexity class)", "Nitrogen-vacancy center", "Nuclear magnetic resonance quantum computer", "One-way quantum computer", "Optical lattice", "Phase qubit", "PostBQP", "Probability", "QMA", "Quantum Fourier transform", "Quantum Turing machine", "Quantum algorithm", "Quantum annealing", "Quantum capacity", "Quantum channel", "Quantum circuit", "Quantum complexity theory", "Quantum computer", "Quantum convolutional code", "Quantum cryptography", "Quantum decoherence", "Quantum energy teleportation", "Quantum error correction", "Quantum gate", "Quantum information", "Quantum information science", "Quantum key distribution", "Quantum network", "Quantum optics", "Quantum phase estimation algorithm", "Quantum programming", "Quantum teleportation", "Qubit", "SIAM Journal on Computing", "Shor's algorithm", "Simon's problem", "Spin (physics)", "Stabilizer code", "Subroutine", "Superconducting quantum computing", "Superdense coding", "Timeline of quantum computing", "Topological quantum computer", "Trapped ion quantum computer", "Ultracold atom", "Unitary operator", "Universal quantum simulator", "Vladimir Korepin"], "categories": ["Quantum algorithms", "Search algorithms"], "title": "Grover's algorithm"}
{"summary": "The hidden subgroup problem (HSP) is a topic of research in mathematics and theoretical computer science. The framework captures problems like factoring, graph isomorphism, and the shortest vector problem. This makes it especially important in the theory of quantum computing because Shor's quantum algorithm for factoring is essentially equivalent to the hidden subgroup problem for finite Abelian groups, while the other problems correspond to finite groups that are not Abelian.", "links": ["Abelian group", "ArXiv", "Coset", "Dihedral group", "Discrete logarithm", "Graph isomorphism problem", "Group (mathematics)", "Group homomorphism", "Integer factorization", "Kernel (algebra)", "Leonard Schulman", "Mathematics", "Non-abelian group", "Oracle machine", "Polynomial time", "Quantum Fourier transform", "Quantum algorithms", "Quantum computer", "Shor's algorithm", "Shortest vector problem", "Subgroup", "Symmetric group", "Theoretical computer science"], "categories": ["Group theory", "Quantum algorithms"], "title": "Hidden subgroup problem"}
{"summary": "In quantum computing, the quantum Fourier transform is a linear transformation on quantum bits, and is the quantum analogue of the discrete Fourier transform. The quantum Fourier transform is a part of many quantum algorithms, notably Shor's algorithm for factoring and computing the discrete logarithm, the quantum phase estimation algorithm for estimating the eigenvalues of a unitary operator, and algorithms for the hidden subgroup problem.\nThe quantum Fourier transform can be performed efficiently on a quantum computer, with a particular decomposition into a product of simpler unitary matrices. Using a simple decomposition, the discrete Fourier transform on  amplitudes can be implemented as a quantum circuit consisting of only  Hadamard gates and controlled phase shift gates, where  is the number of qubits. This can be compared with the classical discrete Fourier transform, which takes  gates (where  is the number of bits), which is exponentially more than . However, the quantum Fourier transform acts on a quantum state, whereas the classical Fourier transform acts on a vector, so not every task that uses the classical Fourier transform can take advantage of this exponential speedup.\nThe best quantum Fourier transform algorithms known today require only  gates to achieve an efficient approximation.", "links": ["Adiabatic quantum computation", "Algorithmic cooling", "BQP", "Cavity quantum electrodynamics", "Charge qubit", "Circuit quantum electrodynamics", "Classical capacity", "Cluster state", "Deutsch\u2013Jozsa algorithm", "Discrete Fourier transform", "Discrete logarithm", "EQP (complexity)", "Eigenvalue", "Entanglement-assisted classical capacity", "Entanglement-assisted stabilizer formalism", "Entanglement distillation", "Flux qubit", "Grover's algorithm", "Hadamard gate", "Hermitian adjoint", "Hidden subgroup problem", "International Standard Book Number", "John Preskill", "K. R. Parthasarathy (probabilist)", "Kane quantum computer", "LOCC", "Linear optical quantum computing", "Linear transformation", "Loss\u2013DiVincenzo quantum computer", "Matrix multiplication", "Michael Nielsen", "Nitrogen-vacancy center", "Norm (mathematics)", "Nuclear magnetic resonance quantum computer", "OCLC", "One-way quantum computer", "Optical lattice", "Phase qubit", "Phase shift gate", "PostBQP", "QMA", "Quantum Turing machine", "Quantum algorithm", "Quantum algorithms", "Quantum annealing", "Quantum capacity", "Quantum channel", "Quantum circuit", "Quantum complexity theory", "Quantum computer", "Quantum computing", "Quantum convolutional code", "Quantum cryptography", "Quantum decoherence", "Quantum energy teleportation", "Quantum error correction", "Quantum gate", "Quantum information", "Quantum information science", "Quantum key distribution", "Quantum network", "Quantum optics", "Quantum phase estimation algorithm", "Quantum programming", "Quantum teleportation", "Qubit", "Root of unity", "Shor's algorithm", "Simon's problem", "Spin (physics)", "Stabilizer code", "Superconducting quantum computing", "Superdense coding", "Timeline of quantum computing", "Topological quantum computer", "Trapped ion quantum computer", "Ultracold atom", "Unitary matrix", "Unitary operator", "Unitary transformation", "Universal quantum simulator", "Vector (mathematics and physics)"], "categories": ["Quantum algorithms", "Transforms"], "title": "Quantum Fourier transform"}
{"summary": "In quantum computing, the quantum phase estimation algorithm is a quantum algorithm that finds many applications as a subroutine in other algorithms. The quantum phase estimation algorithm allows one to estimate the eigenphase of an eigenvector of a unitary gate, given access to a quantum state proportional to the eigenvector and a procedure to implement the unitary conditionally.", "links": ["Adiabatic quantum computation", "Algorithmic cooling", "BQP", "Born probabilities", "Cavity quantum electrodynamics", "Charge qubit", "Circuit quantum electrodynamics", "Classical capacity", "Cluster state", "Deutsch\u2013Jozsa algorithm", "EQP (complexity)", "Entanglement-assisted classical capacity", "Entanglement-assisted stabilizer formalism", "Entanglement distillation", "Flux qubit", "Grover's algorithm", "Kane quantum computer", "LOCC", "Linear optical quantum computing", "Loss\u2013DiVincenzo quantum computer", "Nitrogen-vacancy center", "Nuclear magnetic resonance quantum computer", "One-way quantum computer", "Optical lattice", "Phase qubit", "PostBQP", "QMA", "Quantum Fourier transform", "Quantum Turing machine", "Quantum algorithm", "Quantum annealing", "Quantum capacity", "Quantum channel", "Quantum circuit", "Quantum complexity theory", "Quantum computer", "Quantum computing", "Quantum convolutional code", "Quantum cryptography", "Quantum decoherence", "Quantum energy teleportation", "Quantum error correction", "Quantum gate", "Quantum information", "Quantum information science", "Quantum key distribution", "Quantum network", "Quantum optics", "Quantum programming", "Quantum teleportation", "Qubit", "Shor's algorithm", "Simon's problem", "Spectrum", "Spin (physics)", "Stabilizer code", "Superconducting quantum computing", "Superdense coding", "Timeline of quantum computing", "Topological quantum computer", "Trapped ion quantum computer", "Ultracold atom", "Unitary operator", "Universal quantum simulator"], "categories": ["All Wikipedia articles needing context", "All articles needing expert attention", "All pages needing cleanup", "Articles needing expert attention from September 2010", "Articles needing expert attention with no reason or talk parameter", "Articles needing unspecified expert attention", "Quantum algorithms", "Wikipedia articles needing context from September 2010", "Wikipedia introduction cleanup from September 2010"], "title": "Quantum phase estimation algorithm"}
{"summary": "In computational complexity theory and quantum computing, Simon's problem is a computational problem in the model of decision tree complexity or query complexity, conceived by Daniel Simon in 1994. Simon exhibited a quantum algorithm, usually called Simon's algorithm, that solves the problem exponentially faster than any (deterministic or probabilistic) classical algorithm.\nSimon's algorithm uses  queries to the black box, whereas the best classical probabilistic algorithm necessarily needs at least  queries. It is also known that Simon's algorithm is optimal in the sense that any quantum algorithm to solve this problem requires  queries. This problem yields an oracle separation between BPP and BQP, unlike the separation provided by the Deutsch-Jozsa algorithm, which separates P and EQP.\nAlthough the problem itself is of little practical value it is interesting because it provides an exponential speedup over any classical algorithm. Moreover, it was also the inspiration for Shor's algorithm. Both problems are special cases of the abelian hidden subgroup problem, which is now known to have efficient quantum algorithms.", "links": ["Adiabatic quantum computation", "Algorithmic cooling", "ArXiv", "BPP (complexity)", "BQP", "Cavity quantum electrodynamics", "Charge qubit", "Circuit quantum electrodynamics", "Classical capacity", "Cluster state", "Computational complexity theory", "Coset", "Decision tree complexity", "Destructive interference", "Deutsch-Jozsa algorithm", "Deutsch\u2013Jozsa algorithm", "Digital object identifier", "EQP (complexity)", "Entanglement-assisted classical capacity", "Entanglement-assisted stabilizer formalism", "Entanglement distillation", "Flux qubit", "Grover's algorithm", "Hadamard transform", "Hidden subgroup problem", "Hilbert space", "Kane quantum computer", "LOCC", "Linear optical quantum computing", "Loss\u2013DiVincenzo quantum computer", "Nitrogen-vacancy center", "Nuclear magnetic resonance quantum computer", "One-way quantum computer", "Optical lattice", "P (complexity)", "Phase qubit", "PostBQP", "Probabilistic algorithms", "QMA", "Quantum Fourier transform", "Quantum Turing machine", "Quantum algorithm", "Quantum annealing", "Quantum capacity", "Quantum channel", "Quantum circuit", "Quantum complexity theory", "Quantum computer", "Quantum computing", "Quantum convolutional code", "Quantum cryptography", "Quantum decoherence", "Quantum energy teleportation", "Quantum error correction", "Quantum gate", "Quantum information", "Quantum information science", "Quantum key distribution", "Quantum network", "Quantum optics", "Quantum phase estimation algorithm", "Quantum programming", "Quantum teleportation", "Qubit", "Shor's algorithm", "Spin (physics)", "Stabilizer code", "Superconducting quantum computing", "Superdense coding", "Timeline of quantum computing", "Topological quantum computer", "Trapped ion quantum computer", "Ultracold atom", "Universal quantum simulator", "XOR"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from May 2012", "Quantum algorithms"], "title": "Simon's problem"}
{"summary": "A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the \"average case\" over all possible choices of random bits. Formally, the algorithm's performance will be a random variable determined by the random bits; thus either the running time, or the output (or both) are random variables.\nOne has to distinguish between algorithms that use the random input to reduce the expected running time or memory usage, but always terminate with a correct result (Las Vegas algorithms) in a bounded amount of time, and probabilistic algorithms, which, depending on the random input, have a chance of producing an incorrect result (Monte Carlo algorithms) or fail to produce a result either by signalling a failure or failing to terminate.\nIn the second case, random performance and random output, the term \"algorithm\" for a procedure is somewhat questionable. In the case of random output, it is no longer formally effective. However, in some cases, probabilistic algorithms are the only practical means of solving a problem.\nIn common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior.", "links": ["AKS primality test", "Adi Shamir", "Algorithm", "Analysis of algorithms", "Anne Condon", "Array data structure", "Arto Salomaa", "Atlantic City algorithm", "Attacker", "Big O notation", "Bloom filter", "Bounded-error probabilistic polynomial", "Charles E. Leiserson", "Chemical reaction network", "Christos Papadimitriou", "Clifford Stein", "Closest pair of points problem", "Communication complexity", "Competitive analysis (online algorithm)", "Complexity class", "Computational complexity theory", "Computational geometry", "Computer software", "Conditional probability", "Convex hull", "Cosmic radiation", "Count\u2013min sketch", "Cryptographically secure pseudo-random number generator", "Cryptography", "Cut (graph theory)", "Cyber-physical system", "Data structure", "David Harel", "Decision problem", "Delaunay triangulation", "Deterministic algorithm", "Digital object identifier", "Discrepancy theory", "Disperser", "Edge contraction", "Effective method", "Eli Upfal", "Embedded systems", "Erik Winfree", "Expander graph", "Fermat primality test", "Freivalds' algorithm", "Gerald J. Sussman", "Graph theory", "Hal Abelson", "IP (complexity)", "Imre B\u00e1r\u00e1ny", "International Standard Book Number", "Introduction to Algorithms", "Jon Kleinberg", "Journal of the ACM", "Karger\u2019s algorithm", "Las Vegas algorithm", "MIT Press", "Markov's inequality", "Matthew Cook", "Method of conditional probabilities", "Michael Mitzenmacher", "Michael O. Rabin", "Miller-Rabin primality test", "Monte Carlo algorithm", "Monte Carlo method", "NP (complexity)", "PSPACE", "P (complexity)", "Pairwise independence", "Pessimistic estimator", "Polynomial time", "Primality test", "Prime number", "Primitive recursive", "Prisoner's dilemma", "Probabilistic Turing machine", "Probabilistic analysis of algorithms", "Pseudorandom", "Pseudorandom number generator", "Quantum computer", "Quicksort", "Quotient filter", "RP (complexity)", "Rajeev Motwani", "Random", "Random access machine", "Random binary tree", "Random tree (disambiguation)", "Random variable", "Randomized incremental construction", "Randomness", "Rapidly exploring random tree", "Robert M. Solovay", "Ronald L. Rivest", "Skip list", "Solovay-Strassen primality test", "Structure and Interpretation of Computer Programs", "Symposium on Theory of Computing", "Thomas H. Cormen", "Treap", "Turing machine", "Uniform distribution (discrete)", "Universal hashing", "Volker Strassen", "Worst-case complexity", "ZPP (complexity)", "Zolt\u00e1n F\u00fcredi", "\u00c9va Tardos"], "categories": ["Analysis of algorithms", "Probabilistic complexity theory", "Randomized algorithms", "Stochastic algorithms"], "title": "Randomized algorithm"}
{"summary": "The approximate counting algorithm allows the counting of a large number of events using a small amount of memory. Invented in 1977 by Robert Morris (cryptographer) of Bell Labs, it uses probabilistic techniques to increment the counter. It was fully analyzed in the early 1980s by Philippe Flajolet of INRIA Rocquencourt, who coined the name Approximate Counting, and strongly contributed to its recognition among the research community. The algorithm is considered one of the precursors of streaming algorithms, and the more general problem of determining the frequency moments of a data stream has been central to the field.", "links": ["Artificial intelligence", "Bell Labs", "Counter (digital)", "Data compression", "Exponent", "INRIA", "Order of magnitude", "Philippe Flajolet", "Powers of two", "Pseudo-random", "Randomized algorithm", "Robert Morris (cryptographer)", "Unbiased estimator"], "categories": ["Randomized algorithms"], "title": "Approximate counting algorithm"}
{"summary": "An Atlantic City algorithm is a probabilistic polynomial-time algorithm that answers correctly at least 75% of the time (or, in some versions, some other value greater than 50%). The term \"Atlantic City\" was first introduced in 1982 by J. Finn in an unpublished manuscript entitled Comparison of probabilistic tests for primality.\nTwo other common classes of probabilistic algorithms are Monte Carlo algorithms and Las Vegas algorithms. Monte Carlo algorithms are always fast, but only probably correct. On the other hand, Las Vegas algorithms are always correct, but only probably fast. The Atlantic City algorithms which are bounded probabilistic polynomial time algorithms are probably correct and probably fast.", "links": ["J. Finn", "Las Vegas algorithm", "Monte Carlo algorithm", "Probabilistic algorithm", "Randomized algorithm"], "categories": ["Cryptography", "Randomized algorithms"], "title": "Atlantic City algorithm"}
{"summary": "In mathematics and theoretical computer science, entropy compression is an information theoretic method for proving that a random process terminates, originally used by Robin Moser to prove an algorithmic version of the Lov\u00e1sz local lemma.", "links": ["Acyclic coloring", "Algorithmic Lov\u00e1sz local lemma", "ArXiv", "Boolean expression", "Boolean satisfiability problem", "Conjunctive normal form", "Degree (graph theory)", "Digital object identifier", "Edge coloring", "Existence theorem", "G\u00e1bor Tardos", "Infinite recursion", "Information theory", "Journal of the ACM", "Kolmogorov complexity", "Lance Fortnow", "Lov\u00e1sz local lemma", "Mathematical Reviews", "Natural logarithm", "Parameterized complexity", "Random process", "Richard J. Lipton", "Terence Tao"], "categories": ["Analysis of algorithms", "Probabilistic complexity theory", "Randomized algorithms"], "title": "Entropy compression"}
{"summary": "A randomized algorithm for computing the minimum spanning forest of a weighted graph with no isolated vertices. It was developed by David Karger, Philip Klein, and Robert Tarjan. The algorithm relies on techniques from Bor\u016fvka's algorithm along with an algorithm for verifying a minimum spanning tree in linear time. It combines the design paradigms of divide and conquer algorithms, greedy algorithms, and randomized algorithms to achieve expected linear performance.\nDeterministic algorithms that find the minimum spanning tree include Prim's algorithm, Kruskal's algorithm, reverse-delete algorithm, and Bor\u016fvka's algorithm.", "links": ["Binary tree", "Bor\u016fvka's algorithm", "David Karger", "Digital object identifier", "Divide and conquer algorithms", "Expected value", "Geometric series", "Glossary of graph theory", "Graph (mathematics)", "Greedy algorithms", "Isolated vertex", "Journal of the Association for Computing Machinery", "Kruskal's algorithm", "Linear time", "Linearity of expectation", "Mathematical Reviews", "Minimum spanning forest", "Minimum spanning tree", "Negative binomial distribution", "Prim's algorithm", "Randomized algorithm", "Randomized algorithms", "Recursion (computer science)", "Reverse-delete algorithm", "Robert Tarjan", "Valerie King", "Weighted graph"], "categories": ["Randomized algorithms", "Spanning tree"], "title": "Expected linear time MST algorithm"}
{"summary": "In computing, a Las Vegas algorithm is a randomized algorithm that always gives correct results; that is, it always produces the correct result or it informs about the failure. In other words, a Las Vegas algorithm does not gamble with the correctness of the result; it gambles only with the resources used for the computation. A simple example is randomized quicksort, where the pivot is chosen randomly, but the result is always sorted. The usual definition of a Las Vegas algorithm includes the restriction that the expected run time always be finite, when the expectation is carried out over the space of random information, or entropy, used in the algorithm. An alternative definition requires that a Las Vegas algorithm always terminate (be effective), but it may output a symbol not part of the solution space to indicate failure in finding a solution.\nLas Vegas algorithms were introduced by L\u00e1szl\u00f3 Babai in 1979, in the context of the graph isomorphism problem, as a stronger version of Monte Carlo algorithms. Las Vegas algorithms can be used in situations where the number of possible solutions is relatively limited, and where verifying the correctness of a candidate solution is relatively easy while actually calculating the solution is complex.\nThe name refers to the city of Las Vegas, Nevada, which is well known within the United States as an icon of gambling.", "links": ["Atlantic City algorithm", "Complexity class", "Computing", "Correctness (computer science)", "Decision problem", "Effective method", "Expected value", "Graph isomorphism problem", "International Standard Book Number", "Las Vegas, Nevada", "Leonid Levin", "L\u00e1szl\u00f3 Babai", "Markov's inequality", "Monte Carlo algorithm", "National Institute of Standards and Technology", "Partial function", "Quicksort", "RP (complexity)", "Randomized algorithm", "Randomness", "Zero-error Probabilistic Polynomial time"], "categories": ["Randomized algorithms"], "title": "Las Vegas algorithm"}
{"summary": "In computing, a Monte Carlo algorithm is a randomized algorithm whose running time is deterministic, but whose output may be incorrect with a certain (typically small) probability.\nThe related class of Las Vegas algorithms are also randomized, but in a different way: they take an amount of time that varies randomly, but always produce the correct answer. A Monte Carlo algorithm can be converted into a Las Vegas algorithm whenever there exists a procedure to verify that the output produced by the algorithm is indeed correct. If so, then the resulting Las Vegas algorithm is merely to repeatedly run the Monte Carlo algorithm until one of the runs produces an output that can be verified to be correct.\nThe name refers to the grand casino in the Principality of Monaco at Monte Carlo, which is well-known around the world as an icon of gambling.", "links": ["Atlantic City algorithm", "Baillie-PSW primality test", "Boston", "Bounded-error probabilistic polynomial", "Charles E. Leiserson", "Clifford Stein", "Complexity class", "Computational group theory", "Computational statistics", "Computing", "Decision problem", "Deterministic algorithm", "International Standard Book Number", "Introduction to Algorithms", "Las Vegas algorithm", "Majority function", "Miller\u2013Rabin primality test", "Monte Carlo", "Monte Carlo Casino", "Monte Carlo method", "New York", "PP (complexity)", "Prabhakar Raghavan", "Prime number", "Probability", "RP (complexity)", "Rajeev Motwani", "Randomized algorithm", "Ronald Rivest", "Schreier\u2013Sims algorithm", "Solovay\u2013Strassen primality test", "Thomas H. Cormen", "ZPP (complexity)"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from August 2011", "Randomized algorithms"], "title": "Monte Carlo algorithm"}
{"summary": "Principle of Deferred Decisions is a technique used in analysis of randomized algorithms.", "links": ["Algorithm", "Data structure", "Eli Upfal", "Randomized algorithm"], "categories": ["Algorithms and data structures stubs", "All articles covered by WikiProject Wikify", "All articles with too few wikilinks", "All orphaned articles", "All stub articles", "Articles covered by WikiProject Wikify from December 2013", "Articles with too few wikilinks from December 2013", "Computer science stubs", "Orphaned articles from January 2009", "Randomized algorithms"], "title": "Principle of deferred decision"}
{"summary": "A rapidly exploring random tree (RRT) is an algorithm designed to efficiently search nonconvex, high-dimensional spaces by randomly building a space-filling tree. The tree is constructed incrementally from samples drawn randomly from the search space and is inherently biased to grow towards large unsearched areas of the problem. RRTs were developed by Steven M. LaValle and James J. Kuffner Jr.  . They easily handle problems with obstacles and differential constraints (nonholonomic and kinodynamic) and have been widely used in autonomous robotic path planning.\nRRTs can be viewed as a technique to generate open-loop trajectories for nonlinear systems with state constraints. An RRT can also be considered as a Monte-Carlo method to bias search into the largest Voronoi regions of a graph in a configuration space. Some variations can even be considered stochastic fractals.", "links": ["Algorithm", "Autonomous", "Bloom filter", "Configuration space", "Convex space", "Count\u2013min sketch", "Data structure", "Degrees of freedom (engineering)", "Digital object identifier", "Fractal", "James J. Kuffner Jr.", "Monte Carlo method", "Motion planning", "Path planning", "Probabilistic roadmap", "Pseudocode", "Quotient filter", "Random binary tree", "Random tree (disambiguation)", "Randomized algorithm", "Robotics", "Skip list", "Space-filling tree", "Steven M. LaValle", "Stochastic", "Treap", "Voronoi diagram"], "categories": ["Probabilistic data structures", "Robot control", "Search algorithms"], "title": "Rapidly exploring random tree"}
{"summary": "Estimation of distribution algorithms (EDAs), sometimes called probabilistic model-building genetic algorithms (PMBGAs), are stochastic optimization methods that guide the search for the optimum by building and sampling explicit probabilistic models of promising candidate solutions. Optimization is viewed as a series of incremental updates of a probabilistic model, starting with the model encoding the uniform distribution over admissible solutions and ending with the model that generates only the global optima.\nEDAs belong to the class of evolutionary algorithms. The main difference between EDAs and most conventional evolutionary algorithms is that evolutionary algorithms generate new candidate solutions using an implicit distribution defined by one or more variation operators, whereas EDAs use an explicit probability distribution encoded by a Bayesian network, a multivariate normal distribution, or another model class. Similarly as other evolutionary algorithms, EDAs can be used to solve optimization problems defined over a number of representations from vectors to LISP style S expressions, and the quality of candidate solutions is often evaluated using one or more objective functions.\nThe general procedure of an EDA is outlined in the following:\nt = 0\ninitialize model M(0) to represent uniform distribution over admissible solutions\nwhile (termination criteria not met)\n\nP = generate N>0 candidate solutions by sampling M(t)\nF = evaluate all candidate solutions in P\nM(t+1) = adjust_model(P,F,M(t))\nt = t + 1\n\nUsing explicit probabilistic models in optimization allowed EDAs to feasibly solve optimization problems that were notoriously difficult for most conventional evolutionary algorithms and traditional optimization techniques, such as problems with high levels of epistasis. Nonetheless, the advantage of EDAs is also that these algorithms provide an optimization practitioner with a series of probabilistic models that reveal a lot of information about the problem being solved. This information can in turn be used to design problem-speci\ufb01c neighborhood operators for local search, to bias future runs of EDAs on a similar problem, or to create an efficient computational model of the problem.\nFor example, if the population is represented by bit strings of length 4, the EDA can represent the population of promising solution using a single vector of four probabilities (p1, p2, p3, p4) where each component of p defines the probability of that position being a 1. Using this probability vector it is possible to create an arbitrary number of candidate solutions.\nBetter-known EDAs include\nPopulation-based incremental learning (PBIL)\nHill Climbing with Learning (HCwL)\nCompact Genetic Algorithm (cGA)\nUnivariate Marginal Distribution Algorithm (UMDA)\nEstimation of Multivariate Normal Algorithm (EMNA)\nMutual Information Maximization for Input Clustering (MIMIC)\nBivariate Marginal Distribution Algorithm (BMDA)\nExtended Compact Genetic Algorithm (ECGA)\nBayesian Optimization Algorithm (BOA)\nEstimation of Bayesian Networks Algorithm (EBNA)\nStochastic hill climbing with learning by vectors of normal distributions (SHCLVND)\nReal-coded PBIL\nProbabilistic Incremental Program Evolution (PIPE)\nEstimation of Gaussian Networks Algorithm (EGNA)", "links": ["Bayesian Optimization Algorithm", "Bayesian network", "Bivariate Marginal Distribution Algorithm", "Compact Genetic Algorithm", "Density estimation", "Epistasis", "Estimation of Bayesian Networks Algorithm", "Estimation of Gaussian Networks Algorithm", "Estimation of Multivariate Normal Algorithm", "Evolutionary algorithms", "Evolutionary computation", "Extended Compact Genetic Algorithm", "Hill Climbing with Learning", "LISP", "Multivariate normal distribution", "Mutual Information Maximization for Input Clustering", "Population-based incremental learning", "Probabilistic Incremental Program Evolution", "Probability distribution", "Real-coded PBIL", "Stochastic hill climbing with learning by vectors of normal distributions", "Stochastic optimization", "Univariate Marginal Distribution Algorithm"], "categories": ["All Wikipedia articles needing context", "All articles covered by WikiProject Wikify", "All articles needing additional references", "All articles needing references cleanup", "All pages needing cleanup", "Articles covered by WikiProject Wikify from September 2009", "Articles needing additional references from January 2008", "Evolutionary computation", "Stochastic algorithms", "Wikipedia articles needing context from September 2009", "Wikipedia introduction cleanup from September 2009", "Wikipedia references cleanup from September 2009"], "title": "Estimation of distribution algorithm"}
{"summary": "Stochastic computing is a collection of techniques that represent continuous values by streams of random bits. Complex computations can then be computed by simple bit-wise operations on the streams.\nDespite the similarity in their names, stochastic computing is distinct from the study of randomized algorithms.", "links": ["Adder (electronics)", "Analog computer", "And gate", "Arithmetic logic unit", "Belief propagation", "Bernoulli process", "Computer", "Digital object identifier", "Edge detection", "Field-programmable gate array", "Gibbs sampling", "Image processing", "International Standard Book Number", "John von Neumann", "Least significant bit", "Logical and", "Low-density parity-check code", "Most significant bit", "Multiplication algorithm", "PRNG", "Randomized algorithm"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from October 2014", "History of computing hardware", "Models of computation", "Stochastic algorithms"], "title": "Stochastic computing"}
{"summary": "Stochastic diffusion search (SDS) was first described in 1989 as a population-based, pattern-matching algorithm [Bishop, 1989]. It belongs to a family of swarm intelligence and naturally inspired search and optimisation algorithms which includes ant colony optimization, particle swarm optimization and genetic algorithms. Unlike stigmergetic communication employed in ant colony optimization, which is based on modification of the physical properties of a simulated environment, SDS uses a form of direct (one-to-one) communication between the agents similar to the tandem calling mechanism employed by one species of ants, Leptothorax acervorum.\nIn SDS agents perform cheap, partial evaluations of a hypothesis (a candidate solution to the search problem). They then share information about hypotheses (diffusion of information) through direct one-to-one communication. As a result of the diffusion mechanism, high-quality solutions can be identified from clusters of agents with the same hypothesis. The operation of SDS is most easily understood by means of a simple analogy \u2013 The Restaurant Game.", "links": ["Ant colony optimization", "Genetic algorithm", "Particle swarm optimization", "Swarm intelligence"], "categories": ["Artificial intelligence", "Stochastic algorithms"], "title": "Stochastic diffusion search"}
{"summary": "In the analysis of algorithms, the master theorem provides a solution in asymptotic terms (using Big O notation) for recurrence relations of types that occur in the analysis of many divide and conquer algorithms. It was popularized by the canonical algorithms textbook Introduction to Algorithms by Cormen, Leiserson, Rivest, and Stein, in which it is both introduced and proved. Not all recurrence relations can be solved with the use of the master theorem; its generalizations include the Akra\u2013Bazzi method.", "links": ["Akra-Bazzi theorem", "Akra\u2013Bazzi method", "Analysis of algorithms", "Asymptotic", "Big O notation", "Binary search", "Charles E. Leiserson", "Clifford Stein", "Divide and conquer algorithm", "Introduction to Algorithms", "MacMahon Master theorem", "Merge Sort", "Michael T. Goodrich", "Ramanujan's master theorem", "Recurrence relation", "Recursive algorithm", "Roberto Tamassia", "Ron Rivest", "Ronald L. Rivest", "Thomas H. Cormen"], "categories": ["Analysis of algorithms", "Asymptotic analysis", "Recurrence relations", "Theorems in computational complexity theory"], "title": "Master theorem"}
{"summary": "In computer science, Atropos is a real-time scheduling algorithm developed at Cambridge University. It combines the earliest deadline first algorithm with a best effort scheduler to make use of slack time, while exercising strict admission control.", "links": ["Admission control", "Computer science", "Earliest deadline first scheduling", "Real-time computing", "Scheduling algorithm", "Slack time", "University of Cambridge"], "categories": ["All articles lacking sources", "All stub articles", "Articles lacking sources from December 2009", "Computer science stubs", "Real-time computing", "Scheduling algorithms"], "title": "Atropos scheduler"}
{"summary": "The critical path method (CPM) is an algorithm for scheduling a set of project activities.", "links": ["Algorithm", "Booz Allen Hamilton", "Critical chain", "Critical path drag", "Dependency (project management)", "DuPont", "Event chain methodology", "Float (project management)", "Gantt chart", "Harold Kerzner", "International Standard Book Number", "List of project management software", "List of project management topics", "Longest path", "Manhattan Project", "Milestone (project management)", "National Diet Library", "Program Evaluation and Review Technique", "Program evaluation and review technique (PERT)", "Project", "Project management", "Project planning", "Remington Rand", "Resource Leveling", "U.S. Navy", "Work breakdown structure"], "categories": ["All articles needing additional references", "Articles needing additional references from May 2009", "Business terms", "Management", "Network theory", "Operations research", "Production and manufacturing", "Project management", "Scheduling algorithms"], "title": "Critical path method"}
{"summary": "Definition: Dynamic scheduling is a method in which the hardware determines which instructions to execute, as opposed to a statically scheduled machine, in which the compiler determines the order of execution. In essence, the processor is executing instructions out of order\nA major driving force in the microprocessor industry is the never ending desire to miniaturize things. Smaller transistors require less voltage to operate and thus consume less power and produce less heat. Smaller interconnect distances also allow for faster clock speeds. Perhaps most important of all, smaller die areas lead to cheaper processors since more chips can fit in a single wafer. The first microprocessor made by Intel was the 4004, which had 2300 transistors. Today's chips, on the other hand, incorporate 5 to 20 million transistors. So what do they do with all those transistors?\nA major hog of real estate is, of course, caches. Caches, and any IC (integrated circuit) based memory device, must have many wires running to and from the read and write ports. For on-chip caches, the load/store unit in the CPU must be able to access every location in the cache from both the read and write ports. The situation is even worse when there are more than one load/store units. That's a lot of wires! In the Pentium Pro, for example, a single package includes both the CPU chip and a L2 cache chip; the CPU chip has about 5 million transistors, while the cache chip has about 15 million transistors.\nHowever, even accounting for caches, there is still a large increase in the number of transistors in today's chips compared to the 4004. Obviously, microprocessors are becoming increasingly more complex. We can understand this increasing complexity since chip designers want to create fast processors which are at the same time affordable. As process technology improved and more transistors could be fitted in the same die area, it became cost effective to add newer or improved features to the processor in an attempt to increase its effective speed. One of these improvements is dynamic scheduling.\nAs its name implies, is a method in which the hardware determines which instructions to execute, as opposed to a statically scheduled machine, in which the compiler determines the order of execution. In essence, the processor is executing instructions out of order. Dynamic scheduling is akin to a data flow machine, in which instructions don't execute based on the order in which they appear, but rather on the availability of the source operands. Of course, a real processor also has to take into account the limited amount of resources available. Thus instructions execute based on the availability of the source operands as well as the availability of the requested functional units.\nDynamically scheduled machines can take advantage of parallelism which would not be visible at compile time. They are also more versatile as code does not necessarily have to be recompiled to run efficiently since the hardware takes care of much of the scheduling. In a statically scheduled machine, code would have to be recompiled to take advantage of the machine's particular hardware. (All of this is assuming the machines use the same instruction set architecture. Of course, the code would have to be recompiled no matter what if the machines used different ISAs.)\nDynamic priority scheduling is a type of scheduling algorithm in which the priorities are calculated during the execution of the system. The goal of dynamic priority scheduling is to adapt to dynamically changing progress and form an optimal configuration in self-sustained manner. It can be very hard to produce well-defined policies to achieve the goal depending on the difficulty of a given problem.\nEarliest deadline first scheduling and Least slack time scheduling are examples of Dynamic priority scheduling algorithms.", "links": ["Computer science", "Earliest deadline first scheduling", "Least slack time scheduling", "Scheduling algorithm"], "categories": ["All articles lacking sources", "All articles needing expert attention", "All stub articles", "Articles lacking sources from August 2009", "Articles needing expert attention from February 2009", "Articles needing expert attention with no reason or talk parameter", "Computer science articles needing expert attention", "Computer science stubs", "Scheduling algorithms"], "title": "Dynamic priority scheduling"}
{"summary": "FIFO is an acronym for first in, first out, a method for organizing and manipulating a data buffer, where the oldest (first) entry, or 'head' of the queue, is processed first. It is analogous to processing a queue with first-come, first-served (FCFS) behaviour: where the people leave the queue in the order in which they arrive.\nFCFS is also the jargon term for the FIFO operating system scheduling algorithm, which gives every process central processing unit (CPU) time in the order in which it is demanded.\nFIFO's opposite is LIFO, last-in-first-out, where the youngest entry or 'top of the stack' is processed first.\nA priority queue is neither FIFO or LIFO but may adopt similar behaviour temporarily or by default.\nQueueing theory encompasses these methods for processing data structures, as well as interactions between strict-FIFO queues.", "links": ["Acronym", "C++", "Central processing unit", "Circular buffer", "Circular queue", "Computer networks", "Data structure", "Electronics", "FIFO (disambiguation)", "FINO", "First-come, first-served", "Garbage in, garbage out", "Gray code", "Integrated Authority File", "International Standard Book Number", "Interprocess communication", "Jargon", "LIFO (computing)", "Leaky bucket", "Linked list", "Linux", "List (abstract data type)", "Metastability", "Named pipe", "Network bridge", "Network router", "Network switch", "Operating system", "Pipes and filters", "Priority queue", "Queue (data structure)", "Queueing theory", "Scheduling (computing)", "Standard Template Library", "Static random access memory", "Xilinx"], "categories": ["All articles needing additional references", "Articles needing additional references from March 2015", "Cybernetics", "Inter-process communication", "Queue management", "Scheduling algorithms", "Wikipedia articles with GND identifiers"], "title": "FIFO (computing and electronics)"}
{"summary": "In computer science, FINO (Sometimes seen as \"FISH\", for First In, Still Here) is a humorous scheduling algorithm. It is an acronym for \"First In Never Out\" as opposed to traditional \"first in first out\" (FIFO) and \"last in first out\" (LIFO) algorithms.\nFINO works by withholding all scheduled tasks permanently. No matter how many tasks are scheduled at any time, no task ever actually takes place. This makes FINO extremely simple to implement, but useless in practice. A stateful FINO queue can be used to implement a memory leak.\nA mention of FINO appears in the Signetics 25120 write-only memory joke datasheet.", "links": ["/dev/null", "Acronym", "Bit bucket", "Black hole (networking)", "Computer science", "FIFO (computing and electronics)", "Fino", "LIFO (computing)", "Memory leak", "Null route", "Scheduling algorithm", "State (computer science)", "Wayback Machine", "Write-only memory (joke)"], "categories": ["All stub articles", "Computer humor", "Computer science stubs", "Scheduling algorithms"], "title": "FINO"}
{"summary": "Generalized processor sharing (GPS) is an ideal scheduling algorithm for network schedulers.\nIt is related to the Fair queuing principle, that groups the packets into classes, and share the service capacity between them. The GPS shares this capacity according to some fixed weights.\nIn processor scheduling, generalized processor sharing is \"an idealized scheduling algorithm that achieves perfect fairness. All practical schedulers approximate GPS and use it as a reference to measure fairness.\" Generalized processor sharing assumes that traffic is fluid (infinitesimal packet sizes), and can be arbitrarily split. There are several service disciplines which track the performance of GPS quite closely such as weighted fair queuing (WFQ), also known as packet-by-packet generalized processor sharing (PGPS).", "links": ["Deficit round robin", "Digital object identifier", "Fair queuing", "Fairness measure", "First-come, first-served", "IEEE/ACM Transactions on Networking", "Infinitesimal", "International Standard Book Number", "Low latency", "Network scheduler", "Processor sharing", "Robert G. Gallager", "Scheduling (computing)", "Scheduling algorithm", "Scott Shenker", "Statistical multiplexing", "Store and forward", "Videoconferencing", "Weighted fair queuing", "Weighted round robin"], "categories": ["Scheduling algorithms"], "title": "Generalized processor sharing"}
{"summary": "The Graphical Path Method (GPM) is a mathematically based algorithm used in project management for planning, scheduling and resource control. GPM represents logical relationships of dated objects \u2013 such as activities, milestones, and benchmarks \u2013 in a time-scaled network diagram.", "links": ["Activity (project management)", "Arrow Diagramming Method", "Construction management", "Critical Path Method", "Critical path method", "Dependency (project management)", "Float (project management)", "Gantt chart", "List of project management topics", "Milestone (project management)", "Precedence Diagram Method", "Project management", "Project management software", "Project planning", "Resource (project management)", "Scheduling (production processes)"], "categories": ["Critical Path Scheduling", "Project management", "Scheduling algorithms"], "title": "Graphical path method"}
{"summary": "Heterogeneous Earliest Finish Time (or HEFT) is a heuristic to schedule a set of dependent tasks onto a network of heterogeneous workers taking communication time into account. For inputs HEFT takes a set of tasks, represented as a directed acyclic graph, a set of workers, the times to execute each task on each worker, and the times to communicate the results from each job to each of its children between each pair of workers. It descends from list scheduling algorithms.", "links": ["Digital object identifier", "Directed acyclic graph", "List scheduling"], "categories": ["Pages containing cite templates with deprecated parameters", "Scheduling algorithms"], "title": "Heterogeneous Earliest Finish Time"}
{"summary": "Interval scheduling is a class of problems in computer science, particularly in the area of algorithm design. The problems consider a set of tasks. Each task is represented by an interval describing the time in which it needs to be executed. For instance, task A might run from 2:00 to 5:00, task B might run from 4:00 to 10:00 and task C might run from 9:00 to 11:00. A subset of intervals is compatible if no two intervals overlap. For example, the subset {A,C} is compatible, as is the subset {B}; but neither {A,B} nor {B,C} are compatible subsets, because the corresponding intervals within each subset overlap.\nThe interval scheduling maximization problem (ISMP) is to find a largest compatible set - a set of non-overlapping intervals of maximum size. The goal here is to execute as many tasks as possible.\nIn an upgraded version of the problem, the intervals are partitioned into groups. A subset of intervals is compatible if no two intervals overlap, and moreover, no two intervals belong to the same group (i.e. the subset contains at most a single representative interval of each group).\nThe group interval scheduling decision problem (GISDP) is to decide whether there exists a compatible set in which all groups are represented. The goal here is to execute a single representative task from each group. GISDPk is a restricted version of GISDP in which the number of intervals in each group is at most k.\nThe group interval scheduling maximization problem (GISMP) is to find a largest compatible set - a set of non-overlapping representatives of maximum size. The goal here is to execute a representative task from as many groups as possible. GISMPk is a restricted version of GISMP in which the number of intervals in each group is at most k. This problem is often called JISPk, where J stands for Job.\nGISMP is the most general problem; the other two problems can be seen as special cases of it:\nISMP is the special case in which each task belongs to its own group (i.e. it is equal to GISMP1).\nGISDP is the problem of deciding whether the maximum is exactly equal to the number of groups.", "links": ["2-SAT", "2-satisfiability", "Algorithm", "Boolean satisfiability problem", "Charging argument", "Computer science", "Digital object identifier", "Earliest deadline first scheduling", "Greedy algorithm", "Independent set (graph theory)", "International Standard Book Number", "Intersection graph", "Job stream", "Linear programming relaxation", "MaxSNP", "Maximum disjoint set", "NP-complete", "Scheduling (computing)"], "categories": ["Scheduling algorithms"], "title": "Interval scheduling"}
{"summary": "Least slack time (LST) scheduling is a scheduling algorithm. It assigns priority based on the slack time of a process. Slack time is the amount of time left after a job if the job was started now. This algorithm is also known as least laxity first. Its most common use is in embedded systems, especially those with multiple processors. It imposes the simple constraint that each process on each available processor possesses the same run time, and that individual processes do not have an affinity to a certain processor. This is what lends it a suitability to embedded systems.", "links": ["Earliest deadline first scheduling", "Rate-monotonic scheduling", "Scheduling algorithm"], "categories": ["All articles lacking sources", "Articles lacking sources from December 2009", "Scheduling algorithms"], "title": "Least slack time scheduling"}
{"summary": "The basic idea of list scheduling is to make an ordered list of processes by assigning them some priorities, and then repeatedly execute the following two steps until a valid schedule is obtained :\nSelect from the list, the process with the highest priority for scheduling.\nSelect a resource to accommodate this process.\nIf no resource can be found, we select the next process in the list.\nThe priorities are determined statically before scheduling process begins. The first step chooses the process with the highest priority, the second step selects the best possible resource. Some known list scheduling strategies are :\nHighest level first algorithm or HLF\nLongest path algorithm or LP\nLongest processing time\nCritical path method\nHeterogeneous Earliest Finish Time or HEFT. For the case heterogeneous workers.", "links": ["Critical path method", "Heterogeneous Earliest Finish Time", "Highest level first", "International Standard Book Number", "Longest path", "Longest processing time"], "categories": ["Pages using citations with accessdate and no URL", "Scheduling algorithms"], "title": "List scheduling"}
{"summary": "The modified due-date scheduling heuristic is used in scheduling tasks to resources (for example, to answer the question \"What order should we make sandwiches in, so as to please our customers?\").\nIt assumes that the objective of the scheduling process is to minimise the total amount of time spent on tasks after their due dates. This strategy is most relevant when completing all tasks carries a certainty that at least some of them will be completed late.\nThe modified due date forms the basis of an algorithm that attempts first to complete tasks early or on time, and second to complete tasks as soon as possible when the requested due date is unattainable: Given a list of tasks, with a range of due dates (dj), and a range of times it takes to complete the tasks (pj), then at any moment (t) you should do the task that has the smallest modified due date. The modified due date itself is the highest of either the due date, or the completion date if you started the task now:\nmddj = max( dj, t+pj ).\nThus, if the due date of the PB and J is 5 minutes from now, but its time to complete is 6 minutes, the modified due date of the PB and J is 6 minutes from now. Imagine a second customer is requesting that a Reuben be prepared in 7 minutes from now, but the Reuben takes 5 minutes to complete: in this case, the MDD of the Reuben is 7 minutes; the PB and J has a smaller MDD, and thus you should make the PB and J first, while the second customer waits patiently for the Reuben to be started. In this case, the PB and J is 1 minute late, and the Reuben is 4 minutes late. Both sandwiches are made late, but the total lateness is only 5 minutes. If you had made the Reuben first, the second customer may have gotten the sandwich early, but the first customer would have received their sandwich 6 minutes late.", "links": ["Computer science", "Heuristic", "Scheduling (computing)"], "categories": ["All Wikipedia articles needing context", "All articles lacking sources", "All pages needing cleanup", "All stub articles", "Articles lacking sources from October 2007", "Computer science stubs", "Scheduling algorithms", "Wikipedia articles needing context from October 2009", "Wikipedia introduction cleanup from October 2009"], "title": "Modified due-date scheduling heuristic"}
{"summary": "Multi-level queueing, used at least since the late 1950s/early 1960s, is a queue with a predefined number of levels. Unlike the multilevel feedback queue, items get assigned to a particular level at insert (using some predefined algorithm), and thus cannot be moved to another level. Items get removed from the queue by removing all items from a level, and then moving to the next. If an item is added to a level above, the \"fetching\" restarts from there. Each level of the queue is free to use its own scheduling, thus adding greater flexibility than merely having multiple levels in a queue.", "links": ["Computer science", "FIFO (computing and electronics)", "Fair-share scheduling", "International Standard Book Number", "Lottery scheduling", "Multilevel feedback queue", "Round-robin scheduling", "Scheduling (computing)"], "categories": ["All articles needing additional references", "All stub articles", "Articles needing additional references from September 2014", "Computer science stubs", "Scheduling algorithms"], "title": "Multilevel queue"}
{"summary": "In computer science, starvation is a problem encountered in concurrent computing where a process is perpetually denied necessary resources to process its work. Starvation may be caused by errors in a scheduling or mutual exclusion algorithm, but can also be caused by resource leaks, and can be intentionally caused via a denial-of-service attack such as a fork bomb.\nThe impossibility of starvation in a concurrent algorithm is called starvation-freedom, lockout-freedom or finite bypass, is an instance of liveness, and is one of the two requirements for any mutual exclusion algorithm (the other being correctness). The name \"finite bypass\" means that any process (concurrent part) of the algorithm is bypassed at most a finite number times before being allowed access to the shared resource.", "links": ["Aging (scheduling)", "Andrew Tanenbaum", "CPU time", "Computer process", "Computer science", "Concurrent computing", "Critical section", "Deadlock", "Denial-of-service attack", "Fork bomb", "International Standard Book Number", "Kernel (computer science)", "Liveness", "Maurice Herlihy", "Maximum throughput scheduling", "Michel Raynal", "Mutual exclusion", "Nir Shavit", "Priority inversion", "Resource (computer science)", "Resource leak", "Room synchronization", "Scheduling algorithm"], "categories": ["All articles needing additional references", "All stub articles", "Articles needing additional references from January 2011", "Computer science stubs", "Concurrency (computer science)", "Processor scheduling algorithms"], "title": "Starvation (computer science)"}
{"summary": "A sequence step algorithm (SQS-AL) is an algorithm implemented in a discrete event simulation system to maximize resource utilization. This is achieved by running through two main nested loops: A sequence step loop and a replication loop. For each sequence step, each replication loop is a simulation run that collects crew idle time for activities in that sequence step. The collected crew idle times are, then, used to determine resource arrival dates for user-specified confidence levels. The process of collecting the crew idle times and determining crew arrival times for activities on a considered sequence step is repeated from the first to the last sequence step.", "links": ["Algorithm", "Data structure", "Discrete event simulation", "Linear scheduling method"], "categories": ["Algorithms and data structures stubs", "All articles covered by WikiProject Wikify", "All articles needing expert attention", "All articles with too few wikilinks", "All orphaned articles", "All stub articles", "Articles covered by WikiProject Wikify from March 2013", "Articles needing expert attention from October 2009", "Articles needing expert attention with no reason or talk parameter", "Articles needing unspecified expert attention", "Articles with too few wikilinks from March 2013", "Computer science stubs", "Network theory", "Orphaned articles from February 2009", "Project management", "Scheduling algorithms"], "title": "Sequence step algorithm"}
{"summary": "Anticipatory scheduling is an algorithm for scheduling hard disk input/output (I/O scheduling). It seeks to increase the efficiency of disk utilization by \"anticipating\" future synchronous read operations.\n\"Deceptive idleness\" is a situation where a process appears to be finished reading from the disk when it is actually processing data in preparation of the next read operation. This will cause a normal work-conserving I/O scheduler to switch to servicing I/O from an unrelated process. This situation is detrimental to the throughput of synchronous reads, as it degenerates into a seeking workload. Anticipatory scheduling overcomes deceptive idleness by pausing for a short time (a few milliseconds) after a read operation in anticipation of another close-by read requests.\nAnticipatory scheduling yields significant improvements in disk utilization for some workloads. In some situations the Apache web server may achieve up to 71% more throughput from using anticipatory scheduling.\nThe Linux anticipatory scheduler may reduce performance on disks using Tagged Command Queuing (TCQ), high performance disks, and hardware RAID arrays. An anticipatory scheduler (AS) was the default Linux kernel scheduler between 2.6.0 and 2.6.18, by which time it was replaced by the CFQ scheduler.\nAs of kernel version 2.6.33, the Anticipatory scheduler (AS) has been removed from the Linux kernel. The reason being that while useful, the scheduler's effects could be achieved through tuned use of other schedulers (mostly CFQ, which can also be configured to idle with the slice_idle tunable). Since the anticipatory scheduler added maintenance overhead while not improving the workload coverage of the Linux kernel, it was deemed redundant.", "links": ["Algorithm", "Andrew Morton (computer programmer)", "Apache web server", "CFQ", "Deadline scheduler", "Hard disk", "I/O scheduling", "Input/output", "Linux", "Linux kernel", "Native Command Queuing", "Noop scheduler", "Process (computing)", "RAID", "Scheduling (computing)", "Tagged Command Queuing", "Work-conserving"], "categories": ["All stub articles", "Disk scheduling algorithms", "Linux kernel features", "Linux stubs", "Operating system kernels"], "title": "Anticipatory scheduling"}
{"summary": "FScan is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests. It uses two subqueues. During the scan, all of the requests are in the first queue and all new requests are put into the second queue. Thus, service of new requests is deferred until all of the old requests have been processed. When the scan ends, the arm is taken to the first queue entries and is started all over again.", "links": ["C-SCAN", "Elevator algorithm", "I/O scheduling", "LOOK algorithm", "N-Step-SCAN", "Queue (data structure)", "Shortest seek first"], "categories": ["All articles lacking sources", "Articles lacking sources from December 2009", "Disk scheduling algorithms"], "title": "FSCAN"}
{"summary": "Input/output (I/O) scheduling is the method that computer operating systems use to decide in which order the block I/O operations will be submitted to storage volumes. I/O scheduling is sometimes called disk scheduling.\nI/O scheduling usually has to work with hard disk drives that have long access times for requests placed far away from the current position of the disk head (this operation is called a seek). To minimize the effect this has on system performance, most I/O schedulers implement a variant of the elevator algorithm that reorders the incoming randomly ordered requests so the associated data would be accessed with minimal arm/head movement.\nI/O schedulers can have many purposes depending on the goals; common purposes include the following:\nTo minimize time wasted by hard disk seeks\nTo prioritize a certain processes' I/O requests\nTo give a share of the disk bandwidth to each running process\nTo guarantee that certain requests will be issued before a particular deadline\nCommon scheduling disciplines include the following:\nRandom scheduling (RSS)\nFirst In, First Out (FIFO), also known as First Come First Served (FCFS)\nLast In, First Out (LIFO)\nShortest seek first, also known as Shortest Seek / Service Time First (SSTF)\nElevator algorithm, also known as SCAN (including its variants, C-SCAN, LOOK, and C-LOOK)\nN-Step-SCAN SCAN of N records at a time\nFSCAN, N-Step-SCAN where N equals queue size at start of the SCAN cycle\nCompletely Fair Queuing (CFQ) on Linux\nAnticipatory scheduling\nNoop scheduler\nDeadline scheduler\nmClock scheduler", "links": ["Access Time", "Anticipatory scheduling", "CFQ", "Computer", "Deadline scheduler", "Elevator algorithm", "FIFO (computing and electronics)", "FSCAN", "Hard disk", "Hard disk drive", "Input/output", "LIFO (computing)", "Linux kernel", "N-Step-SCAN", "Native Command Queuing", "Noop scheduler", "Operating system", "Process (computing)", "Process management (computing)", "Random scheduling", "Robert Love", "Scheduling (computing)", "Shortest seek first", "Tagged Command Queuing", "Volume (computing)"], "categories": ["All articles needing additional references", "Articles needing additional references from February 2013", "Disk scheduling algorithms"], "title": "I/O scheduling"}
{"summary": "LOOK is a disk scheduling algorithm used to determine the order in which new disk read and write requests are processed.", "links": ["Elevator algorithm", "FSCAN", "I/O scheduling", "N-Step-SCAN", "Shortest seek time first"], "categories": ["All articles needing additional references", "All articles that may contain original research", "Articles needing additional references from April 2014", "Articles that may contain original research from April 2014", "Disk scheduling algorithms"], "title": "LOOK algorithm"}
{"summary": "N-Step-SCAN (also referred to as N-Step LOOK) is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests. It segments the request queue into subqueues of length N. Breaking the queue into segments of N requests makes service guarantees possible. Subsequent requests entering the request queue won't get pushed into N sized subqueues which are already full by the elevator algorithm. As such, starvation is eliminated and guarantees of service within N requests is possible.\nAnother way to look at N-step SCAN is this: A buffer for N requests is kept. All the requests in this buffer are serviced in any particular sweep. All the incoming requests in this period are not added to this buffer but are kept up in a separate buffer. When these top N requests are serviced, the IO scheduler chooses the next N requests and this process continues. This allows for better throughput and avoids starvation.", "links": ["C-SCAN", "Elevator algorithm", "FSCAN", "I/O scheduling", "LOOK algorithm", "Shortest seek first"], "categories": ["All articles needing additional references", "Articles needing additional references from February 2008", "Disk scheduling algorithms"], "title": "N-Step-SCAN"}
{"summary": "Shortest seek first (or shortest seek time first) is a secondary storage scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.", "links": ["Disk storage", "Elevator algorithm", "FCFS (computing and electronics)", "I/O scheduling", "Resource starvation"], "categories": ["All articles lacking sources", "Articles lacking sources from December 2009", "Disk scheduling algorithms"], "title": "Shortest seek first"}
{"summary": "Deadline-monotonic priority assignment is a priority assignment policy used with fixed priority pre-emptive scheduling.\nWith deadline-monotonic priority assignment, tasks are assigned priorities according to their deadlines; the task with the shortest deadline being assigned the highest priority.\nThis priority assignment policy is optimal for a set of periodic or sporadic tasks which comply with the following restrictive system model:\nAll tasks have deadlines less than or equal to their minimum inter-arrival times (or periods).\nAll tasks have worst-case execution times (WCET) that are less than or equal to their deadlines.\nAll tasks are independent and so do not block each other's execution (for example by accessing mutually exclusive shared resources).\nNo task voluntarily suspends itself.\nThere is some point in time, referred to as a critical instant, where all of the tasks become ready to execute simultaneously.\nScheduling overheads (switching from one task to another) are zero.\nAll tasks have zero release jitter (the time from the task arriving to it becoming ready to execute).\nIf restriction 7 is lifted, then \"deadline minus jitter\" monotonic priority assignment is optimal.\nIf restriction 1. is lifted allowing deadlines greater than periods, then Audsley's optimal priority assignment algorithm may be used to find the optimal priority assignment.\nDeadline monotonic priority assignment is not optimal for fixed priority non-pre-emptive scheduling.\nA fixed priority assignment policy P is referred to as optimal if no task set exists which is schedulable using a different priority assignment policy which is not also schedulable using priority assignment policy P. Or in other words: Deadline-monotonic priority assignment (DMPA) policy is optimal if any process set, Q, that is schedulable by priority scheme, W, is also schedulable by DMPA", "links": ["Assignment (computer programming)", "Computer science", "Fixed priority pre-emptive scheduling", "International Standard Book Number", "Task (computing)", "Worst-case execution time"], "categories": ["All articles covered by WikiProject Wikify", "All articles with too few wikilinks", "All stub articles", "Articles covered by WikiProject Wikify from December 2012", "Articles with too few wikilinks from December 2012", "Computer science stubs", "Processor scheduling algorithms"], "title": "Deadline-monotonic scheduling"}
{"summary": "Earliest deadline first (EDF) or least time to go is a dynamic scheduling algorithm used in real-time operating systems to place processes in a priority queue. Whenever a scheduling event occurs (task finishes, new task released, etc.) the queue will be searched for the process closest to its deadline. This process is the next to be scheduled for execution.\nEDF is an optimal scheduling algorithm on preemptive uniprocessors, in the following sense: if a collection of independent jobs, each characterized by an arrival time, an execution requirement and a deadline, can be scheduled (by any algorithm) in a way that ensures all the jobs complete by their deadline, the EDF will schedule this collection of jobs so they all complete by their deadline.\nWith scheduling periodic processes that have deadlines equal to their periods, EDF has a utilization bound of 100%. Thus, the schedulability test for EDF is:\n\nwhere the  are the worst-case computation-times of the  processes and the  are their respective inter-arrival periods (assumed to be equal to the relative deadlines).\nThat is, EDF can guarantee that all deadlines are met provided that the total CPU utilization is not more than 100%. Compared to fixed priority scheduling techniques like rate-monotonic scheduling, EDF can guarantee all the deadlines in the system at higher loading.\nHowever, when the system is overloaded, the set of processes that will miss deadlines is largely unpredictable (it will be a function of the exact deadlines and time at which the overload occurs.) This is a considerable disadvantage to a real time systems designer. The algorithm is also difficult to implement in hardware and there is a tricky issue of representing deadlines in different ranges (deadlines can't be more precise than the granularity of the clock used for the scheduling). If a modular arithmetic is used to calculate future deadlines relative to now, the field storing a future relative deadline must accommodate at least the value of the ((\"duration\" {of the longest expected time to completion} * 2) + \"now\"). Therefore EDF is not commonly found in industrial real-time computer systems.\nInstead, most real-time computer systems use fixed priority scheduling (usually rate-monotonic scheduling). With fixed priorities, it is easy to predict that overload conditions will cause the low-priority processes to miss deadlines, while the highest-priority process will still meet its deadline.\nThere is a significant body of research dealing with EDF scheduling in real-time computing; it is possible to calculate worst case response times of processes in EDF, to deal with other types of processes than periodic processes and to use servers to regulate overloads.", "links": ["AQuoSA", "Central processing unit", "Computer hardware", "Dynamic priority scheduling", "Fixed priority pre-emptive scheduling", "Hierarchical scheduler", "Kernel-based Virtual Machine", "Least common multiple", "Linux kernel", "OSEK", "Priority inheritance", "Priority inversion", "Priority queue", "Rate-monotonic scheduling", "Real-time computing", "Real-time operating system", "SCHED DEADLINE", "Scheduling algorithm", "Temporal isolation among virtual machines", "Time slice", "Virtual machine"], "categories": ["All Wikipedia articles needing clarification", "Processor scheduling algorithms", "Real-time computing", "Wikipedia articles needing clarification from June 2013"], "title": "Earliest deadline first scheduling"}
{"summary": "Fair-share scheduling is a scheduling algorithm for computer operating systems in which the CPU usage is equally distributed among system users or groups, as opposed to equal distribution among processes.\nOne common method of logically implementing the fair-share scheduling strategy is to recursively apply the round-robin scheduling strategy at each level of abstraction (processes, users, groups, etc.) The time quantum required by round-robin is arbitrary, as any equal division of time will produce the same results.\nThis was first developed by Judy Kay and Piers Lauder through their research at Sydney University in the 1980s.", "links": ["Central processing unit", "Completely Fair Scheduler", "Operating systems", "Round-robin scheduling", "Scheduling algorithm"], "categories": ["All articles needing additional references", "Articles needing additional references from June 2012", "Processor scheduling algorithms"], "title": "Fair-share scheduling"}
{"summary": "Foreground-background is a scheduling algorithm that is used to control execution of multiple processes on a single processor. It is based on two waiting lists, the first one is called foreground because this is the one in which all processes initially enter, and the second one is called background because all processes, after using all of their execution time in foreground, are moved to background.\nWhen a process becomes ready it begins its execution in foreground immediately, forcing the processor to give up execution of current process in the background and execute newly created process for a predefined period. This period is usually 2 or more quanta. If the process is not finished after its execution in the foreground it is moved to background waiting list where it will be executed only when the foreground list is empty. After being moved to background, process is then run longer than before, usually 4 quanta. The time of execution is increased because the process obviously needs more than 2 quanta to finish (this is the reason it was moved to background). This gives the process the opportunity to finish within this newly designated time. If the process does not finish after this, it is then preempted and moved to the end of the background list.\nThe advantage of the foreground-background algorithm is that it gives process the opportunity to execute immediately after its creation, but scheduling in the background list is pure round-robin scheduling.", "links": ["Preemption (computing)", "Round-robin scheduling", "Run time (program lifecycle phase)", "Scheduling algorithm"], "categories": ["All articles lacking sources", "Articles lacking sources from February 2007", "Processor scheduling algorithms"], "title": "Foreground-background"}
{"summary": "In computer science, gang scheduling is a scheduling algorithm for parallel systems that schedules related threads or processes to run simultaneously on different processors. Usually these will be threads all belonging to the same process, but they may also be from different processes, for example when the processes have a producer-consumer relationship, or when they all come from the same MPI program.\nGang scheduling is used so that if two or more threads or processes communicate with each other, they will all be ready to communicate at the same time. If they were not gang-scheduled, then one could wait to send or receive a message to another while it is sleeping, and vice versa. When processors are over-subscribed and gang scheduling is not used within a group of processes or threads which communicate with each other, it can lead to situations where each communication event suffers the overhead of a context switch.\nTechnically, gang scheduling is based on a data structure called the Ousterhout matrix. In this matrix each row represents a time slice, and each column a processor. The threads or processes of each job are packed into a single row of the matrix. During execution, coordinated context switching is performed across all nodes to switch from the processes in one row to those in the next row.\nGang scheduling is stricter than coscheduling. It requires all threads of the same process to run concurrently, while coscheduling allows for fragments, which are sets of threads that do not run concurrently with the rest of the gang.\nGang scheduling was implemented and used in production mode on several parallel machines, most notably the Connection Machine CM-5.", "links": ["Central processing unit", "CiteSeer", "Computer science", "Connection Machine", "Coscheduling", "Message Passing Interface", "Parallel computing", "Process (computing)", "Scheduling algorithm", "Thread (computer science)"], "categories": ["All stub articles", "Computer science stubs", "Processor scheduling algorithms"], "title": "Gang scheduling"}
{"summary": "Lottery Scheduling is a probabilistic scheduling algorithm for processes in an operating system. Processes are each assigned some number of lottery tickets, and the scheduler draws a random ticket to select the next process. The distribution of tickets need not be uniform; granting a process more tickets provides it a relative higher chance of selection. This technique can be used to approximate other scheduling algorithms, such as Shortest job next and Fair-share scheduling.\nLottery scheduling solves the problem of starvation. Giving each process at least one lottery ticket guarantees that it has non-zero probability of being selected at each scheduling operation.", "links": ["Algorithms", "Computer process", "Fair-share scheduling", "Lottery ticket", "Operating system", "Probability", "Resource starvation", "Scheduling (computing)", "Shortest job next"], "categories": ["Processor scheduling algorithms"], "title": "Lottery scheduling"}
{"summary": "In computer science, a multilevel feedback queue is a scheduling algorithm. Solaris 2.6 Time-Sharing (TS) scheduler implements this algorithm. The Mac OS X and Microsoft Windows schedulers can both be regarded as examples of the broader class of multilevel feedback queue schedulers. This scheduling algorithm is intended to meet the following design requirements for multimode systems:\nGive preference to short jobs.\nGive preference to I/O bound processes.\nSeparate processes into categories based on their need for the processor.\nThe Multi-level Feedback Queue scheduler was first developed by Fernando J. Corbat\u00f3 et al. in 1962, and this work, along with other work on Multics, led the ACM to award Corbat\u00f3 the Turing Award.", "links": ["Aging (scheduling)", "Central processing unit", "Computer science", "Digital object identifier", "FIFO (computing and electronics)", "Fair-share scheduling", "Fernando J. Corbat\u00f3", "First-come, first-served", "I/O bound", "International Standard Book Number", "Lottery scheduling", "Multilevel queue", "Multimode systems", "Preemption (computing)", "Resource starvation", "Round-robin scheduling", "Scheduling (computing)", "Turing Award"], "categories": ["Processor scheduling algorithms"], "title": "Multilevel feedback queue"}
{"summary": "Processor affinity, or CPU pinning enables the binding and unbinding of a process or a thread to a central processing unit (CPU) or a range of CPUs, so that the process or thread will execute only on the designated CPU or CPUs rather than any CPU. This can be viewed as a modification of the native central queue scheduling algorithm in a symmetric multiprocessing operating system. Each item in the queue has a tag indicating its kin processor. At the time of resource allocation, each task is allocated to its kin processor in preference to others.\nProcessor affinity takes advantage of the fact that some remnants of a process that was run on a given processor may remain in that processor's memory state (for example, data in the CPU cache) after another process is run on that CPU. Scheduling that process to execute on the same processor could result in an efficient use of process by reducing performance-degrading situations such as cache misses. A practical example of processor affinity is executing multiple instances of a non-threaded application, such as some graphics-rendering software.\nScheduling-algorithm implementations vary in adherence to processor affinity. Under certain circumstances, some implementations will allow a task to change to another processor if it results in higher efficiency. For example, when two processor-intensive tasks (A and B) have affinity to one processor while another processor remains unused, many schedulers will shift task B to the second processor in order to maximize processor use. Task B will then acquire affinity with the second processor, while task A will continue to have affinity with the original processor.", "links": ["Affinity mask", "CPU cache", "Cache miss", "Central processing unit", "FreeBSD", "Hyper-threading", "Linux", "Load balancing (computing)", "MSDN Library", "Multi-core processor", "NetBSD", "OS X", "Process (computing)", "Scheduling algorithm", "Silicon Graphics", "Solaris (operating system)", "Symmetric multiprocessing", "Thread (computing)", "Windows NT"], "categories": ["Processor scheduling algorithms"], "title": "Processor affinity"}
{"summary": "Proportional Share Scheduling is a type of scheduling which preallocates certain amount of CPU time to each of the processes. In a proportional share algorithm every job has a weight, and jobs receive a share of the available resources proportional to the weight of every job.", "links": ["CPU time", "Computer science", "Scheduling (computing)"], "categories": ["All articles needing additional references", "All stub articles", "Articles needing additional references from December 2009", "Computer science stubs", "Processor scheduling algorithms"], "title": "Proportional share scheduling"}
{"summary": "In computer science, rate-monotonic scheduling (RMS) is a scheduling algorithm used in real-time operating systems (RTOS) with a static-priority scheduling class. The static priorities are assigned on the basis of the cycle duration of the job: the shorter the cycle duration is, the higher is the job's priority.\nThese operating systems are generally preemptive and have deterministic guarantees with regard to response times. Rate monotonic analysis is used in conjunction with those systems to provide scheduling guarantees for a particular application.", "links": ["Busy waiting", "Butler Lampson", "Central processing unit", "Chung Laung Liu", "Computer hardware", "Computer science", "Deadline-monotonic scheduling", "Deadlock", "Deos", "Digital object identifier", "Dynamic priority scheduling", "Earliest deadline first scheduling", "Infinity", "International Standard Book Number", "Linux kernel", "Lock-free and wait-free algorithms", "Mars Pathfinder", "MicroC/OS-II", "Preemption (computing)", "Priority ceiling protocol", "Priority inheritance", "Priority inversion", "RTEMS", "Real-time operating system", "Round-robin scheduling", "Scheduling (computing)", "Scheduling algorithm", "Semaphore (programming)", "Time-sharing", "VxWorks"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from October 2007", "Processor scheduling algorithms", "Real-time computing"], "title": "Rate-monotonic scheduling"}
{"summary": "Shortest job next (SJN), also known as Shortest Job First (SJF) or Shortest Process Next (SPN), is a scheduling policy that selects the waiting process with the smallest execution time to execute next. SJN is a non-preemptive algorithm. Shortest remaining time is a preemptive variant of SJN.\nShortest job next is advantageous because of its simplicity and because it minimizes the average amount of time each process has to wait until its execution is complete. However, it has the potential for process starvation for processes which will require a long time to complete if short processes are continually added. Highest response ratio next is similar but provides a solution to this problem.\nAnother disadvantage of using shortest job next is that the total execution time of a job must be known before execution. While it is not possible to perfectly predict execution time, several methods can be used to estimate the execution time for a job, such as a weighted average of previous execution times.\nShortest job next can be effectively used with interactive processes which generally follow a pattern of alternating between waiting for a command and executing it. If the execution burst of a process is regarded as a separate \"job\", past behaviour can indicate which process to run next, based on an estimate of its running time.\nShortest job next is used in specialized environments where accurate estimates of running time are available. Estimating the running time of queued processes is sometimes done using a technique called aging.", "links": ["Adversarial queueing network", "Aging (scheduling)", "Arrival theorem", "BCMP network", "Balance equation", "Bene\u0161 method", "Bulk queue", "Burke's theorem", "Buzen's algorithm", "Continuous-time Markov chain", "D/M/1 queue", "Data buffer", "Decomposition method (queueing theory)", "Erlang (unit)", "Erlang distribution", "FIFO (computing and electronics)", "Flow-equivalent server method", "Flow control (data)", "Fluid limit", "Fluid queue", "Fork\u2013join queue", "G-network", "G/G/1 queue", "G/M/1 queue", "Gordon\u2013Newell theorem", "Heavy traffic approximation", "Highest response ratio next", "Information system", "International Standard Book Number", "Jackson network", "Kelly network", "Kendall's notation", "Kingman's formula", "LIFO (computing)", "Layered queueing network", "Lindley equation", "Little's law", "Loss network", "M/D/1 queue", "M/D/c queue", "M/G/1 queue", "M/G/k queue", "M/M/1 queue", "M/M/c queue", "M/M/\u221e queue", "Markovian arrival process", "Matrix analytic method", "Mean field theory", "Mean value analysis", "Message queue", "Network congestion", "Network scheduler", "Pipeline (software)", "Poisson process", "Pollaczek\u2013Khinchine formula", "Polling system", "Preemption (computing)", "Process (computing)", "Processor sharing", "Product-form solution", "Quality of service", "Quasireversibility", "Queueing theory", "Rational arrival process", "Reflected Brownian motion", "Retrial queue", "Round-robin scheduling", "Scheduling (computing)", "Scheduling algorithm", "Shortest job first", "Shortest remaining time", "Starvation (computing)", "Teletraffic engineering", "Traffic equations"], "categories": ["Processor scheduling algorithms"], "title": "Shortest job next"}
{"summary": "Shortest remaining time, also known as shortest remaining time first (SRTF), is a scheduling method that is a preemptive version of shortest job next scheduling. In this scheduling algorithm, the process with the smallest amount of time remaining until completion is selected to execute. Since the currently executing process is the one with the shortest amount of time remaining by definition, and since that time should only reduce as execution progresses, processes will always run until they complete or a new process is added that requires a smaller amount of time.\nShortest remaining time is advantageous because short processes are handled very quickly. The system also requires very little overhead since it only makes a decision when a process completes or a new process is added, and when a new process is added the algorithm only needs to compare the currently executing process with the new process, ignoring all other processes currently waiting to execute.\nLike shortest job first, it has the potential for process starvation; long processes may be held off indefinitely if short processes are continually added. This threat can be minimal when process times follow a heavy-tailed distribution.\nLike shortest job next scheduling, shortest remaining time scheduling is rarely used outside of specialized environments because it requires accurate estimations of the runtime of all processes that are waiting to execute.", "links": ["Adversarial queueing network", "Arrival theorem", "BCMP network", "Balance equation", "Bene\u0161 method", "Bulk queue", "Burke's theorem", "Buzen's algorithm", "Continuous-time Markov chain", "D/M/1 queue", "Data buffer", "Decomposition method (queueing theory)", "Digital object identifier", "Erlang (unit)", "Erlang distribution", "FIFO (computing and electronics)", "Flow-equivalent server method", "Flow control (data)", "Fluid limit", "Fluid queue", "Fork\u2013join queue", "G-network", "G/G/1 queue", "G/M/1 queue", "Gordon\u2013Newell theorem", "Heavy-tailed distribution", "Heavy traffic approximation", "Information system", "Jackson network", "Kelly network", "Kendall's notation", "Kingman's formula", "LIFO (computing)", "Layered queueing network", "Lindley equation", "Little's law", "Loss network", "M/D/1 queue", "M/D/c queue", "M/G/1 queue", "M/G/k queue", "M/M/1 queue", "M/M/c queue", "M/M/\u221e queue", "Markovian arrival process", "Matrix analytic method", "Mean field theory", "Mean value analysis", "Message queue", "Network congestion", "Network scheduler", "Pipeline (software)", "Poisson process", "Pollaczek\u2013Khinchine formula", "Polling system", "Preemption (computing)", "Process (computing)", "Processor sharing", "Product-form solution", "Quality of service", "Quasireversibility", "Queueing theory", "Rational arrival process", "Reflected Brownian motion", "Retrial queue", "Round-robin scheduling", "Scheduling (computing)", "Shortest job first", "Shortest job next", "Starvation (computing)", "Teletraffic engineering", "Traffic equations"], "categories": ["Processor scheduling algorithms"], "title": "Shortest remaining time"}
{"summary": "In computer science, starvation is a problem encountered in concurrent computing where a process is perpetually denied necessary resources to process its work. Starvation may be caused by errors in a scheduling or mutual exclusion algorithm, but can also be caused by resource leaks, and can be intentionally caused via a denial-of-service attack such as a fork bomb.\nThe impossibility of starvation in a concurrent algorithm is called starvation-freedom, lockout-freedom or finite bypass, is an instance of liveness, and is one of the two requirements for any mutual exclusion algorithm (the other being correctness). The name \"finite bypass\" means that any process (concurrent part) of the algorithm is bypassed at most a finite number times before being allowed access to the shared resource.", "links": ["Aging (scheduling)", "Andrew Tanenbaum", "CPU time", "Computer process", "Computer science", "Concurrent computing", "Critical section", "Deadlock", "Denial-of-service attack", "Fork bomb", "International Standard Book Number", "Kernel (computer science)", "Liveness", "Maurice Herlihy", "Maximum throughput scheduling", "Michel Raynal", "Mutual exclusion", "Nir Shavit", "Priority inversion", "Resource (computer science)", "Resource leak", "Room synchronization", "Scheduling algorithm"], "categories": ["All articles needing additional references", "All stub articles", "Articles needing additional references from January 2011", "Computer science stubs", "Concurrency (computer science)", "Processor scheduling algorithms"], "title": "Starvation (computer science)"}
{"summary": "YDS is a scheduling algorithm for dynamic speed scaling processors which minimizes the total energy consumption. It was named after and developed by Yao et al. There is both an online and an offline version of the algorithm.", "links": ["CPU", "Dynamic frequency scaling", "Earliest deadline first scheduling", "Recursive algorithm", "Scheduling algorithm", "Symposium on Foundations of Computer Science"], "categories": ["All articles needing expert attention", "All articles that are too technical", "Articles needing expert attention from July 2013", "Processor scheduling algorithms", "Real-time computing", "Wikipedia articles that are too technical from July 2013"], "title": "YDS algorithm"}
{"summary": "In computer science, a search algorithm is an algorithm for finding an item with specified properties among a collection of items which are coded into a computer program, that look for clues to return what is wanted. The items may be stored individually as records in a database; or may be elements of a search space defined by a mathematical formula or procedure, such as the roots of an equation with integer variables; or a combination of the two, such as the Hamiltonian circuits of a graph.", "links": ["A* search algorithm", "Algorithm", "Alpha\u2013beta pruning", "Artificial intelligence", "Backgammon", "Backtracking", "Backward induction", "Best-first search", "Boyer\u2013Moore string search algorithm", "Branch and bound", "Breadth-first search", "Brute-force search", "Chess", "Collection (abstract data type)", "Combinatorial optimization", "Combinatorial search", "Completeness (logic)", "Computer science", "Constraint satisfaction problem", "Content-addressable memory", "Database", "Depth-first search", "Dijkstra's algorithm", "Diophantine equation", "Discrete mathematics", "Discrete optimization", "Donald Knuth", "Dual-phase evolution", "Equation", "Fibonacci search technique", "Finance", "Game tree", "Genetic programming", "Glossary of graph theory", "Gradient descent", "Graph theory", "Graph traversal", "Group (mathematics)", "Grover's algorithm", "Hamiltonian path", "Heuristic function", "Inequation", "Integer", "Jack Kiefer (statistician)", "Knuth\u2013Morris\u2013Pratt algorithm", "Kruskal's algorithm", "Linear search problem", "List of algorithms", "Local consistency", "Local search (optimization)", "Marketing", "Mathematical optimization", "Metaheuristic", "Military", "Minimax", "Nearest neighbour algorithm", "No free lunch in search and optimization", "PageRank", "Path (graph theory)", "Prim's algorithm", "Pruning (decision trees)", "Quantum computing", "Recommender system", "Record (computer science)", "Robot", "Search engine (computing)", "Search game", "Secondary source", "Selection algorithm", "Simulated annealing", "Solver", "Sorting algorithm", "Statistics", "Stochastic optimization", "String (computer science)", "String searching algorithm", "Suffix tree", "Tabu search", "The Art of Computer Programming", "Tree (graph theory)", "Tree traversal", "Variable (mathematics)", "Vertex (graph theory)", "Wikiversity", "Zero of a function"], "categories": ["All Wikipedia articles needing context", "All articles lacking sources", "All articles needing expert attention", "All pages needing cleanup", "Articles lacking sources from December 2014", "Articles needing expert attention from December 2014", "Computer science articles needing expert attention", "Search algorithms", "Wikipedia articles needing context from December 2014", "Wikipedia introduction cleanup from December 2014"], "title": "Search algorithm"}
{"summary": "In computer science, the all nearest smaller values problem is the following task: for each position in a sequence of numbers, search among the previous positions for the last position that contains a smaller value. This problem can be solved efficiently both by parallel and non-parallel algorithms: Berkman, Schieber & Vishkin (1993), who first identified the procedure as a useful subroutine for other parallel programs, developed efficient algorithms to solve it in the Parallel Random Access Machine model; it may also be solved in linear time on a non-parallel computer using a stack-based algorithm. Later researchers have studied algorithms to solve it in other models of parallel computation.", "links": ["Array data structure", "Binary search", "Bulk synchronous parallel", "Cartesian tree", "Computer science", "Convex hull", "Data structure", "David Eppstein", "Digital object identifier", "Donald Knuth", "Graham scan", "Hypercube graph", "International Standard Book Number", "Jon Bentley", "Linear time", "Merge algorithm", "Merge sort", "Parallel Random Access Machine", "Parallel algorithms", "Parenthesis", "Polygon triangulation", "Prefix sum", "Pseudocode", "Randomized binary search tree", "Range searching", "Robert Tarjan", "Shang-Hua Teng", "Stack-sortable permutation", "Stack (data structure)", "The Art of Computer Programming", "Treap", "Uzi Vishkin", "Van der Corput sequence"], "categories": ["Parallel computing", "Search algorithms"], "title": "All nearest smaller values"}
{"summary": "Any-angle path planning algorithms search for paths on a cell decomposition of a continuous configuration space (such as a two-dimensional terrain).", "links": ["A*", "A* search algorithm", "Alpha\u2013beta pruning", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Big O notation", "Block A*", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Configuration space", "D*", "Depth-first search", "Depth-limited search", "Digital object identifier", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph traversal", "Grid graph", "Hill climbing", "IEEE", "Incremental heuristic search", "International Standard Book Number", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Motion planning", "Pathfinding", "Prim's algorithm", "Real-time strategy", "Regular grid", "Robot navigation", "SMA*", "Search game", "Tree traversal", "Visibility (geometry)", "Visibility graph", "Weighted region problem"], "categories": ["Artificial intelligence", "Robot navigation", "Search algorithms"], "title": "Any-angle path planning"}
{"summary": "In backtracking algorithms, backjumping is a technique that reduces search space, therefore increasing efficiency. While backtracking always goes up one level in the search tree when all values for a variable have been tested, backjumping may go up more levels. In this article, a fixed order of evaluation of variables  is used, but the same considerations apply to a dynamic order of evaluation.", "links": ["Algorithm", "Backtracking", "Candidate solution", "Constraint Satisfaction Problems", "Constraint learning", "Constraint satisfaction", "International Standard Book Number", "Patrick Prosser", "Recursion", "Search tree"], "categories": ["Constraint programming", "Search algorithms"], "title": "Backjumping"}
{"summary": "In computer science, beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic which attempts to predict how close a partial solution is to a complete solution (goal state). But in beam search, only a predetermined number of best partial solutions are kept as candidates.\nBeam search uses breadth-first search to build its search tree. At each level of the tree, it generates all successors of the states at the current level, sorting them in increasing order of heuristic cost. However, it only stores a predetermined number of best states at each level (called the beam width). Only those states are expanded next. The greater the beam width, the fewer states are pruned. With an infinite beam width, no states are pruned and beam search is identical to breadth-first search. The beam width bounds the memory required to perform the search. Since a goal state could potentially be pruned, beam search sacrifices completeness (the guarantee that an algorithm will terminate with a solution, if one exists). Beam search is not optimal (that is, there is no guarantee that it will find the best solution). It returns the first solution found.\nThe beam width can either be fixed or variable. One approach that uses a variable beam width starts with the width at a minimum. If no solution is found, the beam is widened and the procedure is repeated.", "links": ["A* search algorithm", "Alpha\u2013beta pruning", "Anytime algorithm", "B*", "Backtracking", "Beam stack search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Carnegie Mellon University", "CiteSeer", "Completeness (logic)", "Computer science", "D*", "Depth-first search", "Depth-limited search", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph traversal", "Heuristic (computer science)", "Hill climbing", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Machine translation", "Prim's algorithm", "Raj Reddy", "SMA*", "Search algorithm", "Search game", "Tree traversal"], "categories": ["Search algorithms"], "title": "Beam search"}
{"summary": "Beam Stack Search is a search algorithm that combines chronological backtracking (that is, depth-first search) with beam search and is similar to Depth-First Beam Search. Both search algorithms are anytime algorithms that find good but likely sub-optimal solutions quickly, like beam search, then backtrack and continue to find improved solutions until convergence to an optimal solution.", "links": ["Algorithm", "Anytime algorithm", "Backtracking", "Beam search", "CiteSeer", "Data structure", "Depth-first search", "Divide and conquer algorithm", "Search algorithm"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Search algorithms"], "title": "Beam stack search"}
{"summary": "Best bin first is a search algorithm that is designed to efficiently find an approximate solution to the nearest neighbor search problem in very-high-dimensional spaces. The algorithm is based on a variant of the kd-tree search algorithm which makes indexing higher-dimensional spaces possible. Best bin first is an approximate algorithm which returns the nearest neighbor for a large fraction of queries and a very close neighbor otherwise.", "links": ["Algorithm", "CiteSeer", "Data structure", "Kd-tree", "Nearest neighbor search", "Search algorithm"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Search algorithms"], "title": "Best bin first"}
{"summary": "Best-first search is a search algorithm which explores a graph by expanding the most promising node chosen according to a specified rule.\nJudea Pearl described best-first search as estimating the promise of node n by a \"heuristic evaluation function  which, in general, may depend on the description of n, the description of the goal, the information gathered by the search up to that point, and most important, on any extra knowledge about the problem domain.\"\nSome authors have used \"best-first search\" to refer specifically to a search with a heuristic that attempts to predict how close the end of a path is to a solution, so that paths which are judged to be closer to a solution are extended first. This specific type of search is called greedy best-first search or pure heuristic search.\nEfficient selection of the current best candidate for extension is typically implemented using a priority queue.\nThe A* search algorithm is an example of best-first search, as is B*. Best-first algorithms are often used for path finding in combinatorial search. (Note that neither A* nor B* is a greedy best-first search as they incorporate the distance from start in addition to estimated distances to the goal.)", "links": ["A* search algorithm", "Alpha\u2013beta pruning", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "Combinatorial search", "D*", "Depth-first search", "Depth-limited search", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph (data structure)", "Graph traversal", "Greedy algorithm", "Heuristic function", "Hill climbing", "International Standard Book Number", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Judea Pearl", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Peter Norvig", "Prim's algorithm", "Priority queue", "SMA*", "Search algorithm", "Search game", "Stuart J. Russell", "Tree traversal"], "categories": ["All articles with dead external links", "Articles with dead external links from August 2014", "Search algorithms"], "title": "Best-first search"}
{"summary": "In computer science, a binary search or half-interval search algorithm finds the position of a target value within a sorted array. The binary search algorithm can be classified as a dichotomic divide-and-conquer search algorithm and executes in logarithmic time.", "links": [".NET Framework", "Addison-Wesley", "Algorithm", "Algorithm function", "Array data structure", "Best, worst and average case", "Big O notation", "Binary logarithm", "Binary search algorithm", "Bisection method", "Branch table", "C++", "COBOL", "CPAN", "C (programming language)", "C standard library", "Cache (computing)", "Charles E. Leiserson", "Cocoa (API)", "Computer science", "Core Foundation", "Decision problem", "Dichotomic search", "Digital object identifier", "Divide and conquer algorithm", "Donald Knuth", "Eric W. Weisstein", "Exponential search", "Floor function", "Fractional cascading", "Function overloading", "Generic programming", "Go (programming language)", "Index (database)", "International Standard Book Number", "Interpolation search", "Introduction to Algorithms", "Iteration", "Java (programming language)", "Jon Bentley", "Linear search", "Locality of reference", "Logarithmic algorithm", "Logarithmic time", "MathWorld", "Microsoft", "Niklaus Wirth", "Objective-C", "Oracle Corporation", "Perl", "Permanent", "Prentice Hall", "Python (programming language)", "Recursion", "Reduction (complexity)", "Ron Rivest", "Ruby (programming language)", "Run-time analysis", "SIGCSE", "Search algorithm", "Self-balancing binary search tree", "Signedness", "Sorted array", "Standard Template Library", "Tail recursive", "Telephone book", "The Art of Computer Programming", "Thomas H. Cormen", "Worst case analysis"], "categories": ["All articles needing cleanup", "All articles with unsourced statements", "Articles needing cleanup from April 2011", "Articles with unsourced statements from August 2009", "Articles with unsourced statements from October 2011", "Cleanup tagged articles without a reason field from April 2011", "Search algorithms", "Wikipedia articles needing clarification from January 2015", "Wikipedia pages needing cleanup from April 2011"], "title": "Binary search algorithm"}
{"summary": "In computer science, brute-force search or exhaustive search, also known as generate and test, is a very general problem-solving technique that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problem's statement.\nA brute-force algorithm to find the divisors of a natural number n would enumerate all integers from 1 to the square root of n, and check whether each of them divides n without remainder. A brute-force approach for the eight queens puzzle would examine all possible arrangements of 8 pieces on the 64-square chessboard, and, for each arrangement, check whether each (queen) piece can attack any other.\nWhile a brute-force search is simple to implement, and will always find a solution if it exists, its cost is proportional to the number of candidate solutions \u2013 which in many practical problems tends to grow very quickly as the size of the problem increases. Therefore, brute-force search is typically used when the problem size is limited, or when there are problem-specific heuristics that can be used to reduce the set of candidate solutions to a manageable size. The method is also used when the simplicity of implementation is more important than speed.\nThis is the case, for example, in critical applications where any errors in the algorithm would have very serious consequences; or when using a computer to prove a mathematical theorem. Brute-force search is also useful as a baseline method when benchmarking other algorithms or metaheuristics. Indeed, brute-force search can be viewed as the simplest metaheuristic. Brute force search should not be confused with backtracking, where large sets of solutions can be discarded without being explicitly enumerated (as in the textbook computer solution to the eight queens problem above). The brute-force method for finding an item in a table \u2014 namely, check all entries of the latter, sequentially \u2014 is called linear search.", "links": ["Automated theorem proving", "Backtracking", "Benchmarking", "Big O notation", "Binary digit", "Brute-force attack", "Brute force (disambiguation)", "Brute force attack", "Central processing unit", "Chart parsing", "Chessboard", "Combinations", "Combinatorial explosion", "Computer chess", "Computer science", "Constraint Satisfaction Problem", "Constraint programming", "Constraint propagation", "Cryptography", "Curse of dimensionality", "Divisor", "Eight queens puzzle", "Evaluation function", "Expected value", "Heuristic", "Heuristic (computer science)", "International Standard Book Number", "Key (cryptography)", "Key length", "Linear search", "Metaheuristic", "Minimax", "Natural number", "Obfuscation", "One-time pad", "Personal computer", "Quintillion", "Strategy", "Subroutine"], "categories": ["All articles needing additional references", "Articles needing additional references from February 2008", "Search algorithms"], "title": "Brute-force search"}
{"summary": "In computer science, dancing links, also known as DLX, is the technique suggested by Donald Knuth to efficiently implement his Algorithm X. Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm that finds all solutions to the exact cover problem. Some of the better-known exact cover problems include tiling, the n queens problem, and Sudoku.\nThe name dancing links stems from the way the algorithm works, as iterations of the algorithm cause the links to \"dance\" with partner links so as to resemble an \"exquisitely choreographed dance.\" Knuth credits Hiroshi Hitotsumatsu and K\u014dhei Noshita with having invented the idea in 1979, but it is his paper which has popularized it.", "links": ["-yllion", "AMS Euler", "Algorithm", "Algorithm X", "ArXiv", "Backtracking", "Big O notation", "CWEB", "Computer Modern", "Computer science", "Computers and Typesetting", "Concrete Mathematics", "Concrete Roman", "Depth-first", "Digital object identifier", "Dijkstra's algorithm", "Donald Knuth", "Doubly linked list", "Eight queens puzzle", "Exact cover", "Fisher\u2013Yates shuffle", "Font", "GNU MIX Development Kit", "Hadoop", "Knuth's Algorithm X", "Knuth's Simpath algorithm", "Knuth Prize", "Knuth reward check", "Knuth\u2013Bendix completion algorithm", "Knuth\u2013Morris\u2013Pratt algorithm", "Literate programming", "METAFONT", "MIX", "MMIX", "Man or boy test", "MapReduce", "Nondeterministic algorithm", "Polycube", "Potrzebie", "Quater-imaginary base", "Recursion (computer science)", "Robinson\u2013Schensted\u2013Knuth correspondence", "Selected papers series of Knuth", "Software", "Sparse matrix", "Sudoku", "Surreal Numbers (book)", "TeX", "Tessellation", "The Art of Computer Programming", "The Complexity of Songs", "Things a Computer Scientist Rarely Talks About", "Trabb Pardo\u2013Knuth algorithm", "WEB"], "categories": ["Articles containing video clips", "Donald Knuth", "Linked lists", "Search algorithms", "Sudoku"], "title": "Dancing Links"}
{"summary": "In computer science, a dichotomic search is a search algorithm that operates by selecting between two distinct alternatives (dichotomies) at each step. It is a specific type of divide and conquer algorithm. A well-known example is binary search.\nAbstractly, a dichotomic search can be viewed as following edges of an implicit binary tree structure until it reaches a leaf (a goal or final state). This creates a theoretical tradeoff between the number of possible states and the running time: given k comparisons, the algorithm can only reach O(2k) possible states and/or possible goals.\nSome dichotomic searches only have results at the leaves of the tree, such as the Huffman tree used in Huffman compression, or the implicit classification tree used in Twenty Questions. Other dichotomic searches also have results in at least some internal nodes of the tree, such as a dichotomic search table for Morse code. There is thus some looseness in the definition. Though there may indeed be only two paths from any node, there are thus three possibilities at each step: choose one onwards path or the other, or stop at this node.\n\nDichotomic searches are often used in repair manuals, sometimes graphically illustrated with a flowchart similar to a fault tree.", "links": ["Binary search", "Binary tree", "Classification tree", "Computer science", "Divide and conquer algorithm", "Fault tree", "Flowchart", "Huffman tree", "Morse code", "Search algorithm", "Twenty Questions"], "categories": ["All articles lacking in-text citations", "All stub articles", "Articles lacking in-text citations from December 2014", "Computer science stubs", "Search algorithms"], "title": "Dichotomic search"}
{"summary": "The difference-map algorithm is a search algorithm for general constraint satisfaction problems. It is a meta-algorithm in the sense that it is built from more basic algorithms that perform projections onto constraint sets. From a mathematical perspective, the difference-map algorithm is a dynamical system based on a mapping of Euclidean space. Solutions are encoded as fixed points of the mapping.\nAlthough originally conceived as a general method for solving the phase problem, the difference-map algorithm has been used for the boolean satisfiability problem, protein structure prediction, Ramsey numbers, diophantine equations, and Sudoku, as well as sphere- and disk-packing problems. Since these applications include NP-complete problems, the scope of the difference map is that of an incomplete algorithm. Whereas incomplete algorithms can efficiently verify solutions (once a candidate is found), they cannot prove that a solution does not exist.\nThe difference-map algorithm is a generalization of two iterative methods: Fienup's Hybrid input output (HIO) algorithm for phase retrieval  and the Douglas-Rachford algorithm for convex optimization. Iterative methods, in general, have a long history in phase retrieval and convex optimization. The use of this style of algorithm for hard, non-convex problems is a more recent development.", "links": ["2-SAT", "3-SAT", "Absolute value", "Applied Optics", "Boolean satisfiability problem", "Chaos theory", "Coherent light", "Constraint (mathematics)", "Constraint satisfaction", "Convex optimization", "Diophantine equations", "Discrete Fourier transform", "Douglas-Rachford algorithm", "Dynamical system", "Euclidean space", "Fixed point (mathematics)", "Fraunhofer diffraction", "Hybrid input output (HIO) algorithm for phase retrieval", "Incomplete algorithm", "Iterative methods", "Journal of the Optical Society of America A", "Literal (mathematical logic)", "Local search (optimization)", "Map (mathematics)", "Meta-algorithm", "NP-complete", "Phase problem", "Physical Review", "Proceedings of the National Academy of Sciences", "Projection (linear algebra)", "Protein structure prediction", "Ramsey numbers", "Search algorithm", "Set intersection", "Strange attractor", "Sudoku", "Support (mathematics)", "Unitary transformation"], "categories": ["Constraint programming", "Search algorithms"], "title": "Difference-map algorithm"}
{"summary": "In computer science, an exponential search (also called doubling search or galloping search) is an algorithm, created by Jon Bentley and Andrew Chi-Chih Yao in 1976, for searching sorted, unbounded/infinite lists. There are numerous ways to implement this with the most common being to determine a range that the search key resides in and performing a binary search within that range. This takes O(log i) where i is the position of the search key in the list, if the search key is in the list, or the position where the search key should be, if the search key is not in the list.\nExponential search can also be used to search in bounded lists. Exponential search can even out-perform more traditional searches for bounded lists, such as binary search, when the element being searched for is near the beginning of the array. This is because exponential search will run in O(log i) time, where i is the index of the element being searched for in the list, whereas binary search would run in O(log n) time, where n is the number of elements in the list.", "links": ["Algorithm", "Andrew Chi-Chih Yao", "Array data structure", "Best, worst and average case", "Big-O notation", "Big O notation", "Binary search", "Computer science", "Digital object identifier", "Exponentiation", "Hash table", "International Standard Book Number", "International Standard Serial Number", "Interpolation search", "Jon Bentley", "Linear search", "Search algorithm", "Splay tree", "Ternary search"], "categories": ["Search algorithms"], "title": "Exponential search"}
{"summary": "In computer science, the Fibonacci search technique is a method of searching a sorted array using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers. Compared to binary search, Fibonacci search examines locations whose addresses have lower dispersion. Therefore, when the elements being searched have non-uniform access memory storage (i.e., the time needed to access a storage location varies depending on the location previously accessed), the Fibonacci search has an advantage over binary search in slightly reducing the average time needed to access a storage location. The typical example of non-uniform access storage is that of a magnetic tape, where the time to access a particular element is proportional to its distance from the element currently under the tape's head. Note, however, that large arrays not fitting in CPU cache or even in RAM can also be considered as non-uniform access examples. Fibonacci search has a complexity of O(log(n)) (see Big O notation).\nFibonacci search was first devised by Jack Kiefer (1953) as a minimax search for the maximum (minimum) of a unimodal function in an interval.", "links": ["Big O notation", "Binary search algorithm", "CPU cache", "Computer science", "Divide and conquer algorithm", "Fibonacci number", "Golden section search", "Jack Kiefer (statistician)", "Magnetic tape", "Minimax", "RAM", "Search algorithms", "Sorted array", "The Art of Computer Programming", "Unimodal function"], "categories": ["All articles needing expert attention", "All articles that are too technical", "Articles needing expert attention from July 2013", "Search algorithms", "Wikipedia articles that are too technical from July 2013"], "title": "Fibonacci search technique"}
{"summary": "In computer science, finger search trees are a type of binary search tree that keeps pointers to interior nodes, called fingers. The fingers speed up searches, insertions, and deletions for elements close to the fingers, giving amortized O(log n) lookups, and amortized O(1) insertions and deletions. It should not be confused with a finger tree nor a splay tree, although both can be used to implement finger search trees.\nGuibas et al. introduced \ufb01nger search trees, by building upon B-trees. The original version supports \ufb01nger searches in O(log d) time, where d is the number of elements between the \ufb01nger and the search target. Updates take O(1) time, when only O(1) moveable \ufb01ngers are maintained. Moving a \ufb01nger p positions requires O(log p) time. Huddleston and Mehlhorn refined this idea as level-linked B-trees.\nTsakalidis proposed a version based on AVL trees that facilitates searching from the ends of the tree; it can be used to implement a data structure with multiple fingers by using multiple of such trees.\nTo perform a finger search on a binary tree, the ideal way is to start from the finger, and search upwards to the root, until we reach the turning node or the least common ancestor of x and y, and then go downwards to find the element we're looking for. Determining if a node is the ancestor of another is non-trivial.\n\nTreaps, a randomized tree structure proposed by Seidel and Aragon, has the property that the expected path length of two elements of distance d is O(log d). For finger searching, they proposed adding pointers to determine the least common ancestor(LCA) quickly, or in every node maintain the minimum and maximum values of its subtree.\nA book chapter has been written that covers finger search trees in depth. In which, Brodal suggested an algorithm to perform finger search on treaps in O(log d) time, without needing any extra bookkeeping information; this algorithm accomplishes this by concurrently searching downward from the last candidate LCA.", "links": ["(a,b)-tree", "2\u20133 tree", "2\u20133\u20134 tree", "AA tree", "AVL tree", "Algorithm", "Associative array", "Athanasios Tsakalidis", "B*-tree", "B+ tree", "B-tree", "BK-tree", "BSP tree", "Big O notation", "Binary heap", "Binary search tree", "Binomial heap", "Bx-tree", "C-trie", "CRC Press", "Cartesian tree", "Cecilia R. Aragon", "Chapman & Hall", "Cover tree", "Ctrie", "Dancing tree", "Data structure", "Digital object identifier", "Exponential tree", "Fenwick tree", "Fibonacci heap", "Finger search", "Finger tree", "Fusion tree", "HTree", "Hash calendar", "Hash tree (persistent data structure)", "Heap (data structure)", "Hilbert R-tree", "IDistance", "Implicit k-d tree", "International Standard Book Number", "Interval tree", "K-ary tree", "K-d tree", "Left-child right-sibling binary tree", "Left-leaning red\u2013black tree", "Leftist tree", "Link/cut tree", "Log-structured merge-tree", "M-tree", "MVP tree", "Merkle tree", "Metric tree", "Octree", "Optimal binary search tree", "Order statistic tree", "PQ tree", "Pairing heap", "Priority R-tree", "Quadtree", "R* tree", "R+ tree", "R-tree", "Radix tree", "Raimund Seidel", "Range tree", "Red\u2013black tree", "SPQR tree", "Sartaj Sahni", "Scapegoat tree", "Search tree", "Segment tree", "Set (abstract data type)", "Skew heap", "Spatial index", "Splay tree", "Suffix tree", "T-tree", "Ternary search tree", "Top tree", "Treap", "Tree (data structure)", "Trie", "UB-tree", "Van Emde Boas tree", "Vantage-point tree", "Weight-balanced tree", "X-fast trie", "X-tree", "Y-fast trie"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Pages using citations with accessdate and no URL", "Search algorithms", "Trees (data structures)"], "title": "Finger search tree"}
{"summary": "God's algorithm is a notion originating in discussions of ways to solve the Rubik's Cube puzzle, but which can also be applied to other combinatorial puzzles and mathematical games. It refers to any algorithm which produces a solution having the fewest possible number of moves, the idea being that an omniscient being would know an optimal step from any given configuration.", "links": ["15 puzzle", "Alexander's Star", "Anthony Michael Brooks", "Antoine Cantin", "BBC News", "Bob Burton, Jr.", "BrainTwist", "Chess", "Chris Hardwick (speed cuber)", "Collin Burns", "Combination puzzle", "Combinatorial", "David Singmaster", "Deep Blue (chess computer)", "Divine move", "Dogic", "Draughts", "Eric Limeback", "Erik Akkersdijk", "Ern\u0151 Rubik", "Feliks Zemdegs", "Fifteen puzzle", "Frank Morris (cube solver)", "Fridrich Method", "Gilles Roux", "Go (game)", "Graph (mathematics)", "Helicopter Cube", "Impossiball", "Jessica Fridrich", "Kevin Hays (speedsolver)", "Lars Petrus", "Layer by Layer", "Leyan Lo", "Logic puzzle", "Lookup table", "Mathematical game", "Mathematical jargon", "Mathematical model", "Mechanical puzzle", "Megaminx", "Micha\u0142 Halczuk", "Missing Link (puzzle)", "Missionaries and cannibals problem", "Morwen Thistlethwaite", "N-dimensional sequential move puzzle", "NP-hard", "Omniscient", "Optimal solutions for Rubik's Cube", "Oracle machine", "Peg solitaire", "Pocket Cube", "Professor's Cube", "Proofs from THE BOOK", "Puzzle", "Pyraminx", "Pyraminx Crystal", "Pyramorphix", "Ron van Bruchem", "Rowe Hessler", "Rubik's 360", "Rubik's Clock", "Rubik's Cube", "Rubik's Cube group", "Rubik's Domino", "Rubik's Magic", "Rubik's Magic: Master Edition", "Rubik's Revenge", "Rubik's Revolution", "Rubik's Snake", "Rubik's Triamid", "Sebastian Weyer", "Shotaro \"Macky\" Makisumi", "Skewb", "Skewb Diamond", "Skewb Ultimate", "Speedcubing", "Square One (puzzle)", "Sudoku Cube", "Toby Mao", "Towers of Hanoi", "Tuttminx", "Tyson Mao", "Upper bound", "V-Cube 6", "V-Cube 7", "Void Cube", "Weston Mizumoto", "World Cube Association", "Yu Da-Hyun", "Yu Nakajima"], "categories": ["Logic puzzles", "Mathematical games", "Rubik's Cube", "Search algorithms"], "title": "God's algorithm"}
{"summary": "Graphplan is an algorithm for automated planning developed by Avrim Blum and Merrick Furst in 1995. Graphplan takes as input a planning problem expressed in STRIPS and produces, if one is possible, a sequence of operations for reaching a goal state.\nThe name graphplan is due to the use of a novel planning graph, to reduce the amount of search needed to find the solution from straightforward exploration of the state space graph.\nIn the state space graph:\nthe nodes are possible states,\nand the edges indicate reachability through a certain action.\nOn the contrary, in Graphplan's planning graph:\nthe nodes are actions and atomic facts, arranged into alternate levels,\nand the edges are of two kinds:\nfrom an atomic fact to the actions for which it is a condition,\nfrom an action to the atomic facts it makes true or false.\n\nthe first level contains true atomic facts identifying the initial state.\nLists of incompatible facts that cannot be true at the same time and incompatible actions that cannot be executed together are also maintained.\nThe algorithm then iteratively extends the planning graph, proving that there are no solutions of length l-1 before looking for plans of length l by backward chaining: supposing the goals are true, Graphplan looks for the actions and previous states from which the goals can be reached, pruning as many of them as possible thanks to incompatibility information.\nA closely related approach to planning is the Planning as Satisfiability (Satplan). Both reduce the automated planning problem to search for plans of different fixed horizon lengths.", "links": ["Algorithm", "Automated planning", "Avrim Blum", "Fact", "Graph (mathematics)", "Length", "Merrick L. Furst", "STRIPS", "Satplan"], "categories": ["Automated planning and scheduling", "Search algorithms"], "title": "Graphplan"}
{"summary": "Hopscotch hashing is a scheme in computer programming for resolving hash collisions of values of hash functions in a table using open addressing. It is also well suited for implementing a concurrent hash table. Hopscotch hashing was introduced by Maurice Herlihy, Nir Shavit and Moran Tzafrir in 2008. The name is derived from the sequence of hops that characterize the table's insertion algorithm.\n\nThe algorithm uses a single array of n buckets. For each bucket, its neighborhood is a small collection of nearby consecutive buckets (i.e. one with close indexes to the original hashed bucket). The desired property of the neighborhood is that the cost of finding an item in the buckets of the neighborhood is close to the cost of finding it in the bucket itself (for example, by having buckets in the neighborhood fall within the same cache line). The size of the neighborhood must be sufficient to accommodate a logarithmic number of items in the worst case (i.e. it must accommodate log(n) items), but only a constant number on average. If some bucket's neighborhood is filled, the table is resized.\nIn hopscotch hashing, as in cuckoo hashing, and unlike in linear probing, a given item will always be inserted-into and found-in the neighborhood of its hashed bucket. In other words, it will always be found either in its original hashed array entry, or in one of the next H-1 neighboring entries. H could, for example, be 32, the standard machine word size. The neighborhood is thus a \"virtual\" bucket that has fixed size and overlaps with the next H-1 buckets. To speed the search, each bucket (array entry) includes a \"hop-information\" word, an H-bit bitmap that indicates which of the next H-1 entries contain items that hashed to the current entry's virtual bucket. In this way, an item can be found quickly by looking at the word to see which entries belong to the bucket, and then scanning through the constant number of entries (most modern processors support special bit manipulation operations that make the lookup in the \"hop-information\" bitmap very fast).\nHere is how to add item x which was hashed to bucket i:\nIf the entry i is empty, add x to i and return.\nStarting at entry i, use a linear probe to find an empty entry at index j.\nIf the empty entry's index j is within H-1 of entry i, place x there and return. Otherwise, entry j is too far from i. To create an empty entry closer to i, find an item y whose hash value lies between i and j, but within H-1 of j. Displacing y to j creates a new empty slot closer to i. Repeat until the empty entry is within H-1 of entry i, place x there and return. If no such item y exists, or if the bucket i already contains H items, resize and rehash the table.\nThe idea is that hopscotch hashing \"moves the empty slot towards the desired bucket\". This distinguishes it from linear probing which leaves the empty slot where it was found, possibly far away from the original bucket, or from cuckoo hashing that, in order to create a free bucket, moves an item out of one of the desired buckets in the target arrays, and only then tries to find the displaced item a new place.\nTo remove an item from the table, one simply removes it from the table entry. If the neighborhood buckets are cache aligned, then one could apply a reorganization operation in which items are moved into the now vacant location in order to improve alignment.\nOne advantage of hopscotch hashing is that it provides good performance at very high table load factors, even ones exceeding 0.9. Part of this efficiency is due to using a linear probe only to find an empty slot during insertion, not for every lookup as in the original linear probing hash table algorithm. Another advantage is that one can use any hash function, in particular simple ones that are close-to-universal.", "links": ["Cache line", "Computer programming", "Concurrent hash table", "Cuckoo hashing", "Hash collision", "Hash function", "Hash table", "Linear probing", "Maurice Herlihy", "Nir Shavit", "Open addressing", "Perfect hashing", "Quadratic probing"], "categories": ["Hashing", "Search algorithms"], "title": "Hopscotch hashing"}
{"summary": "Incremental heuristic search algorithms combine both incremental and heuristic search to speed up searches of sequences of similar search problems, which is important in domains that are only incompletely known or change dynamically. Incremental search has been studied at least since the late 1960s. Incremental search algorithms reuse information from previous searches to speed up the current search and solve search problems potentially much faster than solving them repeatedly from scratch. Similarly, heuristic search has been studied at least since the late 1960s. Heuristic search algorithms, often based on A*, use heuristic knowledge in the form of approximations of the goal distances to focus the search and solve search problems potentially much faster than uninformed search algorithms. The resulting search problems, sometimes called dynamic path planning problems, are graph search problems where paths have to be found repeatedly because the topology of the graph, its edge costs, the start vertex or the goal vertices change over time.\nSo far, three main classes of incremental heuristic search algorithms have been developed:\nThe first class restarts A* at the point where its current search deviates from the previous one (example: Fringe Saving A*).\nThe second class updates the h-values from the previous search during the current search to make them more informed (example: Generalized Adaptive A*).\nThe third class updates the g-values from the previous search during the current search to correct them when necessary, which can be interpreted as transforming the A* search tree from the previous search into the A* search tree for the current search (examples: Lifelong Planning A*, D*, D* Lite).\nAll three classes of incremental heuristic search algorithms are different from other replanning algorithms, such as planning by analogy, in that their plan quality does not deteriorate with the number of replanning episodes.", "links": ["A*", "A* search algorithm", "Alpha\u2013beta pruning", "B*", "Backtracking", "Beam search", "Bellman\u2013Ford algorithm", "Best-first search", "Bidirectional search", "Bor\u016fvka's algorithm", "Branch and bound", "Breadth-first search", "British Museum algorithm", "D*", "Depth-first search", "Depth-limited search", "Dijkstra's algorithm", "Dynamic programming", "Edmonds' algorithm", "Floyd\u2013Warshall algorithm", "Fringe search", "Graph traversal", "Hill climbing", "Iterative deepening A*", "Iterative deepening depth-first search", "Johnson's algorithm", "Jump point search", "Kruskal's algorithm", "Lexicographic breadth-first search", "List of algorithms", "Narsingh Deo", "Prim's algorithm", "Robotics", "SMA*", "Search game", "Topology", "Tree traversal"], "categories": ["Artificial intelligence", "Robot control", "Search algorithms"], "title": "Incremental heuristic search"}
{"summary": "Interpolation search (sometimes referred to as extrapolation search) is an algorithm for searching for a given key value in an indexed array that has been ordered by the values of the key. It parallels how humans search through a telephone book for a particular name, the key value by which the book's entries are ordered. In each search step it calculates where in the remaining search space the sought item might be, based on the key values at the bounds of the search space and the value of the sought key, usually via a linear interpolation. The key value actually found at this estimated position is then compared to the key value being sought. If it is not equal, then depending on the comparison, the remaining search space is reduced to the part before or after the estimated position. This method will only work if calculations on the size of differences between key values are sensible.\nBy comparison, the binary search always chooses the middle of the remaining search space, discarding one half or the other, again depending on the comparison between the key value found at the estimated position and the key value sought. The remaining search space is reduced to the part before or after the estimated position. The linear search uses equality only as it compares elements one-by-one from the start, ignoring any sorting.\nOn average the interpolation search makes about log(log(n)) comparisons (if the elements are uniformly distributed), where n is the number of elements to be searched. In the worst case (for instance where the numerical values of the keys increase exponentially) it can make up to O(n) comparisons.\nIn interpolation-sequential search, interpolation is used to find an item near the one being searched for, then linear search is used to find the exact item.", "links": ["Algorithm", "B-tree", "Big-O notation", "Big O notation", "Binary search", "Binary search algorithm", "C++", "Collation", "Exponential search", "Hash table", "Linear search", "Mathematical optimization", "Online algorithm", "Search algorithm", "Ternary search", "Three-way comparison"], "categories": ["Search algorithms"], "title": "Interpolation search"}
{"summary": "In computer science, a jump search or block search refers to a search algorithm for ordered lists. It works by first checking all items Lkm, where  and m is the block size, until an item is found that is larger than the search key. To find the exact position of the search key in the list a linear search is performed on the sublist L[(k-1)m, km].\nThe optimal value of m is \u221an, where n is the length of the list L. Because both steps of the algorithm look at, at most, \u221an items the algorithm runs in O(\u221an) time. This is better than a linear search, but worse than a binary search. The advantage over the latter is that a jump search only needs to jump backwards once, while a binary can jump backwards up to log n times. This can be important if a jumping backwards takes significantly more time than jumping forward.\nThe algorithm can be modified by performing multiple levels of jump search on the sublists, before finally performing the linear search. For an k-level jump search the optimum block size ml for the lth level (counting from 1) is n(k-l)/k. The modified algorithm will perform k backward jumps and runs in O(kn1/(k+1)) time.", "links": ["Algorithm", "Ben Shneiderman", "Binary search", "Computer science", "Dictionary of Algorithms and Data Structures", "Interpolation search", "Jump list", "Linear search", "List (computing)", "National Institute of Standards and Technology", "Search algorithm", "Search key", "Sublist"], "categories": ["Search algorithms"], "title": "Jump search"}
{"summary": "\"Algorithm X\" is the name Donald Knuth used in his paper \"Dancing Links\" to refer to \"the most obvious trial-and-error approach\" for finding all solutions to the exact cover problem. Technically, Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm. While Algorithm X is generally useful as a succinct explanation of how the exact cover problem may be solved, Knuth's intent in presenting it was merely to demonstrate the utility of the dancing links technique via an efficient implementation he called DLX.\nThe exact cover problem is represented in Algorithm X using a matrix A consisting of 0s and 1s. The goal is to select a subset of the rows so that the digit 1 appears in each column exactly once.\nAlgorithm X functions as follows:\n\nThe nondeterministic choice of r means that the algorithm essentially clones itself into independent subalgorithms; each subalgorithm inherits the current matrix A, but reduces it with respect to a different row r. If column c is entirely zero, there are no subalgorithms and the process terminates unsuccessfully.\nThe subalgorithms form a search tree in a natural way, with the original problem at the root and with level k containing each subalgorithm that corresponds to k chosen rows. Backtracking is the process of traversing the tree in preorder, depth first.\nAny systematic rule for choosing column c in this procedure will find all solutions, but some rules work much better than others. To reduce the number of iterations, Knuth suggests that the column choosing algorithm select a column with the lowest number of 1s in it.", "links": ["-yllion", "AMS Euler", "Algorithm", "Algorithm X", "ArXiv", "Backtracking", "CWEB", "Computer Modern", "Computers and Typesetting", "Concrete Mathematics", "Concrete Roman", "Dancing Links", "Dancing links", "Depth-first", "Deterministic algorithm", "Dijkstra's algorithm", "Donald Knuth", "Doubly linked list", "Exact cover", "Fisher\u2013Yates shuffle", "Font", "GNU MIX Development Kit", "International Standard Book Number", "Knuth's Simpath algorithm", "Knuth Prize", "Knuth reward check", "Knuth\u2013Bendix completion algorithm", "Knuth\u2013Morris\u2013Pratt algorithm", "Literate programming", "METAFONT", "MIX", "MMIX", "Man or boy test", "Nondeterministic algorithm", "Potrzebie", "Quater-imaginary base", "Recursion (computer science)", "Robinson\u2013Schensted\u2013Knuth correspondence", "Search tree", "Selected papers series of Knuth", "Software", "Surreal Numbers (book)", "TeX", "The Art of Computer Programming", "The Complexity of Songs", "Things a Computer Scientist Rarely Talks About", "Trabb Pardo\u2013Knuth algorithm", "WEB"], "categories": ["Donald Knuth", "Search algorithms"], "title": "Knuth's Algorithm X"}
{"summary": "Late Move Reductions (LMR) is a non-game specific enhancement to the alpha-beta algorithm and its variants which attempts to examine a game search tree more efficiently. It uses the assumption that good game-specific move ordering causes a program to search the most likely moves early. If a cut-off is going to happen in a search, the first few moves are the ones most likely to cause them. In games like chess, most programs search winning captures and \"killers\" first. LMR will reduce the search depth for moves searched later at a given node. This allows the program to search deeper along the critical lines, and play better.\nMost chess programs will search the first several moves at a node to full depth. Often, they do not reduce moves considered to be very tactical, such as captures or promotions. If the score of the move at a reduced depth is smaller than the alpha, the move is assumed to be bad. However, if the score is larger than alpha, the reduced tells us nothing so we will have to do a full search (fail-low).\nThis search reduction can lead to a different search space than the pure alpha-beta method which can give different results. Care must be taken to select the reduction criteria or the search will miss some deep threats.", "links": ["Alpha-beta pruning", "Chess", "Computer chess", "Game tree", "Killer heuristic"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from March 2014", "Computer chess", "Search algorithms"], "title": "Late Move Reductions"}
{"summary": "Linear hashing is a dynamic hash table algorithm invented by Witold Litwin (1980), and later popularized by Paul Larson. Linear hashing allows for the expansion of the hash table one slot at a time. The frequent single slot expansion can very effectively control the length of the collision chain. The cost of hash table expansion is spread out across each hash table insertion operation, as opposed to being incurred all at once. Linear hashing is therefore well suited for interactive applications.", "links": ["Bill Griswold", "Consistent hashing", "Dictionary of Algorithms and Data Structures", "Digital object identifier", "Dynamic array", "Extendible hashing", "Hash function", "Hash table", "Icon language", "National Institute of Standards and Technology", "Paul Larson"], "categories": ["Hashing", "Search algorithms"], "title": "Linear hashing"}
{"summary": "In computer science, linear search or sequential search is a method for finding a particular value in a list that checks each element in sequence until the desired element is found or the list is exhausted. The list need not be ordered.\nLinear search is the simplest search algorithm; it is a special case of brute-force search. Its worst case cost is proportional to the number of elements in the list. Its expected cost is also proportional to the number of elements if all elements are searched equally. If the list has more than a few elements and is searched often, then more complicated search methods such as binary search or hashing may be appropriate. Those methods have faster search times but require additional resources to attain that speed.", "links": ["Array data structure", "Array index", "Assembly language", "Asymptotic complexity", "Average-case complexity", "Big O notation", "Binary search", "Branch instruction", "Brute-force search", "Compiler", "Computer science", "Donald Knuth", "Geometric distribution", "Hash table", "High-level programming languages", "Instruction (computer science)", "International Standard Book Number", "Linear search problem", "Linked list", "List (computing)", "Machine language", "Null pointer", "Pseudocode", "Recursion", "Reference (computer science)", "Search algorithm", "Search data structure", "Sentinel value", "Sort (computing)", "Ternary search", "Worst-case complexity"], "categories": ["All articles needing additional references", "Articles needing additional references from November 2010", "Articles with example Java code", "Articles with example pseudocode", "Search algorithms"], "title": "Linear search"}
{"summary": "In backtracking algorithms, look ahead is the generic term for a subprocedure that attempts to foresee the effects of choosing a branching variable to evaluate or one of its values. The two main aims of look-ahead are to choose a variable to evaluate next and the order of values to assign to it.", "links": ["Algorithm", "Arc consistency", "Backtracking", "Branch (computer science)", "Consistent", "Constraint satisfaction problem", "Digital object identifier", "Recursion", "Subroutine", "Variable (programming)"], "categories": ["Constraint programming", "Search algorithms"], "title": "Look-ahead (backtracking)"}
{"summary": "MaMF, or Mammalian Motif Finder, is an algorithm for identifying motifs to which transcription factors bind.\nThe algorithm takes as input a set of promoter sequences, and a motif width(w), and as output, produces a ranked list of 30 predicted motifs(each motif is defined by a set of N sequences, where N is a parameter).\nThe algorithm firstly indexes each sub-sequence of length n, where n is a parameter around 4-6 base pairs, in each promoter, so they can be looked up efficiently. This index is then used to build a list of all pairs of sequences of length w, such that each sequence shares an n-mer, and each sequence forms an ungapped alignment with a substring of length w from the string of length 2w around the match, with a score exceeding a cut-off.\nThe pairs of sequences are then scored. The scoring function favours pairs which are very similar, but disfavours sequences which are very common in the target genome. The 1000 highest scoring pairs are kept, and the others are discarded. Each of these 1000 'seed' motifs are then used to search iteratively search for further sequences of length which maximise the score(a greedy algorithm), until N sequences for that motif are reached.\nVery similar motifs are discarded, and the 30 highest scoring motifs are returned as output.", "links": ["Base pairs", "Greedy algorithm", "Index (database)", "N-mer", "Promoter (biology)", "Sequence alignment", "Sequence motif", "Transcription factors"], "categories": ["All orphaned articles", "Bioinformatics", "Orphaned articles from October 2008", "Search algorithms"], "title": "MaMF"}
{"summary": "Mobilegeddon is a name given by webmasters and web-developers to the Google's algorithm update of April 21, 2015. The main effect of this update is give priority to web sites that display well on smartphones and other mobile devices. The change does not affect searches made from a desktop computer or a laptop.\nGoogle announced its intention to make the change in February 2015. The Economist found the timing \"awkward\" because they said \"It comes less than a week after the European Union accused the firm...\" of anti-competitive behaviors.\nThe protologism is a blend word of \"mobile\" and \"Armageddon\" because the change \"could cause massive disruption to page rankings.\" But, writing for Forbes, Robert Hof says that concerns about the change were \"overblown\" in part because \"Google is providing a test to see if sites look good on smartphones\".\nSearch engine results pages on smartphones now show URLs in \"breadcrumb\" format, as opposed to the previous explicit format.", "links": ["Algorithm", "Armageddon", "Blend word", "Forbes", "Mobile Web", "Moz (marketing software)", "Search engine optimization", "Search engine results page", "Searchmetrics", "The Economist"], "categories": ["All orphaned articles", "Google", "Orphaned articles from July 2015", "Search algorithms", "Search engine optimization"], "title": "Mobilegeddon"}
{"summary": "MTD(f), is a minimax search algorithm, developed in 1994 by Aske Plaat, Jonathan Schaeffer, Wim Pijls, and Arie de Bruin. Experiments with tournament-quality chess, checkers, and Othello programs show it to be the most efficient minimax algorithm. The name MTD(f) is an abbreviation for MTD(n,f) (Memory-enhanced Test Driver with node n and value f). It is an alternative to the alpha-beta pruning algorithm.", "links": ["Alpha-Beta", "Alpha-beta pruning", "Arie de Bruin", "Aske Plaat", "Chess engine", "Chinook (draughts player)", "Digital object identifier", "George Stockman", "Jonathan Schaeffer", "Judea Pearl", "Keyano", "Minimax", "NegaScout", "Negascout", "Phoenix (chess program)", "Recursion (computer science)", "SSS*", "Transposition table", "Wim Pijls"], "categories": ["Articles with example pseudocode", "Search algorithms"], "title": "MTD-f"}
{"summary": "In computer chess programs, the null-move heuristic is a heuristic technique used to enhance the speed of the alpha-beta pruning algorithm.", "links": ["Alpha-beta pruning", "ArXiv", "Chess", "Computer chess", "Game tree", "Heuristic (computer science)", "ICGA Journal", "Jonathan Schaeffer", "Minimax algorithm", "Murray Campbell", "Nathan Netanyahu", "Null move", "Ply (game theory)", "Search algorithm", "Zugzwang"], "categories": ["Computer chess", "Heuristics", "Search algorithms"], "title": "Null-move heuristic"}
{"summary": "In computer science, Union Find is an algorithm for doing certain operations on sets. This page is about proof of O(log*n) amortized time  of Union Find\nStatement: If m operations, either Union or Find, are applied to n elements, the total run time is O(m log*n), where log* is the iterated logarithm.", "links": ["Algorithm", "Amortized analysis", "Computer science", "Digital object identifier", "Disjoint-set data structure", "Iterated logarithm", "Jan van Leeuwen", "Raimund Seidel", "Robert E. Tarjan"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from July 2012", "Search algorithms"], "title": "Proof of O(log*n) time complexity of union\u2013find"}
{"summary": "Rapidly exploring dense trees is a family of planning algorithms that includes the rapidly exploring random tree.", "links": ["Algorithms", "CiteSeer", "Rapidly exploring random tree"], "categories": ["All articles covered by WikiProject Wikify", "All articles with too few wikilinks", "Articles covered by WikiProject Wikify from October 2013", "Articles with too few wikilinks from October 2013", "Search algorithms"], "title": "Rapidly exploring dense trees"}
{"summary": "A search game is a two-person zero-sum game which takes place in a set called the search space. The searcher can choose any continuous trajectory subject to a maximal velocity constraint. It is always assumed that neither the searcher nor the hider has any knowledge about the movement of the other player until their distance apart is less than or equal to the discovery radius and at this very moment capture occurs. As mathematical models, search games can be applied to areas such as hide-and-seek games that children play or representations of some tactical military situations. The area of search games was introduced in the last chapter of Rufus Isaacs' classic book \"Differential Games\" and has been developed further by Shmuel Gal  and Steve Alpern.\nWhat is the best way to search a stationary target in a graph? A natural strategy is to find a minimal closed curve L that covers all the arcs of the graph. (L is called a Chinese postman tour). Then, traverse L with probability 1/2 for each direction. This strategy seems to work well if the graph is Eulerian. In general, this random Chinese postman tour is indeed an optimal search strategy if and only if the graph consists of a set of Eulerian graphs connected in a tree-like structure. A misleadingly simple example of a graph not in this family consists of two nodes connected by three arcs. The random Chinese postman tour (equivalent to traversing the three arcs in a random order) is not optimal. The optimal way to search these three arcs is surprisingly complicated [2] .\nThe princess and monster game deals with a moving target.\nSearching unbounded domains is also interesting. In general, the reasonable framework, as in the case of an online algorithm, is to use a normalized cost function (called the competitive ratio in Computer Science literature). The minimax trajectory for problems of these types is always a geometric sequence (or exponential function for continuous problems). This result yields an easy method to find the minimax trajectory by minimizing over a single parameter (the generator of this sequence) instead of searching over the whole trajectory space. This tool has been used for the linear search problem, i.e., finding a target on the infinite line, which has attracted much attention over several decades and has been analyzed as a search game. It has also been used to find a minimax trajectory for searching a set of concurrent rays. Optimal searching in the plane is performed by using exponential spirals. Searching a set of concurrent rays was later re-discovered in Computer Science literature as the 'cow-path problem'.", "links": ["Albert W. Tucker", "All-pay auction", "Alpha\u2013beta pruning", "Amos Tversky", "Ariel Rubinstein", "Arrow's impossibility theorem", "Backward induction", "Bargaining problem", "Battle of the sexes (game theory)", "Bayesian game", "Bertrand paradox (economics)", "Blotto games", "Bounded rationality", "Centipede game", "Cheap talk", "Chicken (game)", "Chinese postman", "Collusion", "Combinatorial game theory", "Competitive analysis (online algorithm)", "Confrontation analysis", "Cooperative game", "Coopetition", "Coordination game", "Core (game theory)", "Correlated equilibrium", "Cournot competition", "Daniel Kahneman", "David K. Levine", "David M. Kreps", "Deadlock (game theory)", "Dictator game", "Dollar auction", "Dominance (game theory)", "Donald B. Gillies", "Drew Fudenberg", "Economic equilibrium", "El Farol Bar problem", "Epsilon-equilibrium", "Eric Maskin", "Escalation of commitment", "Eulerian path", "Evolutionarily stable strategy", "Extensive-form game", "Fair cake-cutting", "Fair division", "Folk theorem (game theory)", "Forward induction", "Game theory", "Global games", "Graphical game theory", "Grim trigger", "Guess 2/3 of the average", "Harold W. Kuhn", "Herbert A. Simon", "Herv\u00e9 Moulin", "Hierarchy of beliefs", "Information set (game theory)", "Jean-Fran\u00e7ois Mertens", "Jean Tirole", "John Forbes Nash, Jr.", "John Harsanyi", "John Maynard Smith", "John von Neumann", "Kenneth Arrow", "Kenneth Binmore", "Kuhn poker", "Large Poisson game", "Leonid Hurwicz", "Linear search problem", "List of game theorists", "List of games in game theory", "Lloyd Shapley", "Loss function", "Markov perfect equilibrium", "Markov strategy", "Matching pennies", "Mechanism design", "Melvin Dresher", "Merrill M. Flood", "Mertens-stable equilibrium", "Minimax", "Monty Hall problem", "N-player game", "Nash bargaining game", "Nash equilibrium", "No-win situation", "Nontransitive game", "Normal-form game", "Online algorithm", "Oskar Morgenstern", "Pareto efficiency", "Paul Milgrom", "Perfect information", "Peyton Young", "Pirate game", "Preference (economics)", "Princess and monster game", "Prisoner's dilemma", "Prisoners and hats puzzle", "Proper equilibrium", "Public goods game", "Purification theorem", "Quantal response equilibrium", "Quasi-perfect equilibrium", "Reinhard Selten", "Rendezvous problem", "Repeated game", "Revelation principle", "Risk dominance", "Robert Aumann", "Robert B. Wilson", "Rock-paper-scissors", "Roger Myerson", "Rufus Isaacs (game theorist)", "Samuel Bowles (economist)", "Screening game", "Self-confirming equilibrium", "Sequential equilibrium", "Sequential game", "Set (mathematics)", "Shapley value", "Shmuel Gal", "Signaling game", "Simultaneous game", "Solution concept", "Stag hunt", "Steve Alpern", "Stochastic game", "Strategy (game theory)", "Strictly determined game", "Strong Nash equilibrium", "Subgame perfect equilibrium", "Succinct game", "Symmetric game", "Thomas Schelling", "Tit for tat", "Tragedy of the commons", "Traveler's dilemma", "Trembling hand perfect equilibrium", "Tyranny of small decisions", "Ultimatum game", "Unscrupulous diner's dilemma", "Volunteer's dilemma", "War of attrition (game)", "William Vickrey", "Zero-sum game"], "categories": ["Game theory", "Search algorithms"], "title": "Search game"}
{"summary": "SSS* is a search algorithm, introduced by George Stockman in 1979, that conducts a state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithm.\nSSS* is based on the notion of solution trees. Informally, a solution tree can be formed from any arbitrary game tree by pruning the number of branches at each MAX node to one. Such a tree represents a complete strategy for MAX, since it specifies exactly one MAX action for every possible sequence of moves might be made by the opponent. Given a game tree, SSS* searches through the space of partial solution trees, gradually analyzing larger and larger subtrees, eventually producing a single solution tree with the same root and Minimax value as the original game tree. SSS* never examines a node that alpha-beta pruning would prune, and may prune some branches that alpha-beta would not. Stockman speculated that SSS* may therefore be a better general algorithm than alpha-beta. However, Igor Roizen and Judea Pearl have shown that the savings in the number of positions that SSS* evaluates relative to alpha/beta is limited and generally not enough to compensate for the increase in other resources (e.g., the storing and sorting of a list of nodes made necessary by the best-first nature of the algorithm). However, Aske Plaat, Jonathan Schaeffer, Wim Pijls and Arie de Bruin have shown that a sequence of null-window alpha-beta calls is equivalent to SSS* (i.e., it expands the same nodes in the same order) when alpha-beta is used with a transposition table, as is the case in all game-playing programs for chess, checkers, etc. Now the storing and sorting of the OPEN list were no longer necessary. This allowed the implementation of (an algorithm equivalent to) SSS* in tournament quality game-playing programs. Experiments showed that it did indeed perform better than Alpha-Beta in practice, but that it did not beat NegaScout.\nThe reformulation of a best-first algorithm as a sequence of depth-first calls prompted the formulation of a class of null-window alpha-beta algorithms, of which MTD-f is the best known example.", "links": ["A* search algorithm", "Alpha-Beta", "Alpha-beta pruning", "Aske Plaat", "Best-first search", "Dewey's notation", "Digital object identifier", "Game tree", "Igor Roizen", "Jonathan Schaeffer", "Judea Pearl", "MTD-f", "Minimax", "NegaScout", "Priority queue", "Search algorithm", "Solution tree", "State space search", "Transposition table"], "categories": ["All articles needing additional references", "Articles needing additional references from February 2010", "Search algorithms"], "title": "SSS*"}
{"summary": "Stack search (also known as Stack decoding algorithm) is a search algorithm similar to beam search. It can be used to explore tree-structured search spaces and is often employed in Natural language processing applications, such as parsing of natural languages, or for decoding of error correcting codes where the technique goes under the name of sequential decoding.\nStack search keeps a list of the best n candidates seen so far. These candidates are incomplete solutions to the search problems, e.g. partial parse trees. It then iteratively expands the best partial solution, putting all resulting partial solutions onto the stack and then trimming the resulting list of partial solutions to the top n candidates, until a real solution (i.e. complete parse tree) has been found.\nStack search is not guaranteed to find the optimal solution to the search problem. The quality of the result depends on the quality of the search heuristic.", "links": ["Beam search", "Error correcting code", "Natural language processing", "Sequential decoding"], "categories": ["All stub articles", "Computing stubs", "Search algorithms"], "title": "Stack search"}
{"summary": "Uniform binary search is an optimization of the classic binary search algorithm invented by Donald Knuth and given in Knuth's The Art of Computer Programming. It uses a lookup table to update a single array index, rather than taking the midpoint of an upper and a lower bound on each iteration; therefore, it is optimized for architectures (such as Knuth's MIX) on which\na table lookup is generally faster than an addition and a shift, and\nmany searches will be performed on the same array, or on several arrays of the same length", "links": ["Binary search", "Binary search algorithm", "C (programming language)", "Donald Knuth", "Lookup table", "MIX", "Pascal (programming language)", "The Art of Computer Programming"], "categories": ["All orphaned articles", "Articles with example C code", "Orphaned articles from February 2009", "Search algorithms"], "title": "Uniform binary search"}
{"summary": "UUHash is a hash algorithm employed by clients on the FastTrack network. It is employed for its ability to hash very large files in a very short period of time, even on older computers. However, this is achieved by only hashing a fraction of the file. This weakness makes it trivial to create a hash collision, allowing large sections to be completely altered without altering the checksum.\nThis method is used by Kazaa. The weakness of UUHash is exploited by anti-p2p agencies to corrupt downloads.", "links": ["Base64", "CRC32", "Checksum", "Data corruption", "FastTrack", "Hash function", "Kazaa", "Kibibyte", "Little endian", "MD5", "Peer-to-peer file sharing", "Request for Comments", "URI", "Uniform Resource Identifier"], "categories": ["Search algorithms"], "title": "UUHash"}
{"summary": "A Variation can refer to a specific sequence of successive moves in a turn-based game, often used to specify a hypothetical future state of a game that is being played. Although the term is most commonly used in the context of Chess analysis, it has been applied to other games. It also is a useful term used when describing computer tree-search algorithms (for example minimax) for playing games such as Go or Chess.\nA variation can be any number of steps as long as each step would be legal if it were to be played. It is often as far ahead as a human or computer can calculate; or however long is necessary to reach a particular position of interest. It may also lead to a terminal state in the game, in which case the term \"Winning Variation\" or \"Losing Variation\" is sometimes used.", "links": ["Alpha-beta pruning", "Artificial Intelligence", "Backtracking", "Chess", "Game tree", "Go (board game)", "Minimax", "Minimax algorithm", "Negamax", "Negascout"], "categories": ["Combinatorial game theory", "Computer chess", "Game artificial intelligence", "Search algorithms", "Trees (graph theory)"], "title": "Variation (game tree)"}
{"summary": "Perceptual hashing is the use of an algorithm that produces a snippet or fingerprint of various forms of multimedia. Perceptual hash functions are analogous if features are similar, whereas cryptographic rely on the avalanche effect of a small change in input value creating a drastic change in output value. Perceptual hash functions are widely used to protect against copyright infringement and digital forensics because of the ability to have a correlation between hashes so you can compare and map source data. For example, Wikipedia could maintain a database of text hashes of popular online books or articles for which the authors hold copyrights to, anytime a Wikipedia user uploads an online book or article that has a copyright, the hashes will be almost exactly the same and could be flagged as plagiarism. This same flagging system can be used for any multimedia or text file.", "links": ["Algorithm", "Cryptographic hash function", "Data structure", "Multimedia"], "categories": ["Algorithms and data structures stubs", "All articles covered by WikiProject Wikify", "All articles with too few wikilinks", "All stub articles", "Articles covered by WikiProject Wikify from November 2014", "Articles with too few wikilinks from November 2014", "Computer science stubs", "Hashing"], "title": "Perceptual hashing"}
{"summary": "A focused crawler is a web crawler that collects Web pages that satisfy some specific property, by carefully prioritizing the crawl frontier and managing the hyperlink exploration process. Some predicates may be based on simple, deterministic and surface properties. For example, a crawler's mission may be to crawl pages from only the .jp domain. Other predicates may be softer or comparative, e.g., \"crawl pages with large PageRank\", or \"crawl pages about baseball\". An important page property pertains to topics, leading to topical crawlers. For example, a topical crawler may be deployed to collect pages about solar power, or swine flu, while minimizing resources spent fetching pages on other topics. Crawl frontier management may not be the only device used by focused crawlers; they may use a Web directory, an Web text index, backlinks, or any other Web artifact.\nA focused crawler must predict the probability that an unvisited page will be relevant before actually downloading the page. A possible predictor is the anchor text of links; this was the approach taken by Pinkerton  in a crawler developed in the early days of the Web. Topical crawling was first introduced by Filippo Menczer Chakrabarti et al. coined the term focused crawler and used a text classifier to prioritize the crawl frontier. Andrew McCallum and co-authors also used reinforcement learning to focus crawlers. Diligenti 'et al. traced the context graph leading up to relevant pages, and their text content, to train classifiers. A form of online reinforcement learning has been used along with features extracted from the DOM tree and text of linking pages, to continually train classifiers that guide the crawl. In a review of topical crawling algorithms, Menczer et al.  show that such simple strategies are very effective for short crawls, while more sophisticated techniques such as reinforcement learning and evolutionary adaptation can give the best performance over longer crawls.\nAnother type of focused crawlers is semantic focused crawler, which makes use of domain ontologies to represent topical maps and link Web pages with relevant ontological concepts for the selection and categorization purposes. In addition, ontologies can be automatically updated in the crawling process. Dong et al. introduced such an ontology-learning-based crawler using support vector machine to update the content of ontological concepts when crawling Web Pages.\nCrawlers are also focused on page properties other than topics. Cho et al. study a variety of crawl prioritization policies and their effects on the link popularity of fetched pages. Najork and Weiner show that breadth-first crawling, starting from popular seed pages, leads to collecting large-PageRank pages early in the crawl. Refinements involving detection of stale (poorly maintained) pages have been reported by Eiron et al.\nThe performance of a focused crawler depends on the richness of links in the specific topic being searched, and focused crawling usually relies on a general web search engine for providing starting points. Davison presented studies on Web links and text that explain why focused crawling succeeds on broad topics; similar studies were presented by Chakrabarti et al. Seed selection can be important for focused crawlers and significantly influence the crawling efficiency. A whitelist strategy is to start the focus crawl from a list of high quality seed URLs and limit the crawling scope to the domains of these URLs. These high quality seeds should be selected based on a list of URL candidates which are accumulated over a sufficient long period of general web crawling. The whitelist should be updated periodically after it is created.", "links": ["80legs", "Andrew McCallum", "Backlink", "Bingbot", "Breadth-first", "Collaborative search engine", "DOM tree", "Desktop search", "Distributed web crawler", "Distributed web crawling", "Document retrieval", "Domain name", "Enterprise search", "FAST Crawler", "Federated search", "Fetcher", "Filippo Menczer", "Googlebot", "HTTrack", "Heritrix", "Human flesh search engine", "ICDL crawler", "Image retrieval", "Index (search engine)", "Internet bot", "Internet search", "Inverted index", "List of search engines", "Local search (Internet)", "Metasearch engine", "Msnbot", "Multisearch", "Natural language search engine", "Online search", "OpenSearch", "PHP-Crawler", "PageRank", "Pandemonium (Webcrawler)", "PowerMapper", "RBSE", "Reinforcement learning", "Representational State Transfer", "Robots exclusion standard", "Search/Retrieve Web Service", "Search/Retrieve via URL", "Search aggregator", "Search engine", "Search engine (computing)", "Search engine marketing", "Search engine optimization", "Search oriented architecture", "Selection-based search", "Semantic search", "Social search", "Spider trap", "Text mining", "TkWWW robot", "Twiceler", "Uniform resource locator", "Vertical search", "Video search engine", "Voice search", "Web archiving", "Web crawler", "Web crawling", "Web directory", "Web indexing", "Web query classification", "Web search engine", "Web search query", "Website Parse Template", "Website mirroring software", "Wget", "Whitelist", "Wide area information server", "Yahoo! Slurp", "Z39.50"], "categories": ["Internet search algorithms", "Web crawlers", "World Wide Web"], "title": "Focused crawler"}
{"summary": "PageRank is an algorithm used by Google Search to rank websites in their search engine results. PageRank was named after Larry Page, one of the founders of Google. PageRank is a way of measuring the importance of website pages. According to Google:\n\nPageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites.\n\nIt is not the only algorithm used by Google to order search engine results, but it is the first algorithm that was used by the company, and it is the best-known.", "links": ["111 Eighth Avenue", "AI Challenge", "Aardvark (search engine)", "AdMob", "AdSense", "AdWords", "Adjacency matrix", "Adscape", "Al Gore", "Alan Eustace", "Alan Mulally", "Algorithm", "Alphabet Inc.", "Amit Singhal", "Analytic Hierarchy Process", "Android (operating system)", "Android Auto", "Android Pay", "Android TV", "Android Wear", "Android software development", "Android version history", "Ann Mather", "Apache Wave", "App Inventor for Android", "ArXiv", "Ask.com", "AtGoogleTalks", "Author Rank", "Baidu", "Bibcode", "BigTable", "Blog", "Blogger (service)", "Blogosphere", "CLEVER project", "Caja project", "Calico (company)", "Censorship by Google", "CheiRank", "Chrome OS", "Chrome Web Store", "Chrome Zone", "Chromebit", "Chromebook", "Chromebox", "Chromecast", "Citation analysis", "Cnn.com", "Coverage of Google Street View", "Criticism of Google", "Dart (programming language)", "David Drummond (Google)", "Digital object identifier", "Distributed algorithms", "Dodgeball (service)", "Don't be evil", "DoubleClick", "DoubleClick for Publishers", "EigenTrust", "Eigenfactor", "Eigenvalue", "Eigenvector", "Eigenvector centrality", "Eric Schmidt", "Eugene Garfield", "Expected value", "FeedBurner", "GData", "GNU Octave", "GOOG-411", "Game the system", "Gears (software)", "Gmail", "Gmail interface", "Go (programming language)", "Google", "Google+", "Google.org", "Google APIs", "Google Account", "Google Alerts", "Google Analytics", "Google Answers", "Google App Engine", "Google Apps Marketplace", "Google Apps Script", "Google Apps for Work", "Google Art Project", "Google Audio Indexing", "Google Authenticator", "Google Base", "Google Blog Search", "Google Bookmarks", "Google Books", "Google Books Library Project", "Google Browser Sync", "Google Business Groups", "Google Buzz", "Google Calendar", "Google Cardboard", "Google Checkout", "Google China", "Google Chrome", "Google Chrome Apps", "Google Chrome Experiments", "Google Chrome Frame", "Google Chrome extension", "Google Chrome for Android", "Google Chrome for iOS", "Google Classroom", "Google Closure Tools", "Google Cloud Connect", "Google Cloud Print", "Google Code-in", "Google Code Jam", "Google Code Search", "Google Compute Engine", "Google Contact Lens", "Google Contacts", "Google Contributor", "Google Current", "Google Currents", "Google Custom Search", "Google Data Liberation Front", "Google Desktop", "Google Developer Day", "Google Developer Expert", "Google Developers", "Google Dictionary", "Google Directory", "Google Docs, Sheets, and Slides", "Google Domains", "Google Doodle", "Google Drive", "Google Earth", "Google Earth Engine", "Google Earth Outreach", "Google Express", "Google Fast Flip", "Google Fiber", "Google File System", "Google Finance", "Google Fit", "Google Flights", "Google Friend Connect", "Google Gadgets", "Google Gadgets API", "Google Glass", "Google Goggles", "Google Grants", "Google Groups", "Google Guava", "Google Guice", "Google Hangouts", "Google Health", "Google Highly Open Participation Contest", "Google Hummingbird", "Google I/O", "Google IME", "Google Image Labeler", "Google Images", "Google Inc.", "Google Insights for Search", "Google Japanese Input", "Google Keep", "Google Kythe", "Google Labs", "Google Latitude", "Google Lively", "Google Lunar X Prize", "Google Map Maker", "Google Maps", "Google Mars", "Google Mashup Editor", "Google Moon", "Google My Maps", "Google Native Client", "Google News", "Google News & Weather", "Google News Archive", "Google Nexus", "Google Notebook", "Google Now", "Google Offers", "Google Pack", "Google Page Creator", "Google Panda", "Google Partners", "Google Patents", "Google Penguin", "Google Personalized Search", "Google Photos", "Google Pinyin", "Google Places", "Google Play", "Google Play Books", "Google Play Games", "Google Play Movies & TV", "Google Play Music", "Google Play Newsstand", "Google PowerMeter", "Google Public DNS", "Google Questions and Answers", "Google Reader", "Google Real-Time Search", "Google Scholar", "Google Science Fair", "Google Script Converter", "Google Search", "Google SearchWiki", "Google Search Appliance", "Google Search Console", "Google Searchology", "Google Shopping", "Google Sidewiki", "Google Sites", "Google Sky", "Google Squared", "Google Storage", "Google Street View", "Google Street View privacy concerns", "Google Summer of Code", "Google Swiffy", "Google Sync", "Google TV", "Google Takeout", "Google Talk", "Google Text-to-Speech", "Google Toolbar", "Google Translate", "Google Trends", "Google Ventures", "Google Video Marketplace", "Google Videos", "Google Voice", "Google Voice Search", "Google Wallet", "Google Web Accelerator", "Google Web History", "Google Web Toolkit", "Google Website Optimizer", "Google WiFi", "Google X", "Google bomb", "Google driverless car", "Google for Work", "Google logo", "Google matrix", "Google platform", "Google search", "Google search algorithm", "Google transliteration", "Googlebot", "Googleplex", "Googlization", "Goojje", "Graph (data structure)", "HITS algorithm", "HTML attribute", "HTTP 302", "Hilltop algorithm", "History of Gmail", "History of Google", "Hyper Search", "Hyperlink", "IGoogle", "Identity matrix", "Impact factor", "Inbox by Gmail", "Incoming link", "Institute for Scientific Information", "International Standard Book Number", "International Standard Serial Number", "Jaiku", "Jeff Dean (computer scientist)", "John Doerr", "John L. Hennessy", "Jon Kleinberg", "Keyhole Markup Language", "Knol", "Knowledge Graph", "Knowledge Vault", "Krishna Bharat", "Larry Page", "Lexical semantics", "Link farm", "Link love", "List of Google Doodles (1998\u20132009)", "List of Google Doodles in 2010", "List of Google Doodles in 2011", "List of Google Doodles in 2012", "List of Google Doodles in 2013", "List of Google Doodles in 2014", "List of Google Doodles in 2015", "List of Google domains", "List of Google easter eggs", "List of Google products", "List of mergers and acquisitions by Google", "List of street view services", "Logarithmic scale", "MATLAB", "Made with Code", "MapReduce", "Markov chain", "Markov process", "Massimo Marchiori", "Material Design", "Matt Cutts", "Mediabot", "Meta tag", "Methods of website linking", "Monopoly City Streets", "Motorola Mobility", "Network theory", "New Straits Times", "Nofollow", "Omid Kordestani", "OpenRefine", "OpenSocial", "Orkut", "Outline of Google", "Panoramio", "Patrick Pichette", "Paul Otellini", "Perron\u2013Frobenius theorem", "Picasa", "Picasa Web Albums", "Picnik", "Power iteration", "Power method", "Probability distribution", "Project Ara", "Project Loon", "Project Sunroof", "Project Tango", "PubMed Identifier", "Rachel Whetstone", "Rajeev Motwani", "Rajen Sheth", "Ram Shriram", "Rand Fishkin", "RankDex", "Ray Kurzweil", "Reciprocal link", "Recursion", "Robin Li", "Ruth Porat", "SCImago", "SEO", "SafeSearch", "Salar Kamangar", "Scale-free network", "Scientometrics", "Search Engine Optimization Metrics", "Search engine optimization", "Search engine results page", "Semantic link", "Semantic similarity", "Seomoz.org", "Sergey Brin", "Set (computer science)", "Shirley M. Tilghman", "SimRank", "Sitemaps", "Slide.com", "Software patent", "Spam in blogs", "Spamdexing", "Stanford University", "Steady state", "Stochastic matrix", "Sundar Pichai", "Susan Wojcicki", "Swiftype", "Synsets", "Teoma", "Terry Winograd", "Thomas Saaty", "Timeline of Google Search", "Topic-Sensitive PageRank", "Trade secret", "TrustRank", "Twitter", "Uniform Resource Locator", "United States dollar", "Unity (cable system)", "Urchin (software)", "Urs H\u00f6lzle", "Usa.gov", "Vevo", "Vint Cerf", "VisualRank", "Wayback Machine", "Web crawler", "Web page", "Webgraph", "Website spoofing", "Weighting", "Wikipedia", "WordNet", "Word Sense Disambiguation", "World Wide Web", "YouTube", "Zagat", "ZygoteBody"], "categories": ["All Wikipedia articles in need of updating", "All articles lacking reliable references", "All articles with unsourced statements", "American inventions", "Articles lacking reliable references from October 2012", "Articles with example MATLAB/Octave code", "Articles with inconsistent citation formats", "Articles with unsourced statements from June 2013", "Articles with unsourced statements from October 2015", "Crowdsourcing", "Google Search", "Internet search algorithms", "Link analysis", "Markov models", "Pages containing cite templates with deprecated parameters", "Reputation management", "Search engine optimization", "Wikipedia articles in need of updating from February 2014"], "title": "PageRank"}
{"summary": "In signal processing, the fast folding algorithm (Staelin, 1969) is an efficient algorithm for the detection of approximately-periodic events within time series data. It computes superpositions of the signal modulo various window sizes simultaneously.\nThe FFA is best known for its use in the detection of pulsars, as popularised by SETI@home and Astropulse.", "links": ["Algorithm", "Astropulse", "Data structure", "Periodic function", "Pulsar", "SETI@home", "Signal processing", "Time series"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Computer science stubs", "Signal processing"], "title": "Fast folding algorithm"}
{"summary": "Algorithms for calculating variance play a major role in computational statistics. A key problem in the design of good algorithms for this problem is that formulas for the variance may involve sums of squares, which can lead to numerical instability as well as to arithmetic overflow when dealing with large values.", "links": ["Algebraic formula for the variance", "Algorithm", "Algorithms for calculating variance", "Arithmetic overflow", "Assumed mean", "Bessel's correction", "Catastrophic cancellation", "Central moment", "Communications of the ACM", "Compensated summation", "Computational statistics", "Covariance", "Digital object identifier", "Donald E. Knuth", "Eric W. Weisstein", "Estimator bias", "Floating-point", "Gene H. Golub", "IEEE 754", "International Standard Book Number", "Invariant (mathematics)", "Kahan summation algorithm", "Kurtosis", "Location parameter", "Loss of significance", "MathWorld", "Mean", "Numerical instability", "One-pass algorithm", "Online algorithm", "Precision (arithmetic)", "Python (programming language)", "Recurrence relation", "Skewness", "Statistical population", "Statistical sample", "Technometrics", "The Art of Computer Programming", "Tony F. Chan", "Variance"], "categories": ["Articles with example Python code", "Articles with example pseudocode", "Statistical algorithms", "Statistical deviation and dispersion"], "title": "Algorithms for calculating variance"}
{"summary": "In queueing theory, a discipline within the mathematical theory of probability, Buzen's algorithm (or convolution algorithm) is an algorithm for calculating the normalization constant G(N) in the Gordon\u2013Newell theorem. This method was first proposed by Jeffrey P. Buzen in 1973. Computing G(N) is required to compute the stationary probability distribution of a closed queueing network.\nPerforming a na\u00efve computation of the normalising constant requires enumeration of all states. For a system with N jobs and M states there are  states. Buzen's algorithm \"computes G(1), G(2), ..., G(N) using a total of NM multiplications and NM additions.\" This is a significant improvement and allows for computations to be performed with much larger networks.", "links": ["Adversarial queueing network", "Arrival theorem", "BCMP network", "Balance equation", "Bene\u0161 method", "Bulk queue", "Burke's theorem", "Continuous-time Markov chain", "D/M/1 queue", "Data buffer", "Decomposition method (queueing theory)", "Digital object identifier", "Erlang (unit)", "Erlang distribution", "Expected value", "Exponentially distributed", "FIFO (computing and electronics)", "Flow-equivalent server method", "Flow control (data)", "Fluid limit", "Fluid queue", "Fork\u2013join queue", "G-network", "G/G/1 queue", "G/M/1 queue", "Gordon F. Newell", "Gordon\u2013Newell theorem", "Heavy traffic approximation", "Information system", "JSTOR", "Jackson network", "Jeffrey P. Buzen", "Kelly network", "Kendall's notation", "Kingman's formula", "LIFO (computing)", "Layered queueing network", "Lindley equation", "Little's law", "Loss network", "M/D/1 queue", "M/D/c queue", "M/G/1 queue", "M/G/k queue", "M/M/1 queue", "M/M/c queue", "M/M/\u221e queue", "Marginal distribution", "Markovian arrival process", "Matrix analytic method", "Mean field theory", "Mean value analysis", "Message queue", "Network congestion", "Network scheduler", "Normalization constant", "Operations Research (journal)", "Pipeline (software)", "Poisson process", "Pollaczek\u2013Khinchine formula", "Polling system", "Probability distribution", "Probability theory", "Processor sharing", "Product-form solution", "Quality of service", "Quasireversibility", "Queueing theory", "Rational arrival process", "Reflected Brownian motion", "Retrial queue", "Round-robin scheduling", "Scheduling (computing)", "Shortest job first", "Shortest remaining time", "Teletraffic engineering", "Traffic equations"], "categories": ["Queueing theory", "Statistical algorithms", "Stochastic processes"], "title": "Buzen's algorithm"}
{"summary": "The Elston\u2013Stewart algorithm is an algorithm for computing the likelihood of observed genotype data given a pedigree. It is due to Robert Elston and John Stewart. It can handle relatively large pedigrees providing they are (almost) outbred. Its computation time is exponential in the number of markers. It is used in the analysis of genetic linkage.", "links": ["Biometrics (journal)", "Digital object identifier", "Genetics", "John Stewart (scientist)", "Lander-Green algorithm", "Likelihood", "Pedigree chart", "Robert Elston", "Statistics"], "categories": ["All stub articles", "Genetic epidemiology", "Genetic linkage analysis", "Genetics stubs", "Statistical algorithms", "Statistical genetics", "Statistics stubs"], "title": "Elston\u2013Stewart algorithm"}
{"summary": "The false nearest neighbor algorithm is an algorithm for estimating the embedding dimension. The concept was proposed by Kennel et al. The main idea is to examine how the number of neighbors of a point along a signal trajectory change with increasing embedding dimension. In too low an embedding dimension, many of the neighbors will be false, but in an appropriate embedding dimension or higher, the neighbors are real. With increasing dimension, the false neighbors will no longer be neighbors. Therefore, by examining how the number of neighbors change as a function of dimension, an appropriate embedding can be determined.", "links": ["Algorithm", "Bibcode", "Data structure", "Digital object identifier", "Embedding dimension", "Nearest neighbor", "PubMed Identifier", "Time series"], "categories": ["Algorithms and data structures stubs", "All Wikipedia articles needing context", "All pages needing cleanup", "All stub articles", "Computer science stubs", "Dynamical systems", "Nonlinear time series analysis", "Statistical algorithms", "Wikipedia articles needing context from October 2009", "Wikipedia introduction cleanup from October 2009"], "title": "FNN algorithm"}
{"summary": "The iterative proportional fitting procedure (IPFP, also known as biproportional fitting in statistics, RAS algorithm in economics and matrix raking or matrix scaling in computer science) is an iterative algorithm for estimating cell values of a contingency table such that the marginal totals remain fixed and the estimated table decomposes into an outer product.\nFirst introduced by Deming and Stephan in 1940 (they proposed IPFP as an algorithm leading to a minimizer of the Pearson X-squared statistic, which it does not, and even failed to prove convergence), it has seen various extensions and related research. A rigorous proof of convergence by means of differential geometry is due to Fienberg (1970). He interpreted the family of contingency tables of constant crossproduct ratios as a particular (IJ \u2212 1)-dimensional manifold of constant interaction and showed that the IPFP is a fixed-point iteration on that manifold. Nevertheless, he assumed strictly positive observations. Generalization to tables with zero entries is still considered a hard and only partly solved problem.\nAn exhaustive treatment of the algorithm and its mathematical foundations can be found in the book of Bishop et al. (1975). The first general proof of convergence, built on non-trivial measure theoretic theorems and entropy minimization, is due to Csisz\u00e1r (1975). Relatively new results on convergence and error behavior have been published by Pukelsheim and Simeone (2009) . They proved simple necessary and sufficient conditions for the convergence of the IPFP for arbitrary two-way tables (i.e. tables with zero entries) by analysing an -error function.\nOther general algorithms can be modified to yield the same limit as the IPFP, for instance the Newton\u2013Raphson method and the EM algorithm. In most cases, IPFP is preferred due to its computational speed, numerical stability and algebraic simplicity.", "links": ["Annals of Mathematical Statistics", "Contingency table", "Differential geometry", "Digital object identifier", "EM algorithm", "G-test", "Imre Csisz\u00e1r", "International Standard Book Number", "Iterative algorithm", "JSTOR", "Likelihood-ratio test", "Log-linear model", "Mathematical Reviews", "Maximum likelihood", "Newton\u2013Raphson method", "Outer product", "P-value", "Pearson X-squared statistic", "Stephen Fienberg", "W. Edwards Deming", "Zentralblatt MATH"], "categories": ["Categorical data", "Statistical algorithms"], "title": "Iterative proportional fitting"}
{"summary": "The Lander\u2013Green algorithm is an algorithm, due to Eric Lander and Philip Green for computing the likelihood of observed genotype data given a pedigree. It is appropriate for relatively small pedigrees and a large number of markers. It is used in the analysis of genetic linkage.", "links": ["Elston\u2013Stewart algorithm", "Eric Lander", "Genetic linkage", "Genetics", "Likelihood", "Pedigree chart", "Philip Green (geneticist)", "Statistics"], "categories": ["All stub articles", "Genetic epidemiology", "Genetic linkage analysis", "Genetics stubs", "Statistical algorithms", "Statistical genetics", "Statistics stubs"], "title": "Lander\u2013Green algorithm"}
{"summary": "The nested sampling algorithm is a computational approach to the problem of comparing models in Bayesian statistics, developed in 2004 by physicist John Skilling.", "links": ["ArXiv", "Astronomy", "Bayes' theorem", "Bayes factor", "Bayesian model comparison", "Bayesian statistics", "Bibcode", "C (programming language)", "Computation", "Condensed matter physics", "Cosmology", "Digital object identifier", "Finite element", "Finite element updating", "International Standard Book Number", "Lebesgue integration", "Model selection", "Physicist", "Python (programming language)", "R (programming language)", "Statistical physics", "Structural dynamics"], "categories": ["All Wikipedia articles needing context", "All pages needing cleanup", "Bayesian statistics", "Model selection", "Statistical algorithms", "Wikipedia articles needing context from October 2009", "Wikipedia introduction cleanup from October 2009"], "title": "Nested sampling algorithm"}
{"summary": "Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data which contains outliers. It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981.They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations.\nA basic assumption is that the data consists of \"inliers\", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and \"outliers\" which are data that do not fit the model. The outliers can come, e.g., from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data.", "links": ["Andrew Zisserman", "Computer vision", "Conference on Computer Vision and Pattern Recognition", "Confidence interval", "Correspondence problem", "Digital object identifier", "Fundamental matrix (computer vision)", "Hough transform", "International Standard Book Number", "Iterative method", "Loss function", "MATLAB", "Ordinary least squares", "Outlier", "Outliers", "Regression analysis", "Robust statistics", "SRI International", "Standard deviation"], "categories": ["All articles needing additional references", "All articles with unsourced statements", "Articles needing additional references from September 2014", "Articles with example pseudocode", "Articles with unsourced statements from September 2014", "Geometry in computer vision", "Robust statistics", "SRI International", "Statistical algorithms", "Statistical outliers", "Wikipedia articles needing clarification from April 2014", "Wikipedia articles needing clarification from September 2014"], "title": "RANSAC"}
{"summary": "The Yamartino method (introduced by Robert J. Yamartino in 1984) is an algorithm for calculating an approximation to the standard deviation \u03c3\u03b8 of wind direction \u03b8 during a single pass through the incoming data. The standard deviation of wind direction is a measure of lateral turbulence, and is used in a method for estimating the Pasquill stability category.\nThe simple method for calculating standard deviation requires two passes through the list of values. The first pass determines the average of those values; the second pass determines the sum of the squares of the differences between the values and the average. This double-pass method requires access to all values. A single-pass method can be used for normal data but is unsuitable for angular data such as wind direction where the 0\u00b0/360\u00b0 (or +180\u00b0/-180\u00b0) discontinuity forces special consideration. For example, the directions 1\u00b0, 0\u00b0, and 359\u00b0 (or -1\u00b0) should not average to the direction 120\u00b0!\nThe Yamartino method solves both problems. The United States Environmental Protection Agency (EPA) has chosen it as the preferred way to compute the standard deviation of wind direction. A further discussion of the Yamartino method, along with other methods of estimating the standard deviation of wind direction can be found in Farrugia & Micallef.\nIt should be mentioned that it is also possible to calculate the exact standard deviation in one pass. However, that method needs slightly more calculation effort.", "links": ["Air pollution dispersion terminology", "Algorithms for calculating variance", "Arcsine", "Bibcode", "Digital object identifier", "Directional statistics", "Polar coordinate system", "Right angle", "Standard deviation", "Turbulence", "United States Environmental Protection Agency", "Wind direction"], "categories": ["Atmospheric dispersion modeling", "Boundary layer meteorology", "Directional statistics", "Statistical algorithms"], "title": "Yamartino method"}
{"summary": "Starting with a sample  observed from a random variable X having a given distribution law with a set of non fixed parameters which we denote with a vector , a parametric inference problem consists of computing suitable values \u2013 call them estimates \u2013 of these parameters precisely on the basis of the sample. An estimate is suitable if replacing it with the unknown parameter does not cause major damage in next computations. In Algorithmic inference, suitability of an estimate reads in terms of compatibility with the observed sample.\nIn this framework, resampling methods are aimed at generating a set of candidate values to replace the unknown parameters that we read as compatible replicas of them. They represent a population of specifications of a random vector   compatible with an observed sample, where the compatibility of its values has the properties of a probability distribution. By plugging parameters into the expression of the questioned distribution law, we bootstrap entire populations of random variables compatible with the observed sample.\nThe rationale of the algorithms computing the replicas, which we denote population bootstrap procedures, is to identify a set of statistics  exhibiting specific properties, denoting a well behavior, w.r.t. the unknown parameters. The statistics are expressed as functions of the observed values , by definition. The  may be expressed as a function of the unknown parameters and a random seed specification  through the sampling mechanism , in turn. Then, by plugging the second expression in the former, we obtain  expressions as functions of seeds and parameters \u2013 the master equations \u2013 that we invert to find values of the latter as a function of: i) the statistics, whose values in turn are fixed at the observed ones; and ii) the seeds, which are random according to their own distribution. Hence from a set of seed samples we obtain a set of parameter replicas.", "links": ["Algorithmic inference", "Bootstrapping (statistics)", "Cumulative distribution function", "Digital object identifier", "Estimator", "Indicator function", "Parametric statistics", "Pareto distribution", "PubMed Identifier", "Random variable", "Resampling (statistics)", "Statistical sample", "Sufficiency (statistics)", "Uniform distribution (continuous)", "Well-behaved statistic"], "categories": ["Algorithmic inference", "All Wikipedia articles needing context", "All articles needing cleanup", "All pages needing cleanup", "Articles needing cleanup from January 2009", "Cleanup tagged articles without a reason field from January 2009", "Computational statistics", "Resampling (statistics)", "Wikipedia articles needing context from October 2009", "Wikipedia introduction cleanup from October 2009", "Wikipedia pages needing cleanup from January 2009"], "title": "Bootstrapping populations"}
{"summary": "Starting with a sample  observed from a random variable X having a given distribution law with a non-set parameter, a parametric inference problem consists of computing suitable values \u2013 call them estimates \u2013 of this parameter precisely on the basis of the sample. An estimate is suitable if replacing it with the unknown parameter does not cause major damage in next computations. In algorithmic inference, suitability of an estimate reads in terms of compatibility with the observed sample.\nIn turn, parameter compatibility is a probability measure that we derive from the probability distribution of the random variable to which the parameter refers. In this way we identify a random parameter \u0398 compatible with an observed sample. Given a sampling mechanism , the rationale of this operation lies in using the Z seed distribution law to determine both the X distribution law for the given \u03b8, and the \u0398 distribution law given an X sample. Hence, we may derive the latter distribution directly from the former if we are able to relate domains of the sample space to subsets of \u0398 support. In more abstract terms, we speak about twisting properties of samples with properties of parameters and identify the former with statistics that are suitable for this exchange, so denoting a well behavior w.r.t. the unknown parameters. The operational goal is to write the analytic expression of the cumulative distribution function , in light of the observed value s of a statistic S, as a function of the S distribution law when the X parameter is exactly \u03b8.", "links": ["Algorithmic inference", "Cumulative distribution function", "Digital object identifier", "Estimator", "Fiducial inference", "Fox's H function", "Gamma distribution", "Incomplete Gamma function", "Method of moments (statistics)", "Parametric statistics", "Random variable", "Statistical sample", "Support (mathematics)", "Well-behaved statistic", "Well-behaved statistics"], "categories": ["Algorithmic inference", "All Wikipedia articles needing context", "All articles lacking in-text citations", "All pages needing cleanup", "Articles lacking in-text citations from September 2009", "Computational statistics", "Wikipedia articles needing context from January 2009", "Wikipedia introduction cleanup from January 2009"], "title": "Twisting properties"}
{"summary": "In computer science, the Hunt\u2013McIlroy algorithm is a solution to the longest common subsequence problem. It was one of the first non-heuristic algorithms used in diff. To this day, variations of this algorithm are found in incremental version control systems, wiki engines, and molecular phylogenetics research software.\nThe research accompanying the final version of Unix diff, written by Douglas McIlroy, was published in the 1976 paper \"An Algorithm for Differential File Comparison\", co-written with James W. Hunt, who developed an initial prototype of diff.", "links": ["Big O notation", "Computer science", "Diff", "Douglas McIlroy", "James W. Hunt", "Levenshtein distance", "Longest common subsequence problem", "Molecular phylogenetics", "Unix", "Version control system", "Wagner\u2013Fischer algorithm", "Wiki software"], "categories": ["Algorithms on strings", "Combinatorics", "Dynamic programming"], "title": "Hunt\u2013McIlroy algorithm"}
{"summary": "In machine learning and data mining, a string kernel is a kernel function that operates on strings, i.e. finite sequences of symbols that need not be of the same length. String kernels can be intuitively understood as functions measuring the similarity of pairs of strings: the more similar two strings a and b are, the higher the value of a string kernel K(a, b) will be.\nUsing string kernels with kernelized learning algorithms such as support vector machines allow such algorithms to work with strings, without having to translate these to fixed-length, real-valued feature vectors. String kernels are used in domains where sequence data are to be clustered or classified, e.g. in text mining and gene analysis.", "links": ["Alphabet (computer science)", "Approximation", "Association for Computing Machinery", "Bioinformatics", "Cluster analysis", "Continuous function", "Data mining", "Feature vector", "Genes", "Homology (biology)", "Inner product space", "Journal of Machine Learning Research", "Kernel methods", "Kernel trick", "Machine learning", "Mercer's theorem", "Multiindices", "Mutated", "Positive-definite kernel", "Positive-semidefinite function", "Spam (electronic)", "Statistical classification", "String (computer science)", "Support vector machine", "Symmetric", "Text mining"], "categories": ["Algorithms on strings", "Kernel methods for machine learning", "Natural language processing", "String similarity measures"], "title": "String kernel"}
{"summary": "In computer science, the Wagner\u2013Fischer algorithm is a dynamic programming algorithm that computes the edit distance between two strings of characters.", "links": ["Aho\u2013Corasick algorithm", "Apostolico\u2013Giancarlo algorithm", "Approximate string matching", "Big O notation", "Bitap algorithm", "Boyer\u2013Moore string search algorithm", "Boyer\u2013Moore\u2013Horspool algorithm", "Commentz-Walter algorithm", "Comparison of regular expression engines", "Compressed pattern matching", "Computer science", "Damerau\u2013Levenshtein distance", "Data dependency", "Deterministic acyclic finite state automaton", "Digital object identifier", "Directed acyclic word graph", "Dynamic programming", "Edit distance", "European Summer School in Logic, Language and Information", "Flood fill", "Fuzzy string searching", "Generalized suffix tree", "Hamming distance", "Hirschberg's algorithm", "International Standard Book Number", "Invariant (mathematics)", "Jaro\u2013Winkler distance", "Knuth\u2013Morris\u2013Pratt algorithm", "Lazy evaluation", "Lee distance", "Levenshtein automaton", "Levenshtein distance", "List of regular expression software", "Longest common subsequence", "Longest common substring", "Matrix (mathematics)", "Multiple invention", "Needleman\u2013Wunsch algorithm", "Nondeterministic finite automaton", "Parallel computing", "Parsing", "Pattern matching", "Prefix (computer science)", "Pseudocode", "Rabin\u2013Karp algorithm", "Rabin\u2013Karp string search algorithm", "Reductio ad absurdum", "Regular expression", "Regular tree grammar", "Rope (data structure)", "Sequence alignment", "Sequential pattern mining", "Smith\u2013Waterman algorithm", "String (computer science)", "String metric", "String searching algorithm", "Suffix array", "Suffix automaton", "Suffix tree", "Ternary search tree", "Thompson's construction", "Trie"], "categories": ["Algorithms on strings", "String similarity measures"], "title": "Wagner\u2013Fischer algorithm"}
{"summary": "In computer science, the Cocke\u2013Younger\u2013Kasami algorithm (alternatively called CYK, or CKY) is a parsing algorithm for context-free grammars, named after its inventors, John Cocke, Daniel Younger and Tadao Kasami. It employs bottom-up parsing and dynamic programming.\nThe standard version of CYK operates only on context-free grammars given in Chomsky normal form (CNF). However any context-free grammar may be transformed to a CNF grammar expressing the same language (Sipser 1997).\nThe importance of the CYK algorithm stems from its high efficiency in certain situations. Using Landau symbols, the worst case running time of CYK is , where n is the length of the parsed string and |G| is the size of the CNF grammar G. This makes it one of the most efficient parsing algorithms in terms of worst-case asymptotic complexity, although other algorithms exist with better average running time in many practical scenarios.", "links": ["Air Force Cambridge Research Laboratories", "Algorithm", "Analysis of algorithms", "Asymptotic complexity", "Big O Notation", "Boolean matrix", "Bottom-up parsing", "Chomsky normal form", "CiteSeer", "Computational Intelligence (journal)", "Computer science", "Context-free grammar", "Coppersmith\u2013Winograd algorithm", "Courant Institute of Mathematical Sciences", "Digital object identifier", "Donald Knuth", "Dynamic programming", "Earley parser", "Formal grammar", "GLR parser", "Information and Computation", "International Standard Book Number", "John Cocke", "Journal of Computer and System Sciences", "Journal of the ACM", "Landau symbol", "Leslie Valiant", "Matrix multiplication algorithm", "Michael Sipser", "New York University", "Packrat parser", "Parse tree", "Parser", "Parsing", "Pseudocode", "Recognizer", "Stochastic context-free grammar", "Tadao Kasami", "The Art of Computer Programming", "Weighted context-free grammar"], "categories": ["Parsing algorithms"], "title": "CYK algorithm"}
{"summary": "In computer science, the Earley parser is an algorithm for parsing strings that belong to a given context-free language, though (depending on the variant) it may suffer problems with certain nullable grammars. The algorithm, named after its inventor, Jay Earley, is a chart parser that uses dynamic programming; it is mainly used for parsing in computational linguistics. It was first introduced in his dissertation in 1968 (and later appeared in abbreviated, more legible form in a journal).\nEarley parsers are appealing because they can parse all context-free languages, unlike LR parsers and LL parsers, which are more typically used in compilers but which can only handle restricted classes of languages. The Earley parser executes in cubic time in the general case , where n is the length of the parsed string, quadratic time for unambiguous grammars , and linear time for almost all LR(k) grammars. It performs particularly well when the rules are written left-recursively.", "links": ["Algorithm", "C++", "CYK algorithm", "C (programming language)", "Chart parser", "Communications of the ACM", "Compiler", "Computational linguistics", "Computer science", "Context-free grammar", "Context-free language", "Digital object identifier", "Domain-specific language", "Dynamic programming", "Empty string", "Formal grammar", "Haskell (programming language)", "International Standard Book Number", "JavaScript", "Jay Earley", "LL parser", "LR parser", "Left recursion", "Lexical analysis", "List of algorithms", "Mathematical Reviews", "Nigel Horspool", "Parsing", "Perl", "Python (programming language)", "Racket (programming language)", "Scheme (programming language)", "String (computer science)", "Terminal and nonterminal symbols", "The Computer Journal", "Theoretical Computer Science (journal)", "Tuple"], "categories": ["Dynamic programming", "Parsing algorithms"], "title": "Earley parser"}
{"summary": "A GLR parser (GLR standing for \"generalized LR\", where L stands for \"left-to-right\" and R stands for \"rightmost (derivation)\") is an extension of an LR parser algorithm to handle nondeterministic and ambiguous grammars. The theoretical foundation was provided in a 1974 paper by Bernard Lang (along with other general Context-Free parsers such as GLL). It describes a systematic way to produce such algorithms, and provides uniform results regarding correctness proofs, complexity with respect to grammar classes, and optimization techniques. The first actual implementation of GLR was described in a 1984 paper by Masaru Tomita, it has also been referred to as a \"parallel parser\". Tomita presented five stages in his original work, though in practice it is the second stage that is recognized as the GLR parser.\nThough the algorithm has evolved since its original forms, the principles have remained intact. As shown by an earlier publication, Lang was primarily interested in more easily used and more flexible parsers for extensible programming languages. Tomita's goal was to parse natural language text thoroughly and efficiently. Standard LR parsers cannot accommodate the nondeterministic and ambiguous nature of natural language, and the GLR algorithm can.", "links": ["ASF+SDF Meta Environment", "Ambiguous grammar", "Breadth-first search", "CYK algorithm", "Comparison of parser generators", "DMS Software Reengineering Toolkit", "Digital object identifier", "Directed acyclic graph", "Earley algorithm", "GNU Bison", "International Standard Book Number", "International Standard Serial Number", "LALR", "LR parser", "LR parsers", "Masaru Tomita", "Mathematical optimization", "Natural language", "Nondeterministic grammar", "Parser generator", "State transition"], "categories": ["All articles covered by WikiProject Wikify", "All articles lacking in-text citations", "All articles with unsourced statements", "All pages needing cleanup", "Articles covered by WikiProject Wikify from February 2015", "Articles lacking in-text citations from May 2011", "Articles with unsourced statements from May 2011", "Parsing algorithms", "Wikipedia introduction cleanup from February 2015"], "title": "GLR parser"}
{"summary": "In computer science, the inside\u2013outside algorithm is a way of re-estimating production probabilities in a probabilistic context-free grammar. It was introduced James K. Baker in 1979 as a generalization of the forward\u2013backward algorithm for parameter estimation on hidden Markov models to stochastic context-free grammars. It is used to compute expectations, for example as part of the expectation\u2013maximization algorithm (an unsupervised learning algorithm).", "links": ["Algorithm", "Computer science", "Data structure", "Expectation\u2013maximization algorithm", "Forward\u2013backward algorithm", "Hidden Markov model", "International Standard Book Number", "James K. Baker", "Karim Lari", "Probabilistic context-free grammar", "Steve J. Young", "Stochastic context-free grammar"], "categories": ["Algorithms and data structures stubs", "All Wikipedia articles needing context", "All pages needing cleanup", "All stub articles", "Computer science stubs", "Parsing algorithms", "Wikipedia articles needing context from June 2012", "Wikipedia introduction cleanup from June 2012"], "title": "Inside\u2013outside algorithm"}
{"summary": "In computer science, an LALR parser or Look-Ahead LR parser is a simplified version of a canonical LR parser, to parse (separate and analyze) a text according to a set of production rules specified by a formal grammar for a computer language. (\"LR\" means left-to-right, rightmost derivation.)\nThe LALR parser was invented by Frank DeRemer in his 1969 PhD dissertation, Practical Translators for LR(k) languages, in his treatment of the practical difficulties at that time of implementing LR(1) parsers. He showed that the LALR parser has more language recognition power than the LR(0) parser, while requiring the same number of states as the LR(0) parser for a language that can be recognized by both parsers. This makes the LALR parser a memory-efficient alternative to the LR(1) parser for languages that are not LR(0). It was also proved that there exist LR(1) languages that are not LALR. Despite this weakness, the power of the LALR parser is enough for many mainstream computer languages, including Java, though the reference grammars for many languages fail to be LALR due to being ambiguous.\nThe original dissertation gave no algorithm for constructing such a parser given some formal grammar. The first algorithms for LALR parser generation were published in 1973. In 1982, DeRemer and Tom Pennello published an algorithm that generated highly memory-efficient LALR parsers. LALR parsers can be automatically generated from some grammar by an LALR parser generator such as Yacc or GNU Bison. The automatically generated code may be augmented by hand-written code to augment the power of the resulting parser.\n\n", "links": ["Ambiguous grammar", "Backtracking", "Bottom-up parsing", "C++ language", "C language", "Canonical LR parser", "Comparison of parser generators", "Computer language", "Computer memory", "Computer science", "Context-free grammar", "Deterministic context-free language", "Digital object identifier", "Donald Knuth", "Formal grammar", "Frank DeRemer", "GNU Bison", "Gnu Compiler Collection", "Initialism", "Java technology", "LALR parser generator", "LL grammar", "LL parser", "LR parser", "Parser generator", "Parsing", "Production (computer science)", "Recursive descent parser", "Rightmost derivation", "SLR parser", "Token (parser)", "Token scanner", "Tom Pennello", "Yacc"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from December 2012", "Parsing algorithms", "Use dmy dates from July 2012"], "title": "LALR parser"}
{"summary": "In computer science, LR parsers are a type of bottom-up parsers that efficiently handle deterministic context-free languages in guaranteed linear time. The LALR parsers and the SLR parsers are common variants of LR parsers. LR parsers are often mechanically generated from a formal grammar for the language by a parser generator tool. They are very widely used for the processing of computer languages, more than other kinds of generated parsers.\nThe name LR is an acronym. The L means that the parser reads input text in one direction without backing up; that direction is typically Left to right within each line, and top to bottom across the lines of the full input file. (This is true for most parsers.) The R means that the parser produces a reversed Rightmost derivation; it does a bottom-up parse, not a top-down LL parse or ad-hoc parse. The name LR is often followed by a numeric qualifier, as in LR(1) or sometimes LR(k). To avoid backtracking or guessing, the LR parser is allowed to peek ahead at k lookahead input symbols before deciding how to parse earlier symbols. Typically k is 1 and is not mentioned. The name LR is often preceded by other qualifiers, as in SLR and LALR.\nLR parsers are deterministic; they produce a single correct parse without guesswork or backtracking, in linear time. This is ideal for computer languages. But LR parsers are not suited for human languages which need more flexible but slower methods. Other parser methods (CYK algorithm, Earley parser, and GLR parser) that backtrack or yield multiple parses may take O(n2), O(n3) or even exponential time when they guess badly.\nThe above properties of L, R, and k are actually shared by all shift-reduce parsers, including precedence parsers. But by convention, the LR name stands for the form of parsing invented by Donald Knuth, and excludes the earlier, less powerful precedence methods (for example Operator-precedence parser). LR parsers can handle a larger range of languages and grammars than precedence parsers or top-down LL parsing. This is because the LR parser waits until it has seen an entire instance of some grammar pattern before committing to what it has found. An LL parser has to decide or guess what it is seeing much sooner, when it has only seen the leftmost input symbol of that pattern. LR is also better at error reporting. It detects syntax errors as early in the input stream as possible.", "links": ["Alphabet (formal languages)", "Ambiguous grammar", "Backtracking", "Boldface", "Bottom-up parsing", "CYK algorithm", "Cambridge University Press", "Canonical LR parser", "Computer language", "Computer science", "Concatenation", "Context-free grammar", "Context-free language", "Dangling else", "Deterministic context-free language", "Digital object identifier", "Donald Knuth", "Earley parser", "Finite state automaton", "Formal grammar", "Frank DeRemer", "GLR parser", "GNU Bison", "Generalized LR parser", "International Standard Book Number", "LALR", "LALR parser", "LL parser", "LL parsing", "Left corner parser", "Lexical analysis", "Nonterminal symbol", "Operator-precedence parser", "Parse tree", "Parser", "Parser generator", "Parsing", "Prefix (formal languages)", "Prolog", "Recursive ascent parser", "Recursive descent parser", "Rightmost derivation", "SLR grammar", "SLR parser", "Semantics", "Shift-reduce parser", "Simple LR parser", "Simple precedence parser", "Stack (abstract data type)", "Terminal symbol", "Top-down parser", "Top-down parsing", "Yacc"], "categories": ["All articles with unsourced statements", "Articles with unsourced statements from June 2012", "Parsing algorithms"], "title": "LR parser"}
{"summary": "In computer science, an operator precedence parser is a bottom-up parser that interprets an operator-precedence grammar. For example, most calculators use operator precedence parsers to convert from the human-readable infix notation relying on order of operations to a format that is optimized for evaluation such as Reverse Polish notation (RPN).\nEdsger Dijkstra's shunting yard algorithm is commonly used to implement operator precedence parsers. Other algorithms include the precedence climbing method and the top down operator precedence method.", "links": ["Bottom-up parsing", "Calculator", "Compiler compiler", "Computer science", "Digital object identifier", "EBNF", "Edsger Dijkstra", "Friedrich L. Bauer", "GNU Compiler Collection", "Haskell (programming language)", "Infix notation", "International Standard Book Number", "Klaus Samelson", "LR parser", "Nonterminal", "Operator-precedence grammar", "Order of operations", "Parrot virtual machine", "Parser Grammar Engine", "Perl 6", "Pratt parser", "Recursive descent parser", "Reverse Polish notation", "Run time (program lifecycle phase)", "Shift-reduce parser", "Shunting yard algorithm"], "categories": ["All articles with unsourced statements", "Articles with example C code", "Articles with unsourced statements from November 2010", "Parsing algorithms"], "title": "Operator-precedence parser"}
{"summary": "In computer science, a parsing expression grammar, or PEG, is a type of analytic formal grammar, i.e. it describes a formal language in terms of a set of rules for recognizing strings in the language. The formalism was introduced by Bryan Ford in 2004 and is closely related to the family of top-down parsing languages introduced in the early 1970s. Syntactically, PEGs also look similar to context-free grammars (CFGs), but they have a different interpretation: the choice operator selects the first match in PEG, while it is ambiguous in CFG. This is closer to how string recognition tends to be done in practice, e.g. by a recursive descent parser.\nUnlike CFGs, PEGs cannot be ambiguous; if a string parses, it has exactly one valid parse tree. It is conjectured that there exist context-free languages that cannot be parsed by a PEG, but this is not yet proven. PEGs are well-suited to parsing computer languages, but not natural languages where their performance is comparable to general CFG algorithms such as the Earley algorithm.", "links": ["Addison-Wesley Longman", "Ambiguous", "Ambiguous grammar", "Association for Computing Machinery", "Backtracking", "Bellman\u2013Ford algorithm", "Boolean grammar", "CYK algorithm", "Circular definition", "Commutativity", "Comparison of parser generators", "Computer science", "Constructed language", "Context-free grammar", "Context-free grammars", "Context-free language", "Cut (logic programming)", "Dangling else", "Digital object identifier", "Earley algorithm", "Exponential time", "Expression (mathematics)", "Floyd\u2013Warshall algorithm", "Formal grammar", "Formal language", "Function (mathematics)", "GLR parser", "Graph algorithms", "Greedy algorithm", "International Standard Book Number", "LL parser", "LR parser", "Left recursion", "Linear time", "Logic programming", "Lojban", "Memoization", "Mutual recursion", "Natural language", "Nonterminal symbol", "OMeta", "Parse tree", "Parser combinators", "Parsing", "Portable Document Format", "Recursion", "Recursive descent parser", "Regular expression", "Regular expressions", "Roberto Ierusalimschy", "String (computer science)", "Syntactic predicate", "Terminal symbol", "Tokenization (lexical analysis)", "Top-down parsing language"], "categories": ["All accuracy disputes", "All articles with unsourced statements", "Articles with disputed statements from July 2014", "Articles with unsourced statements from November 2011", "Formal languages", "Parsing algorithms", "Wikipedia external links cleanup from September 2011", "Wikipedia spam cleanup from September 2011"], "title": "Parsing expression grammar"}
{"summary": "In computer science, the shunting-yard algorithm is a method for parsing mathematical expressions specified in infix notation. It can be used to produce output in Reverse Polish notation (RPN) or as an abstract syntax tree (AST). The algorithm was invented by Edsger Dijkstra and named the \"shunting yard\" algorithm because its operation resembles that of a railroad shunting yard. Dijkstra first described the Shunting Yard Algorithm in the Mathematisch Centrum report MR 34/61.\nLike the evaluation of RPN, the shunting yard algorithm is stack-based. Infix expressions are the form of mathematical notation most people are used to, for instance 3+4 or 3+4*(2\u22121). For the conversion there are two text variables (strings), the input and the output. There is also a stack that holds operators not yet added to the output queue. To convert, the program reads each symbol in order and does something based on that symbol.\nThe shunting-yard algorithm has been later generalized into operator-precedence parsing.", "links": ["Abstract syntax tree", "Algorithm", "Classification yard", "Computer science", "Constant folding", "Edsger Dijkstra", "Function (mathematics)", "Infix notation", "Interpreter (computing)", "Mathematisch Centrum", "Operator-precedence parser", "Operator associativity", "Order of operations", "Polish notation", "Queue (data structure)", "Reverse Polish Notation", "Reverse Polish notation", "Stack (data structure)", "String (computer science)", "Token (parser)", "Tokenize", "Variable (programming)"], "categories": ["All articles lacking in-text citations", "Articles lacking in-text citations from August 2013", "Dutch inventions", "Parsing algorithms"], "title": "Shunting-yard algorithm"}
{"summary": "In computer science, a Simple LR or SLR parser is a type of LR parser with small parse tables and a relatively simple parser generator algorithm. As with other types of LR(1) parser, an SLR parser is quite efficient at finding the single correct bottom-up parse in a single left-to-right scan over the input stream, without guesswork or backtracking. The parser is mechanically generated from a formal grammar for the language.\nSLR and the more-general methods LALR parser and Canonical LR parser have identical methods and similar tables at parse time; they differ only in the mathematical grammar analysis algorithms used by the parser generator tool. SLR and LALR generators create tables of identical size and identical parser states. SLR generators accept fewer grammars than do LALR generators like yacc and Bison. Many computer languages don't readily fit the restrictions of SLR, as is. Bending the language's natural grammar into SLR grammar form requires more compromises and grammar hackery. So LALR generators have become much more widely used than SLR generators, despite being somewhat more complicated tools. SLR methods remain a useful learning step in college classes on compiler theory.\nSLR and LALR were both developed by Frank DeRemer as the first practical uses of Donald Knuth's LR parser theory. The tables created for real grammars by full LR methods were impractically large, larger than most computer memories of that decade, with 100 times or more parser states than the SLR and LALR methods.", "links": ["Bottom-up parsing", "Canonical LR parser", "Computer science", "Donald Knuth", "Formal grammar", "Frank DeRemer", "GNU bison", "LALR parser", "LL parser", "LR parser", "SLR grammar", "Yacc"], "categories": ["All articles lacking sources", "All articles with unsourced statements", "Articles lacking sources from December 2012", "Articles with unsourced statements from January 2015", "Articles with unsourced statements from June 2012", "Parsing algorithms"], "title": "Simple LR parser"}
{"summary": "ISO/IEC 14651:2007, Information technology -- International string ordering and comparison -- Method for comparing character strings and description of the common template tailorable ordering, is an ISO Standard specifying an algorithm that can be used when comparing two strings. This comparison can be used when collating a set of strings. The standard also specifies a datafile specifying the comparison order, the Common Tailorable Template, CTT. The comparison order is supposed to be tailored for different languages (hence the CTT is regarded as a template and not a default, though the empty tailoring, not changing any weighting, is appropriate in many cases), since different languages have incompatible ordering requirements. One such tailoring is European ordering rules (EOR), which in turn is supposed to be tailored for different European languages.\nThe Common Tailorable Template (CTT) datafile of this ISO Standard is aligned with the Default Unicode Collation Entity Table (DUCET) datafile of the Unicode Collation Algorithm (UCA) specified in Unicode Technical Standard #10.", "links": ["'Phags-pa script", "Ahom alphabet", "Algorithm", "Anatolian hieroglyphs", "Ancient North Arabian", "Arabic diacritics", "Arabic script", "Aramaic alphabet", "Armenian alphabet", "Avestan alphabet", "Balinese alphabet", "Bamum script", "Bassa alphabet", "Batak alphabet", "Baybayin", "Bengali alphabet", "Bi-directional text", "Binary Ordered Compression for Unicode", "Bopomofo", "Brahmi script", "Braille", "Buhid alphabet", "Burmese alphabet", "Byte order mark", "CESU-8", "CJK Unified Ideographs", "Canadian Aboriginal syllabics", "Carian alphabets", "Caucasian Albanian alphabet", "Chakma alphabet", "Cham alphabet", "Cherokee syllabary", "Code point", "Collation", "Combining Grapheme Joiner", "Combining character", "Common Locale Data Repository", "Comparison of Unicode encodings", "ConScript Unicode Registry", "Coptic alphabet", "Cuneiform", "Currency symbol", "Cypriot syllabary", "Cyrillic script", "Deseret alphabet", "Devanagari", "Diacritic", "Duplicate characters in Unicode", "Duployan shorthand", "Egyptian hieroglyphs", "Elbasan", "Emoji", "European ordering rules", "Fraser alphabet", "GB 18030", "Ge'ez script", "Georgian scripts", "Glagolitic alphabet", "Gothic alphabet", "Grantha alphabet", "Greek alphabet", "Gujarati alphabet", "Gurmukh\u012b alphabet", "Halfwidth and fullwidth forms", "Han unification", "Hangul", "Hanja", "Hanun\u00f3'o alphabet", "Hatran alphabet", "Hebrew alphabet", "Hebrew diacritics", "Hiragana", "Homoglyph", "ISO/IEC 8859", "ISO/IEC JTC 1/SC 2", "ISO 15924", "Ideographic Rapporteur Group", "International Components for Unicode", "International Organization for Standardization", "Internationalized domain name", "Javanese script", "Kaithi", "Kanji", "Kannada alphabet", "Katakana", "Kayah Li alphabet", "Kharosthi", "Khmer alphabet", "Khojki", "Khudabadi script", "Lao alphabet", "Latin script in Unicode", "Left-to-right mark", "Lepcha alphabet", "Limbu alphabet", "Linear A", "Linear B", "List of Unicode characters", "List of XML and HTML character entity references", "List of precomposed Latin characters in Unicode", "Lontara alphabet", "Lycian alphabet", "Lydian alphabet", "Mahajani", "Malayalam script", "Mandaic alphabet", "Manichaean alphabet", "Mathematical operators and symbols in Unicode", "Measurement", "Meithei script", "Mende Kikakui script", "Meroitic alphabet", "Modi alphabet", "Mongolian script", "Mro people", "Multani alphabet", "N'Ko alphabet", "Nabataean alphabet", "New Tai Lue alphabet", "Numerals in Unicode", "Numeric character reference", "Ogham", "Ol Chiki alphabet", "Old Hungarian alphabet", "Old Italic script", "Old Permic alphabet", "Old Persian cuneiform", "Old Turkic alphabet", "Oriya alphabet", "Osmanya alphabet", "Pahawh Hmong", "Pahlavi scripts", "Palmyrene alphabet", "Parthian language", "Pau Cin Hau", "Phoenician alphabet", "Phonetic symbols in Unicode", "Plane (Unicode)", "Pollard script", "Precomposed character", "Private Use Areas", "Punctuation", "Punycode", "Rejang alphabet", "Religious and political symbols in Unicode", "Right-to-left mark", "Runes", "Samaritan alphabet", "Saurashtra alphabet", "Script (Unicode)", "Shavian alphabet", "Siddha\u1e43 script", "SignWriting", "Sinhala alphabet", "Soft hyphen", "Sorang Sompeng alphabet", "South Arabian alphabet", "Space (punctuation)", "Standard Compression Scheme for Unicode", "Standardization", "String (computer science)", "Sundanese alphabet", "Sylheti Nagari", "Syriac alphabet", "Tagbanwa alphabet", "Tai Dam language", "Tai Le alphabet", "Tai Tham alphabet", "Takri alphabet", "Tamil script", "Telugu script", "Thaana", "Thai alphabet", "Tibetan alphabet", "Tifinagh", "Tirhuta", "UTF-1", "UTF-16", "UTF-32", "UTF-7", "UTF-8", "UTF-9 and UTF-18", "UTF-EBCDIC", "Ugaritic alphabet", "Unicode", "Unicode Collation Algorithm", "Unicode Consortium", "Unicode and HTML", "Unicode and email", "Unicode anomaly", "Unicode block", "Unicode character property", "Unicode collation algorithm", "Unicode compatibility characters", "Unicode equivalence", "Unicode font", "Unicode input", "Unicode symbols", "Universal Character Set characters", "Universal Coded Character Set", "Vai syllabary", "Varang Kshiti", "Word joiner", "Yi script", "Z-variant", "Zero-width joiner", "Zero-width non-joiner", "Zero-width space", "\u015a\u0101rad\u0101 script"], "categories": ["All stub articles", "Collation", "Computing stubs", "ISO standards", "Standards and measurement stubs", "String collation algorithms", "Unicode algorithms"], "title": "ISO 14651"}
{"summary": "The Unicode collation algorithm (UCA) is an algorithm defined in Unicode Technical Report #10, which defines a customizable method to compare two strings. These comparisons can then be used to collate or sort text in any writing system and language that can be represented with Unicode.\nUnicode Technical Report #10 also specifies the Default Unicode Collation Element Table (DUCET). This datafile specifies the default collation ordering. The DUCET is customizable for different languages. Some such customisations can be found in Common Locale Data Repository (CLDR).\nAn important open source implementation of UCA is included with the International Components for Unicode, ICU. ICU also supports tailoring and the collation tailorings from CLDR are included in ICU. You can see the effects of tailoring and a large number of language specific tailorings in the on-line ICU Locale Explorer.", "links": ["'Phags-pa script", "Ahom alphabet", "Algorithm", "Anatolian hieroglyphs", "Ancient North Arabian", "Arabic diacritics", "Arabic script", "Aramaic alphabet", "Armenian alphabet", "Avestan alphabet", "Balinese alphabet", "Bamum script", "Bassa alphabet", "Batak alphabet", "Baybayin", "Bengali alphabet", "Bi-directional text", "Binary Ordered Compression for Unicode", "Bopomofo", "Brahmi script", "Braille", "Buhid alphabet", "Burmese alphabet", "Byte order mark", "CESU-8", "CJK Unified Ideographs", "Canadian Aboriginal syllabics", "Carian alphabets", "Caucasian Albanian alphabet", "Chakma alphabet", "Cham alphabet", "Cherokee syllabary", "Code point", "Collate", "Collation", "Combining Grapheme Joiner", "Combining character", "Common Locale Data Repository", "Comparison of Unicode encodings", "ConScript Unicode Registry", "Coptic alphabet", "Cuneiform", "Currency symbol", "Cypriot syllabary", "Cyrillic script", "Data structure", "Deseret alphabet", "Devanagari", "Diacritic", "Duplicate characters in Unicode", "Duployan shorthand", "Egyptian hieroglyphs", "Elbasan", "Emoji", "European ordering rules", "Fraser alphabet", "GB 18030", "Ge'ez script", "Georgian scripts", "Glagolitic alphabet", "Gothic alphabet", "Grantha alphabet", "Greek alphabet", "Gujarati alphabet", "Gurmukh\u012b alphabet", "Halfwidth and fullwidth forms", "Han unification", "Hangul", "Hanja", "Hanun\u00f3'o alphabet", "Hatran alphabet", "Hebrew alphabet", "Hebrew diacritics", "Hiragana", "Homoglyph", "ISO/IEC 8859", "ISO 14651", "ISO 15924", "Ideographic Rapporteur Group", "International Components for Unicode", "Internationalized domain name", "Javanese script", "Kaithi", "Kanji", "Kannada alphabet", "Katakana", "Kayah Li alphabet", "Kharosthi", "Khmer alphabet", "Khojki", "Khudabadi script", "Language", "Lao alphabet", "Latin script in Unicode", "Left-to-right mark", "Lepcha alphabet", "Limbu alphabet", "Linear A", "Linear B", "List of Unicode characters", "List of XML and HTML character entity references", "List of precomposed Latin characters in Unicode", "Lontara alphabet", "Lycian alphabet", "Lydian alphabet", "Mahajani", "Malayalam script", "Mandaic alphabet", "Manichaean alphabet", "Mathematical operators and symbols in Unicode", "Measurement", "Meithei script", "Mende Kikakui script", "Meroitic alphabet", "Modi alphabet", "Mongolian script", "Mro people", "Multani alphabet", "N'Ko alphabet", "Nabataean alphabet", "New Tai Lue alphabet", "Numerals in Unicode", "Numeric character reference", "Ogham", "Ol Chiki alphabet", "Old Hungarian alphabet", "Old Italic script", "Old Permic alphabet", "Old Persian cuneiform", "Old Turkic alphabet", "Oriya alphabet", "Osmanya alphabet", "Pahawh Hmong", "Pahlavi scripts", "Palmyrene alphabet", "Parthian language", "Pau Cin Hau", "Phoenician alphabet", "Phonetic symbols in Unicode", "Plane (Unicode)", "Pollard script", "Precomposed character", "Private Use Areas", "Punctuation", "Punycode", "Rejang alphabet", "Religious and political symbols in Unicode", "Right-to-left mark", "Runes", "Samaritan alphabet", "Saurashtra alphabet", "Script (Unicode)", "Shavian alphabet", "Siddha\u1e43 script", "SignWriting", "Sinhala alphabet", "Soft hyphen", "Sorang Sompeng alphabet", "South Arabian alphabet", "Space (punctuation)", "Standard Compression Scheme for Unicode", "Standardization", "String (computer science)", "Sundanese alphabet", "Sylheti Nagari", "Syriac alphabet", "Tagbanwa alphabet", "Tai Dam language", "Tai Le alphabet", "Tai Tham alphabet", "Takri alphabet", "Tamil script", "Telugu script", "Thaana", "Thai alphabet", "Tibetan alphabet", "Tifinagh", "Tirhuta", "UTF-1", "UTF-16", "UTF-32", "UTF-7", "UTF-8", "UTF-9 and UTF-18", "UTF-EBCDIC", "Ugaritic alphabet", "Unicode", "Unicode Consortium", "Unicode and HTML", "Unicode and email", "Unicode anomaly", "Unicode block", "Unicode character property", "Unicode compatibility characters", "Unicode equivalence", "Unicode font", "Unicode input", "Unicode symbols", "Universal Character Set characters", "Universal Coded Character Set", "Vai syllabary", "Varang Kshiti", "Word joiner", "Writing system", "Yi script", "Z-variant", "Zero-width joiner", "Zero-width non-joiner", "Zero-width space", "\u015a\u0101rad\u0101 script"], "categories": ["Algorithms and data structures stubs", "All stub articles", "Collation", "Computer science stubs", "Standards and measurement stubs", "String collation algorithms", "Unicode algorithms"], "title": "Unicode collation algorithm"}

3Dc,"3Dc (FourCC : ATI2), also known as DXN, BC5, or Block Compression 5 is a lossy data compression algorithm for normal maps invented and first implemented by ATI. It builds upon the earlier DXT5 algorithm and is an open standard. 3Dc is now implemented by both ATI and Nvidia."
A*,"In computer science, A* (pronounced as ""A star"" ( listen)) is a computer algorithm that is widely used in pathfinding and graph traversal, the process of plotting an efficiently traversable path between multiple points, called nodes. Noted for its performance and accuracy, it enjoys widespread use. However, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, although other work has found A* to be superior to other approaches.
Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first described the algorithm in 1968. It is an extension of Edsger Dijkstra's 1959 algorithm. A* achieves better performance by using heuristics to guide its search."
ALOPEX,"ALOPEX (an acronym from ""ALgorithms Of Pattern EXtraction"") is a correlation based machine learning algorithm first proposed by Tzanakou and Harth in 1974."
AdaBoost,"AdaBoost, short for ""Adaptive Boosting"", is a machine learning meta-algorithm formulated by Yoav Freund and Robert Schapire who won the Gödel Prize in 2003 for their work. It can be used in conjunction with many other types of learning algorithms to improve their performance. The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. AdaBoost is sensitive to noisy data and outliers. In some problems, however, it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing (i.e., their error rate is smaller than 0.5 for binary classification), the final model can be proven to converge to a strong learner
While every learning algorithm will tend to suit some problem types better than others, and will typically have many different parameters and configurations to be adjusted before achieving optimal performance on a dataset, AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier. When used with decision tree learning, information gathered at each stage of the AdaBoost algorithm about the relative 'hardness' of each training sample is fed into the tree growing algorithm such that later trees tend to focus on harder to classify examples."
Adaptive coding,"Adaptive coding refers to variants of entropy encoding methods of lossless data compression. They are particularly suited to streaming data, as they adapt to localized changes in the characteristics of the data, and don't require a first pass over the data to calculate a probability model. The cost paid for these advantages is that the encoder and decoder must be more complex to keep their states synchronized, and more computational power is needed to keep adapting the encoder/decoder state.
Almost all data compression methods involve the use of a model, a prediction of the composition of the data. When the data matches the prediction made by the model, the encoder can usually transmit the content of the data at a lower information cost, by making reference to the model. This general statement is a bit misleading as general data compression algorithm would include the popular LZW and LZ77 algorithms, which are hardly comparable to compression techniques typically called adaptive. Run length encoding and the typical JPEG compression with run length encoding and predefined Huffman codes do not transmit a model. A lot of other methods adapt their model to the current file and need to transmit it in addition to the encoded data, because both the encoder and the decoder need to use the model.
In adaptive coding, the encoder and decoder are instead equipped with a predefined meta-model about how they will alter their models in response to the actual content of the data, and otherwise start with a blank slate, meaning that no initial model needs to be transmitted. As the data is transmitted, both encoder and decoder adapt their models, so that unless the character of the data changes radically, the model becomes better-adapted to the data its handling and compresses it more efficiently approaching the efficiency of the static coding.

"
Adaptive replacement cache,"Adaptive Replacement Cache (ARC) is a page replacement algorithm with better performance than LRU (Least Recently Used) developed at the IBM Almaden Research Center. This is accomplished by keeping track of both Frequently Used and Recently Used pages plus a recent eviction history for both. In 2006, IBM was granted a patent for the adaptive replacement cache policy.

"
Addition-chain exponentiation,"In mathematics and computer science, optimal addition-chain exponentiation is a method of exponentiation by positive integer powers that requires a minimal number of multiplications. It works by creating the shortest addition chain that generates the desired exponent. Each exponentiation in the chain can be evaluated by multiplying two of the earlier exponentiation results. More generally, addition-chain exponentiation may also refer to exponentiation by non-minimal addition chains constructed by a variety of algorithms (since a shortest addition chain is very difficult to find).
The shortest addition-chain algorithm requires no more multiplications than binary exponentiation and usually less. The first example of where it does better is for a15, where the binary method needs six multiplications but a shortest addition chain requires only five:
 (binary, 6 multiplications)
 (shortest addition chain, 5 multiplications).
On the other hand, the determination of a shortest addition chain is hard: no efficient optimal methods are currently known for arbitrary exponents, and the related problem of finding a shortest addition chain for a given set of exponents has been proven NP-complete. Even given a shortest chain, addition-chain exponentiation requires more memory than the binary method, because it must potentially store many previous exponents from the chain. So in practice, shortest addition-chain exponentiation is primarily used for small fixed exponents for which a shortest chain can be precomputed and is not too large.
There are also several methods to approximate a shortest addition chain, and which often require fewer multiplications than binary exponentiation; binary exponentiation itself is a suboptimal addition-chain algorithm. The optimal algorithm choice depends on the context (such as the relative cost of the multiplication and the number of times a given exponent is re-used).
The problem of finding the shortest addition chain cannot be solved by dynamic programming, because it does not satisfy the assumption of optimal substructure. That is, it is not sufficient to decompose the power into smaller powers, each of which is computed minimally, since the addition chains for the smaller powers may be related (to share computations). For example, in the shortest addition chain for a15 above, the subproblem for a6 must be computed as (a3)2 since a3 is re-used (as opposed to, say, a6 = a2(a2)2, which also requires three multiplies)."
Adler-32,"Adler-32 is a checksum algorithm which was invented by Mark Adler in 1995, and is a modification of the Fletcher checksum. Compared to a cyclic redundancy check of the same length, it trades reliability for speed (preferring the latter). Adler-32 is more reliable than Fletcher-16, and slightly less reliable than Fletcher-32."
Aho–Corasick string matching algorithm,"In computer science, the Aho–Corasick algorithm is a string searching algorithm invented by Alfred V. Aho and Margaret J. Corasick. It is a kind of dictionary-matching algorithm that locates elements of a finite set of strings (the ""dictionary"") within an input text. It matches all patterns simultaneously. The complexity of the algorithm is linear in the length of the patterns plus the length of the searched text plus the number of output matches. Note that because all matches are found, there can be a quadratic number of matches if every substring matches (e.g. dictionary = a, aa, aaa, aaaa and input string is aaaa).
Informally, the algorithm constructs a finite state machine that resembles a trie with additional links between the various internal nodes. These extra internal links allow fast transitions between failed pattern matches (e.g. a search for cat in a trie that does not contain cat, but contains cart, and thus would fail at the node prefixed by ca), to other branches of the trie that share a common prefix (e.g., in the previous case, a branch for attribute might be the best lateral transition). This allows the automaton to transition between pattern matches without the need for backtracking.
When the pattern dictionary is known in advance (e.g. a computer virus database), the construction of the automaton can be performed once off-line and the compiled automaton stored for later use. In this case, its run time is linear in the length of the input plus the number of matched entries.
The Aho–Corasick string matching algorithm formed the basis of the original Unix command fgrep."
Algorithm X,"""Algorithm X"" is the name Donald Knuth used in his paper ""Dancing Links"" to refer to ""the most obvious trial-and-error approach"" for finding all solutions to the exact cover problem. Technically, Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm. While Algorithm X is generally useful as a succinct explanation of how the exact cover problem may be solved, Knuth's intent in presenting it was merely to demonstrate the utility of the dancing links technique via an efficient implementation he called DLX.
The exact cover problem is represented in Algorithm X using a matrix A consisting of 0s and 1s. The goal is to select a subset of the rows so that the digit 1 appears in each column exactly once.
Algorithm X functions as follows:

The nondeterministic choice of r means that the algorithm essentially clones itself into independent subalgorithms; each subalgorithm inherits the current matrix A, but reduces it with respect to a different row r. If column c is entirely zero, there are no subalgorithms and the process terminates unsuccessfully.
The subalgorithms form a search tree in a natural way, with the original problem at the root and with level k containing each subalgorithm that corresponds to k chosen rows. Backtracking is the process of traversing the tree in preorder, depth first.
Any systematic rule for choosing column c in this procedure will find all solutions, but some rules work much better than others. To reduce the number of iterations, Knuth suggests that the column choosing algorithm select a column with the lowest number of 1s in it."
Algorithms for Recovery and Isolation Exploiting Semantics,"In computer science, Algorithms for Recovery and Isolation Exploiting Semantics, or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; it is used by IBM DB2, Microsoft SQL Server and many other database systems.
Three main principles lie behind ARIES
Write ahead logging: Any change to an object is first recorded in the log, and the log must be written to stable storage before changes to the object are written to disk.
Repeating history during Redo: On restart after a crash, ARIES retraces the actions of a database before the crash and brings the system back to the exact state that it was in before the crash. Then it undoes the transactions still active at crash time.
Logging changes during Undo: Changes made to the database while undoing transactions are logged to ensure such an action isn't repeated in the event of repeated restarts."
Algorithms for calculating variance,"Algorithms for calculating variance play a major role in computational statistics. A key problem in the design of good algorithms for this problem is that formulas for the variance may involve sums of squares, which can lead to numerical instability as well as to arithmetic overflow when dealing with large values."
Alpha-beta pruning,"Alpha–beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Go, etc.). It stops completely evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. Such moves need not be evaluated further. When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision."
Alpha max plus beta min algorithm,"The alpha max plus beta min algorithm is a high-speed approximation of the square root of the sum of two squares. The square root of the sum of two squares, also known as Pythagorean addition, is a useful function, because it finds the hypotenuse of a right triangle given the two side lengths, the norm of a 2-D vector, or the magnitude of a complex number z=a+bi given the real and imaginary parts.

The algorithm avoids performing the square and square-root operations, instead using simple operations such as comparison, multiplication, and addition. Some choices of the α and β parameters of the algorithm allow the multiplication operation to be reduced to a simple shift of binary digits that is particularly well suited to implementation in high-speed digital circuitry.
The approximation is expressed as:

Where  is the maximum absolute value of a and b and  is the minimum absolute value of a and b.
For the closest approximation, the optimum values for  and  are  and , giving a maximum error of 3.96%."
Approximate counting algorithm,"The approximate counting algorithm allows the counting of a large number of events using a small amount of memory. Invented in 1977 by Robert Morris (cryptographer) of Bell Labs, it uses probabilistic techniques to increment the counter. It was fully analyzed in the early 1980s by Philippe Flajolet of INRIA Rocquencourt, who coined the name Approximate Counting, and strongly contributed to its recognition among the research community. The algorithm is considered one of the precursors of streaming algorithms, and the more general problem of determining the frequency moments of a data stream has been central to the field."
Apriori algorithm,Apriori is an algorithm for frequent item set mining and association rule learning over transactional databases. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often in the database. The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: this has applications in domains such as market basket analysis.
Astronomical algorithms,"Astronomical algorithms are the algorithms used to calculate ephemerides, calendars, and positions (as in celestial navigation or satellite navigation). Examples of large and complex astronomical algorithms are those used to calculate the position of the Moon. A simple example is the calculation of the Julian day.
Numerical model of solar system discusses a generalized approach to local astronomical modeling. The variations séculaires des orbites planétaires describes an often used model.

"
B*,"In computer science, B* (pronounced ""B star"") is a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals). First published by Hans Berliner in 1979, it is related to the A* search algorithm."
BFGS method,"In numerical optimization, the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is an iterative method for solving unconstrained nonlinear optimization problems.
The BFGS method approximates Newton's method, a class of hill-climbing optimization techniques that seeks a stationary point of a (preferably twice continuously differentiable) function. For such problems, a necessary condition for optimality is that the gradient be zero. Newton's method and the BFGS methods are not guaranteed to converge unless the function has a quadratic Taylor expansion near an optimum. These methods use both the first and second derivatives of the function. However, BFGS has proven to have good performance even for non-smooth optimizations.
In quasi-Newton methods, the Hessian matrix of second derivatives doesn't need to be evaluated directly. Instead, the Hessian matrix is approximated using rank-one updates specified by gradient evaluations (or approximate gradient evaluations). Quasi-Newton methods are generalizations of the secant method to find the root of the first derivative for multidimensional problems. In multi-dimensional problems, the secant equation does not specify a unique solution, and quasi-Newton methods differ in how they constrain the solution. The BFGS method is one of the most popular members of this class. Also in common use is L-BFGS, which is a limited-memory version of BFGS that is particularly suited to problems with very large numbers of variables (e.g., >1000). The BFGS-B variant handles simple box constraints."
Baby-step giant-step,"In group theory, a branch of mathematics, the baby-step giant-step is a meet-in-the-middle algorithm computing the discrete logarithm. The discrete log problem is of fundamental importance to the area of public key cryptography. Many of the most commonly used cryptography systems are based on the assumption that the discrete log is extremely difficult to compute; the more difficult it is, the more security it provides a data transfer. One way to increase the difficulty of the discrete log problem is to base the cryptosystem on a larger group."
Backtracking,"Backtracking is a general algorithm for finding all (or some) solutions to some computational problems, notably constraint satisfaction problems, that incrementally builds candidates to the solutions, and abandons each partial candidate c (""backtracks"") as soon as it determines that c cannot possibly be completed to a valid solution.
The classic textbook example of the use of backtracking is the eight queens puzzle, that asks for all arrangements of eight chess queens on a standard chessboard so that no queen attacks any other. In the common backtracking approach, the partial candidates are arrangements of k queens in the first k rows of the board, all in different rows and columns. Any partial solution that contains two mutually attacking queens can be abandoned.
Backtracking can be applied only for problems which admit the concept of a ""partial candidate solution"" and a relatively quick test of whether it can possibly be completed to a valid solution. It is useless, for example, for locating a given value in an unordered table. When it is applicable, however, backtracking is often much faster than brute force enumeration of all complete candidates, since it can eliminate a large number of candidates with a single test.
Backtracking is an important tool for solving constraint satisfaction problems, such as crosswords, verbal arithmetic, Sudoku, and many other puzzles. It is often the most convenient (if not the most efficient) technique for parsing, for the knapsack problem and other combinatorial optimization problems. It is also the basis of the so-called logic programming languages such as Icon, Planner and Prolog.
Backtracking depends on user-given ""black box procedures"" that define the problem to be solved, the nature of the partial candidates, and how they are extended into complete candidates. It is therefore a metaheuristic rather than a specific algorithm – although, unlike many other meta-heuristics, it is guaranteed to find all solutions to a finite problem in a bounded amount of time.
The term ""backtrack"" was coined by American mathematician D. H. Lehmer in the 1950s. The pioneer string-processing language SNOBOL (1962) may have been the first to provide a built-in general backtracking facility."
Bailey–Borwein–Plouffe formula,"The Bailey–Borwein–Plouffe formula (BBP formula) is a spigot algorithm for computing the nth binary digit of pi (symbol: π) using base 16 math. The formula can directly calculate the value of any given digit of π without calculating the preceding digits. The BBP is a summation-style formula that was discovered in 1995 by Simon Plouffe and was named after the authors of the paper in which the formula was published, David H. Bailey, Peter Borwein, and Simon Plouffe. Before that paper, it had been published by Plouffe on his own site. The formula is
.
The discovery of this formula came as a surprise. For centuries it had been assumed that there was no way to compute the nth digit of π without calculating all of the preceding n − 1 digits.
Since this discovery, many formulas for other irrational constants have been discovered of the general form

where α is the constant and p and q are polynomials in integer coefficients and b ≥ 2 is an integer base.
Formulas in this form are known as BBP-type formulas. Certain combinations of specific p, q and b result in well-known constants, but there is no systematic algorithm for finding the appropriate combinations; known formulas are discovered through experimental mathematics."
Banker's algorithm,"The Banker's algorithm, sometimes referred to as the avoidance algorithm, is a resource allocation and deadlock avoidance algorithm developed by Edsger Dijkstra that tests for safety by simulating the allocation of predetermined maximum possible amounts of all resources, and then makes an ""s-state"" check to test for possible deadlock conditions for all other pending activities, before deciding whether allocation should be allowed to continue.
The algorithm was developed in the design process for the THE operating system and originally described (in Dutch) in EWD108. When a new process enters a system, it must declare the maximum number of instances of each resource type that it may ever claim; clearly, that number may not exceed the total number of resources in the system. Also, when a process gets all its requested resources it must return them in a finite amount of time."
Basic Local Alignment Search Tool,"In bioinformatics, BLAST for Basic Local Alignment Search Tool is an algorithm for comparing primary biological sequence information, such as the amino-acid sequences of different proteins or the nucleotides of DNA sequences. A BLAST search enables a researcher to compare a query sequence with a library or database of sequences, and identify library sequences that resemble the query sequence above a certain threshold.
Different types of BLASTs are available according to the query sequences. For example, following the discovery of a previously unknown gene in the mouse, a scientist will typically perform a BLAST search of the human genome to see if humans carry a similar gene; BLAST will identify sequences in the human genome that resemble the mouse gene based on similarity of sequence. The BLAST algorithm and program were designed by Stephen Altschul, Warren Gish, Webb Miller, Eugene Myers, and David J. Lipman at the National Institutes of Health and was published in the Journal of Molecular Biology in 1990 and cited over 50,000 times."
Baum–Welch algorithm,"In electrical engineering, computer science, statistical computing and bioinformatics, the Baum–Welch algorithm is used to find the unknown parameters of a hidden Markov model (HMM). It makes use of the forward-backward algorithm and is named for Leonard E. Baum and Lloyd R. Welch."
Bead sort,"Bead sort is a natural sorting algorithm, developed by Joshua J. Arulanandham, Cristian S. Calude and Michael J. Dinneen in 2002, and published in The Bulletin of the European Association for Theoretical Computer Science. Both digital and analog hardware implementations of bead sort can achieve a sorting time of O(n); however, the implementation of this algorithm tends to be significantly slower in software and can only be used to sort lists of positive integers. Also, it would seem that even in the best case, the algorithm requires O(n2) space.

"
Beam search,"In computer science, beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic which attempts to predict how close a partial solution is to a complete solution (goal state). But in beam search, only a predetermined number of best partial solutions are kept as candidates.
Beam search uses breadth-first search to build its search tree. At each level of the tree, it generates all successors of the states at the current level, sorting them in increasing order of heuristic cost. However, it only stores a predetermined number of best states at each level (called the beam width). Only those states are expanded next. The greater the beam width, the fewer states are pruned. With an infinite beam width, no states are pruned and beam search is identical to breadth-first search. The beam width bounds the memory required to perform the search. Since a goal state could potentially be pruned, beam search sacrifices completeness (the guarantee that an algorithm will terminate with a solution, if one exists). Beam search is not optimal (that is, there is no guarantee that it will find the best solution). It returns the first solution found.
The beam width can either be fixed or variable. One approach that uses a variable beam width starts with the width at a minimum. If no solution is found, the beam is widened and the procedure is repeated."
Beam stack search,"Beam Stack Search is a search algorithm that combines chronological backtracking (that is, depth-first search) with beam search and is similar to Depth-First Beam Search. Both search algorithms are anytime algorithms that find good but likely sub-optimal solutions quickly, like beam search, then backtrack and continue to find improved solutions until convergence to an optimal solution.

"
Beam tracing,"Beam tracing is an algorithm to simulate wave propagation. It was developed in the context of computer graphics to render 3D scenes, but it has been also used in other similar areas such as acoustics and electromagnetism simulations.
Beam tracing is a derivative of the ray tracing algorithm that replaces rays, which have no thickness, with beams. Beams are shaped like unbounded pyramids, with (possibly complex) polygonal cross sections. Beam tracing was first proposed by Paul Heckbert and Pat Hanrahan.
In beam tracing, a pyramidal beam is initially cast through the entire viewing frustum. This initial viewing beam is intersected with each polygon in the environment, typically from nearest to farthest. Each polygon that intersects with the beam must be visible, and is removed from the shape of the beam and added to a render queue. When a beam intersects with a reflective or refractive polygon, a new beam is created in a similar fashion to ray-tracing.
A variant of beam tracing casts a pyramidal beam through each pixel of the image plane. This is then split up into sub-beams based on its intersection with scene geometry. Reflection and transmission (refraction) rays are also replaced by beams.This sort of implementation is rarely used, as the geometric processes involved are much more complex and therefore expensive than simply casting more rays through the pixel. Cone tracing is a similar technique using a cone instead of a complex pyramid.
Beam tracing solves certain problems related to sampling and aliasing, which can plague conventional ray tracing approaches. Since beam tracing effectively calculates the path of every possible ray within each beam (which can be viewed as a dense bundle of adjacent rays), it is not as prone to under-sampling (missing rays) or over-sampling (wasted computational resources). The computational complexity associated with beams has made them unpopular for many visualization applications. In recent years, Monte Carlo algorithms like distributed ray tracing (and Metropolis light transport?) have become more popular for rendering calculations.
A 'backwards' variant of beam tracing casts beams from the light source into the environment. Similar to backwards raytracing and photon mapping, backwards beam tracing may be used to efficiently model lighting effects such as caustics. Recently the backwards beam tracing technique has also been extended to handle glossy to diffuse material interactions (glossy backward beam tracing) such as from polished metal surfaces.
Beam tracing has been successfully applied to the fields of acoustic modelling and electromagnetic propagation modelling. In both of these applications, beams are used as an efficient way to track deep reflections from a source to a receiver (or vice versa). Beams can provide a convenient and compact way to represent visibility. Once a beam tree has been calculated, one can use it to readily account for moving transmitters or receivers.
Beam tracing is related in concept to cone tracing."
Bees algorithm,"In computer science and operations research, the Bees Algorithm is a population-based search algorithm which was developed in 2005. It mimics the food foraging behaviour of honey bee colonies. In its basic version the algorithm performs a kind of neighbourhood search combined with global search, and can be used for both combinatorial optimization and continuous optimization. The only condition for the application of the Bees Algorithm is that some measure of topological distance between the solutions is defined. The effectiveness and specific abilities of the Bees Algorithm have been proven in a number of studies."
Bellman–Ford algorithm,"The Bellman–Ford algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph. It is slower than Dijkstra's algorithm for the same problem, but more versatile, as it is capable of handling graphs in which some of the edge weights are negative numbers. The algorithm is named after two of its developers, Richard Bellman and Lester Ford, Jr., who published it in 1958 and 1956, respectively; however, Edward F. Moore also published the same algorithm in 1957, and for this reason it is also sometimes called the Bellman–Ford–Moore algorithm.
Negative edge weights are found in various applications of graphs, hence the usefulness of this algorithm. If a graph contains a ""negative cycle"" (i.e. a cycle whose edges sum to a negative value) that is reachable from the source, then there is no cheapest path: any path can be made cheaper by one more walk around the negative cycle. In such a case, the Bellman–Ford algorithm can detect negative cycles and report their existence."
Benson's algorithm,"Benson's algorithm, named after Harold Benson, is a method for solving linear multi-objective optimization problems. This works by finding the ""efficient extreme points in the outcome set"". The primary concept in Benson's algorithm is to evaluate the upper image of the vector optimization problem by cutting planes."
Berkeley algorithm,"The Berkeley algorithm is a method of clock synchronisation in distributed computing which assumes no machine has an accurate time source. It was developed by Gusella and Zatti at the University of California, Berkeley in 1989  and like Cristian's algorithm is intended for use within intranets."
Berlekamp–Massey algorithm,"The Berlekamp–Massey algorithm is an algorithm that will find the shortest linear feedback shift register (LFSR) for a given binary output sequence. The algorithm will also find the minimal polynomial of a linearly recurrent sequence in an arbitrary field. The field requirement means that the Berlekamp–Massey algorithm requires all non-zero elements to have a multiplicative inverse. Reeds and Sloane offer an extension to handle a ring.
Elwyn Berlekamp invented an algorithm for decoding Bose–Chaudhuri–Hocquenghem (BCH) codes. James Massey recognized its application to linear feedback shift registers and simplified the algorithm. Massey termed the algorithm the LFSR Synthesis Algorithm (Berlekamp Iterative Algorithm), but it is now known as the Berlekamp–Massey algorithm."
Best-first search,"Best-first search is a search algorithm which explores a graph by expanding the most promising node chosen according to a specified rule.
Judea Pearl described best-first search as estimating the promise of node n by a ""heuristic evaluation function  which, in general, may depend on the description of n, the description of the goal, the information gathered by the search up to that point, and most important, on any extra knowledge about the problem domain.""
Some authors have used ""best-first search"" to refer specifically to a search with a heuristic that attempts to predict how close the end of a path is to a solution, so that paths which are judged to be closer to a solution are extended first. This specific type of search is called greedy best-first search or pure heuristic search.
Efficient selection of the current best candidate for extension is typically implemented using a priority queue.
The A* search algorithm is an example of best-first search, as is B*. Best-first algorithms are often used for path finding in combinatorial search. (Note that neither A* nor B* is a greedy best-first search as they incorporate the distance from start in addition to estimated distances to the goal.)"
Best Bin First,"Best bin first is a search algorithm that is designed to efficiently find an approximate solution to the nearest neighbor search problem in very-high-dimensional spaces. The algorithm is based on a variant of the kd-tree search algorithm which makes indexing higher-dimensional spaces possible. Best bin first is an approximate algorithm which returns the nearest neighbor for a large fraction of queries and a very close neighbor otherwise.

"
Bidirectional search,"Bidirectional search is a graph search algorithm that finds a shortest path from an initial vertex to a goal vertex in a directed graph. It runs two simultaneous searches: one forward from the initial state, and one backward from the goal, stopping when the two meet in the middle. The reason for this approach is that in many cases it is faster: for instance, in a simplified model of search problem complexity in which both searches expand a tree with branching factor b, and the distance from start to goal is d, each of the two searches has complexity O(bd/2) (in Big O notation), and the sum of these two search times is much less than the O(bd) complexity that would result from a single search from the beginning to the goal.
As in A* search, bi-directional search can be guided by a heuristic estimate of the remaining distance to the goal (in the forward tree) or from the start (in the backward tree).
Ira Pohl (1971) was the first one to design and implement a bi-directional heuristic search algorithm. Andrew Goldberg and others explained the correct termination conditions for the bidirectional version of Dijkstra’s Algorithm."
Binary GCD algorithm,"The binary GCD algorithm, also known as Stein's algorithm, is an algorithm that computes the greatest common divisor of two nonnegative integers. Stein's algorithm uses simpler arithmetic operations than the conventional Euclidean algorithm; it replaces division with arithmetic shifts, comparisons, and subtraction. Although the algorithm was first published by the Israeli physicist and programmer Josef Stein in 1967, it may have been known in 1st-century China.

"
Binary search algorithm,"In computer science, a binary search or half-interval search algorithm finds the position of a target value within a sorted array. The binary search algorithm can be classified as a dichotomic divide-and-conquer search algorithm and executes in logarithmic time."
Binary splitting,"In mathematics, binary splitting is a technique for speeding up numerical evaluation of many types of series with rational terms. In particular, it can be used to evaluate hypergeometric series at rational points. Given a series

where pn and qn are integers, the goal of binary splitting is to compute integers P(a, b) and Q(a, b) such that

The splitting consists of setting m = [(a + b)/2] and recursively computing P(a, b) and Q(a, b) from P(a, m), P(m, b), Q(a, m), and Q(m, b). When a and b are sufficiently close, P(a, b) and Q(a, b) can be computed directly from pa...pb and qa...qb.
Binary splitting requires more memory than direct term-by-term summation, but is asymptotically faster since the sizes of all occurring subproducts are reduced. Additionally, whereas the most naive evaluation scheme for a rational series uses a full-precision division for each term in the series, binary splitting requires only one final division at the target precision; this is not only faster, but conveniently eliminates rounding errors. To take full advantage of the scheme, fast multiplication algorithms such as Toom–Cook and Schönhage–Strassen must be used; with ordinary O(n2) multiplication, binary splitting may render no speedup at all or be slower.
Since all subdivisions of the series can be computed independently of each other, binary splitting lends well to parallelization and checkpointing.
In a less specific sense, binary splitting may also refer to any divide and conquer algorithm that always divides the problem in two halves."
Bitap algorithm,"The bitap algorithm (also known as the shift-or, shift-and or Baeza-Yates–Gonnet algorithm) is an approximate string matching algorithm. The algorithm tells whether a given text contains a substring which is ""approximately equal"" to a given pattern, where approximate equality is defined in terms of Levenshtein distance — if the substring and pattern are within a given distance k of each other, then the algorithm considers them equal. The algorithm begins by precomputing a set of bitmasks containing one bit for each element of the pattern. Then it is able to do most of the work with bitwise operations, which are extremely fast.
The bitap algorithm is perhaps best known as one of the underlying algorithms of the Unix utility agrep, written by Udi Manber, Sun Wu, and Burra Gopal. Manber and Wu's original paper gives extensions of the algorithm to deal with fuzzy matching of general regular expressions.
Due to the data structures required by the algorithm, it performs best on patterns less than a constant length (typically the word length of the machine in question), and also prefers inputs over a small alphabet. Once it has been implemented for a given alphabet and word length m, however, its running time is completely predictable — it runs in O(mn) operations, no matter the structure of the text or the pattern.
The bitap algorithm for exact string searching was invented by Bálint Dömölki in 1964[1][2] and extended by R. K. Shyamasundar in 1977[3], before being reinvented in the context of fuzzy string searching by Manber and Wu in 1991[4][5] based on work done by Ricardo Baeza-Yates and Gaston Gonnet[6]. The algorithm was improved by Baeza-Yates and Navarro in 1996[7] and later by Gene Myers for long patterns in 1998[8]."
Bitonic sorter,"Bitonic mergesort is a parallel algorithm for sorting. It is also used as a construction method for building a sorting network. The algorithm was devised by Ken Batcher. The resulting sorting networks consist of  comparators and have a delay of , where  is the number of items to be sorted.
A sorted sequence is a monotonically non-decreasing (or non-increasing) sequence. A bitonic sequence is a sequence with  for some , or a circular shift of such a sequence."
Block Truncation Coding,"Block Truncation Coding, or BTC, is a type of lossy image compression technique for greyscale images. It divides the original images into blocks and then uses a quantiser to reduce the number of grey levels in each block whilst maintaining the same mean and standard deviation. It is an early predecessor of the popular hardware DXTC technique, although BTC compression method was first adapted to colour long before DXTC using a very similar approach called Color Cell Compression. BTC has also been adapted to video compression 
BTC was first proposed by E.J Delp and O.R. Mitchell  at Purdue University. Another variation of BTC is Absolute Moment Block Truncation Coding or AMBTC, in which instead of using the standard deviation the first absolute moment is preserved along with the mean. AMBTC is computationally simpler than BTC and also typically results in a lower Mean Squared Error (MSE). AMBTC was proposed by Maximo Lema and Robert Mitchell.
Using sub-blocks of 4x4 pixels gives a compression ratio of 4:1 assuming 8-bit integer values are used during transmission or storage. Larger blocks allow greater compression (""a"" and ""b"" values spread over more pixels) however quality also reduces with the increase in block size due to the nature of the algorithm.
The BTC algorithm was used for compressing Mars Pathfinder's rover images."
Block nested loop,"A block-nested loop (BNL) is an algorithm used to join two relations in a relational database.
This algorithm is a variation on the simple nested loop join used to join two relations  and  (the ""outer"" and ""inner"" join operands, respectively). Suppose . In a traditional nested loop join,  will be scanned once for every tuple of . If there are many qualifying  tuples, and particularly if there is no applicable index for the join key on , this operation will be very expensive.
The block nested loop join algorithm improves on the simple nested loop join by only scanning  once for every group of  tuples. For example, one variant of the block nested loop join reads an entire page of  tuples into memory and loads them into a hash table. It then scans , and probes the hash table to find  tuples that match any of the tuples in the current page of . This reduces the number of scans of  that are necessary.
A more aggressive variant of this algorithm loads as many pages of  as can be fit in the available memory, loading all such tuples into a hash table, and then repeatedly scans . This further reduces the number of scans of  that are necessary. In fact, this algorithm is essentially a special-case of the classic hash join algorithm.
The block nested loop runs in  I/Os where  is the number of available pages of internal memory and  and  is size of  and  respectively in pages. Note that block nested loop runs in  I/Os if  fits in the available internal memory."
Bloom filter,"A Bloom filter is a space-efficient probabilistic data structure, conceived by Burton Howard Bloom in 1970, that is used to test whether an element is a member of a set. False positive matches are possible, but false negatives are not, thus a Bloom filter has a 100% recall rate. In other words, a query returns either ""possibly in set"" or ""definitely not in set"". Elements can be added to the set, but not removed (though this can be addressed with a ""counting"" filter). The more elements that are added to the set, the larger the probability of false positives.
Bloom proposed the technique for applications where the amount of source data would require an impractically large amount of memory if ""conventional"" error-free hashing techniques were applied. He gave the example of a hyphenation algorithm for a dictionary of 500,000 words, out of which 90% follow simple hyphenation rules, but the remaining 10% require expensive disk accesses to retrieve specific hyphenation patterns. With sufficient core memory, an error-free hash could be used to eliminate all unnecessary disk accesses; on the other hand, with limited core memory, Bloom's technique uses a smaller hash area but still eliminates most unnecessary accesses. For example, a hash area only 15% of the size needed by an ideal error-free hash still eliminates 85% of the disk accesses, an 85–15 form of the Pareto principle (Bloom (1970)).
More generally, fewer than 10 bits per element are required for a 1% false positive probability, independent of the size or number of elements in the set (Bonomi et al. (2006))."
Bogosort,"In computer science, bogosort (also stupid sort, slowsort, random sort, shotgun sort or monkey sort) is a particularly ineffective sorting algorithm based on the generate and test paradigm. It is not useful for sorting, but may be used for educational purposes, to contrast it with other more realistic algorithms; it has also been used as an example in logic programming. If bogosort were used to sort a deck of cards, it would consist of checking if the deck were in order, and if it were not, throwing the deck into the air, picking the cards up at random, and repeating the process until the deck is sorted. Its name comes from the word bogus."
Boosting (meta-algorithm),"Boosting is a machine learning ensemble meta-algorithm for reducing bias primarily and also variance in supervised learning, and a family of machine learning algorithms which convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): Can a set of weak learners create a single strong learner? A weak learner is defined to be a classifier which is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.
Robert Schapire's affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.
When first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. ""Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm […] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner]."" Algorithms that achieve hypothesis boosting quickly became simply known as ""boosting"". Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting."
Bootstrap aggregating,"Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach."
Borwein's algorithm,"In mathematics, Borwein's algorithm is an algorithm devised by Jonathan and Peter Borwein to calculate the value of 1/π. They devised several other algorithms. They published a book: Jonathon M. Borwein, Peter B. Borwein, Pi and the AGM - A Study in Analytic Number Theory and Computational Complexity, Wiley, New York, 1987. Many of their results are available in: Jorg Arndt, Christoph Haenel, Pi Unleashed, Springer, Berlin, 2001, ISBN 3-540-66572-2."
Borůvka's algorithm,"Borůvka's algorithm is an algorithm for finding a minimum spanning tree in a graph for which all edge weights are distinct.
It was first published in 1926 by Otakar Borůvka as a method of constructing an efficient electricity network for Moravia. The algorithm was rediscovered by Choquet in 1938; again by Florek, Łukasiewicz, Perkal, Steinhaus, and Zubrzycki in 1951; and again by Sollin  in 1965. Because Sollin was the only computer scientist in this list living in an English speaking country, this algorithm is frequently called Sollin's algorithm, especially in the parallel computing literature.
The algorithm begins by first examining each vertex and adding the cheapest edge from that vertex to another in the graph, without regard to already added edges, and continues joining these groupings in a like manner until a tree spanning all vertices is completed."
Bowyer–Watson algorithm,"In computational geometry, the Bowyer–Watson algorithm is a method for computing the Delaunay triangulation of a finite set of points in any number of dimensions. The algorithm can be used to obtain a Voronoi diagram of the points, which is the dual graph of the Delaunay triangulation.
The Bowyer–Watson algorithm is an incremental algorithm. It works by adding points, one at a time, to a valid Delaunay triangulation of a subset of the desired points. After every insertion, any triangles whose circumcircles contain the new point are deleted, leaving a star-shaped polygonal hole which is then re-triangulated using the new point. By using the connectivity of the triangulation to efficiently locate triangles to remove, the algorithm can take O(N log N) operations to triangulate N points, although special degenerate cases exist where this goes up to O(N2).
The algorithm is sometimes known just as the Bowyer Algorithm or the Watson Algorithm. Adrian Bowyer and David Watson devised it independently of each other at the same time, and each published a paper on it in the same issue of The Computer Journal (see below)."
Boyer–Moore string search algorithm,"In computer science, the Boyer–Moore string search algorithm is an efficient string searching algorithm that is the standard benchmark for practical string search literature. It was developed by Robert S. Boyer and J Strother Moore in 1977. The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. The Boyer-Moore algorithm uses information gathered during the preprocess step to skip sections of the text, resulting in a lower constant factor than many other string algorithms. In general, the algorithm runs faster as the pattern length increases. The key features of the algorithm are to match on the tail of the pattern rather than the head, and to skip along the text in jumps of multiple characters rather than searching every single character in the text."
Boyer–Moore–Horspool algorithm,"In computer science, the Boyer–Moore–Horspool algorithm or Horspool's algorithm is an algorithm for finding substrings in strings. It was published by Nigel Horspool in 1980.
It is a simplification of the Boyer–Moore string search algorithm which is related to the Knuth–Morris–Pratt algorithm. The algorithm trades space for time in order to obtain an average-case complexity of O(N) on random text, although it has O(MN) in the worst case, where the length of the pattern is M and the length of the search string is N."
Branch and bound,"Branch and bound (BB or B&B) is an algorithm design paradigm for discrete and combinatorial optimization problems, as well as general real valued problems. A branch-and-bound algorithm consists of a systematic enumeration of candidate solutions by means of state space search: the set of candidate solutions is thought of as forming a rooted tree with the full set at the root. The algorithm explores branches of this tree, which represent subsets of the solution set. Before enumerating the candidate solutions of a branch, the branch is checked against upper and lower estimated bounds on the optimal solution, and is discarded if it cannot produce a better solution than the best one found so far by the algorithm.
The algorithm depends on the efficient estimation of the lower and upper bounds of a region/branch of the search space and approaches exhaustive enumeration as the size (n-dimensional volume) of the region tends to zero.
The method was first proposed by A. H. Land and A. G. Doig in 1960 for discrete programming, and has become the most commonly used tool for solving NP-hard optimization problems. The name ""branch and bound"" first occurred in the work of Little et al. on the traveling salesman problem."
Branch and cut,"Branch and cut is a method of combinatorial optimization for solving integer linear programs (ILPs), that is, linear programming (LP) problems where some or all the unknowns are restricted to integer values. Branch and cut involves running a branch and bound algorithm and using cutting planes to tighten the linear programming relaxations. Note that if cuts are only used to tighten the initial LP relaxation, the algorithm is called cut and branch."
Breadth-first search,"Breadth-first search (BFS) is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key') and explores the neighbor nodes first, before moving to the next level neighbors.
BFS was invented in the late 1950s by E. F. Moore, who used it to find the shortest path out of a maze, and discovered independently by C. Y. Lee as a wire routing algorithm (published 1961)."
Bresenham's line algorithm,"Bresenham's line algorithm is an algorithm that determines the points of an n-dimensional raster that should be selected in order to form a close approximation to a straight line between two points. It is commonly used to draw lines on a computer screen, as it uses only integer addition, subtraction and bit shifting, all of which are very cheap operations in standard computer architectures. It is one of the earliest algorithms developed in the field of computer graphics. An extension to the original algorithm may be used for drawing circles.
While algorithms such as Wu's algorithm are also frequently used in modern computer graphics because they can support antialiasing, the speed and simplicity of Bresenham's line algorithm means that it is still important. The algorithm is used in hardware such as plotters and in the graphics chips of modern graphics cards. It can also be found in many software graphics libraries. Because the algorithm is very simple, it is often implemented in either the firmware or the graphics hardware of modern graphics cards.
The label ""Bresenham"" is used today for a family of algorithms extending or modifying Bresenham's original algorithm."
Bron–Kerbosch algorithm,"In computer science, the Bron–Kerbosch algorithm is an algorithm for finding maximal cliques in an undirected graph. That is, it lists all subsets of vertices with the two properties that each pair of vertices in one of the listed subsets is connected by an edge, and no listed subset can have any additional vertices added to it while preserving its complete connectivity. The Bron–Kerbosch algorithm was designed by Dutch scientists Joep Kerbosch and Coenraad Bron, who published its description in 1973. Although other algorithms for solving the clique problem have running times that are, in theory, better on inputs that have few maximal independent sets, the Bron–Kerbosch algorithm and subsequent improvements to it are frequently reported as being more efficient in practice than the alternatives. It is well-known and widely used in application areas of graph algorithms such as computational chemistry.
A contemporaneous algorithm of Akkoyunlu (1973), although presented in different terms, can be viewed as being the same as the Bron–Kerbosch algorithm, as it generates the same recursive search tree."
BrownBoost,"BrownBoost is a boosting algorithm that may be robust to noisy datasets. BrownBoost is an adaptive version of the boost by majority algorithm. As is true for all boosting algorithms, BrownBoost is used in conjunction with other machine learning methods. BrownBoost was introduced by Yoav Freund in 2001."
Bruun's FFT algorithm,"Bruun's algorithm is a fast Fourier transform (FFT) algorithm based on an unusual recursive polynomial-factorization approach, proposed for powers of two by G. Bruun in 1978 and generalized to arbitrary even composite sizes by H. Murakami in 1996. Because its operations involve only real coefficients until the last computation stage, it was initially proposed as a way to efficiently compute the discrete Fourier transform (DFT) of real data. Bruun's algorithm has not seen widespread use, however, as approaches based on the ordinary Cooley–Tukey FFT algorithm have been successfully adapted to real data with at least as much efficiency. Furthermore, there is evidence that Bruun's algorithm may be intrinsically less accurate than Cooley–Tukey in the face of finite numerical precision (Storn, 1993).
Nevertheless, Bruun's algorithm illustrates an alternative algorithmic framework that can express both itself and the Cooley–Tukey algorithm, and thus provides an interesting perspective on FFTs that permits mixtures of the two algorithms and other generalizations.

"
Bubble sort,"Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the list to be sorted, compares each pair of adjacent items and swaps them if they are in the wrong order. The pass through the list is repeated until no swaps are needed, which indicates that the list is sorted. The algorithm, which is a comparison sort, is named for the way smaller elements ""bubble"" to the top of the list. Although the algorithm is simple, it is too slow and impractical for most problems even when compared to insertion sort. It can be practical if the input is usually in sort order but may occasionally have some out-of-order elements nearly in position."
Bucket sort,"Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a distribution sort, and is a cousin of radix sort in the most to least significant digit flavour. Bucket sort is a generalization of pigeonhole sort. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity estimates involve the number of buckets.
Bucket sort works as follows:
Set up an array of initially empty ""buckets"".
Scatter: Go over the original array, putting each object in its bucket.
Sort each non-empty bucket.
Gather: Visit the buckets in order and put all elements back into the original array."
Buddy memory allocation,"The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as possible. This system makes use of splitting memory into halves to try to give a best-fit. According to Donald Knuth, the buddy system was invented in 1963 by Harry Markowitz, who won the 1990 Nobel Memorial Prize in Economics, and was first described by Kenneth C. Knowlton (published 1965). Buddy memory allocation is relatively easy to implement. It supports limited but efficient splitting and coalescing of memory blocks."
Bully algorithm,"The bully algorithm is a programming mechanism that applies a hierarchy to nodes on a system, making a process coordinator or slave. This is used as a method in distributed computing for dynamically electing a coordinator by process ID number. The process with the highest process ID number is selected as the coordinator."
Burrows–Wheeler transform,"The Burrows–Wheeler transform (BWT, also called block-sorting compression) rearranges a character string into runs of similar characters. This is useful for compression, since it tends to be easy to compress a string that has runs of repeated characters by techniques such as move-to-front transform and run-length encoding. More importantly, the transformation is reversible, without needing to store any additional data. The BWT is thus a ""free"" method of improving the efficiency of text compression algorithms, costing only some extra computation."
Burstsort,"Burstsort and its variants are cache-efficient algorithms for sorting strings and are faster than radix sort for large data sets of common strings, first published in 2003.
Burstsort algorithms use a trie to store prefixes of strings, with growable arrays of pointers as end nodes containing sorted, unique, suffixes (referred to as buckets). Some variants copy the string tails into the buckets. As the buckets grow beyond a predetermined threshold, the buckets are ""burst"", giving the sort its name. A more recent variant uses a bucket index with smaller sub-buckets to reduce memory usage. Most implementations delegate to multikey quicksort, an extension of three-way radix quicksort, to sort the contents of the buckets. By dividing the input into buckets with common prefixes, the sorting can be done in a cache-efficient manner.
Burstsort was introduced as a sort that is similar to MSD Radix Sort, but is faster due to being aware of caching and related radixes being stored closer to each other due to specifics of trie structure. It exploits specifics of strings that are usually encountered in real world. And even though asymptotically it is the same as radix sort, with time complexity of O(wn) (w - word length and n - number of strings to be sorted), but due to more optimal memory distribution it tends to be twice as fast on big data sets of strings."
Buzen's algorithm,"In queueing theory, a discipline within the mathematical theory of probability, Buzen's algorithm (or convolution algorithm) is an algorithm for calculating the normalization constant G(N) in the Gordon–Newell theorem. This method was first proposed by Jeffrey P. Buzen in 1973. Computing G(N) is required to compute the stationary probability distribution of a closed queueing network.
Performing a naïve computation of the normalising constant requires enumeration of all states. For a system with N jobs and M states there are  states. Buzen's algorithm ""computes G(1), G(2), ..., G(N) using a total of NM multiplications and NM additions."" This is a significant improvement and allows for computations to be performed with much larger networks.

"
Byte pair encoding,"Byte pair encoding or digram coding is a simple form of data compression in which the most common pair of consecutive bytes of data is replaced with a byte that does not occur within that data. A table of the replacements is required to rebuild the original data. The algorithm was first described publicly by Philip Gage in a February 1994 article ""A New Algorithm for Data Compression"" in the C Users Journal."
C4.5 algorithm,"C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan. C4.5 is an extension of Quinlan's earlier ID3 algorithm. The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier.
It became quite popular after ranking #1 in the Top 10 Algorithms in Data Mining pre-eminent paper published by Springer LNCS in 2008.

"
CYK algorithm,"In computer science, the Cocke–Younger–Kasami algorithm (alternatively called CYK, or CKY) is a parsing algorithm for context-free grammars, named after its inventors, John Cocke, Daniel Younger and Tadao Kasami. It employs bottom-up parsing and dynamic programming.
The standard version of CYK operates only on context-free grammars given in Chomsky normal form (CNF). However any context-free grammar may be transformed to a CNF grammar expressing the same language (Sipser 1997).
The importance of the CYK algorithm stems from its high efficiency in certain situations. Using Landau symbols, the worst case running time of CYK is , where n is the length of the parsed string and |G| is the size of the CNF grammar G. This makes it one of the most efficient parsing algorithms in terms of worst-case asymptotic complexity, although other algorithms exist with better average running time in many practical scenarios."
Cache algorithms,"In computing, cache algorithms (also frequently called cache replacement algorithms or cache replacement policies) are optimizing instructions—​or algorithms—​that a computer program or a hardware-maintained structure can follow in order to manage a cache of information stored on the computer. When the cache is full, the algorithm must choose which items to discard to make room for the new ones."
Cannon's algorithm,"In computer science, Cannon's algorithm is a distributed algorithm for matrix multiplication for two-dimensional meshes first described in 1969 by Lynn Elliot Cannon.
It is especially suitable for computers laid out in an N × N mesh. While Cannon's algorithm works well in homogeneous 2D grids, extending it to heterogeneous 2D grids has been shown to be difficult.
The main advantage of the algorithm is that its storage requirements remain constant and are independent of the number of processors.
The Scalable Universal Matrix Multiplication Algorithm (SUMMA) is a more practical algorithm that requires less workspace and overcomes the need for a square 2D grid. It is used by the ScaLAPACK, PLAPACK, and Elemental libraries."
Canopy clustering algorithm,"The canopy clustering algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and Lyle Ungar in 2000. It is often used as preprocessing step for the K-means algorithm or the Hierarchical clustering algorithm. It is intended to speed up clustering operations on large data sets, where using another algorithm directly may be impractical due to the size of the data set.
The algorithm proceeds as follows, using two thresholds  (the loose distance) and  (the tight distance), where  .
Begin with the set of data points to be clustered.
Remove a point from the set, beginning a new 'canopy'.
For each point left in the set, assign it to the new canopy if the distance less than the loose distance .
If the distance of the point is additionally less than the tight distance , remove it from the original set.
Repeat from step 2 until there are no more data points in the set to cluster.
These relatively cheaply clustered canopies can be sub-clustered using a more expensive but accurate algorithm.
An important note is that individual data points may be part of several canopies. As an additional speed-up, an approximate and fast distance metric can be used for 3, where a more accurate and slow distance metric can be used for step 4.
Since the algorithm uses distance functions and requires the specification of distance thresholds, its applicability for high-dimensional data is limited by the curse of dimensionality. Only when a cheap and approximative – low-dimensional – distance function is available, the produced canopies will preserve the clusters produced by K-means.

"
Chaitin's algorithm,"Chaitin's algorithm is a bottom-up, graph coloring register allocation algorithm that uses cost/degree as its spill metric. It is named after its designer, Gregory Chaitin. Chaitin's algorithm was the first register allocation algorithm that made use of coloring of the interference graph for both register allocations and spilling.
Chaitin's algorithm was presented on the 1982 SIGPLAN Symposium on Compiler Construction, and published in the symposium proceedings. It was extension of an earlier 1981 paper on the use of graph coloring for register allocation. Chaitin's algorithm formed the basis of a large section of research into register allocators.

"
Chakravala method,"The chakravala method (Sanskrit: चक्रवाल विधि) is a cyclic algorithm to solve indeterminate quadratic equations, including Pell's equation. It is commonly attributed to Bhāskara II, (c. 1114 – 1185 CE) although some attribute it to Jayadeva (c. 950 ~ 1000 CE). Jayadeva pointed out that Brahmagupta's approach to solving equations of this type could be generalized, and he then described this general method, which was later refined by Bhāskara II in his Bijaganita treatise. He called it the Chakravala method: chakra meaning ""wheel"" in Sanskrit, a reference to the cyclic nature of the algorithm. E. O. Selenius held that no European performances at the time of Bhāskara, nor much later, exceeded its marvellous height of mathematical complexity.
This method is also known as the cyclic method and contains traces of mathematical induction."
Chan's algorithm,"In computational geometry, Chan's algorithm, named after Timothy M. Chan, is an optimal output-sensitive algorithm to compute the convex hull of a set P of n points, in 2- or 3-dimensional space. The algorithm takes O(n log h) time, where h is the number of vertices of the output (the convex hull). In the planar case, the algorithm combines an O(n log n) algorithm (Graham scan, for example) with Jarvis march, in order to obtain an optimal O(n log h) time. Chan's algorithm is notable because it is much simpler than the Kirkpatrick–Seidel algorithm, and it naturally extends to 3-dimensional space. This paradigm has been independently developed by Frank Nielsen in his Ph. D. thesis."
Christofides algorithm,"The goal of the Christofides approximation algorithm (named after Nicos Christofides) is to find a solution to the instances of the traveling salesman problem where the edge weights satisfy the triangle inequality. Let  be an instance of TSP, i.e.  is a complete graph on the set  of vertices with weight function  assigning a nonnegative real weight to every edge of .

"
Clock with Adaptive Replacement,"In a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which memory pages to page out (swap out, write to disk) when a page of memory needs to be allocated. Paging happens when a page fault occurs and a free page cannot be used to satisfy the allocation, either because there are none, or because the number of free pages is lower than some threshold.
When the page that was selected for replacement and paged out is referenced again it has to be paged in (read in from disk), and this involves waiting for I/O completion. This determines the quality of the page replacement algorithm: the less time waiting for page-ins, the better the algorithm. A page replacement algorithm looks at the limited information about accesses to the pages provided by hardware, and tries to guess which pages should be replaced to minimize the total number of page misses, while balancing this with the costs (primary storage and processor time) of the algorithm itself.
The page replacing problem is a typical online problem from the competitive analysis perspective in the sense that the optimal deterministic algorithm is known."
Closest pair problem,"The closest pair of points problem or closest pair problem is a problem of computational geometry: given n points in metric space, find a pair of points with the smallest distance between them. The closest pair problem for points in the Euclidean plane was among the first geometric problems which were treated at the origins of the systematic study of the computational complexity of geometric algorithms.
A naive algorithm of finding distances between all pairs of points and selecting the minimum requires O(dn2) time. It turns out that the problem may be solved in O(n log n) time in a Euclidean space or Lp space of fixed dimension d. In the algebraic decision tree model of computation, the O(n log n) algorithm is optimal. The optimality follows from the observation that the element uniqueness problem (with the lower bound of Ω(n log n) for time complexity) is reducible to the closest pair problem: checking whether the minimal distance is 0 after the solving of the closest pair problem answers the question whether there are two coinciding points.
In the computational model which assumes that the floor function is computable in constant time the problem can be solved in O(n log log n) time. If we allow randomization to be used together with the floor function, the problem can be solved in O(n) time."
Cocktail sort,"Cocktail sort, also known as bidirectional bubble sort, cocktail shaker sort, shaker sort (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is a variation of bubble sort that is both a stable sorting algorithm and a comparison sort. The algorithm differs from a bubble sort in that it sorts in both directions on each pass through the list. This sorting algorithm is only marginally more difficult to implement than a bubble sort, and solves the problem of turtles in bubble sorts. It provides only marginal performance improvements, and does not improve asymptotic performance; like the bubble sort, it is not of practical interest (insertion sort is preferred for simple sorts), though it finds some use in education."
Comb sort,Comb sort is a relatively simple sorting algorithm originally designed by Włodzimierz Dobosiewicz in 1980. Later it was rediscovered by Stephen Lacey and Richard Box in 1991. Comb sort improves on bubble sort.
Cone algorithm,"In computational geometry, the cone algorithm is an algorithm for identifying the particles that are near the surface of an object composed of discrete particles. Its applications include computational surface science and computational nano science. The cone algorithm was first described in a publication about nanogold in 2005.
The cone algorithm works well with clusters in condensed phases, including solid and liquid phases. It can handle the situations when one configuration includes multiple clusters or when holes exist inside clusters. It can also be applied to a cluster iteratively to identify multiple sub-surface layers.

"
Cone tracing,"Cone tracing and beam tracing are a derivative of the ray tracing algorithm that replaces rays, which have no thickness, with thick rays.

"
Congruence of squares,"In number theory, a congruence of squares is a congruence commonly used in integer factorization algorithms."
Context tree weighting,"The context tree weighting method (CTW) is a lossless compression and prediction algorithm by Willems, Shtarkov & Tjalkens 1995. The CTW algorithm is among the very few such algorithms that offer both theoretical guarantees and good practical performance (see, e.g. Begleiter, El-Yaniv & Yona 2004). The CTW algorithm is an “ensemble method,” mixing the predictions of many underlying variable order Markov models, where each such model is constructed using zero-order conditional probability estimators."
Convex hull algorithms,"Algorithms that construct convex hulls of various objects have a broad range of applications in mathematics and computer science.
In computational geometry, numerous algorithms are proposed for computing the convex hull of a finite set of points, with various computational complexities.
Computing the convex hull means that a non-ambiguous and efficient representation of the required convex shape is constructed. The complexity of the corresponding algorithms is usually estimated in terms of n, the number of input points, and h, the number of points on the convex hull."
Cooley–Tukey FFT algorithm,"The Cooley–Tukey algorithm, named after J.W. Cooley and John Tukey, is the most common fast Fourier transform (FFT) algorithm. It re-expresses the discrete Fourier transform (DFT) of an arbitrary composite size N = N1N2 in terms of smaller DFTs of sizes N1 and N2, recursively, to reduce the computation time to O(N log N) for highly composite N (smooth numbers). Because of the algorithm's importance, specific variants and implementation styles have become known by their own names, as described below.
Because the Cooley-Tukey algorithm breaks the DFT into smaller DFTs, it can be combined arbitrarily with any other algorithm for the DFT. For example, Rader's or Bluestein's algorithm can be used to handle large prime factors that cannot be decomposed by Cooley–Tukey, or the prime-factor algorithm can be exploited for greater efficiency in separating out relatively prime factors.
The algorithm, along with its recursive application, was invented by Carl Friedrich Gauss. Cooley and Tukey independently rediscovered and popularized it 160 years later."
Coppersmith–Winograd algorithm,"In linear algebra, the Coppersmith–Winograd algorithm, named after Don Coppersmith and Shmuel Winograd, was the asymptotically fastest known algorithm for square matrix multiplication until 2010. It can multiply two  matrices in  time  (see Big O notation). This is an improvement over the naïve  time algorithm and the  time Strassen algorithm. Algorithms with better asymptotic running time than the Strassen algorithm are rarely used in practice, because the large constant factors in their running times make them impractical. It is possible to improve the exponent further; however, the exponent must be at least 2 (because an  matrix has  values, and all of them have to be read at least once to calculate the exact result).
In 2010, Andrew Stothers gave an improvement to the algorithm,  In 2011, Virginia Williams combined a mathematical short-cut from Stothers' paper with her own insights and automated optimization on computers, improving the bound to  In 2014, François Le Gall simplified the methods of Williams and obtained an improved bound of 
The Coppersmith–Winograd algorithm is frequently used as a building block in other algorithms to prove theoretical time bounds. However, unlike the Strassen algorithm, it is not used in practice because it only provides an advantage for matrices so large that they cannot be processed by modern hardware.
Henry Cohn, Robert Kleinberg, Balázs Szegedy and Chris Umans have re-derived the Coppersmith–Winograd algorithm using a group-theoretic construction. They also showed that either of two different conjectures would imply that the optimal exponent of matrix multiplication is 2, as has long been suspected. However, they were not able to formulate a specific solution leading to a better running-time than Coppersmith-Winograd at the time.

"
Counting sort,"In computer science, counting sort is an algorithm for sorting a collection of objects according to keys that are small integers; that is, it is an integer sorting algorithm. It operates by counting the number of objects that have each distinct key value, and using arithmetic on those counts to determine the positions of each key value in the output sequence. Its running time is linear in the number of items and the difference between the maximum and minimum key values, so it is only suitable for direct use in situations where the variation in keys is not significantly greater than the number of items. However, it is often used as a subroutine in another sorting algorithm, radix sort, that can handle larger keys more efficiently.
Because counting sort uses key values as indexes into an array, it is not a comparison sort, and the Ω(n log n) lower bound for comparison sorting does not apply to it. Bucket sort may be used for many of the same tasks as counting sort, with a similar time analysis; however, compared to counting sort, bucket sort requires linked lists, dynamic arrays or a large amount of preallocated memory to hold the sets of items within each bucket, whereas counting sort instead stores a single number (the count of items) per bucket."
Cristian's algorithm,"Cristian's Algorithm (introduced by Flaviu Cristian in 1989) is a method for clock synchronization which can be used in many fields of distributive computer science but is primarily used in low-latency intranets. Cristian observed that this simple algorithm is probabilistic, in that it only achieves synchronization if the round-trip time (RTT) of the request is short compared to required accuracy. It also suffers in implementations using a single server, making it unsuitable for many distributive applications where redundancy may be crucial.

"
Cuthill–McKee algorithm,"In the mathematical subfield of matrix theory, the Cuthill–McKee algorithm (CM), named for Elizabeth Cuthill and J. McKee , is an algorithm to permute a sparse matrix that has a symmetric sparsity pattern into a band matrix form with a small bandwidth. The reverse Cuthill–McKee algorithm (RCM) due to Alan George is the same algorithm but with the resulting index numbers reversed. In practice this generally results in less fill-in than the CM ordering when Gaussian elimination is applied.
The Cuthill McKee algorithm is a variant of the standard breadth-first search algorithm used in graph algorithms. It starts with a peripheral node and then generates levels  for  until all nodes are exhausted. The set  is created from set  by listing all vertices adjacent to all nodes in . These nodes are listed in increasing degree. This last detail is the only difference with the breadth-first search algorithm.

"
Cycle detection,"In computer science, cycle detection is the algorithmic problem of finding a cycle in a sequence of iterated function values.
For any function ƒ that maps a finite set S to itself, and any initial value x0 in S, the sequence of iterated function values

must eventually use the same value twice: there must be some i ≠ j such that xi = xj. Once this happens, the sequence must continue periodically, by repeating the same sequence of values from xi to xj−1. Cycle detection is the problem of finding i and j, given ƒ and x0.

"
Cycle sort,"Cycle sort is an in-place, unstable sorting algorithm, a comparison sort that is theoretically optimal in terms of the total number of writes to the original array, unlike any other in-place sorting algorithm. It is based on the idea that the permutation to be sorted can be factored into cycles, which can individually be rotated to give a sorted result.
Unlike nearly every other sort, items are never written elsewhere in the array simply to push them out of the way of the action. Each value is either written zero times, if it's already in its correct position, or written one time to its correct position. This matches the minimal number of overwrites required for a completed in-place sort.
Minimizing the number of writes is useful when making writes to some huge data set is very expensive, such as with EEPROMs like Flash memory where each write reduces the lifespan of the memory."
D*,"D* (pronounced ""D star"") is any one of the following three related incremental search algorithms:
The original D*, by Anthony Stentz, is an informed incremental search algorithm.
Focused D* is an informed incremental heuristic search algorithm by Anthony Stentz that combines ideas of A* and the original D*. Focused D* resulted from a further development of the original D*.
D* Lite is an incremental heuristic search algorithm by Sven Koenig and Maxim Likhachev that builds on LPA*, an incremental heuristic search algorithm that combines ideas of A* and Dynamic SWSF-FP.
All three search algorithms solve the same assumption-based path planning problems, including planning with the freespace assumption, where a robot has to navigate to given goal coordinates in unknown terrain. It makes assumptions about the unknown part of the terrain (for example: that it contains no obstacles) and finds a shortest path from its current coordinates to the goal coordinates under these assumptions. The robot then follows the path. When it observes new map information (such as previously unknown obstacles), it adds the information to its map and, if necessary, replans a new shortest path from its current coordinates to the given goal coordinates. It repeats the process until it reaches the goal coordinates or determines that the goal coordinates cannot be reached. When traversing unknown terrain, new obstacles may be discovered frequently, so this replanning needs to be fast. Incremental (heuristic) search algorithms speed up searches for sequences of similar search problems by using experience with the previous problems to speed up the search for the current one. Assuming the goal coordinates do not change, all three search algorithms are more efficient than repeated A* searches.
D* and its variants have been widely used for mobile robot and autonomous vehicle navigation. Current systems are typically based on D* Lite rather than the original D* or Focused D*. In fact, even Stentz's lab uses D* Lite rather than D* in some implementations. Such navigation systems include a prototype system tested on the Mars rovers Opportunity and Spirit and the navigation system of the winning entry in the DARPA Urban Challenge, both developed at Carnegie Mellon University.
The original D* was introduced by Anthony Stentz in 1994. The name D* comes from the term ""Dynamic A*"", because the algorithm behaves like A* except that the arc costs can change as the algorithm runs."
DBSCAN,"Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996. It is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.
In 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, KDD."
DEFLATE (algorithm),"In computing, deflate is a data compression algorithm that uses a combination of the LZ77 algorithm and Huffman coding. It was originally defined by Phil Katz for version 2 of his PKZIP archiving tool and was later specified in RFC 1951.
The original algorithm as designed by Katz was patented as U.S. Patent 5,051,745 and assigned to PKWARE, Inc. As stated in the RFC document, Deflate is widely thought to be implementable in a manner not covered by patents. This has led to its widespread use, for example in gzip compressed files, PNG image files and the .ZIP file format for which Katz originally designed it."
Daitch–Mokotoff Soundex,"Daitch–Mokotoff Soundex (D–M Soundex) is a phonetic algorithm invented in 1985 by Jewish genealogists Gary Mokotoff and Randy Daitch. It is a refinement of the Russell and American Soundex algorithms designed to allow greater accuracy in matching of Slavic and Yiddish surnames with similar pronunciation but differences in spelling.
Daitch–Mokotoff Soundex is sometimes referred to as ""Jewish Soundex"" and ""Eastern European Soundex"", although the authors discourage use of these nicknames for the algorithm because the algorithm itself is independent of the fact the motivation for creating the new system was the poor results of predecessor systems when dealing with Slavic and Yiddish surnames."
Damm algorithm,"In error detection, the Damm algorithm is a check digit algorithm that detects all single-digit errors and all adjacent transposition errors. It was presented by H. Michael Damm in 2004.

"
Dancing Links,"In computer science, dancing links, also known as DLX, is the technique suggested by Donald Knuth to efficiently implement his Algorithm X. Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm that finds all solutions to the exact cover problem. Some of the better-known exact cover problems include tiling, the n queens problem, and Sudoku.
The name dancing links stems from the way the algorithm works, as iterations of the algorithm cause the links to ""dance"" with partner links so as to resemble an ""exquisitely choreographed dance."" Knuth credits Hiroshi Hitotsumatsu and Kōhei Noshita with having invented the idea in 1979, but it is his paper which has popularized it."
Dekker's algorithm,"Dekker's algorithm is the first known correct solution to the mutual exclusion problem in concurrent programming. The solution is attributed to Dutch mathematician Th. J. Dekker by Edsger W. Dijkstra in an unpublished paper on sequential process descriptions and his manuscript on cooperating sequential processes. It allows two threads to share a single-use resource without conflict, using only shared memory for communication.
It avoids the strict alternation of a naïve turn-taking algorithm, and was one of the first mutual exclusion algorithms to be invented."
Delayed column generation,"Column generation or delayed column generation is an efficient algorithm for solving larger linear programs.
The overarching idea is that many linear programs are too large to consider all the variables explicitly. Since most of the variables will be non-basic and assume a value of zero in the optimal solution, only a subset of variables need to be considered in theory when solving the problem. Column generation leverages this idea to generate only the variables which have the potential to improve the objective function—that is, to find variables with negative reduced cost (assuming without loss of generality that the problem is a minimization problem).
The problem being solved is split into two problems: the master problem and the subproblem. The master problem is the original problem with only a subset of variables being considered. The subproblem is a new problem created to identify a new variable. The objective function of the subproblem is the reduced cost of the new variable with respect to the current dual variables, and the constraints require that the variable obey the naturally occurring constraints.
The process works as follows. The master problem is solved—from this solution, we are able to obtain dual prices for each of the constraints in the master problem. This information is then utilized in the objective function of the subproblem. The subproblem is solved. If the objective value of the subproblem is negative, a variable with negative reduced cost has been identified. This variable is then added to the master problem, and the master problem is re-solved. Re-solving the master problem will generate a new set of dual values, and the process is repeated until no negative reduced cost variables are identified. The subproblem returns a solution with non-negative reduced cost, we can conclude that the solution to the master problem is optimal.
In many cases, this allows large linear programs that had been previously considered intractable to be solved. The classical example of a problem where this is successfully used is the cutting stock problem. One particular technique in linear programming which uses this kind of approach is the Dantzig–Wolfe decomposition algorithm. Additionally, column generation has been applied to many problems such as crew scheduling, vehicle routing, and the capacitated p-median problem."
Depth-first search,"Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. One starts at the root (selecting some arbitrary node as the root in the case of a graph) and explores as far as possible along each branch before backtracking.
A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Trémaux as a strategy for solving mazes."
Deutsch-Jozsa algorithm,"The Deutsch–Jozsa algorithm is a quantum algorithm, proposed by David Deutsch and Richard Jozsa in 1992 with improvements by Richard Cleve, Artur Ekert, Chiara Macchiavello, and Michele Mosca in 1998. Although of little practical use, it is one of the first examples of a quantum algorithm that is exponentially faster than any possible deterministic classical algorithm. It is also a deterministic algorithm, meaning that it always produces an answer, and that answer is always correct.

"
Dictionary coder,"A dictionary coder, also sometimes known as a substitution coder, is a class of lossless data compression algorithms which operate by searching for matches between the text to be compressed and a set of strings contained in a data structure (called the 'dictionary') maintained by the encoder. When the encoder finds such a match, it substitutes a reference to the string's position in the data structure."
Difference-map algorithm,"The difference-map algorithm is a search algorithm for general constraint satisfaction problems. It is a meta-algorithm in the sense that it is built from more basic algorithms that perform projections onto constraint sets. From a mathematical perspective, the difference-map algorithm is a dynamical system based on a mapping of Euclidean space. Solutions are encoded as fixed points of the mapping.
Although originally conceived as a general method for solving the phase problem, the difference-map algorithm has been used for the boolean satisfiability problem, protein structure prediction, Ramsey numbers, diophantine equations, and Sudoku, as well as sphere- and disk-packing problems. Since these applications include NP-complete problems, the scope of the difference map is that of an incomplete algorithm. Whereas incomplete algorithms can efficiently verify solutions (once a candidate is found), they cannot prove that a solution does not exist.
The difference-map algorithm is a generalization of two iterative methods: Fienup's Hybrid input output (HIO) algorithm for phase retrieval  and the Douglas-Rachford algorithm for convex optimization. Iterative methods, in general, have a long history in phase retrieval and convex optimization. The use of this style of algorithm for hard, non-convex problems is a more recent development."
Difference map algorithm,"The difference-map algorithm is a search algorithm for general constraint satisfaction problems. It is a meta-algorithm in the sense that it is built from more basic algorithms that perform projections onto constraint sets. From a mathematical perspective, the difference-map algorithm is a dynamical system based on a mapping of Euclidean space. Solutions are encoded as fixed points of the mapping.
Although originally conceived as a general method for solving the phase problem, the difference-map algorithm has been used for the boolean satisfiability problem, protein structure prediction, Ramsey numbers, diophantine equations, and Sudoku, as well as sphere- and disk-packing problems. Since these applications include NP-complete problems, the scope of the difference map is that of an incomplete algorithm. Whereas incomplete algorithms can efficiently verify solutions (once a candidate is found), they cannot prove that a solution does not exist.
The difference-map algorithm is a generalization of two iterative methods: Fienup's Hybrid input output (HIO) algorithm for phase retrieval  and the Douglas-Rachford algorithm for convex optimization. Iterative methods, in general, have a long history in phase retrieval and convex optimization. The use of this style of algorithm for hard, non-convex problems is a more recent development."
Digital Differential Analyzer (graphics algorithm),"In computer graphics, a digital differential analyzer (DDA) is hardware or software used for linear interpolation of variables over an interval between start and end point. DDAs are used for rasterization of lines, triangles and polygons. In its simplest implementation, the DDA algorithm interpolates values in interval by computing for each xi the equations xi = xi−1+1/m, yi = yi−1 + m, where Δx = xend − xstart and Δy = yend − ystart and m = Δy/Δx

"
Dijkstra's algorithm,"Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.
The algorithm exists in many variants; Dijkstra's original variant found the shortest path between two nodes, but a more common variant fixes a single node as the ""source"" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest path tree.
For a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road, Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. As a result, the shortest path algorithm is widely used in network routing protocols, most notably IS-IS and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson's.
Dijkstra's original algorithm does not use a min-priority queue and runs in time  (where  is the number of nodes). The idea of this algorithm is also given in (Leyzorek et al. 1957). The implementation based on a min-priority queue implemented by a Fibonacci heap and running in  (where  is the number of edges) is due to (Fredman & Tarjan 1984). This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights.
In some fields, artificial intelligence in particular, Dijkstra's algorithm or a variant of it is known as uniform-cost search and formulated as an instance of the more general idea of best-first search."
Dijkstra-Scholten algorithm,"The Dijkstra–Scholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Dijkstra and Scholten in 1980.
First, let us consider the case of a simple process graph which is a tree. A distributed computation which is tree-structured is not uncommon. Such a process graph may arise when the computation is strictly a divide-and-conquer type. A node starts the computation and divides the problem in two (or more, usually a multiple of 2) roughly equal parts and distribute those parts to other processors. This process continues recursively until the problems are of sufficiently small size to solve in a single processor."
Dinic's algorithm,"Dinic's algorithm or Dinitz's algorithm is a strongly polynomial algorithm for computing the maximum flow in a flow network, conceived in 1970 by Israeli (formerly Soviet) computer scientist Yefim (Chaim) A. Dinitz. The algorithm runs in  time and is similar to the Edmonds–Karp algorithm, which runs in  time, in that it uses shortest augmenting paths. The introduction of the concepts of the level graph and blocking flow enable Dinic's algorithm to achieve its performance."
Disk scheduling,"Input/output (I/O) scheduling is the method that computer operating systems use to decide in which order the block I/O operations will be submitted to storage volumes. I/O scheduling is sometimes called disk scheduling.
I/O scheduling usually has to work with hard disk drives that have long access times for requests placed far away from the current position of the disk head (this operation is called a seek). To minimize the effect this has on system performance, most I/O schedulers implement a variant of the elevator algorithm that reorders the incoming randomly ordered requests so the associated data would be accessed with minimal arm/head movement.
I/O schedulers can have many purposes depending on the goals; common purposes include the following:
To minimize time wasted by hard disk seeks
To prioritize a certain processes' I/O requests
To give a share of the disk bandwidth to each running process
To guarantee that certain requests will be issued before a particular deadline
Common scheduling disciplines include the following:
Random scheduling (RSS)
First In, First Out (FIFO), also known as First Come First Served (FCFS)
Last In, First Out (LIFO)
Shortest seek first, also known as Shortest Seek / Service Time First (SSTF)
Elevator algorithm, also known as SCAN (including its variants, C-SCAN, LOOK, and C-LOOK)
N-Step-SCAN SCAN of N records at a time
FSCAN, N-Step-SCAN where N equals queue size at start of the SCAN cycle
Completely Fair Queuing (CFQ) on Linux
Anticipatory scheduling
Noop scheduler
Deadline scheduler
mClock scheduler"
Distributed algorithm,"A distributed algorithm is an algorithm designed to run on computer hardware constructed from interconnected processors. Distributed algorithms are used in many varied application areas of distributed computing, such as telecommunications, scientific computing, distributed information processing, and real-time process control. Standard problems solved by distributed algorithms include leader election, consensus, distributed search, spanning tree generation, mutual exclusion, and resource allocation.
Distributed algorithms are a sub-type of parallel algorithm, typically executed concurrently, with separate parts of the algorithm being run simultaneously on independent processors, and having limited information about what the other parts of the algorithm are doing. One of the major challenges in developing and implementing distributed algorithms is successfully coordinating the behavior of the independent parts of the algorithm in the face of processor failures and unreliable communications links. The choice of an appropriate distributed algorithm to solve a given problem depends on both the characteristics of the problem, and characteristics of the system the algorithm will run on such as the type and probability of processor or link failures, the kind of inter-process communication that can be performed, and the level of timing synchronization between separate processes.

"
Divide and conquer algorithm,"In computer science, divide and conquer (D&C) is an algorithm design paradigm based on multi-branched recursion. A divide and conquer algorithm works by recursively breaking down a problem into two or more sub-problems of the same (or related) type (divide), until these become simple enough to be solved directly (conquer). The solutions to the sub-problems are then combined to give a solution to the original problem.
This divide and conquer technique is the basis of efficient algorithms for all kinds of problems, such as sorting (e.g., quicksort, merge sort), multiplying large numbers (e.g. Karatsuba), syntactic analysis (e.g., top-down parsers), and computing the discrete Fourier transform (FFTs).
Understanding and designing D&C algorithms is a complex skill that requires a good understanding of the nature of the underlying problem to be solved. As when proving a theorem by induction, it is often necessary to replace the original problem with a more general or complicated problem in order to initialize the recursion, and there is no systematic method for finding the proper generalization. These D&C complications are seen when optimizing the calculation of a Fibonacci number with efficient double recursion.
The correctness of a divide and conquer algorithm is usually proved by mathematical induction, and its computational cost is often determined by solving recurrence relations."
Division algorithm,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.
Discussion will refer to the form , where
N = Numerator (dividend)
D = Denominator (divisor)
is the input, and
Q = Quotient
R = Remainder
is the output."
Dixon's algorithm,"In number theory, Dixon's factorization method (also Dixon's random squares method or Dixon's algorithm) is a general-purpose integer factorization algorithm; it is the prototypical factor base method. Unlike for other factor base methods, its run-time bound comes with a rigorous proof that does not rely on conjectures about the smoothness properties of the values taken by polynomial.
The algorithm was designed by John D. Dixon, a mathematician at Carleton University, and was published in 1981."
Doomsday algorithm,"The Doomsday rule or Doomsday algorithm is a way of calculating the day of the week of a given date. It provides a perpetual calendar because the Gregorian calendar moves in cycles of 400 years.
This algorithm for mental calculation was devised by John Conway after drawing inspiration from Lewis Carroll's work on a perpetual calendar algorithm. It takes advantage of each year having a certain day of the week (the doomsday) upon which certain easy-to-remember dates fall; for example, 4/4, 6/6, 8/8, 10/10, 12/12, and the last day of February all occur on the same day of the week in any given year. Applying the Doomsday algorithm involves three steps:
Determine the ""anchor day"" for the century.
Use the anchor day for the century to calculate the doomsday for the year.
Choose the closest date out of the ones that always fall on the doomsday (e.g. 4/4, 6/6, 8/8), and count the number of days (modulo 7) between that date and the date in question to arrive at the day of the week.
This technique applies to both the Gregorian calendar A.D. and the Julian calendar, although their doomsdays will usually be different days of the week.
Since this algorithm involves treating days of the week like numbers modulo 7, John Conway suggests thinking of the days of the week as ""Noneday"" or ""Sansday"" (for Sunday), ""Oneday"", ""Twosday"", ""Treblesday"", ""Foursday"", ""Fiveday"", and ""Six-a-day"".
The algorithm is simple enough for anyone with basic arithmetic ability to do the calculations mentally. Conway can usually give the correct answer in under two seconds. To improve his speed, he practices his calendrical calculations on his computer, which is programmed to quiz him with random dates every time he logs on."
Double Metaphone,"Lawrence Philips redirects here. For the football player, see Lawrence Phillips.
Metaphone is a phonetic algorithm, published by Lawrence Philips in 1990, for indexing words by their English pronunciation. It fundamentally improves on the Soundex algorithm by using information about variations and inconsistencies in English spelling and pronunciation to produce a more accurate encoding, which does a better job of matching words and names which sound similar. As with Soundex, similar sounding words should share the same keys. Metaphone is available as a built-in operator in a number of systems.
The original author later produced a new version of the algorithm, which he named Double Metaphone. Contrary to the original algorithm whose application is limited to English only, this version takes into account spelling peculiarities of a number of other languages. In 2009 Lawrence Philips released a third version, called Metaphone 3, which achieves an accuracy of approximately 99% for English words, non-English words familiar to Americans, and first names and family names commonly found in the United States, having been developed according to modern engineering standards against a test harness of prepared correct encodings."
Double dabble,"In computer science, the double dabble algorithm is used to convert binary numbers into binary-coded decimal (BCD) notation. It is also known as the shift and add 3 algorithm, and can be implemented using a small number of gates in computer hardware, but at the expense of high latency. The algorithm operates as follows:
Suppose the original number to be converted is stored in a register that is n bits wide. Reserve a scratch space wide enough to hold both the original number and its BCD representation; 4×ceil(n/3) bits will be enough. It takes a maximum of 4 bits in binary to store each decimal digit.
Then partition the scratch space into BCD digits (on the left) and the original register (on the right). For example, if the original number to be converted is eight bits wide, the scratch space would be partitioned as follows:

100s Tens Ones   Original
0010 0100 0011   11110011

The diagram above shows the binary representation of 24310 in the original register, and the BCD representation of 243 on the left.
The scratch space is initialized to all zeros, and then the value to be converted is copied into the ""original register"" space on the right.

0000 0000 0000   11110011

The algorithm then iterates n times. On each iteration, the entire scratch space is left-shifted one bit. However, before the left-shift is done, any BCD digit which is greater than 4 is incremented by 3. The increment ensures that a value of 5, incremented and left-shifted, becomes 16, thus correctly ""carrying"" into the next BCD digit.
The double-dabble algorithm, performed on the value 24310, looks like this:

0000 0000 0000   11110011   Initialization
0000 0000 0001   11100110   Shift
0000 0000 0011   11001100   Shift
0000 0000 0111   10011000   Shift
0000 0000 1010   10011000   Add 3 to ONES, since it was 7
0000 0001 0101   00110000   Shift
0000 0001 1000   00110000   Add 3 to ONES, since it was 5
0000 0011 0000   01100000   Shift
0000 0110 0000   11000000   Shift
0000 1001 0000   11000000   Add 3 to TENS, since it was 6
0001 0010 0001   10000000   Shift
0010 0100 0011   00000000   Shift
   2    4    3
       BCD

Now eight shifts have been performed, so the algorithm terminates. The BCD digits to the left of the ""original register"" space display the BCD encoding of the original value 243.
Another example for the double dabble algorithm - value 6524410.

 104  103  102   101  100    Original binary
0000 0000 0000 0000 0000   1111111011011100   Initialization
0000 0000 0000 0000 0001   1111110110111000   Shift left (1st)
0000 0000 0000 0000 0011   1111101101110000   Shift left (2nd)
0000 0000 0000 0000 0111   1111011011100000   Shift left (3rd)
0000 0000 0000 0000 1010   1111011011100000   Add 3 to 100, since it was 7
0000 0000 0000 0001 0101   1110110111000000   Shift left (4th)
0000 0000 0000 0001 1000   1110110111000000   Add 3 to 100, since it was 5
0000 0000 0000 0011 0001   1101101110000000   Shift left (5th)
0000 0000 0000 0110 0011   1011011100000000   Shift left (6th)
0000 0000 0000 1001 0011   1011011100000000   Add 3 to 101, since it was 6
0000 0000 0001 0010 0111   0110111000000000   Shift left (7th)
0000 0000 0001 0010 1010   0110111000000000   Add 3 to 100, since it was 7
0000 0000 0010 0101 0100   1101110000000000   Shift left (8th)
0000 0000 0010 1000 0100   1101110000000000   Add 3 to 101, since it was 5
0000 0000 0101 0000 1001   1011100000000000   Shift left (9th)
0000 0000 1000 0000 1001   1011100000000000   Add 3 to 102, since it was 5
0000 0000 1000 0000 1100   1011100000000000   Add 3 to 100, since it was 9
0000 0001 0000 0001 1001   0111000000000000   Shift left (10th)
0000 0001 0000 0001 1100   0111000000000000   Add 3 to 100, since it was 9
0000 0010 0000 0011 1000   1110000000000000   Shift left (11th)
0000 0010 0000 0011 1011   1110000000000000   Add 3 to 100, since it was 8
0000 0100 0000 0111 0111   1100000000000000   Shift left (12th)
0000 0100 0000 1010 0111   1100000000000000   Add 3 to 101, since it was 7
0000 0100 0000 1010 1010   1100000000000000   Add 3 to 100, since it was 7
0000 1000 0001 0101 0101   1000000000000000   Shift left (13th)
0000 1011 0001 0101 0101   1000000000000000   Add 3 to 103, since it was 8
0000 1011 0001 1000 0101   1000000000000000   Add 3 to 101, since it was 5
0000 1011 0001 1000 1000   1000000000000000   Add 3 to 100, since it was 5
0001 0110 0011 0001 0001   0000000000000000   Shift left (14th)
0001 1001 0011 0001 0001   0000000000000000   Add 3 to 103, since it was 6
0011 0010 0110 0010 0010   0000000000000000   Shift left (15th)
0011 0010 1001 0010 0010   0000000000000000   Add 3 to 102, since it was 6
0110 0101 0010 0100 0100   0000000000000000   Shift left (16th)
   6    5    2    4    4
            BCD

Sixteen shifts have been performed, so the algorithm terminates. The BCD digits is: 6*104 + 5*103 + 2*102 + 4*101 + 4*100 = 65244."
Dynamic Markov compression,"Dynamic Markov compression (DMC) is a lossless data compression algorithm developed by Gordon Cormack and Nigel Horspool. It uses predictive arithmetic coding similar to prediction by partial matching (PPM), except that the input is predicted one bit at a time (rather than one byte at a time). DMC has a good compression ratio and moderate speed, similar to PPM, but requires somewhat more memory and is not widely implemented. Some recent implementations include the experimental compression programs hook by Nania Francesco Antonio, ocamyd by Frank Schwellinger, and as a submodel in paq8l by Matt Mahoney. These are based on the 1993 implementation in C by Gordon Cormack."
Dynamic Programming,"In mathematics, management science, economics, computer science,and bioinformatics, dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems, solving each of those subproblems just once, and storing their solutions - ideally, using a memory-based data structure. The next time the same subproblem occurs, instead of recomputing its solution, one simply looks up the previously computed solution, thereby saving computation time at the expense of a (hopefully) modest expenditure in storage space. (Each of the subproblem solutions is indexed in some way, typically based on the values of its input parameters, so as to facilitate its lookup.) The act of storing solutions to subproblems is called ""memoization"". In contrast, a more naive method would not recognize that a particular subproblem has already been solved previously, and would repeatedly solve the same subproblem many times.
This approach is especially useful when the number of repeating subproblems grows exponentially as a function of the size of the input.
Dynamic programming is applicable to problems exhibiting the properties of overlapping subproblems and optimal substructure (described below). When applicable, the method takes far less time than other methods that don't take advantage of the subproblem overlap (like depth-first search).
Dynamic programming algorithms are used for optimization (for example, finding the shortest path between two points, or the fastest way to multiply many matrices). A dynamic programming algorithm will examine the previously solved subproblems and will combine their solutions to give the best solution for the given problem. The alternatives are many, such as using a greedy algorithm, which picks the locally optimal choice at each branch in the road. The locally optimal choice may be a poor choice for the overall solution. While a greedy algorithm does not guarantee an optimal solution, it is often faster to calculate. Fortunately, some greedy algorithms (such as minimum spanning trees) are proven to lead to the optimal solution.
For example, let's say that you have to get from point A to point B as fast as possible, in a given city, during rush hour. A dynamic programming algorithm will look at finding the shortest paths to points close to A, and use those solutions to eventually find the shortest path to B. On the other hand, a greedy algorithm will start you driving immediately and will pick the road that looks the fastest at every intersection. As you can imagine, this strategy might not lead to the fastest arrival time, since you might take some ""easy"" streets and then find yourself hopelessly stuck in a traffic jam.
Sometimes, applying memoization to a naive basic recursive solution already results in a dynamic programming solution with asymptotically optimal time complexity; however, the optimal solution to some problems requires more sophisticated dynamic programming algorithms. Some of these may be recursive as well but parametrized differently from the naive solution. Others can be more complicated and cannot be implemented as a recursive function with memoization. Examples of these are the two solutions to the Egg Dropping puzzle below."
Dynamic time warping,"In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences which may vary in time or speed. For instance, similarities in walking patterns could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data — indeed, any data which can be turned into a linear sequence can be analyzed with DTW. A well known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. Also it is seen that it can be used in partial shape matching application.
In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restrictions. The sequences are ""warped"" non-linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. This sequence alignment method is often used in time series classification. Although DTW measures a distance-like quantity between two given sequences, it doesn't guarantee the triangle inequality to hold."
Earley parser,"In computer science, the Earley parser is an algorithm for parsing strings that belong to a given context-free language, though (depending on the variant) it may suffer problems with certain nullable grammars. The algorithm, named after its inventor, Jay Earley, is a chart parser that uses dynamic programming; it is mainly used for parsing in computational linguistics. It was first introduced in his dissertation in 1968 (and later appeared in abbreviated, more legible form in a journal).
Earley parsers are appealing because they can parse all context-free languages, unlike LR parsers and LL parsers, which are more typically used in compilers but which can only handle restricted classes of languages. The Earley parser executes in cubic time in the general case , where n is the length of the parsed string, quadratic time for unambiguous grammars , and linear time for almost all LR(k) grammars. It performs particularly well when the rules are written left-recursively."
Earliest deadline first scheduling,"Earliest deadline first (EDF) or least time to go is a dynamic scheduling algorithm used in real-time operating systems to place processes in a priority queue. Whenever a scheduling event occurs (task finishes, new task released, etc.) the queue will be searched for the process closest to its deadline. This process is the next to be scheduled for execution.
EDF is an optimal scheduling algorithm on preemptive uniprocessors, in the following sense: if a collection of independent jobs, each characterized by an arrival time, an execution requirement and a deadline, can be scheduled (by any algorithm) in a way that ensures all the jobs complete by their deadline, the EDF will schedule this collection of jobs so they all complete by their deadline.
With scheduling periodic processes that have deadlines equal to their periods, EDF has a utilization bound of 100%. Thus, the schedulability test for EDF is:

where the  are the worst-case computation-times of the  processes and the  are their respective inter-arrival periods (assumed to be equal to the relative deadlines).
That is, EDF can guarantee that all deadlines are met provided that the total CPU utilization is not more than 100%. Compared to fixed priority scheduling techniques like rate-monotonic scheduling, EDF can guarantee all the deadlines in the system at higher loading.
However, when the system is overloaded, the set of processes that will miss deadlines is largely unpredictable (it will be a function of the exact deadlines and time at which the overload occurs.) This is a considerable disadvantage to a real time systems designer. The algorithm is also difficult to implement in hardware and there is a tricky issue of representing deadlines in different ranges (deadlines can't be more precise than the granularity of the clock used for the scheduling). If a modular arithmetic is used to calculate future deadlines relative to now, the field storing a future relative deadline must accommodate at least the value of the ((""duration"" {of the longest expected time to completion} * 2) + ""now""). Therefore EDF is not commonly found in industrial real-time computer systems.
Instead, most real-time computer systems use fixed priority scheduling (usually rate-monotonic scheduling). With fixed priorities, it is easy to predict that overload conditions will cause the low-priority processes to miss deadlines, while the highest-priority process will still meet its deadline.
There is a significant body of research dealing with EDF scheduling in real-time computing; it is possible to calculate worst case response times of processes in EDF, to deal with other types of processes than periodic processes and to use servers to regulate overloads."
Edmonds's algorithm,"In graph theory, a branch of mathematics, Edmonds' algorithm or Chu–Liu/Edmonds' algorithm is an algorithm for finding a spanning arborescence of minimum weight (sometimes called an optimum branching). It is the directed analog of the minimum spanning tree problem. The algorithm was proposed independently first by Yoeng-jin Chu and Tseng-hong Liu (1965) and then by Jack Edmonds (1967)."
Edmonds–Karp algorithm,"In computer science, the Edmonds–Karp algorithm is an implementation of the Ford–Fulkerson method for computing the maximum flow in a flow network in O(V E2) time. The algorithm was first published by Yefim (Chaim) Dinic in 1970 and independently published by Jack Edmonds and Richard Karp in 1972. Dinic's algorithm includes additional techniques that reduce the running time to O(V2E)."
Elevator algorithm,"The elevator algorithm (also SCAN) is a disk scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.
This algorithm is named after the behavior of a building elevator, where the elevator continues to travel in its current direction (up or down) until empty, stopping only to let individuals off or to pick up new individuals heading in the same direction.
From an implementation perspective, the drive maintains a buffer of pending read/write requests, along with the associated cylinder number of the request. Lower cylinder numbers indicate that the cylinder is closest to the spindle, and higher numbers indicate the cylinder is farther away.

"
Embedded Zerotree Wavelet,"Embedded Zerotrees of Wavelet transforms (EZW) is a lossy image compression algorithm. At low bit rates, i.e. high compression ratios, most of the coefficients produced by a subband transform (such as the wavelet transform) will be zero, or very close to zero. This occurs because ""real world"" images tend to contain mostly low frequency information (highly correlated). However where high frequency information does occur (such as edges in the image) this is particularly important in terms of human perception of the image quality, and thus must be represented accurately in any high quality coding scheme.
By considering the transformed coefficients as a tree (or trees) with the lowest frequency coefficients at the root node and with the children of each tree node being the spatially related coefficients in the next higher frequency subband, there is a high probability that one or more subtrees will consist entirely of coefficients which are zero or nearly zero, such subtrees are called zerotrees. Due to this, we use the terms node and coefficient interchangeably, and when we refer to the children of a coefficient, we mean the child coefficients of the node in the tree where that coefficient is located. We use children to refer to directly connected nodes lower in the tree and descendants to refer to all nodes which are below a particular node in the tree, even if not directly connected.
In zerotree based image compression scheme such as EZW and SPIHT, the intent is to use the statistical properties of the trees in order to efficiently code the locations of the significant coefficients. Since most of the coefficients will be zero or close to zero, the spatial locations of the significant coefficients make up a large portion of the total size of a typical compressed image. A coefficient (likewise a tree) is considered significant if its magnitude (or magnitudes of a node and all its descendants in the case of a tree) is above a particular threshold. By starting with a threshold which is close to the maximum coefficient magnitudes and iteratively decreasing the threshold, it is possible to create a compressed representation of an image which progressively adds finer detail. Due to the structure of the trees, it is very likely that if a coefficient in a particular frequency band is insignificant, then all its descendants (the spatially related higher frequency band coefficients) will also be insignificant.
EZW uses four symbols to represent (a) a zerotree root, (b) an isolated zero (a coefficient which is insignificant, but which has significant descendants), (c) a significant positive coefficient and (d) a significant negative coefficient. The symbols may be thus represented by two binary bits. The compression algorithm consists of a number of iterations through a dominant pass and a subordinate pass, the threshold is updated (reduced by a factor of two) after each iteration. The dominant pass encodes the significance of the coefficients which have not yet been found significant in earlier iterations, by scanning the trees and emitting one of the four symbols. The children of a coefficient are only scanned if the coefficient was found to be significant, or if the coefficient was an isolated zero. The subordinate pass emits one bit (the most significant bit of each coefficient not so far emitted) for each coefficient which has been found significant in the previous significance passes. The subordinate pass is therefore similar to bit-plane coding.
There are several important features to note. Firstly, it is possible to stop the compression algorithm at any time and obtain an approximation of the original image, the greater the number of bits received, the better the image. Secondly, due to the way in which the compression algorithm is structured as a series of decisions, the same algorithm can be run at the decoder to reconstruct the coefficients, but with the decisions being taken according to the incoming bit stream. In practical implementations, it would be usual to use an entropy code such as arithmetic code to further improve the performance of the dominant pass. Bits from the subordinate pass are usually random enough that entropy coding provides no further coding gain.
The coding performance of EZW has since been exceeded by SPIHT and its many derivatives."
Euclidean algorithm,"In mathematics, the Euclidean algorithm, or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two numbers, the largest number that divides both of them without leaving a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in Euclid's Elements (c. 300 BC). It is an example of an algorithm, a step-by-step procedure for performing a calculation according to well-defined rules, and is one of the oldest numerical algorithms in common use. It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.
The Euclidean algorithm is based on the principle that the greatest common divisor of two numbers does not change if the larger number is replaced by its difference with the smaller number. For example, 21 is the GCD of 252 and 105 (252 = 21 × 12 and 105 = 21 × 5), and the same number 21 is also the GCD of 105 and 147 = 252 − 105. Since this replacement reduces the larger of the two numbers, repeating this process gives successively smaller pairs of numbers until one of the two numbers reaches zero. When that occurs, the other number (the one that is not zero) is the GCD of the original two numbers. By reversing the steps, the GCD can be expressed as a sum of the two original numbers each multiplied by a positive or negative integer, e.g., 21 = 5 × 105 + (−2) × 252. The fact that the GCD can always be expressed in this way is known as Bézout's identity.
The version of the Euclidean algorithm described above (and by Euclid) can take many subtraction steps to find the GCD when one of the given numbers is much bigger than the other. A more efficient version of the algorithm shortcuts these steps, instead replacing the larger of the two numbers by its remainder when divided by the smaller of the two. With this improvement, the algorithm never requires more steps than five times the number of digits (base 10) of the smaller integer. This was proven by Gabriel Lamé in 1844, and marks the beginning of computational complexity theory. Additional methods for improving the algorithm's efficiency were developed in the 20th century.
The Euclidean algorithm has many theoretical and practical applications. It is used for reducing fractions to their simplest form and for performing division in modular arithmetic. Computations using this algorithm form part of the cryptographic protocols that are used to secure internet communications, and in methods for breaking these cryptosystems by factoring large composite numbers. The Euclidean algorithm may be used to solve Diophantine equations, such as finding numbers that satisfy multiple congruences according to the Chinese remainder theorem, to construct continued fractions, and to find accurate rational approximations to real numbers. Finally, it is a basic tool for proving theorems in number theory such as Lagrange's four-square theorem and the uniqueness of prime factorizations. The original algorithm was described only for natural numbers and geometric lengths (real numbers), but the algorithm was generalized in the 19th century to other types of numbers, such as Gaussian integers and polynomials of one variable. This led to modern abstract algebraic notions such as Euclidean domains."
Euclidean shortest path problem,"The Euclidean shortest path problem is a problem in computational geometry: given a set of polyhedral obstacles in a Euclidean space, and two points, find the shortest path between the points that does not intersect any of the obstacles.
In two dimensions, the problem can be solved in polynomial time in a model of computation allowing addition and comparisons of real numbers, despite theoretical difficulties involving the numerical precision needed to perform such calculations. These algorithms are based on two different principles, either performing a shortest path algorithm such as Dijkstra's algorithm on a visibility graph derived from the obstacles or (in an approach called the continuous Dijkstra method) propagating a wavefront from one of the points until it meets the other.
In three (and higher) dimensions the problem is NP-hard in the general case , but there exist efficient approximation algorithms that run in polynomial time based on the idea of finding a suitable sample of points on the obstacle edges and performing a visibility graph calculation using these sample points.
There are many results on computing shortest paths which stays on a polyhedral surface. Given two points s and t, say on the surface of a convex polyhedron, the problem is to compute a shortest path that never leaves the surface and connects s with t. This is a generalization of the problem from 2-dimension but it is much easier than the 3-dimensional problem.
Also, there are variations of this problem, where the obstacles are weighted, i.e., one can go through an obstacle, but it incurs an extra cost to go through an obstacle. The standard problem is the special case where the obstacles have infinite weight. This is termed as the weighted region problem in the literature.

"
Expectation-maximization algorithm,"In statistics, an expectation–maximization (EM) algorithm is an iterative method for finding maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step."
Exponential backoff,"Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, in order to gradually find an acceptable rate."
Exponentiating by squaring,"In mathematics and computer programming, exponentiating by squaring is a general method for fast computation of large positive integer powers of a number, or more generally of an element of a semigroup, like a polynomial or a square matrix. Some variants are commonly referred to as square-and-multiply algorithms or binary exponentiation. These can be of quite general use, for example in modular arithmetic or powering of matrices. For semigroups for which additive notation is commonly used, like elliptic curves used in cryptography, this method is also referred to as double-and-add."
Extended Euclidean algorithm,"In arithmetic and computer programming, the extended Euclidean algorithm is an extension to the Euclidean algorithm, which computes, besides the greatest common divisor of integers a and b, the coefficients of Bézout's identity, that is integers x and y such that

It allows one to compute also, with almost no extra cost, the quotients of a and b by their greatest common divisor.
Extended Euclidean algorithm also refers to a very similar algorithm for computing the polynomial greatest common divisor and the coefficients of Bézout's identity of two univariate polynomials.
The extended Euclidean algorithm is particularly useful when a and b are coprime, since x is the modular multiplicative inverse of a modulo b, and y is the modular multiplicative inverse of b modulo a. Similarly, the polynomial extended Euclidean algorithm allows one to compute the multiplicative inverse in algebraic field extensions and, in particular in finite fields of non prime order. It follows that both extended Euclidean algorithms are widely used in cryptography. In particular, the computation of the modular multiplicative inverse is an essential step in RSA public-key encryption method."
FELICS,"FELICS, which stands for Fast Efficient & Lossless Image Compression System, is a lossless image compression algorithm that performs 5-times faster than the original lossless JPEG codec and achieves a similar compression ratio."
FLAME clustering,"Fuzzy clustering by Local Approximation of MEmberships (FLAME) is a data clustering algorithm that defines clusters in the dense parts of a dataset and performs cluster assignment solely based on the neighborhood relationships among objects. The key feature of this algorithm is that the neighborhood relationships among neighboring objects in the feature space are used to constrain the memberships of neighboring objects in the fuzzy membership space.

"
FNN algorithm,"The false nearest neighbor algorithm is an algorithm for estimating the embedding dimension. The concept was proposed by Kennel et al. The main idea is to examine how the number of neighbors of a point along a signal trajectory change with increasing embedding dimension. In too low an embedding dimension, many of the neighbors will be false, but in an appropriate embedding dimension or higher, the neighbors are real. With increasing dimension, the false neighbors will no longer be neighbors. Therefore, by examining how the number of neighbors change as a function of dimension, an appropriate embedding can be determined.

"
Fair-share scheduling,"Fair-share scheduling is a scheduling algorithm for computer operating systems in which the CPU usage is equally distributed among system users or groups, as opposed to equal distribution among processes.
One common method of logically implementing the fair-share scheduling strategy is to recursively apply the round-robin scheduling strategy at each level of abstraction (processes, users, groups, etc.) The time quantum required by round-robin is arbitrary, as any equal division of time will produce the same results.
This was first developed by Judy Kay and Piers Lauder through their research at Sydney University in the 1980s."
Fast Fourier transform,"A fast Fourier transform (FFT) algorithm computes the discrete Fourier transform (DFT) of a sequence, or its inverse. Fourier analysis converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa. A FFT rapidly computes such transformations by factorizing the DFT matrix into a product of sparse (mostly zero) factors. As a result, it manages to reduce the complexity of computing the DFT from , which arises if one simply applies the definition of DFT, to , where  is the data size.
Fast Fourier transforms are widely used for many applications in engineering, science, and mathematics. The basic ideas were popularized in 1965, but some algorithms had been derived as early as 1805. In 1994 Gilbert Strang described the FFT as ""the most important numerical algorithm of our lifetime"" and it was included in Top 10 Algorithms of 20th Century by the IEEE journal Computing in Science & Engineering."
Fast folding algorithm,"In signal processing, the fast folding algorithm (Staelin, 1969) is an efficient algorithm for the detection of approximately-periodic events within time series data. It computes superpositions of the signal modulo various window sizes simultaneously.
The FFA is best known for its use in the detection of pulsars, as popularised by SETI@home and Astropulse."
Faugère F4 algorithm,"In computer algebra, the Faugère F4 algorithm, by Jean-Charles Faugère, computes the Gröbner basis of an ideal of a multivariate polynomial ring. The algorithm uses the same mathematical principles as the Buchberger algorithm, but computes many normal forms in one go by forming a generally sparse matrix and using fast linear algebra to do the reductions in parallel.
The Faugère F5 algorithm first calculates the Gröbner basis of a pair of generator polynomials of the ideal. Then it uses this basis to reduce the size of the initial matrices of generators for the next larger basis:

If Gprev is an already computed Gröbner basis (f2, …, fm) and we want to compute a Gröbner basis of (f1) + Gprev then we will construct matrices whose rows are m f1 such that m is a monomial not divisible by the leading term of an element of Gprev.

This strategy allows the algorithm to apply two new criteria based on what Faugère calls signatures of polynomials. Thanks to these criteria, the algorithm can compute Gröbner bases for a large class of interesting polynomial systems, called regular sequences, without ever simplifying a single polynomial to zero—the most time-consuming operation in algorithms that compute Gröbner bases. It is also very effective for a large number of non-regular sequences."
Featherstone's algorithm,"Featherstone's algorithm is a technique used for computing the effects of forces applied to a structure of joints and links (an ""open kinematic chain"") such as a skeleton used in ragdoll physics.
The Featherstone's algorithm uses a reduced coordinate representation. This is in contrast to the more popular Lagrange multiplier method, which uses maximal coordinates. Brian Mirtich's PhD Thesis has a very clear and detailed description of the algorithm. Baraff's paper ""Linear-time dynamics using Lagrange multipliers"" has a discussion and comparison of both algorithms."
Fibonacci search technique,"In computer science, the Fibonacci search technique is a method of searching a sorted array using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers. Compared to binary search, Fibonacci search examines locations whose addresses have lower dispersion. Therefore, when the elements being searched have non-uniform access memory storage (i.e., the time needed to access a storage location varies depending on the location previously accessed), the Fibonacci search has an advantage over binary search in slightly reducing the average time needed to access a storage location. The typical example of non-uniform access storage is that of a magnetic tape, where the time to access a particular element is proportional to its distance from the element currently under the tape's head. Note, however, that large arrays not fitting in CPU cache or even in RAM can also be considered as non-uniform access examples. Fibonacci search has a complexity of O(log(n)) (see Big O notation).
Fibonacci search was first devised by Jack Kiefer (1953) as a minimax search for the maximum (minimum) of a unimodal function in an interval."
Fisher–Yates shuffle,"The Fisher–Yates shuffle (named after Ronald Fisher and Frank Yates), also known as the Knuth shuffle (after Donald Knuth), is an algorithm for generating a random permutation of a finite set—in plain terms, for randomly shuffling the set. A variant of the Fisher–Yates shuffle, known as Sattolo's algorithm, may be used to generate random cyclic permutations of length n instead. The Fisher–Yates shuffle is unbiased, so that every permutation is equally likely. The modern version of the algorithm is also rather efficient, requiring only time proportional to the number of items being shuffled and no additional storage space.
Fisher–Yates shuffling is similar to randomly picking numbered tickets (combinatorics: distinguishable objects) out of a hat without replacement until there are none left."
Fitness proportionate selection,"Fitness proportionate selection, also known as roulette wheel selection, is a genetic operator used in genetic algorithms for selecting potentially useful solutions for recombination.
In fitness proportionate selection, as in all selection methods, the fitness function assigns a fitness to possible solutions or chromosomes. This fitness level is used to associate a probability of selection with each individual chromosome. If  is the fitness of individual  in the population, its probability of being selected is , where  is the number of individuals in the population.
This could be imagined similar to a Roulette wheel in a casino. Usually a proportion of the wheel is assigned to each of the possible selections based on their fitness value. This could be achieved by dividing the fitness of a selection by the total fitness of all the selections, thereby normalizing them to 1. Then a random selection is made similar to how the roulette wheel is rotated.
While candidate solutions with a higher fitness will be less likely to be eliminated, there is still a chance that they may be. Contrast this with a less sophisticated selection algorithm, such as truncation selection, which will eliminate a fixed percentage of the weakest candidates. With fitness proportionate selection there is a chance some weaker solutions may survive the selection process; this is an advantage, as though a solution may be weak, it may include some component which could prove useful following the recombination process.
The analogy to a roulette wheel can be envisaged by imagining a roulette wheel in which each candidate solution represents a pocket on the wheel; the size of the pockets are proportionate to the probability of selection of the solution. Selecting N chromosomes from the population is equivalent to playing N games on the roulette wheel, as each candidate is drawn independently.
Other selection techniques, such as stochastic universal sampling or tournament selection, are often used in practice. This is because they have less stochastic noise, or are fast, easy to implement and have a constant selection pressure [Blickle, 1996].
The naive implementation is carried out by first generating the cumulative probability distribution (CDF) over the list of individuals using a probability proportional to the fitness of the individual. A uniform random number from the range [0,1) is chosen and the inverse of the CDF for that number gives an individual. This corresponds to the roulette ball falling in the bin of an individual with a probability proportional to its width. The ""bin"" corresponding to the inverse of the uniform random number can be found most quickly by using a binary search over the elements of the CDF. It takes in the O(log n) time to choose an individual. A faster alternative that generates individuals in O(1) time will be to use the alias method.
Recently, a very simple O(1) algorithm was introduced that is based on ""stochastic acceptance"". The algorithm randomly selects an individual (say ) and accepts the selection with probability , where  is the maximum fitness in the population. Certain analysis indicates that the stochastic acceptance version has a considerably better performance than versions based on linear or binary search, especially in applications where fitness values might change during the run."
Flashsort,"Flashsort is a distribution sorting algorithm showing linear computational complexity  for uniformly distributed data sets and relatively little additional memory requirement. The original work was published in 1998 by Karl-Dietrich Neubert.

"
Fletcher's checksum,The Fletcher checksum is an algorithm for computing a position-dependent checksum devised by John G. Fletcher (1934-2012) at Lawrence Livermore Labs in the late 1970s. The objective of the Fletcher checksum was to provide error-detection properties approaching those of a cyclic redundancy check but with the lower computational effort associated with summation techniques.
Flood fill,"Flood fill, also called seed fill, is an algorithm that determines the area connected to a given node in a multi-dimensional array. It is used in the ""bucket"" fill tool of paint programs to fill connected, similarly-colored areas with a different color, and in games such as Go and Minesweeper for determining which pieces are cleared. When applied on an image to fill a particular bounded area with color, it is also known as boundary fill."
Floyd's cycle-finding algorithm,"In computer science, cycle detection is the algorithmic problem of finding a cycle in a sequence of iterated function values.
For any function ƒ that maps a finite set S to itself, and any initial value x0 in S, the sequence of iterated function values

must eventually use the same value twice: there must be some i ≠ j such that xi = xj. Once this happens, the sequence must continue periodically, by repeating the same sequence of values from xi to xj−1. Cycle detection is the problem of finding i and j, given ƒ and x0.

"
Floyd–Steinberg dithering,"Floyd–Steinberg dithering is an image dithering algorithm first published in 1976 by Robert W. Floyd and Louis Steinberg. It is commonly used by image manipulation software, for example when an image is converted into GIF format that is restricted to a maximum of 256 colors.
The algorithm achieves dithering using error diffusion, meaning it pushes (adds) the residual quantization error of a pixel onto its neighboring pixels, to be dealt with later. It spreads the debt out according to the distribution (shown as a map of the neighboring pixels):

The pixel indicated with a star (*) indicates the pixel currently being scanned, and the blank pixels are the previously-scanned pixels. The algorithm scans the image from left to right, top to bottom, quantizing pixel values one by one. Each time the quantization error is transferred to the neighboring pixels, while not affecting the pixels that already have been quantized. Hence, if a number of pixels have been rounded downwards, it becomes more likely that the next pixel is rounded upwards, such that on average, the quantization error is close to zero.
The diffusion coefficients have the property that if the original pixel values are exactly halfway in between the nearest available colors, the dithered result is a checkerboard pattern. For example 50% grey data could be dithered as a black-and-white checkerboard pattern. For optimal dithering, the counting of quantization errors should be in sufficient accuracy to prevent rounding errors from affecting the result.
In some implementations, the horizontal direction of scan alternates between lines; this is called ""serpentine scanning"" or boustrophedon transform dithering.
In pseudocode:

for each y from top to bottom
   for each x from left to right
      oldpixel  := pixel[x][y]
      newpixel  := find_closest_palette_color(oldpixel)
      pixel[x][y]  := newpixel
      quant_error  := oldpixel - newpixel
      pixel[x+1][y  ] := pixel[x+1][y  ] + quant_error * 7/16
      pixel[x-1][y+1] := pixel[x-1][y+1] + quant_error * 3/16
      pixel[x  ][y+1] := pixel[x  ][y+1] + quant_error * 5/16
      pixel[x+1][y+1] := pixel[x+1][y+1] + quant_error * 1/16

When converting 16 bit greyscale to 8 bit, find_closest_palette_color() may perform just a simple rounding, for example:

find_closest_palette_color(oldpixel) = floor(oldpixel / 256)

"
Floyd–Warshall algorithm,"In computer science, the Floyd–Warshall algorithm is an algorithm for finding shortest paths in a weighted graph with positive or negative edge weights (but with no negative cycles). A single execution of the algorithm will find the lengths (summed weights) of the shortest paths between all pairs of vertices, though it does not return details of the paths themselves. Versions of the algorithm can also be used for finding the transitive closure of a relation , or (in connection with the Schulze voting system) widest paths between all pairs of vertices in a weighted graph."
Force-based algorithms (graph drawing),"Force-directed graph drawing algorithms are a class of algorithms for drawing graphs in an aesthetically pleasing way. Their purpose is to position the nodes of a graph in two-dimensional or three-dimensional space so that all the edges are of more or less equal length and there are as few crossing edges as possible, by assigning forces among the set of edges and the set of nodes, based on their relative positions, and then using these forces either to simulate the motion of the edges and nodes or to minimize their energy.
While graph drawing can be a difficult problem, force-directed algorithms, being physical simulations, usually require no special knowledge about graph theory such as planarity.

"
Ford–Fulkerson algorithm,"The Ford–Fulkerson method or Ford–Fulkerson algorithm (FFA) is an algorithm that computes the maximum flow in a flow network. It is called a ""method"" instead of an ""algorithm"" as the approach to finding augmenting paths in a residual graph is not fully specified or it is specified in several implementations with different running times. It was published in 1956 by L. R. Ford, Jr. and D. R. Fulkerson. The name ""Ford–Fulkerson"" is often also used for the Edmonds–Karp algorithm, which is a specialization of Ford–Fulkerson.
The idea behind the algorithm is as follows: as long as there is a path from the source (start node) to the sink (end node), with available capacity on all edges in the path, we send flow along one of the paths. Then we find another path, and so on. A path with available capacity is called an augmenting path."
Forward-backward algorithm,"The forward–backward algorithm is an inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables given a sequence of observations/emissions , i.e. it computes, for all hidden state variables , the distribution . This inference task is usually called smoothing. The algorithm makes use of the principle of dynamic programming to compute efficiently the values that are required to obtain the posterior marginal distributions in two passes. The first pass goes forward in time while the second goes backward in time; hence the name forward–backward algorithm.
The term forward–backward algorithm is also used to refer to any algorithm belonging to the general class of algorithms that operate on sequence models in a forward–backward manner. In this sense, the descriptions in the remainder of this article refer but to one specific instance of this class."
Fractal compression,"Fractal compression is a lossy compression method for digital images, based on fractals. The method is best suited for textures and natural images, relying on the fact that parts of an image often resemble other parts of the same image. Fractal algorithms convert these parts into mathematical data called ""fractal codes"" which are used to recreate the encoded image."
Freivalds' algorithm,"Freivalds' algorithm (named after Rusins Freivalds) is a probabilistic randomized algorithm used to verify matrix multiplication. Given three n × n matrices A, B, and C, a general problem is to verify whether A × B = C. A naïve algorithm would compute the product A × B explicitly and compare term by term whether this product equals C. However, the best known matrix multiplication algorithm runs in O(n2.3729) time. Freivalds' algorithm utilizes randomization in order to reduce this time bound to O(n2)  with high probability. In O(kn2) time the algorithm can verify a matrix product with probability of failure less than ."
Fuzzy clustering,"Data clustering is the process of dividing data elements into classes or clusters so that items in the same class are as similar as possible, and items in different classes are as dissimilar as possible. Depending on the nature of the data and the purpose for which clustering is being used, different measures of similarity may be used to place items into classes, where the similarity measure controls how the clusters are formed. Some examples of measures that can be used as in clustering include distance, connectivity, and intensity.
In hard clustering, data is divided into distinct clusters, where each data element belongs to exactly one cluster. In fuzzy clustering (also referred to as soft clustering), data elements can belong to more than one cluster, and associated with each element is a set of membership levels. These indicate the strength of the association between that data element and a particular cluster. Fuzzy clustering is a process of assigning these membership levels, and then using them to assign data elements to one or more clusters.
One of the most widely used fuzzy clustering algorithms is the Fuzzy C-Means (FCM) Algorithm (Bezdek 1981). The FCM algorithm attempts to partition a finite collection of  elements  into a collection of c fuzzy clusters with respect to some given criterion. Given a finite set of data, the algorithm returns a list of  cluster centres  and a partition matrix , where each element  tells the degree to which element  belongs to cluster . Like the K-means clustering, the FCM aims to minimize an objective function:

where:

This differs from the k-means objective function by the addition of the membership values  and the fuzzifier , with . The fuzzifier  determines the level of cluster fuzziness. A large  results in smaller memberships  and hence, fuzzier clusters. In the limit , the memberships  converge to 0 or 1, which implies a crisp partitioning. In the absence of experimentation or domain knowledge,  is commonly set to 2."
Fürer's algorithm,"Fürer's algorithm is an integer multiplication algorithm for very large numbers possessing a very low asymptotic complexity. It was created in 2007 by Swiss mathematician Martin Fürer of Pennsylvania State University as an asymptotically faster (when analysed on a multitape Turing machine) algorithm than its predecessor, the Schönhage–Strassen algorithm published in 1971.
The predecessor to the Fürer algorithm, the Schönhage–Strassen algorithm, used fast Fourier transforms to compute integer products in time  (in big O notation) and its authors, Arnold Schönhage and Volker Strassen, also conjectured a lower bound for the problem of . Here,  denotes the total number of bits in the two input numbers. Fürer's algorithm reduces the gap between these two bounds: it can be used to multiply binary integers of length  in time  (or by a circuit with that many logic gates), where log*n represents the iterated logarithm operation. However, the difference between the  and  factors in the time bounds of the Schönhage–Strassen algorithm and the Fürer algorithm for realistic values of  is very small.
In 2008, Anindya De, Chandan Saha, Piyush Kurur and Ramprasad Saptharishi gave a similar algorithm that relies on modular arithmetic instead of complex arithmetic to achieve the same running time.
In 2014, David Harvey, Joris van der Hoeven, and Grégoire Lecerf showed that an optimized version of Fürer's algorithm achieves a running time of , making the implied constant in the  exponent explicit. They also gave a modification to Fürer's algorithm that achieves 
In 2015 Svyatoslav Covanov and Emmanuel Thomé provided another modifications that achieves same running time. Yet, as all the other implementation, it's still not practical.

"
GLR parser,"A GLR parser (GLR standing for ""generalized LR"", where L stands for ""left-to-right"" and R stands for ""rightmost (derivation)"") is an extension of an LR parser algorithm to handle nondeterministic and ambiguous grammars. The theoretical foundation was provided in a 1974 paper by Bernard Lang (along with other general Context-Free parsers such as GLL). It describes a systematic way to produce such algorithms, and provides uniform results regarding correctness proofs, complexity with respect to grammar classes, and optimization techniques. The first actual implementation of GLR was described in a 1984 paper by Masaru Tomita, it has also been referred to as a ""parallel parser"". Tomita presented five stages in his original work, though in practice it is the second stage that is recognized as the GLR parser.
Though the algorithm has evolved since its original forms, the principles have remained intact. As shown by an earlier publication, Lang was primarily interested in more easily used and more flexible parsers for extensible programming languages. Tomita's goal was to parse natural language text thoroughly and efficiently. Standard LR parsers cannot accommodate the nondeterministic and ambiguous nature of natural language, and the GLR algorithm can."
Gaussian elimination,"In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the associated matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 CE (see History section).
To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and in every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.

Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced."
Gauss–Jordan elimination,"In linear algebra, Gaussian elimination (also known as row reduction) is an algorithm for solving systems of linear equations. It is usually understood as a sequence of operations performed on the associated matrix of coefficients. This method can also be used to find the rank of a matrix, to calculate the determinant of a matrix, and to calculate the inverse of an invertible square matrix. The method is named after Carl Friedrich Gauss (1777–1855), although it was known to Chinese mathematicians as early as 179 CE (see History section).
To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. There are three types of elementary row operations: 1) Swapping two rows, 2) Multiplying a row by a non-zero number, 3) Adding a multiple of one row to another row. Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. Once all of the leading coefficients (the left-most non-zero entry in each row) are 1, and in every column containing a leading coefficient has zeros elsewhere, the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. For example, in the following sequence of row operations (where multiple elementary operations might be done at each step), the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form.

Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. Some authors use the term Gaussian elimination to refer to the process until it has reached its upper triangular, or (non-reduced) row echelon form. For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced."
Gauss–Legendre algorithm,"The Gauss–Legendre algorithm is an algorithm to compute the digits of π. It is notable for being rapidly convergent, with only 25 iterations producing 45 million correct digits of π. However, the drawback is that it is memory intensive and it is therefore sometimes not used over Machin-like formulas.
The method is based on the individual work of Carl Friedrich Gauss (1777–1855) and Adrien-Marie Legendre (1752–1833) combined with modern algorithms for multiplication and square roots. It repeatedly replaces two numbers by their arithmetic and geometric mean, in order to approximate their arithmetic-geometric mean.
The version presented below is also known as the Gauss–Euler, Brent–Salamin (or Salamin–Brent) algorithm; it was independently discovered in 1975 by Richard Brent and Eugene Salamin. It was used to compute the first 206,158,430,000 decimal digits of π on September 18 to 20, 1999, and the results were checked with Borwein's algorithm."
Gauss–Newton algorithm,"The Gauss–Newton algorithm is used to solve non-linear least squares problems. It is a modification of Newton's method for finding a minimum of a function. Unlike Newton's method, the Gauss–Newton algorithm can only be used to minimize a sum of squared function values, but it has the advantage that second derivatives, which can be challenging to compute, are not required.
Non-linear least squares problems arise for instance in non-linear regression, where parameters in a model are sought such that the model is in good agreement with available observations.
The method is named after the mathematicians Carl Friedrich Gauss and Isaac Newton.

"
Gene expression programming,"In computer programming, gene expression programming (GEP) is an evolutionary algorithm that creates computer programs or models. These computer programs are complex tree structures that learn and adapt by changing their sizes, shapes, and composition, much like a living organism. And like living organisms, the computer programs of GEP are also encoded in simple linear chromosomes of fixed length. Thus, GEP is a genotype-phenotype system, benefiting from a simple genome to keep and transmit the genetic information and a complex phenotype to explore the environment and adapt to it."
General number field sieve,"In number theory, the general number field sieve (GNFS) is the most efficient classical algorithm known for factoring integers larger than 100 digits. Heuristically, its complexity for factoring an integer n (consisting of  bits) is of the form

(in L-notation), where ln is the natural logarithm. It is a generalization of the special number field sieve: while the latter can only factor numbers of a certain special form, the general number field sieve can factor any number apart from prime powers (which are trivial to factor by taking roots). When the term number field sieve (NFS) is used without qualification, it refers to the general number field sieve.
The principle of the number field sieve (both special and general) can be understood as an improvement to the simpler rational sieve or quadratic sieve. When using such algorithms to factor a large number n, it is necessary to search for smooth numbers (i.e. numbers with small prime factors) of order n1/2. The size of these values is exponential in the size of n (see below). The general number field sieve, on the other hand, manages to search for smooth numbers that are subexponential in the size of n. Since these numbers are smaller, they are more likely to be smooth than the numbers inspected in previous algorithms. This is the key to the efficiency of the number field sieve. In order to achieve this speed-up, the number field sieve has to perform computations and factorizations in number fields. This results in many rather complicated aspects of the algorithm, as compared to the simpler rational sieve.
Note that log2 n is the number of bits in the binary representation of n, that is the size of the input to the algorithm, so any element of the order nc for a constant c is exponential in log n. The running time of the number field sieve is super-polynomial but sub-exponential in the size of the input.

"
Genetic algorithms,"In the field of artificial intelligence, a genetic algorithm (GA) is a search heuristic that mimics the process of natural selection. This heuristic (also sometimes called a metaheuristic) is routinely used to generate useful solutions to optimization and search problems. Genetic algorithms belong to the larger class of evolutionary algorithms (EA), which generate solutions to optimization problems using techniques inspired by natural evolution, such as inheritance, mutation, selection, and crossover."
Gift wrapping algorithm,"In computational geometry, the gift wrapping algorithm is an algorithm for computing the convex hull of a given set of points.

"
Gilbert–Johnson–Keerthi distance algorithm,"The Gilbert–Johnson–Keerthi distance algorithm is a method of determining the minimum distance between two convex sets. Unlike many other distance algorithms, it does not require that the geometry data be stored in any specific format, but instead relies solely on a support function to iteratively generate closer simplices to the correct answer using the Minkowski sum (CSO) of two convex shapes.
""Enhanced GJK"" algorithms use edge information to speed up the algorithm by following edges when looking for the next simplex. This improves performance substantially for polytopes with large numbers of vertices.
GJK algorithms are often used incrementally in simulation systems and video games. In this mode, the final simplex from a previous solution is used as the initial guess in the next iteration, or ""frame"". If the positions in the new frame are close to those in the old frame, the algorithm will converge in one or two iterations. This yields collision detection systems which operate in near-constant time.
The algorithm's stability, speed, and small storage footprint make it popular for realtime collision detection, especially in physics engines for video games."
Girvan–Newman algorithm,"The Girvan–Newman algorithm (named after Michelle Girvan and Mark Newman) is a hierarchical method used to detect communities in complex systems.

"
Global illumination,"Global illumination (shortened as GI) or indirect illumination is a general name for a group of algorithms used in 3D computer graphics that are meant to add more realistic lighting to 3D scenes. Such algorithms take into account not only the light which comes directly from a light source (direct illumination), but also subsequent cases in which light rays from the same source are reflected by other surfaces in the scene, whether reflective or not (indirect illumination).
Theoretically reflections, refractions, and shadows are all examples of global illumination, because when simulating them, one object affects the rendering of another object (as opposed to an object being affected only by a direct light). In practice, however, only the simulation of diffuse inter-reflection or caustics is called global illumination.
Images rendered using global illumination algorithms often appear more photorealistic than images rendered using only direct illumination algorithms. However, such images are computationally more expensive and consequently much slower to generate. One common approach is to compute the global illumination of a scene and store that information with the geometry, e.g., radiosity. That stored data can then be used to generate images from different viewpoints for generating walkthroughs of a scene without having to go through expensive lighting calculations repeatedly.
Radiosity, ray tracing, beam tracing, cone tracing, path tracing, Metropolis light transport, ambient occlusion, photon mapping, and image based lighting are examples of algorithms used in global illumination, some of which may be used together to yield results that are not fast, but accurate.
These algorithms model diffuse inter-reflection which is a very important part of global illumination; however most of these (excluding radiosity) also model specular reflection, which makes them more accurate algorithms to solve the lighting equation and provide a more realistically illuminated scene.
The algorithms used to calculate the distribution of light energy between surfaces of a scene are closely related to heat transfer simulations performed using finite-element methods in engineering design.
In real-time 3D graphics, the diffuse inter-reflection component of global illumination is sometimes approximated by an ""ambient"" term in the lighting equation, which is also called ""ambient lighting"" or ""ambient color"" in 3D software packages. Though this method of approximation (also known as a ""cheat"" because it's not really a global illumination method) is easy to perform computationally, when used alone it does not provide an adequately realistic effect. Ambient lighting is known to ""flatten"" shadows in 3D scenes, making the overall visual effect more bland. However, used properly, ambient lighting can be an efficient way to make up for a lack of processing power."
Gnome sort,"Gnome sort (or Stupid sort) is a sorting algorithm originally proposed by Dr. Hamid Sarbazi-Azad (Professor of Computer Engineering at Sharif University of Technology) in 2000 and called ""stupid sort"" (not to be confused with bogosort), and then later on described by Dick Grune and named ""gnome sort"" from the observation that it is ""how a gnome sorts a line of flower pots."" It is a sorting algorithm which is similar to insertion sort, except that moving an element to its proper place is accomplished by a series of swaps, as in bubble sort. It is conceptually simple, requiring no nested loops. The average, or expected, running time is O(n2), but tends towards O(n) if the list is initially almost sorted. In practice the algorithm can run as fast as insertion sort.
The algorithm always finds the first place where two adjacent elements are in the wrong order, and swaps them. It takes advantage of the fact that performing a swap can introduce a new out-of-order adjacent pair only next to the two swapped elements. It does not assume that elements forward of the current position are sorted, so it only needs to check the position directly previous to the swapped elements."
Goertzel algorithm,"The Goertzel algorithm is a Digital Signal Processing (DSP) technique that provides a means for efficient evaluation of individual terms of the Discrete Fourier Transform (DFT), thus making it useful in certain practical applications, such as recognition of DTMF tones produced by the buttons pushed on a telephone keypad. The algorithm was first described by Gerald Goertzel in 1958.
Like the DFT, the Goertzel algorithm analyses one selectable frequency component from a discrete signal. Unlike direct DFT calculations, the Goertzel algorithm applies a single real-valued coefficient at each iteration, using real-valued arithmetic for real-valued input sequences. For covering a full spectrum, the Goertzel algorithm has a higher order of complexity than Fast Fourier Transform (FFT) algorithms; but for computing a small number of selected frequency components, it is more numerically efficient. The simple structure of the Goertzel algorithm makes it well suited to small processors and embedded applications, though not limited to these.
The Goertzel algorithm can also be used ""in reverse"" as a sinusoid synthesis function, which requires only 1 multiplication and 1 subtraction per generated sample."
Golden section search,The golden section search is a technique for finding the extremum (minimum or maximum) of a strictly unimodal function by successively narrowing the range of values inside which the extremum is known to exist. The technique derives its name from the fact that the algorithm maintains the function values for triples of points whose distances form a golden ratio. The algorithm is the limit of Fibonacci search (also described below) for a large number of function evaluations. Fibonacci search and Golden section search were discovered by Kiefer (1953). (see also Avriel and Wilde (1966)).
Goldschmidt division,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.
Discussion will refer to the form , where
N = Numerator (dividend)
D = Denominator (divisor)
is the input, and
Q = Quotient
R = Remainder
is the output."
Gradient descent,"Gradient descent is a first-order optimization algorithm. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.
Gradient descent is also known as steepest descent, or the method of steepest descent. However, gradient descent should not be confused with the method of steepest descent for approximating integrals."
Graham scan,"Graham's scan is a method of finding the convex hull of a finite set of points in the plane with time complexity O(n log n). It is named after Ronald Graham, who published the original algorithm in 1972. The algorithm finds all vertices of the convex hull ordered along its boundary."
Grover's algorithm,"Grover's algorithm is a quantum algorithm that finds with high probability the unique input to a black box function that produces a particular output value, using just O(N1/2) evaluations of the function, where N is the size of the function's domain.
The analogous problem in classical computation cannot be solved in fewer than O(N) evaluations (because, in the worst case, the correct input might be the last one that is tried). At roughly the same time that Grover published his algorithm, Bennett, Bernstein, Brassard, and Vazirani published a proof that no quantum solution to the problem can evaluate the function fewer than O(N1/2) times, so Grover's algorithm is asymptotically optimal.
Unlike other quantum algorithms, which may provide exponential speedup over their classical counterparts, Grover's algorithm provides only a quadratic speedup. However, even quadratic speedup is considerable when N is large. Grover's algorithm could brute force a 128-bit symmetric cryptographic key in roughly 264 iterations, or a 256-bit key in roughly 2128 iterations. As a result, it is sometimes suggested that symmetric key lengths be doubled to protect against future quantum attacks.
Like many quantum algorithms, Grover's algorithm is probabilistic in the sense that it gives the correct answer with a probability of less than 1. Though there is technically no upper bound on the number of repetitions that might be needed before the correct answer is obtained, the expected number of repetitions is a constant factor that does not grow with N.
Grover's original paper described the algorithm as a database search algorithm, and this description is still common. The database in this analogy is a table of all of the function's outputs, indexed by the corresponding input."
Halley's method,"In numerical analysis, Halley’s method is a root-finding algorithm used for functions of one real variable with a continuous second derivative, i.e., C2 functions. It is named after its inventor Edmond Halley.
The algorithm is second in the class of Householder's methods, right after Newton's method. Like the latter, it produces iteratively a sequence of approximations to the root; their rate of convergence to the root is cubic. Multidimensional versions of this method exist.
Halley's method can be viewed as exactly finding the roots of a linear-over-linear Padé approximation to the function, in contrast to Newton's method/Secant method (approximates/interpolates the function linearly) or Cauchy's method/Muller's method (approximates/interpolates the function quadratically)."
Harmony search,"In computer science and operations research, harmony search (HS) is a phenomenon-mimicking algorithm (also known as metaheuristic algorithm, soft computing algorithm or evolutionary algorithm) inspired by the improvisation process of musicians proposed by Zong Woo Geem in 2001. In the HS algorithm, each musician (= decision variable) plays (= generates) a note (= a value) for finding a best harmony (= global optimum) all together. Proponents claim the following merits:
HS does not require differential gradients, thus it can consider discontinuous functions as well as continuous functions.
HS can handle discrete variables as well as continuous variables.
HS does not require initial value setting for the variables.
HS is free from divergence.
HS may escape local optima.
HS may overcome the drawback of GA's building block theory which works well only if the relationship among variables in a chromosome is carefully considered. If neighbor variables in a chromosome have weaker relationship than remote variables, building block theory may not work well because of crossover operation. However, HS explicitly considers the relationship using ensemble operation.
HS has a novel stochastic derivative applied to discrete variables, which uses musician's experiences as a searching direction.
Certain HS variants do not require algorithm parameters such as HMCR and PAR, thus novice users can easily use the algorithm."
Hash join,"The hash join is an example of a join algorithm and is used in the implementation of a relational database management system.
The task of a join algorithm is to find, for each distinct value of the join attribute, the set of tuples in each relation which have that value.
Hash joins require an equijoin predicate (a predicate comparing values from one table with values from the other table using the equals operator '=').

"
Heap's algorithm,"Heap's algorithm generates all possible permutations of N objects. It was first proposed by B. R. Heap in 1963. The algorithm minimizes movement: it generates each permutation from the previous one by interchanging a single pair of elements; the other N−2 elements are not disturbed. In a 1977 review of permutation-generating algorithms, Robert Sedgewick concluded that it was at that time the most effective algorithm for generating permutations by computer."
Heapsort,"In computer science, heapsort is a comparison-based sorting algorithm. Heapsort can be thought of as an improved selection sort: like that algorithm, it divides its input into a sorted and an unsorted region, and it iteratively shrinks the unsorted region by extracting the largest element and moving that to the sorted region. The improvement consists of the use of a heap data structure rather than a linear-time search to find the maximum.
Although somewhat slower in practice on most machines than a well-implemented quicksort, it has the advantage of a more favorable worst-case O(n log n) runtime. Heapsort is an in-place algorithm, but it is not a stable sort.
Heapsort was invented by J. W. J. Williams in 1964. This was also the birth of the heap, presented already by Williams as a useful data structure in its own right. In the same year, R. W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm."
Hindley-Milner type inference,"In type theory and functional programming, Hindley–Milner (HM) (also known as Damas–Milner or Damas–Hindley–Milner) is a classical type system for the lambda calculus with parametric polymorphism, first described by J. Roger Hindley and later rediscovered by Robin Milner. Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.
Among HM's more notable properties is completeness and its ability to deduce the most general type of a given program without the need of any type annotations or other hints supplied by the programmer. Algorithm W is a fast algorithm, performing type inference in almost linear time with respect to the size of the source, making it practically usable to type large programs. HM is preferably used for functional languages. It was first implemented as part of the type system of the programming language ML. Since then, HM has been extended in various ways, most notably by constrained types as used in Haskell."
Hirschberg's algorithm,"In computer science, Hirschberg's algorithm, named after its inventor, Dan Hirschberg, is a dynamic programming algorithm that finds the optimal sequence alignment between two strings. Optimality is measured with the Levenshtein distance, defined to be the sum of the costs of insertions, replacements, deletions, and null actions needed to change one string into the other. Hirschberg's algorithm is simply described as a divide and conquer version of the Needleman–Wunsch algorithm. Hirschberg's algorithm is commonly used in computational biology to find maximal global alignments of DNA and protein sequences."
Hopcroft–Karp algorithm,"In computer science, the Hopcroft–Karp algorithm is an algorithm that takes as input a bipartite graph and produces as output a maximum cardinality matching – a set of as many edges as possible with the property that no two edges share an endpoint. It runs in  time in the worst case, where  is set of edges in the graph, and  is set of vertices of the graph. In the case of dense graphs the time bound becomes , and for random graphs it runs in near-linear time.
The algorithm was found by John Hopcroft and Richard Karp (1973). As in previous methods for matching such as the Hungarian algorithm and the work of Edmonds (1965), the Hopcroft–Karp algorithm repeatedly increases the size of a partial matching by finding augmenting paths. However, instead of finding just a single augmenting path per iteration, the algorithm finds a maximal set of shortest augmenting paths. As a result, only  iterations are needed. The same principle has also been used to develop more complicated algorithms for non-bipartite matching with the same asymptotic running time as the Hopcroft–Karp algorithm."
Huang's algorithm,Huang's algorithm is an algorithm for detecting termination in a distributed system. The algorithm was proposed by Shing-Tsaan Huang in 1989 in the Journal of Computers.
Huffman coding,"In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and/or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Ph.D. student at MIT, and published in the 1952 paper ""A Method for the Construction of Minimum-Redundancy Codes"".
The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in linear time to the number of input weights if these weights are sorted. However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods."
ID3 algorithm,"In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan used to generate a decision tree from a dataset. ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domains.

"
Incremental encoding,"Incremental encoding, also known as front compression, back compression, or front coding, is a type of delta encoding compression algorithm whereby common prefixes or suffixes and their lengths are recorded so that they need not be duplicated. This algorithm is particularly well-suited for compressing sorted data, e.g., a list of words from a dictionary.
For example:
The encoding used to store the common prefix length itself varies from application to application. Typical techniques are storing the value as a single byte; delta encoding, which stores only the change in the common prefix length; and various universal codes. It may be combined with other general lossless data compression techniques such as entropy encoding and dictionary coders to compress the remaining suffixes."
Incremental heuristic search,"Incremental heuristic search algorithms combine both incremental and heuristic search to speed up searches of sequences of similar search problems, which is important in domains that are only incompletely known or change dynamically. Incremental search has been studied at least since the late 1960s. Incremental search algorithms reuse information from previous searches to speed up the current search and solve search problems potentially much faster than solving them repeatedly from scratch. Similarly, heuristic search has been studied at least since the late 1960s. Heuristic search algorithms, often based on A*, use heuristic knowledge in the form of approximations of the goal distances to focus the search and solve search problems potentially much faster than uninformed search algorithms. The resulting search problems, sometimes called dynamic path planning problems, are graph search problems where paths have to be found repeatedly because the topology of the graph, its edge costs, the start vertex or the goal vertices change over time.
So far, three main classes of incremental heuristic search algorithms have been developed:
The first class restarts A* at the point where its current search deviates from the previous one (example: Fringe Saving A*).
The second class updates the h-values from the previous search during the current search to make them more informed (example: Generalized Adaptive A*).
The third class updates the g-values from the previous search during the current search to correct them when necessary, which can be interpreted as transforming the A* search tree from the previous search into the A* search tree for the current search (examples: Lifelong Planning A*, D*, D* Lite).
All three classes of incremental heuristic search algorithms are different from other replanning algorithms, such as planning by analogy, in that their plan quality does not deteriorate with the number of replanning episodes."
Insertion sort,"Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages:
Simple implementation: Bentley shows a three-line C version, and a five-line optimized version
Efficient for (quite) small data sets
More efficient in practice than most other simple quadratic (i.e., O(n2)) algorithms such as selection sort or bubble sort are usually faster in practice than asymptotically faster algorithms for small data sets
Adaptive, i.e., efficient for data sets that are already substantially sorted: the time complexity is O(nk) when each element in the input is no more than k places away from its sorted position
Stable; i.e., does not change the relative order of elements with equal keys
In-place; i.e., only requires a constant amount O(1) of additional memory space
Online; i.e., can sort a list as it receives it
When people manually sort cards in a bridge hand, most use a method that is similar to insertion sort."
Inside-outside algorithm,"In computer science, the inside–outside algorithm is a way of re-estimating production probabilities in a probabilistic context-free grammar. It was introduced James K. Baker in 1979 as a generalization of the forward–backward algorithm for parameter estimation on hidden Markov models to stochastic context-free grammars. It is used to compute expectations, for example as part of the expectation–maximization algorithm (an unsupervised learning algorithm).

"
Integer factorization,"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.
When the numbers are very large, no efficient, non-quantum integer factorization algorithm is known; an effort by several researchers concluded in 2009, factoring a 232-digit number (RSA-768), utilizing hundreds of machines over a span of two years. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.
Not all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.
Many cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure."
Interior point method,"Interior point methods (also referred to as barrier methods) are a certain class of algorithms that solves linear and nonlinear convex optimization problems.

John von Neumann suggested an interior point method of linear programming which was neither a polynomial time method nor an efficient method in practice. In fact, it turned out to be slower in practice compared to simplex method which is not a polynomial time method. In 1984, Narendra Karmarkar developed a method for linear programming called Karmarkar's algorithm which runs in provably polynomial time and is also very efficient in practice. It enabled solutions of linear programming problems which were beyond the capabilities of simplex method. Contrary to the simplex method, it reaches a best solution by traversing the interior of the feasible region. The method can be generalized to convex programming based on a self-concordant barrier function used to encode the convex set.
Any convex optimization problem can be transformed into minimizing (or maximizing) a linear function over a convex set by converting to the epigraph form. The idea of encoding the feasible set using a barrier and designing barrier methods was studied by Anthony V. Fiacco, Garth P. McCormick, and others in the early 1960s. These ideas were mainly developed for general nonlinear programming, but they were later abandoned due to the presence of more competitive methods for this class of problems (e.g. sequential quadratic programming).
Yurii Nesterov and Arkadi Nemirovski came up with a special class of such barriers that can be used to encode any convex set. They guarantee that the number of iterations of the algorithm is bounded by a polynomial in the dimension and accuracy of the solution.
Karmarkar's breakthrough revitalized the study of interior point methods and barrier problems, showing that it was possible to create an algorithm for linear programming characterized by polynomial complexity and, moreover, that was competitive with the simplex method. Already Khachiyan's ellipsoid method was a polynomial time algorithm; however, it was too slow to be of practical interest.
The class of primal-dual path-following interior point methods is considered the most successful. Mehrotra's predictor-corrector algorithm provides the basis for most implementations of this class of methods."
Interpolation search,"Interpolation search (sometimes referred to as extrapolation search) is an algorithm for searching for a given key value in an indexed array that has been ordered by the values of the key. It parallels how humans search through a telephone book for a particular name, the key value by which the book's entries are ordered. In each search step it calculates where in the remaining search space the sought item might be, based on the key values at the bounds of the search space and the value of the sought key, usually via a linear interpolation. The key value actually found at this estimated position is then compared to the key value being sought. If it is not equal, then depending on the comparison, the remaining search space is reduced to the part before or after the estimated position. This method will only work if calculations on the size of differences between key values are sensible.
By comparison, the binary search always chooses the middle of the remaining search space, discarding one half or the other, again depending on the comparison between the key value found at the estimated position and the key value sought. The remaining search space is reduced to the part before or after the estimated position. The linear search uses equality only as it compares elements one-by-one from the start, ignoring any sorting.
On average the interpolation search makes about log(log(n)) comparisons (if the elements are uniformly distributed), where n is the number of elements to be searched. In the worst case (for instance where the numerical values of the keys increase exponentially) it can make up to O(n) comparisons.
In interpolation-sequential search, interpolation is used to find an item near the one being searched for, then linear search is used to find the exact item."
Intersection algorithm,"The intersection algorithm is an agreement algorithm used to select sources for estimating accurate time from a number of noisy time sources, it forms part of the modern Network Time Protocol. It is a modified form of Marzullo's algorithm.
While Marzullo's algorithm will return the smallest interval consistent with the largest number of sources, the returned interval does not necessarily include the center point (calculated offset) of all the sources in the intersection. The Intersection algorithm returns an interval that includes that returned by Marzullo's algorithm but may be larger since it will include the center points. This larger interval allows using additional statistical data to select a point within the interval, reducing the jitter in repeated execution."
Johnson algorithm,"Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in a sparse, edge weighted, directed graph. It allows some of the edge weights to be negative numbers, but no negative-weight cycles may exist. It works by using the Bellman–Ford algorithm to compute a transformation of the input graph that removes all negative weights, allowing Dijkstra's algorithm to be used on the transformed graph. It is named after Donald B. Johnson, who first published the technique in 1977.
A similar reweighting technique is also used in Suurballe's algorithm for finding two disjoint paths of minimum total length between the same two vertices in a graph with non-negative edge weights."
Jump-and-Walk algorithm,"Jump-and-Walk is an algorithm for point location in triangulations (though most of the theoretical analysis were performed in 2D and 3D random Delaunay triangulations). Surprisingly, the algorithm does not need any preprocessing or complex data structures except some simple representation of the triangulation itself. The predecessor of Jump-and-Walk was due to Lawson (1977) and Green and Sibson (1978), which picks a random starting point S and then walks from S toward the query point Q one triangle at a time. But no theoretical analysis was known for these predecessors until after mid-1990s.
Jump-and-Walk picks a small group of sample points and starts the walk from the sample point which is the closest to Q until the simplex containing Q is found. The algorithm was a folklore in practice for some time, and the formal presentation of the algorithm and the analysis of its performance on 2D random Delaunay triangulation was done by Devroye, Mucke and Zhu in mid-1990s (the paper appeared in Algorithmica, 1998). The analysis on 3D random Delaunay triangulation was done by Mucke, Saias and Zhu (ACM Symposium of Computational Geometry, 1996). In both cases, a boundary condition was assumed, namely, Q must be slightly away from the boundary of the convex domain where the vertices of the random Delaunay triangulation are drawn. In 2004, Devroye, Lemaire and Moreau showed that in 2D the boundary condition can be withdrawn (the paper appeared in Computational Geometry: Theory and Applications, 2004).
Jump-and-Walk has been used in many famous software packages, e.g., QHULL, Triangle and CGAL.

"
Jump point search,"In computer science, jump point search is an optimization to the A* search algorithm pathfinding algorithm for uniform-cost grids. It reduces symmetries in the search procedure by means of graph pruning, eliminating certain nodes in the grid based on assumptions that can be made about the current node's neighbors, as long as certain conditions relating to the grid are satisfied. As a result, the algorithm can consider long ""jumps"" along straight (horizontal, vertical and diagonal) lines in the grid, rather than the small steps from one grid position to the next that ordinary A* considers.
Jump point search preserves A*'s optimality, while potentially reducing its running time by an order of magnitude."
Jump search,"In computer science, a jump search or block search refers to a search algorithm for ordered lists. It works by first checking all items Lkm, where  and m is the block size, until an item is found that is larger than the search key. To find the exact position of the search key in the list a linear search is performed on the sublist L[(k-1)m, km].
The optimal value of m is √n, where n is the length of the list L. Because both steps of the algorithm look at, at most, √n items the algorithm runs in O(√n) time. This is better than a linear search, but worse than a binary search. The advantage over the latter is that a jump search only needs to jump backwards once, while a binary can jump backwards up to log n times. This can be important if a jumping backwards takes significantly more time than jumping forward.
The algorithm can be modified by performing multiple levels of jump search on the sublists, before finally performing the linear search. For an k-level jump search the optimum block size ml for the lth level (counting from 1) is n(k-l)/k. The modified algorithm will perform k backward jumps and runs in O(kn1/(k+1)) time."
K-means++,"In data mining, k-means++ is an algorithm for choosing the initial values (or ""seeds"") for the k-means clustering algorithm. It was proposed in 2007 by David Arthur and Sergei Vassilvitskii, as an approximation algorithm for the NP-hard k-means problem—a way of avoiding the sometimes poor clusterings found by the standard k-means algorithm. It is similar to the first of three seeding methods proposed, in independent work, in 2006 by Rafail Ostrovsky, Yuval Rabani, Leonard Schulman and Chaitanya Swamy. (The distribution of the first seed is different.)"
K-means clustering,"k-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells.
The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.
The algorithm has a loose relationship to the k-nearest neighbor classifier, a popular machine learning technique for classification that is often confused with k-means because of the k in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by k-means to classify new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm."
K-medoids,"The k-medoids algorithm is a clustering algorithm related to the k-means algorithm and the medoidshift algorithm. Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) and both attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. In contrast to the k-means algorithm, k-medoids chooses datapoints as centers (medoids or exemplars) and works with an arbitrary matrix of distances between datapoints instead of . This method was proposed in 1987 for the work with  norm and other distances.
k-medoid is a classical partitioning technique of clustering that clusters the data set of n objects into k clusters known a priori. A useful tool for determining k is the silhouette.
It is more robust to noise and outliers as compared to k-means because it minimizes a sum of pairwise dissimilarities instead of a sum of squared Euclidean distances.
A medoid can be defined as the object of a cluster whose average dissimilarity to all the objects in the cluster is minimal. i.e. it is a most centrally located point in the cluster."
K-nearest neighbors,"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:

In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.

In k-NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.

k-NN is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until classification. The k-NN algorithm is among the simplest of all machine learning algorithms.
Both for classification and regression, it can be useful to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.
The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
A shortcoming of the k-NN algorithm is that it is sensitive to the local structure of the data. The algorithm has nothing to do with and is not to be confused with k-means, another popular machine learning technique."
Kabsch algorithm,"The Kabsch algorithm, named after Wolfgang Kabsch, is a method for calculating the optimal rotation matrix that minimizes the RMSD (root mean squared deviation) between two paired sets of points. It is useful in graphics, cheminformatics to compare molecular structures, and also bioinformatics for comparing protein structures (in particular, see root-mean-square deviation (bioinformatics)).
The algorithm only computes the rotation matrix, but it also requires the computation of a translation vector. When both the translation and rotation are actually performed, the algorithm is sometimes called partial Procrustes superimposition (see also orthogonal Procrustes problem).

"
Kadane's algorithm,"In computer science, the maximum subarray problem is the task of finding the contiguous subarray within a one-dimensional array of numbers (containing at least one positive number) which has the largest sum. For example, for the sequence of values −2, 1, −3, 4, −1, 2, 1, −5, 4; the contiguous subarray with the largest sum is 4, −1, 2, 1, with sum 6.
The problem was first posed by Ulf Grenander of Brown University in 1977, as a simplified model for maximum likelihood estimation of patterns in digitized images. A linear time algorithm was found soon afterwards by Jay Kadane of Carnegie-Mellon University (Bentley 1984)."
Karatsuba algorithm,"The Karatsuba algorithm is a fast multiplication algorithm. It was discovered by Anatolii Alexeevitch Karatsuba in 1960 and published in 1962. It reduces the multiplication of two n-digit numbers to at most  single-digit multiplications in general (and exactly  when n is a power of 2). It is therefore faster than the classical algorithm, which requires n2 single-digit products. For example, the Karatsuba algorithm requires 310 = 59,049 single-digit multiplications to multiply two 1024-digit numbers (n = 1024 = 210), whereas the classical algorithm requires (210)2 = 1,048,576.
The Karatsuba algorithm was the first multiplication algorithm asymptotically faster than the quadratic ""grade school"" algorithm. The Toom–Cook algorithm is a faster generalization of Karatsuba's method, and the Schönhage–Strassen algorithm is even faster, for sufficiently large n."
Karger's algorithm,"In computer science and graph theory, Karger's algorithm is a randomized algorithm to compute a minimum cut of a connected graph. It was invented by David Karger and first published in 1993.
The idea of the algorithm is based on the concept of contraction of an edge  in an undirected graph . Informally speaking, the contraction of an edge merges the nodes  and  into one, reducing the total number of nodes of the graph by one. All other edges connecting either  or  are ""reattached"" to the merged node, effectively producing a multigraph. Karger's basic algorithm iteratively contracts randomly chosen edges until only two nodes remain; those nodes represent a cut in the original graph. By iterating this basic algorithm a sufficient number of times, a minimum cut can be found with high probability."
Karmarkar's algorithm,"Karmarkar's algorithm is an algorithm introduced by Narendra Karmarkar in 1984 for solving linear programming problems. It was the first reasonably efficient algorithm that solves these problems in polynomial time. The ellipsoid method is also polynomial time but proved to be inefficient in practice.
Where  is the number of variables and  is the number of bits of input to the algorithm, Karmarkar's algorithm requires  operations on  digit numbers, as compared to  such operations for the ellipsoid algorithm. The runtime of Karmarkar's algorithm is thus

using FFT-based multiplication (see Big O notation).
Karmarkar's algorithm falls within the class of interior point methods: the current guess for the solution does not follow the boundary of the feasible set as in the simplex method, but it moves through the interior of the feasible region, improving the approximation of the optimal solution by a definite fraction with every iteration, and converging to an optimal solution with rational data."
Karn's Algorithm,"Karn's algorithm addresses the problem of getting accurate estimates of the round-trip time for messages when using the Transmission Control Protocol (TCP) in computer networking. The algorithm was proposed by Phil Karn in 1987.
Accurate round trip estimates in TCP can be difficult to calculate because of an ambiguity created by retransmitted segments. The round trip time is estimated as the difference between the time that a segment was sent and the time that its acknowledgment was returned to the sender, but when packets are re-transmitted there is an ambiguity: the acknowledgment may be a response to the first transmission of the segment or to a subsequent re-transmission.
Karn's Algorithm ignores retransmitted segments when updating the round trip time estimate. Round trip time estimation is based only on unambiguous acknowledgments, which are acknowledgments for segments that were sent only once.
This simplistic implementation of Karn's algorithm can lead to problems as well. Consider what happens when TCP sends a segment after a sharp increase in delay. Using the prior round trip time estimate, TCP computes a timeout and retransmits a segment. If TCP ignores the round trip time of all retransmitted packets, the round trip estimate will never be updated, and TCP will continue retransmitting every segment, never adjusting to the increased delay.
A solution to this problem is to incorporate transmission timeouts with a timer backoff strategy. The timer backoff strategy computes an initial timeout. If the timer expires and causes a retransmission, TCP increases the timeout generally by a factor of 2. This algorithm has proven to be extremely effective in networks with high packet loss."
Kirkpatrick–Seidel algorithm,"The Kirkpatrick–Seidel algorithm, called by its authors ""the ultimate planar convex hull algorithm"" is an algorithm for computing the convex hull of a set of points in the plane, with O(n log h) time complexity, where n is the number of input points and h is the number of points in the hull. Thus, the algorithm is output-sensitive: its running time depends on both the input size and the output size. Another output-sensitive algorithm, the gift wrapping algorithm, was known much earlier, but the Kirkpatrick–Seidel algorithm has an asymptotic running time that is significantly smaller and that always improves on the O(n log n) bounds of non-output-sensitive algorithms. The Kirkpatrick–Seidel algorithm is named after its inventors, David G. Kirkpatrick and Raimund Seidel.
Although the algorithm is asymptotically very efficient, it is not very practical for moderate-sized problems.

"
Knuth–Morris–Pratt algorithm,"In computer science, the Knuth–Morris–Pratt string searching algorithm (or KMP algorithm) searches for occurrences of a ""word"" W within a main ""text string"" S by employing the observation that when a mismatch occurs, the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.
The algorithm was conceived in 1974 by Donald Knuth and Vaughan Pratt, and independently by James H. Morris. The three published it jointly in 1977."
Kosaraju's algorithm,"In computer science, Kosaraju's algorithm (also known as the Kosaraju–Sharir algorithm) is a linear time algorithm to find the strongly connected components of a directed graph. Aho, Hopcroft and Ullman credit it to an unpublished paper from 1978 by S. Rao Kosaraju. The same algorithm was independently discovered by Micha Sharir and published by him in 1981. It makes use of the fact that the transpose graph (the same graph with the direction of every edge reversed) has exactly the same strongly connected components as the original graph."
Kruskal's algorithm,"Kruskal's algorithm is a minimum-spanning-tree algorithm which finds an edge of the least possible weight that connects any two trees in the forest. It is a greedy algorithm in graph theory as it finds a minimum spanning tree for a connected weighted graph adding increasing cost arcs at each step. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. If the graph is not connected, then it finds a minimum spanning forest (a minimum spanning tree for each connected component).
This algorithm first appeared in Proceedings of the American Mathematical Society, pp. 48–50 in 1956, and was written by Joseph Kruskal.
Other algorithms for this problem include Prim's algorithm, Reverse-delete algorithm, and Borůvka's algorithm."
LR parser,"In computer science, LR parsers are a type of bottom-up parsers that efficiently handle deterministic context-free languages in guaranteed linear time. The LALR parsers and the SLR parsers are common variants of LR parsers. LR parsers are often mechanically generated from a formal grammar for the language by a parser generator tool. They are very widely used for the processing of computer languages, more than other kinds of generated parsers.
The name LR is an acronym. The L means that the parser reads input text in one direction without backing up; that direction is typically Left to right within each line, and top to bottom across the lines of the full input file. (This is true for most parsers.) The R means that the parser produces a reversed Rightmost derivation; it does a bottom-up parse, not a top-down LL parse or ad-hoc parse. The name LR is often followed by a numeric qualifier, as in LR(1) or sometimes LR(k). To avoid backtracking or guessing, the LR parser is allowed to peek ahead at k lookahead input symbols before deciding how to parse earlier symbols. Typically k is 1 and is not mentioned. The name LR is often preceded by other qualifiers, as in SLR and LALR.
LR parsers are deterministic; they produce a single correct parse without guesswork or backtracking, in linear time. This is ideal for computer languages. But LR parsers are not suited for human languages which need more flexible but slower methods. Other parser methods (CYK algorithm, Earley parser, and GLR parser) that backtrack or yield multiple parses may take O(n2), O(n3) or even exponential time when they guess badly.
The above properties of L, R, and k are actually shared by all shift-reduce parsers, including precedence parsers. But by convention, the LR name stands for the form of parsing invented by Donald Knuth, and excludes the earlier, less powerful precedence methods (for example Operator-precedence parser). LR parsers can handle a larger range of languages and grammars than precedence parsers or top-down LL parsing. This is because the LR parser waits until it has seen an entire instance of some grammar pattern before committing to what it has found. An LL parser has to decide or guess what it is seeing much sooner, when it has only seen the leftmost input symbol of that pattern. LR is also better at error reporting. It detects syntax errors as early in the input stream as possible."
LZ77 and LZ78,"LZ77 and LZ78 are the two lossless data compression algorithms published in papers by Abraham Lempel and Jacob Ziv in 1977 and 1978. They are also known as LZ1 and LZ2 respectively. These two algorithms form the basis for many variations including LZW, LZSS, LZMA and others. Besides their academic influence, these algorithms formed the basis of several ubiquitous compression schemes, including GIF and the DEFLATE algorithm used in PNG.
They are both theoretically dictionary coders. LZ77 maintains a sliding window during compression. This was later shown to be equivalent to the explicit dictionary constructed by LZ78—however, they are only equivalent when the entire data is intended to be decompressed. LZ78 decompression allows random access to the input as long as the entire dictionary is available, while LZ77 decompression must always start at the beginning of the input.
The algorithms were named an IEEE Milestone in 2004."
LZJB,"LZJB is a lossless data compression algorithm invented by Jeff Bonwick to compress crash dumps and data in ZFS. It includes a number of improvements to the LZRW1 algorithm, a member of the Lempel–Ziv family of compression algorithms. The name LZJB is derived from its parent algorithm and its creator—Lempel Ziv Jeff Bonwick. Bonwick is also one of two architects of ZFS, and the creator of the Slab Allocator."
LZRW,"Lempel–Ziv Ross Williams (LZRW) refers to variants of the LZ77 lossless data compression algorithms with an emphasis on improving compression speed through the use of hash tables and other techniques. This family was explored by Ross Williams, who published a series of algorithms beginning with LZRW1 in 1991.
The variants are:
LZRW1
LZRW1-A
LZRW2
LZRW3
LZRW3-A
LZRW4
LZRW5
The LZJB algorithm used in ZFS is derived from LZRW1."
LZWL,LZWL is a syllable-based variant of the character-based LZW compression algorithm that can work with syllables obtained by all algorithms of decomposition into syllables. The algorithm can be used for words too.
LZX,"LZX is the name of an LZ77 family compression algorithm. It is also the name of a file archiver with the same name. Both were invented by Jonathan Forbes and Tomi Poutanen.

"
Lamport's Bakery algorithm,"Lamport's bakery algorithm is a computer algorithm devised by computer scientist Leslie Lamport, which is intended to improve the safety in the usage of shared resources among multiple threads by means of mutual exclusion.
In computer science, it is common for multiple threads to simultaneously access the same resources. Data corruption can occur if two or more threads try to write into the same memory location, or if one thread reads a memory location before another has finished writing into it. Lamport's bakery algorithm is one of many mutual exclusion algorithms designed to prevent concurrent threads entering critical sections of code concurrently to eliminate the risk of data corruption."
Lamport's Distributed Mutual Exclusion Algorithm,"Lamport's Distributed Mutual Exclusion Algorithm is a contention-based algorithm for mutual exclusion on a distributed system.

"
Least slack time scheduling,"Least slack time (LST) scheduling is a scheduling algorithm. It assigns priority based on the slack time of a process. Slack time is the amount of time left after a job if the job was started now. This algorithm is also known as least laxity first. Its most common use is in embedded systems, especially those with multiple processors. It imposes the simple constraint that each process on each available processor possesses the same run time, and that individual processes do not have an affinity to a certain processor. This is what lends it a suitability to embedded systems."
Lempel–Ziv,"LZ77 and LZ78 are the two lossless data compression algorithms published in papers by Abraham Lempel and Jacob Ziv in 1977 and 1978. They are also known as LZ1 and LZ2 respectively. These two algorithms form the basis for many variations including LZW, LZSS, LZMA and others. Besides their academic influence, these algorithms formed the basis of several ubiquitous compression schemes, including GIF and the DEFLATE algorithm used in PNG.
They are both theoretically dictionary coders. LZ77 maintains a sliding window during compression. This was later shown to be equivalent to the explicit dictionary constructed by LZ78—however, they are only equivalent when the entire data is intended to be decompressed. LZ78 decompression allows random access to the input as long as the entire dictionary is available, while LZ77 decompression must always start at the beginning of the input.
The algorithms were named an IEEE Milestone in 2004."
Lempel–Ziv–Markov chain algorithm,"The Lempel–Ziv–Markov chain algorithm (LZMA) is an algorithm used to perform lossless data compression. It has been under development either since 1998 or 1996 and was first used in the 7z format of the 7-Zip archiver. This algorithm uses a dictionary compression scheme somewhat similar to the LZ77 algorithm published by Abraham Lempel and Jacob Ziv in 1977 and features a high compression ratio (generally higher than bzip2)) and a variable compression-dictionary size (up to 4 GB), while still maintaining decompression speed similar to other commonly used compression algorithms.
LZMA2 is a simple container format that can include both uncompressed data and LZMA data, possibly with multiple different LZMA encoding parameters. LZMA2 supports arbitrarily scalable multithreaded compression and decompression and efficient compression of data which is partially incompressible."
Lempel–Ziv–Oberhumer,"Lempel–Ziv–Oberhumer (LZO) is a lossless data compression algorithm that is focused on decompression speed.

"
Lempel–Ziv–Stac,"Lempel–Ziv–Stac (LZS, or Stac compression) is a lossless data compression algorithm that uses a combination of the LZ77 sliding-window compression algorithm and fixed Huffman coding. It was originally developed by Stac Electronics for tape compression, and subsequently adapted for hard disk compression and sold as the Stacker disk compression software. It was later specified as a compression algorithm for various network protocols. LZS is specified in the Cisco IOS stack."
Lempel–Ziv–Storer–Szymanski,"Lempel–Ziv–Storer–Szymanski (LZSS) is a lossless data compression algorithm, a derivative of LZ77, that was created in 1982 by James Storer and Thomas Szymanski. LZSS was described in article ""Data compression via textual substitution"" published in Journal of the ACM (pp. 928–951).
LZSS is a dictionary encoding technique. It attempts to replace a string of symbols with a reference to a dictionary location of the same string.
The main difference between LZ77 and LZSS is that in LZ77 the dictionary reference could actually be longer than the string it was replacing. In LZSS, such references are omitted if the length is less than the ""break even"" point. Furthermore, LZSS uses one-bit flags to indicate whether the next chunk of data is a literal (byte) or a reference to an offset/length pair."
Lempel–Ziv–Welch,"Lempel–Ziv–Welch (LZW) is a universal lossless data compression algorithm created by Abraham Lempel, Jacob Ziv, and Terry Welch. It was published by Welch in 1984 as an improved implementation of the LZ78 algorithm published by Lempel and Ziv in 1978. The algorithm is simple to implement, and has the potential for very high throughput in hardware implementations. It was the algorithm of the widely used Unix file compression utility compress, and is used in the GIF image format."
Lenstra elliptic curve factorization,"The Lenstra elliptic curve factorization or the elliptic curve factorization method (ECM) is a fast, sub-exponential running time algorithm for integer factorization which employs elliptic curves. For general purpose factoring, ECM is the third-fastest known factoring method. The second fastest is the multiple polynomial quadratic sieve and the fastest is the general number field sieve. The Lenstra elliptic curve factorization is named after Hendrik Lenstra.
Practically speaking, ECM is considered a special purpose factoring algorithm as it is most suitable for finding small factors. Currently, it is still the best algorithm for divisors not greatly exceeding 20 to 25 digits (64 to 83 bits or so), as its running time is dominated by the size of the smallest factor p rather than by the size of the number n to be factored. Frequently, ECM is used to remove small factors from a very large integer with many factors; if the remaining integer is still composite, then it has only large factors and is factored using general purpose techniques. The largest factor found using ECM so far has 83 digits and was discovered on 7 September 2013 by R. Propper. Increasing the number of curves tested improves the chances of finding a factor, but they are not linear with the increase in the number of digits."
Level set method,"Level set methods (LSM) are a conceptual framework for using level sets as a tool for numerical analysis of surfaces and shapes. The advantage of the level set model is that one can perform numerical computations involving curves and surfaces on a fixed Cartesian grid without having to parameterize these objects (this is called the Eulerian approach). Also, the level set method makes it very easy to follow shapes that change topology, for example when a shape splits in two, develops holes, or the reverse of these operations. All these make the level set method a great tool for modeling time-varying objects, like inflation of an airbag, or a drop of oil floating in water.

The figure on the right illustrates several important ideas about the level set method. In the upper-left corner we see a shape; that is, a bounded region with a well-behaved boundary. Below it, the red surface is the graph of a level set function  determining this shape, and the flat blue region represents the xy-plane. The boundary of the shape is then the zero level set of , while the shape itself is the set of points in the plane for which  is positive (interior of the shape) or zero (at the boundary).
In the top row we see the shape changing its topology by splitting in two. It would be quite hard to describe this transformation numerically by parameterizing the boundary of the shape and following its evolution. One would need an algorithm able to detect the moment the shape splits in two, and then construct parameterizations for the two newly obtained curves. On the other hand, if we look at the bottom row, we see that the level set function merely translated downward. This is an example of when it can be much easier to work with a shape through its level set function than with the shape directly, where using the shape directly would need to consider and handle all the possible deformations the shape might undergo.
Thus, in two dimensions, the level set method amounts to representing a closed curve  (such as the shape boundary in our example) using an auxiliary function , called the level set function.  is represented as the zero level set of  by

and the level set method manipulates  implicitly, through the function . This function  is assumed to take positive values inside the region delimited by the curve  and negative values outside.
^ Osher, S.; Sethian, J. A. (1988), ""Fronts propagating with curvature-dependent speed: Algorithms based on Hamilton–Jacobi formulations"" (PDF), J. Comput. Phys. 79: 12–49., Bibcode:1988JCoPh..79...12O, doi:10.1016/0021-9991(88)90002-2 
^ 
^"
Levenberg–Marquardt algorithm,"In mathematics and computing, the Levenberg–Marquardt algorithm (LMA), also known as the damped least-squares (DLS) method, is used to solve non-linear least squares problems. These minimization problems arise especially in least squares curve fitting.
The LMA is used in many software applications for solving generic curve-fitting problems. However, as for many fitting algorithms, the LMA finds only a local minimum, which is not necessarily the global minimum. The LMA interpolates between the Gauss–Newton algorithm (GNA) and the method of gradient descent. The LMA is more robust than the GNA, which means that in many cases it finds a solution even if it starts very far off the final minimum. For well-behaved functions and reasonable starting parameters, the LMA tends to be a bit slower than the GNA. LMA can also be viewed as Gauss–Newton using a trust region approach.
The algorithm was first published in 1944 by Kenneth Levenberg, while working at the Frankford Army Arsenal. It was rediscovered in 1963 by Donald Marquardt who worked as a statistician at DuPont and independently by Girard, Wynn and Morrison."
Lexicographic breadth-first search,"In computer science, lexicographic breadth-first search or Lex-BFS is a linear time algorithm for ordering the vertices of a graph. The algorithm is different from breadth first search, but it produces an ordering that is consistent with breadth-first search.
The lexicographic breadth-first search algorithm is based on the idea of partition refinement and was first developed by Donald J. Rose, Robert E. Tarjan, and George S. Lueker (1976). A more detailed survey of the topic is presented by Corneil (2004). It has been used as a subroutine in other graph algorithms including the recognition of chordal graphs, and optimal coloring of distance-hereditary graphs."
Library sort,"Library sort, or gapped insertion sort is a sorting algorithm that uses an insertion sort, but with gaps in the array to accelerate subsequent insertions. The name comes from an analogy:

Suppose a librarian were to store his books alphabetically on a long shelf, starting with the A's at the left end, and continuing to the right along the shelf with no spaces between the books until the end of the Z's. If the librarian acquired a new book that belongs to the B section, once he finds the correct space in the B section, he will have to move every book over, from the middle of the B's all the way down to the Z's in order to make room for the new book. This is an insertion sort. However, if he were to leave a space after every letter, as long as there was still space after B, he would only have to move a few books to make room for the new one. This is the basic principle of the Library Sort.

The algorithm was proposed by Michael A. Bender, Martín Farach-Colton, and Miguel Mosteiro in 2004 and was published in 2006.
Like the insertion sort it is based on, library sort is a stable comparison sort and can be run as an online algorithm; however, it was shown to have a high probability of running in O(n log n) time (comparable to quicksort), rather than an insertion sort's O(n2). The mechanism used for this improvement is very similar to that of a skip list. There is no full implementation given in the paper, nor the exact algorithms of important parts, such as insertion and rebalancing. Further information would be needed to discuss how the efficiency of library sort compares to that of other sorting methods in reality.
Compared to basic insertion sort, the drawback of library sort is that it requires extra space for the gaps. The amount and distribution of that space would be implementation dependent. In the paper the size of the needed array is (1 + ε)n, but with no further recommendations on how to choose ε.
One weakness of insertion sort is that it may require a high number of swap operations and be costly if memory write is expensive. Library sort may improve that somewhat in the insertion step, as fewer elements need to move to make room, but is also adding an extra cost in the rebalancing step. In addition, locality of reference will be poor compared to mergesort as each insertion from a random data set may access memory that is no longer in cache, especially with large data sets."
Linde–Buzo–Gray algorithm,"The Linde–Buzo–Gray algorithm (introduced by Yoseph Linde, Andrés Buzo and Robert M. Gray in 1980) is a vector quantization algorithm to derive a good codebook.
It is similar to the k-means method in data clustering.

"
Line drawing algorithm,"A line drawing algorithm is a graphical algorithm for approximating a line segment on discrete graphical media. On discrete media, such as pixel-based displays and printers, line drawing requires such an approximation (in nontrivial cases). Basic algorithms rasterize lines in one color. A better representation with multiple color gradations requires an advanced process, anti-aliasing.
On continuous media, by contrast, no algorithm is necessary to draw a line. For example, oscilloscopes use natural phenomena to draw lines and curves.
The Cartesian slope-intercept equation for a straight line is  With m representing the slope of the line and b as the y intercept. Given that the two endpoints of the line segment are specified at positions  and . we can determine values for the slope m and y intercept b with the following calculations,  so, ."
Line segment intersection,"In computational geometry, the line segment intersection problem supplies a list of line segments in the Euclidean plane and asks whether any two of them intersect, or cross.
Simple algorithms examine each pair of segments. However, if a large number of possibly intersecting segments are to be checked, this becomes increasingly inefficient since most pairs of segments are not close to one another in a typical input sequence. The most common, more efficient way to solve this problem for a high number of segments is to use a sweep line algorithm, where we imagine a line sliding across the line segments and we track which line segments it intersects at each point in time using a dynamic data structure based on binary search trees. The Shamos–Hoey algorithm applies this principle to solve the line segment intersection detection problem, as stated above, of determining whether or not a set of line segments has an intersection; the Bentley–Ottmann algorithm works by the same principle to list all intersections in logarithmic time per intersection."
Linear-time,"In computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input. The time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms. When expressed this way, the time complexity is said to be described asymptotically, i.e., as the input size goes to infinity. For example, if the time required by an algorithm on all inputs of size n is at most 5n3 + 3n for any n (bigger than some n0), the asymptotic time complexity is O(n3).
Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, where an elementary operation takes a fixed amount of time to perform. Thus the amount of time taken and the number of elementary operations performed by the algorithm differ by at most a constant factor.
Since an algorithm's performance time may vary with different inputs of the same size, one commonly uses the worst-case time complexity of an algorithm, denoted as T(n), which is defined as the maximum amount of time taken on any input of size n. Less common, and usually specified explicitly, is the measure of average-case complexity. Time complexities are classified by the nature of the function T(n). For instance, an algorithm with T(n) = O(n) is called a linear time algorithm, and an algorithm with T(n) = O(Mn) and mn= O(T(n)) for some M ≥ m > 1 is said to be an exponential time algorithm."
Linear programming,"Linear programming (LP; also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (mathematical optimization).
More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polyhedron where this function has the smallest (or largest) value if such a point exists.
Linear programs are problems that can be expressed in canonical form as

where x represents the vector of variables (to be determined), c and b are vectors of (known) coefficients, A is a (known) matrix of coefficients, and  is the matrix transpose. The expression to be maximized or minimized is called the objective function (cTx in this case). The inequalities Ax ≤ b and x ≥ 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second then we can say the first vector is less-than or equal-to the second vector.
Linear programming can be applied to various fields of study. It is widely used in business and economics, and is also utilized for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proved useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design."
Linear search,"In computer science, linear search or sequential search is a method for finding a particular value in a list that checks each element in sequence until the desired element is found or the list is exhausted. The list need not be ordered.
Linear search is the simplest search algorithm; it is a special case of brute-force search. Its worst case cost is proportional to the number of elements in the list. Its expected cost is also proportional to the number of elements if all elements are searched equally. If the list has more than a few elements and is searched often, then more complicated search methods such as binary search or hashing may be appropriate. Those methods have faster search times but require additional resources to attain that speed."
Linear time,"In computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input. The time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms. When expressed this way, the time complexity is said to be described asymptotically, i.e., as the input size goes to infinity. For example, if the time required by an algorithm on all inputs of size n is at most 5n3 + 3n for any n (bigger than some n0), the asymptotic time complexity is O(n3).
Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, where an elementary operation takes a fixed amount of time to perform. Thus the amount of time taken and the number of elementary operations performed by the algorithm differ by at most a constant factor.
Since an algorithm's performance time may vary with different inputs of the same size, one commonly uses the worst-case time complexity of an algorithm, denoted as T(n), which is defined as the maximum amount of time taken on any input of size n. Less common, and usually specified explicitly, is the measure of average-case complexity. Time complexities are classified by the nature of the function T(n). For instance, an algorithm with T(n) = O(n) is called a linear time algorithm, and an algorithm with T(n) = O(Mn) and mn= O(T(n)) for some M ≥ m > 1 is said to be an exponential time algorithm."
List of algorithm general topics,"This is a list of algorithm general topics.
Analysis of algorithms
Ant colony algorithm
Approximation algorithm
Best and worst cases
Big O notation
Combinatorial search
Competitive analysis
Computability theory
Computational complexity theory
Embarrassingly parallel problem
Emergent algorithm
Evolutionary algorithm
Fast Fourier transform
Genetic algorithm
Graph exploration algorithm
Heuristic
Hill climbing
Implementation
Las Vegas algorithm
Lock-free and wait-free algorithms
Monte Carlo algorithm
Numerical analysis
Online algorithm
Polynomial time approximation scheme
Problem size
Pseudorandom number generator
Quantum algorithm
Random-restart hill climbing
Randomized algorithm
Running time
Sorting algorithm
Search algorithm
Stable algorithm (disambiguation)
Super-recursive algorithm
Tree search algorithm"
List of algorithms,The following is a list of algorithms along with one-line descriptions for each.
List of terms relating to algorithms and data structures,"The NIST Dictionary of Algorithms and Data Structures is a reference work maintained by the U.S. National Institute of Standards and Technology. It defines a large number of terms relating to algorithms and data structures. For algorithms and data structures not necessarily mentioned here, see list of algorithms and list of data structures.
This list of terms was originally derived from the index of that document, and is in the public domain, as it was compiled by a Federal Government employee as part of a Federal Government work. Some of the terms defined are:

"
List scheduling,"The basic idea of list scheduling is to make an ordered list of processes by assigning them some priorities, and then repeatedly execute the following two steps until a valid schedule is obtained :
Select from the list, the process with the highest priority for scheduling.
Select a resource to accommodate this process.
If no resource can be found, we select the next process in the list.
The priorities are determined statically before scheduling process begins. The first step chooses the process with the highest priority, the second step selects the best possible resource. Some known list scheduling strategies are :
Highest level first algorithm or HLF
Longest path algorithm or LP
Longest processing time
Critical path method
Heterogeneous Earliest Finish Time or HEFT. For the case heterogeneous workers."
Lloyd's algorithm,"In computer science and electrical engineering, Lloyd's algorithm, also known as Voronoi iteration or relaxation, is an algorithm named after Stuart P. Lloyd for finding evenly spaced sets of points in subsets of Euclidean spaces, and partitions of these subsets into well-shaped and uniformly sized convex cells. Like the closely related k-means clustering algorithm, it repeatedly finds the centroid of each set in the partition, and then re-partitions the input according to which of these centroids is closest. However, Lloyd's algorithm differs from k-means clustering in that its input is a continuous geometric region rather than a discrete set of points. Thus, when re-partitioning the input, Lloyd's algorithm uses Voronoi diagrams rather than simply determining the nearest center to each of a finite set of points as the k-means algorithm does.
Although the algorithm may be applied most directly to the Euclidean plane, similar algorithms may also be applied to higher-dimensional spaces or to spaces with other non-Euclidean metrics. Lloyd's algorithm can be used to construct close approximations to centroidal Voronoi tessellations of the input, which can be used for quantization, dithering, and stippling. Other applications of Lloyd's algorithm include smoothing of triangle meshes in the finite element method."
Local search (optimization),"In computer science, local search is a metaheuristic method for solving computationally hard optimization problems. Local search can be used on problems that can be formulated as finding a solution maximizing a criterion among a number of candidate solutions. Local search algorithms move from solution to solution in the space of candidate solutions (the search space) by applying local changes, until a solution deemed optimal is found or a time bound is elapsed.
Local search algorithms are widely applied to numerous hard computational problems, including problems from computer science (particularly artificial intelligence), mathematics, operations research, engineering, and bioinformatics. Examples of local search algorithms are WalkSAT and the 2-opt algorithm for the Traveling Salesman Problem."
LogitBoost,"In machine learning and computational learning theory, LogitBoost is a boosting algorithm formulated by Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The original paper casts the AdaBoost algorithm into a statistical framework. Specifically, if one considers AdaBoost as a generalized additive model and then applies the cost functional of logistic regression, one can derive the LogitBoost algorithm.

"
Look-ahead LR parser,"In computer science, an LALR parser or Look-Ahead LR parser is a simplified version of a canonical LR parser, to parse (separate and analyze) a text according to a set of production rules specified by a formal grammar for a computer language. (""LR"" means left-to-right, rightmost derivation.)
The LALR parser was invented by Frank DeRemer in his 1969 PhD dissertation, Practical Translators for LR(k) languages, in his treatment of the practical difficulties at that time of implementing LR(1) parsers. He showed that the LALR parser has more language recognition power than the LR(0) parser, while requiring the same number of states as the LR(0) parser for a language that can be recognized by both parsers. This makes the LALR parser a memory-efficient alternative to the LR(1) parser for languages that are not LR(0). It was also proved that there exist LR(1) languages that are not LALR. Despite this weakness, the power of the LALR parser is enough for many mainstream computer languages, including Java, though the reference grammars for many languages fail to be LALR due to being ambiguous.
The original dissertation gave no algorithm for constructing such a parser given some formal grammar. The first algorithms for LALR parser generation were published in 1973. In 1982, DeRemer and Tom Pennello published an algorithm that generated highly memory-efficient LALR parsers. LALR parsers can be automatically generated from some grammar by an LALR parser generator such as Yacc or GNU Bison. The automatically generated code may be augmented by hand-written code to augment the power of the resulting parser."
Lossless data compression,"Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast, lossy compression permits reconstruction only of an approximation of the original data, though this usually improves compression rates (and therefore reduces file sizes).
Lossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by the LAME MP3 encoder and other lossy audio encoders).
Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data could be deleterious. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods. Lossless audio formats are most often used for archiving or production purposes, while smaller lossy audio files are typically used on portable players and in other cases where storage space is limited or exact replication of the audio is unnecessary.

"
Luhn algorithm,"The Luhn algorithm or Luhn formula, also known as the ""modulus 10"" or ""mod 10"" algorithm, is a simple checksum formula used to validate a variety of identification numbers, such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the US, and Canadian Social Insurance Numbers. It was created by IBM scientist Hans Peter Luhn and described in U.S. Patent No. 2,950,048, filed on January 6, 1954, and granted on August 23, 1960.
The algorithm is in the public domain and is in wide use today. It is specified in ISO/IEC 7812-1. It is not intended to be a cryptographically secure hash function; it was designed to protect against accidental errors, not malicious attacks. Most credit cards and many government identification numbers use the algorithm as a simple method of distinguishing valid numbers from mistyped or otherwise incorrect numbers."
Luhn mod N algorithm,"The Luhn mod N algorithm is an extension to the Luhn algorithm (also known as mod 10 algorithm) that allows it to work with sequences of non-numeric characters. This can be useful when a check digit is required to validate an identification string composed of letters, a combination of letters and digits or even any arbitrary set of characters.

"
Luleå algorithm,"The Luleå algorithm of computer science, designed by Degermark et al. (1997), is a patented technique for storing and searching internet routing tables efficiently. It is named after the Luleå University of Technology, the home institute of the technique's authors. The name of the algorithm does not appear in the original paper describing it, but was used in a message from Craig Partridge to the Internet Engineering Task Force describing that paper prior to its publication.
The key task to be performed in internet routing is to match a given IPv4 address (viewed as a sequence of 32 bits) to the longest prefix of the address for which routing information is available. This prefix matching problem may be solved by a trie, but trie structures use a significant amount of space (a node for each bit of each address) and searching them requires traversing a sequence of nodes with length proportional to the number of bits in the address. The Luleå algorithm shortcuts this process by storing only the nodes at three levels of the trie structure, rather than storing the entire trie.
The main advantage of the Luleå algorithm for the routing task is that it uses very little memory, averaging 4–5 bytes per entry for large routing tables. This small memory footprint often allows the entire data structure to fit into the routing processor's cache, speeding operations. However, it has the disadvantage that it cannot be modified easily: small changes to the routing table may require most or all of the data structure to be reconstructed."
MD5,"The MD5 message-digest algorithm is a widely used cryptographic hash function producing a 128-bit (16-byte) hash value, typically expressed in text format as a 32 digit hexadecimal number. MD5 has been utilized in a wide variety of cryptographic applications, and is also commonly used to verify data integrity.
MD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function, MD4. The source code in RFC 1321 contains a ""by attribution"" RSA license.
In 1996 a flaw was found in the design of MD5. While it was not deemed a fatal weakness at the time, cryptographers began recommending the use of other algorithms, such as SHA-1—which has since been found to be vulnerable as well. In 2004 it was shown that MD5 is not collision resistant. As such, MD5 is not suitable for applications like SSL certificates or digital signatures that rely on this property for digital security. Also in 2004 more serious flaws were discovered in MD5, making further use of the algorithm for security purposes questionable; specifically, a group of researchers described how to create a pair of files that share the same MD5 checksum. Further advances were made in breaking MD5 in 2005, 2006, and 2007. In December 2008, a group of researchers used this technique to fake SSL certificate validity. As of 2010, the CMU Software Engineering Institute considers MD5 ""cryptographically broken and unsuitable for further use"", and most U.S. government applications now require the SHA-2 family of hash functions. In 2012, the Flame malware exploited the weaknesses in MD5 to fake a Microsoft digital signature."
Maekawa's Algorithm,Maekawa's algorithm is an algorithm for mutual exclusion on a distributed system. The basis of this algorithm is a quorum like approach where any one site needs only to seek permissions from a subset of other sites.
Marching cubes,"Marching cubes is a computer graphics algorithm, published in the 1987 SIGGRAPH proceedings by Lorensen and Cline, for extracting a polygonal mesh of an isosurface from a three-dimensional discrete scalar field (sometimes called voxels). This paper is one of the most cited papers in the computer graphics field. The applications of this algorithm are mainly concerned with medical visualizations such as CT and MRI scan data images, and special effects or 3-D modelling with what is usually called metaballs or other metasurfaces. An analogous two-dimensional method is called the marching squares algorithm."
Marching squares,"Marching squares is a computer graphics algorithm that generates contours for a two-dimensional scalar field (rectangular array of individual numerical values). A similar method can be used to contour 2D triangle meshes.
The contours can be of two kinds:
Isolines - lines following a single data level, or isovalue.
Isobands - filled areas between isolines.
Typical applications include the Contour lines on topographic maps or the generation of isobars for weather maps.
Marching squares takes a similar approach to the 3D marching cubes algorithm:
Process each cell in the grid independently.
Calculate a cell index using comparisons of the contour level(s) with the data values at the cell corners.
Use a pre-built lookup table, keyed on the cell index, to describe the output geometry for the cell.
Apply linear interpolation along the boundaries of the cell to calculate the exact contour position."
Marching tetrahedrons,"Marching tetrahedra is an algorithm in the field of computer graphics to render implicit surfaces. It clarifies a minor ambiguity problem of the marching cubes algorithm with some cube configurations.
Since more than 20 years have passed from the patent filing date of the marching cubes (June 5, 1985), the original algorithm can be used freely again, adding only the minor modification to circumvent the aforementioned ambiguity in some configurations.
In marching tetrahedra, each cube is split into six irregular tetrahedra by cutting the cube in half three times, cutting diagonally through each of the three pairs of opposing faces. In this way, the tetrahedra all share one of the main diagonals of the cube. Instead of the twelve edges of the cube, we now have nineteen edges: the original twelve, six face diagonals, and the main diagonal. Just like in marching cubes, the intersections of these edges with the isosurface are approximated by linearly interpolating the values at the grid points.
Adjacent cubes share all edges in the connecting face, including the same diagonal. This is an important property to prevent cracks in the rendered surface, because interpolation of the two distinct diagonals of a face usually gives slightly different intersection points. An added benefit is that up to five computed intersection points can be reused when handling the neighbor cube. This includes the computed surface normals and other graphics attributes at the intersection points.
Each tetrahedron has sixteen possible configurations, falling into three classes: no intersection, intersection in one triangle and intersection in two (adjacent) triangles. It is straightforward to enumerate all sixteen configurations and map them to vertex index lists defining the appropriate triangle strips."
Mark-compact algorithm,"In computer science, a mark-compact algorithm is a type of garbage collection algorithm used to reclaim unreachable memory. Mark-compact algorithms can be regarded as a combination of the mark-sweep algorithm and Cheney's copying algorithm. First, reachable objects are marked, then a compacting step relocates the reachable (marked) objects towards the beginning of the heap area. Compacting garbage collection is used by Microsoft's Common Language Runtime and by the Glasgow Haskell Compiler."
Marr–Hildreth algorithm,"In computer vision, the Marr–Hildreth algorithm is a method of detecting edges in digital images, that is, continuous curves where there are strong and rapid variations in image brightness. The Marr–Hildreth edge detection method is simple and operates by convolving the image with the Laplacian of the Gaussian function, or, as a fast approximation by Difference of Gaussians. Then, zero crossings are detected in the filtered result to obtain the edges. The Laplacian-of-Gaussian image operator is sometimes also referred to as the Mexican hat wavelet due to its visual shape when turned upside-down. David Marr and Ellen C. Hildreth are two of the inventors.
The Marr–Hildreth operator, however, suffers from two main limitations. It generates responses that do not correspond to edges, so-called ""false edges"", and the localization error may be severe at curved edges. Today, there are much better edge detection methods, such as the Canny edge detector based on the search for local directional maxima in the gradient magnitude, or the differential approach based on the search for zero crossings of the differential expression that corresponds to the second-order derivative in the gradient direction (Both of these operations preceded by a Gaussian smoothing step.) For more details, see the article on Edge detection."
Marzullo's algorithm,"Marzullo's algorithm, invented by Keith Marzullo for his Ph.D. dissertation in 1984, is an agreement algorithm used to select sources for estimating accurate time from a number of noisy time sources. A refined version of it, renamed the ""intersection algorithm"", forms part of the modern Network Time Protocol. The Marzullo's algorithm is also used to compute the relaxed intersection of n boxes (or more generally n subsets of Rn), as required by several robust set estimation methods."
Match Rating Approach,"The match rating approach (MRA) is a phonetic algorithm developed by Western Airlines in 1977 for the indexation and comparison of homophonous names.
The algorithm itself has a simple set of encoding rules but a more lengthy set of comparison rules. The main mechanism being the similarity comparison which calculates the number of unmatched characters by comparing the strings from left to right and then from right to left and removing identical characters. This value is subtracted from 6 and then compared to a minimum threshold. The minimum threshold is defined by table A and is dependent upon the length of the strings.
The encoded name is known (perhaps incorrectly) as a personal numeric identifier (PNI). The PNI codex can never contain more than 6 alpha only characters.
Match rating approach performs well with names containing the letter ""y"" unlike the original flavour of the NYSIIS algorithm. For example, the surnames ""Smith"" and ""Smyth"" are successfully matched.
MRA does not perform well with encoded names that differ in length by more than 2.

"
Matrix multiplication algorithm,"Because matrix multiplication is such a central operation in many numerical algorithms, much work has been invested in making matrix multiplication algorithms efficient. Applications of matrix multiplication in computational problems are found in many fields including scientific computing and pattern recognition and in seemingly unrelated problems such counting the paths through a graph. Many different algorithms have been designed for multiplying matrices on different types of hardware, including parallel and distributed systems, where the computational work is spread over multiple processors (perhaps over a network).
Directly applying the mathematical definition of matrix multiplication gives an algorithm that takes time on the order of n3 to multiply two n × n matrices (Θ(n3) in big O notation). Better asymptotic bounds on the time required to multiply matrices have been known since the work of Strassen in the 1960s, but it is still unknown what the optimal time is (i.e., what the complexity of the problem is)."
Medical algorithms,"A medical algorithm is any computation, formula, statistical survey, nomogram, or look-up table, useful in healthcare. Medical algorithms include decision tree approaches to healthcare treatment (e.g., if symptoms A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty."
Memetic algorithm,"Memetic algorithms (MA) represent one of the recent growing areas of research in evolutionary computation. The term MA is now widely used as a synergy of evolutionary or any population-based approach with separate individual learning or local improvement procedures for problem search. Quite often, MA are also referred to in the literature as Baldwinian evolutionary algorithms (EA), Lamarckian EAs, cultural algorithms, or genetic local search."
Merge algorithm,"Merge algorithms are a family of algorithms that run sequentially over multiple sorted lists, typically producing more sorted lists as output. This is well-suited for machines with tape drives.
The general merge algorithm has a set of pointers p0..n that point to positions in a set of lists L0..n. Initially they point to the first item in each list. The algorithm is as follows:
While any of p0..n still point to data inside of L0..n instead of past the end:
do something with the data items p0..n point to in their respective lists
find out which of those pointers points to the item with the lowest key; advance one of those pointers to the next item in its list"
Merge sort,"In computer science, merge sort (also commonly spelled mergesort) is an O(n log n) comparison-based sorting algorithm. Most implementations produce a stable sort, which means that the implementation preserves the input order of equal elements in the sorted output. Mergesort is a divide and conquer algorithm that was invented by John von Neumann in 1945. A detailed description and analysis of bottom-up mergesort appeared in a report by Goldstine and Neumann as early as 1948."
Metaphone,"Lawrence Philips redirects here. For the football player, see Lawrence Phillips.
Metaphone is a phonetic algorithm, published by Lawrence Philips in 1990, for indexing words by their English pronunciation. It fundamentally improves on the Soundex algorithm by using information about variations and inconsistencies in English spelling and pronunciation to produce a more accurate encoding, which does a better job of matching words and names which sound similar. As with Soundex, similar sounding words should share the same keys. Metaphone is available as a built-in operator in a number of systems.
The original author later produced a new version of the algorithm, which he named Double Metaphone. Contrary to the original algorithm whose application is limited to English only, this version takes into account spelling peculiarities of a number of other languages. In 2009 Lawrence Philips released a third version, called Metaphone 3, which achieves an accuracy of approximately 99% for English words, non-English words familiar to Americans, and first names and family names commonly found in the United States, having been developed according to modern engineering standards against a test harness of prepared correct encodings."
Methods of computing square roots,"In numerical analysis, a branch of mathematics, there are several square root algorithms or methods of computing the principal square root of a nonnegative real number. For the square roots of a negative or complex number, see below.
Finding  is the same as solving the equation . Therefore, any general numerical root-finding algorithm can be used. Newton's method, for example, reduces in this case to the so-called Babylonian method:

Generally, these methods yield approximate results. To get a higher precision for the root, a higher precision for the square is required and a larger number of steps must be calculated."
Metropolis light transport,"The Metropolis light transport (MLT) is an application of a variant of the Monte Carlo method called the Metropolis-Hastings algorithm to the rendering equation for generating images from detailed physical descriptions of three-dimensional scenes.
The procedure constructs paths from the eye to a light source using bidirectional path tracing, then constructs slight modifications to the path. Some careful statistical calculation (the Metropolis algorithm) is used to compute the appropriate distribution of brightness over the image. This procedure has the advantage, relative to bidirectional path tracing, that once a path has been found from light to eye, the algorithm can then explore nearby paths; thus difficult-to-find light paths can be explored more thoroughly with the same number of simulated photons. In short, the algorithm generates a path and stores the path's 'nodes' in a list. It can then modify the path by adding extra nodes and creating a new light path. While creating this new path, the algorithm decides how many new 'nodes' to add and whether or not these new nodes will actually create a new path.
Metropolis Light Transport is an unbiased method that, in some cases (but not always), converges to a solution of the rendering equation faster than other unbiased algorithms such as path tracing or bidirectional path tracing."
Metropolis–Hastings algorithm,"In statistics and in statistical physics, the Metropolis–Hastings algorithm is a Markov chain Monte Carlo (MCMC) method for obtaining a sequence of random samples from a probability distribution for which direct sampling is difficult. This sequence can be used to approximate the distribution (i.e., to generate a histogram), or to compute an integral (such as an expected value). Metropolis–Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, especially when the number of dimensions is high. For single-dimensional distributions, other methods are usually available (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, and are free from the problem of auto-correlated samples that is inherent in MCMC methods."
Midpoint circle algorithm,"In computer graphics, the midpoint circle algorithm is an algorithm used to determine the points needed for drawing a circle. Bresenham's circle algorithm is derived from the midpoint circle algorithm. The algorithm can be generalized to conic sections.

The algorithm is related to work by Pitteway and Van Aken.

"
Montgomery reduction,"In modular arithmetic computation, Montgomery modular multiplication, more commonly referred to as Montgomery multiplication, is a method for performing fast modular multiplication, introduced in 1985 by the American mathematician Peter L. Montgomery.  
Given two integers a and b, the classical modular multiplication algorithm computes ab mod N. Montgomery multiplication works by transforming a and b into a special representation known as Montgomery form. For a modulus N, the Montgomery form of a is defined to be aR mod N for some constant R depending only on N and the underlying computer architecture. If aR mod N and bR mod N are the Montgomery forms of a and b, then their Montgomery product is abR mod N. Montgomery multiplication is a fast algorithm to compute the Montgomery product. Transforming the result out of Montgomery form yields the classical modular product ab mod N.
Because of the overhead involved in converting a and b into Montgomery form, computing a single product by Montgomery multiplication is slower than computing the product in the integers and performing a modular reduction by division or Barrett reduction. However, when many products are required, as in modular exponentiation, the conversion to Montgomery form becomes a negligible fraction of the time of the computation, and performing the computation by Montgomery multiplication is faster than the available alternatives. Many important cryptosystems such as RSA and Diffie–Hellman key exchange are based on arithmetic operations modulo a large number, and for these cryptosystems, the increased speed afforded by Montgomery multiplication can be important in practice."
Muller's method,"Muller's method is a root-finding algorithm, a numerical method for solving equations of the form f(x) = 0. It was first presented by David E. Muller in 1956.
Muller's method is based on the secant method, which constructs at every iteration a line through two points on the graph of f. Instead, Muller's method uses three points, constructs the parabola through these three points, and takes the intersection of the x-axis with the parabola to be the next approximation."
Multi level feedback queue,"In computer science, a multilevel feedback queue is a scheduling algorithm. Solaris 2.6 Time-Sharing (TS) scheduler implements this algorithm. The Mac OS X and Microsoft Windows schedulers can both be regarded as examples of the broader class of multilevel feedback queue schedulers. This scheduling algorithm is intended to meet the following design requirements for multimode systems:
Give preference to short jobs.
Give preference to I/O bound processes.
Separate processes into categories based on their need for the processor.
The Multi-level Feedback Queue scheduler was first developed by Fernando J. Corbató et al. in 1962, and this work, along with other work on Multics, led the ACM to award Corbató the Turing Award."
Multiplication algorithm,"A multiplication algorithm is an algorithm (or method) to multiply two numbers. Depending on the size of the numbers, different algorithms are in use. Efficient multiplication algorithms have existed since the advent of the decimal system."
Nagle's algorithm,"Nagle's algorithm, named after John Nagle, is a means of improving the efficiency of TCP/IP networks by reducing the number of packets that need to be sent over the network.
Nagle's document, Congestion Control in IP/TCP Internetworks (RFC 896) describes what he called the ""small packet problem"", where an application repeatedly emits data in small chunks, frequently only 1 byte in size. Since TCP packets have a 40 byte header (20 bytes for TCP, 20 bytes for IPv4), this results in a 41 byte packet for 1 byte of useful information, a huge overhead. This situation often occurs in Telnet sessions, where most keypresses generate a single byte of data that is transmitted immediately. Worse, over slow links, many such packets can be in transit at the same time, potentially leading to congestion collapse.
Nagle's algorithm works by combining a number of small outgoing messages, and sending them all at once. Specifically, as long as there is a sent packet for which the sender has received no acknowledgment, the sender should keep buffering its output until it has a full packet's worth of output, so that output can be sent all at once."
Nearest neighbour algorithm,"The nearest neighbour algorithm was one of the first algorithms used to determine a solution to the travelling salesman problem. In it, the salesman starts at a random city and repeatedly visits the nearest city until all have been visited. It quickly yields a short tour, but usually not the optimal one.
Below is the application of nearest neighbour algorithm on TSP
These are the steps of the algorithm:
start on an arbitrary vertex as current vertex.
find out the shortest edge connecting current vertex and an unvisited vertex V.
set current vertex to V.
mark V as visited.
if all the vertices in domain are visited, then terminate.
Go to step 2.
The sequence of the visited vertices is the output of the algorithm.
The nearest neighbour algorithm is easy to implement and executes quickly, but it can sometimes miss shorter routes which are easily noticed with human insight, due to its ""greedy"" nature. As a general guide, if the last few stages of the tour are comparable in length to the first stages, then the tour is reasonable; if they are much greater, then it is likely that there are much better tours. Another check is to use an algorithm such as the lower bound algorithm to estimate if this tour is good enough.
In the worst case, the algorithm results in a tour that is much longer than the optimal tour. To be precise, for every constant r there is an instance of the traveling salesman problem such that the length of the tour computed by the nearest neighbour algorithm is greater than r times the length of the optimal tour. Moreover, for each number of cities there is an assignment of distances between the cities for which the nearest neighbor heuristic produces the unique worst possible tour.
The nearest neighbour algorithm may not find a feasible tour at all, even when one exists."
Needleman–Wunsch algorithm,"The Needleman–Wunsch algorithm is an algorithm used in bioinformatics to align protein or nucleotide sequences. It was one of the first applications of dynamic programming to compare biological sequences. The algorithm was developed by Saul B. Needleman and Christian D. Wunsch and published in 1970. The algorithm essentially divides a large problem (e.g. the full sequence) into a series of smaller problems and uses the solutions to the smaller problems to reconstruct a solution to the larger problem. It is also sometimes referred to as the optimal matching algorithm and the global alignment technique. The Needleman–Wunsch algorithm is still widely used for optimal global alignment, particularly when the quality of the global alignment is of the utmost importance."
Nelder–Mead method,"See simplex algorithm for Dantzig's algorithm for the problem of linear optimization.
The Nelder–Mead method or downhill simplex method or amoeba method is a commonly applied numerical method used to find the minimum or maximum of an objective function in a many-dimensional space. It is applied to nonlinear optimization problems for which derivatives may not be known. However, the Nelder–Mead technique is a heuristic search method that can converge to non-stationary points on problems that can be solved by alternative methods.
The Nelder–Mead technique was proposed by John Nelder & Roger Mead (1965)."
Nested loop join,"A nested loop join is a naive algorithm that joins two sets by using two nested loops. Join operations are important to database management.

"
Nested sampling algorithm,"The nested sampling algorithm is a computational approach to the problem of comparing models in Bayesian statistics, developed in 2004 by physicist John Skilling."
Network scheduler,"On a node in packet switching communication network, a network scheduler, also called packet scheduler, is an arbiter program that manages the sequence of network packets in the transmit and receive queues of the network interface controller, which is a circular data buffer. There are several network schedulers available for the different operating system kernels, that implement many of the existing network scheduling algorithms.
The network scheduler logic decides, in a way similar to statistical multiplexers, which network packet to forward next from the buffer. The buffer works as a queuing system, storing the network packets temporarily until they are transmitted. The buffer space may be divided into different queues, with each of them holding the packets of one flow according to configured packet classification rules; for example, packets can be divided into flows by their source and destination IP addresses. Network scheduling algorithms and their associated settings determine how the network scheduler manages the buffer.
Also, network schedulers are enabling accomplishment of the active queue management and traffic shaping."
New York State Identification and Intelligence System,"The New York State Identification and Intelligence System Phonetic Code, commonly known as NYSIIS, is a phonetic algorithm devised in 1970 as part of the New York State Identification and Intelligence System (now a part of the New York State Division of Criminal Justice Services). It features an accuracy increase of 2.7% over the traditional Soundex algorithm."
Newell's algorithm,"Newell's Algorithm is a 3D computer graphics procedure for elimination of polygon cycles in the depth sorting required in hidden surface removal. It was proposed in 1972 by brothers Martin Newell and Dick Newell, and Tom Sancha, while all three were working at CADCentre.
In the depth sorting phase of hidden surface removal, if two polygons have no overlapping extents or extreme minimum and maximum values in the x, y, and z directions, then they can be easily sorted. If two polygons, Q and P, do have overlapping extents in the Z direction, then it is possible that cutting is necessary.

In that case Newell's algorithm tests the following:
Test for Z overlap; implied in the selection of the face Q from the sort list
The extreme coordinate values in X of the two faces do not overlap (minimax test in X)
The extreme coordinate values in Y of the two faces do not overlap (minimax test in Y)
All vertices of P lie deeper than the plane of Q
All vertices of Q lie closer to the viewpoint than the plane of P
The rasterisation of P and Q do not overlap
Note that the tests are given in order of increasing computational difficulty.
Note also that the polygons must be planar.
If the tests are all false, then the polygons must be split. Splitting is accomplished by selecting one polygon and cutting it along the line of intersection with the other polygon. The above tests are again performed, and the algorithm continues until all polygons pass the above tests."
Newton's method,"In numerical analysis, Newton's method (also known as the Newton–Raphson method), named after Isaac Newton and Joseph Raphson, is a method for finding successively better approximations to the roots (or zeroes) of a real-valued function.

The Newton–Raphson method in one variable is implemented as follows:
Given a function ƒ defined over the reals x, and its derivative ƒ', we begin with a first guess x0 for a root of the function f. Provided the function satisfies all the assumptions made in the derivation of the formula, a better approximation x1 is

Geometrically, (x1, 0) is the intersection with the x-axis of the tangent to the graph of f at (x0, f (x0)).
The process is repeated as

until a sufficiently accurate value is reached.
This algorithm is first in the class of Householder's methods, succeeded by Halley's method. The method can also be extended to complex functions and to systems of equations."
Newton–Raphson division,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.
Discussion will refer to the form , where
N = Numerator (dividend)
D = Denominator (divisor)
is the input, and
Q = Quotient
R = Remainder
is the output."
Non-restoring division,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.
Discussion will refer to the form , where
N = Numerator (dividend)
D = Denominator (divisor)
is the input, and
Q = Quotient
R = Remainder
is the output."
Nth root algorithm,"The principal nth root  of a positive real number A, is the positive real solution of the equation

(for integer n there are n distinct complex solutions to this equation if , but only one is positive and real).
There is a very fast-converging nth root algorithm for finding :
Make an initial guess 
Set . In practice we do .
Repeat step 2 until the desired precision is reached, i.e.  .
A special case is the familiar square-root algorithm. By setting n = 2, the iteration rule in step 2 becomes the square root iteration rule:

Several different derivations of this algorithm are possible. One derivation shows it is a special case of Newton's method (also called the Newton-Raphson method) for finding zeros of a function  beginning with an initial guess. Although Newton's method is iterative, meaning it approaches the solution through a series of increasingly accurate guesses, it converges very quickly. The rate of convergence is quadratic, meaning roughly that the number of bits of accuracy doubles on each iteration (so improving a guess from 1 bit to 64 bits of precision requires only 6 iterations). For this reason, this algorithm is often used in computers as a very fast method to calculate square roots.
For large n, the nth root algorithm is somewhat less efficient since it requires the computation of  at each step, but can be efficiently implemented with a good exponentiation algorithm."
OPTICS algorithm,"Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander. Its basic idea is similar to DBSCAN, but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. In order to do so, the points of the database are (linearly) ordered such that points which are spatially closest become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that needs to be accepted for a cluster in order to have both points belong to the same cluster. This is represented as a dendrogram."
Odd-even sort,"In computing, an odd–even sort or odd–even transposition sort (also known as brick sort) is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections. It is a comparison sort related to bubble sort, with which it shares many characteristics. It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, if a pair is in the wrong order (the first is larger than the second) the elements are switched. The next step repeats this for even/odd indexed pairs (of adjacent elements). Then it alternates between odd/even and even/odd steps until the list is sorted."
Odds algorithm,"The odds-algorithm is a mathematical method for computing optimal strategies for a class of problems that belong to the domain of optimal stopping problems. Their solution follows from the odds-strategy, and the importance of the odds-strategy lies in its optimality, as explained below.
The odds-algorithm applies to a class of problems called last-success-problems. Formally, the objective in these problems is to maximize the probability of identifying in a sequence of sequentially observed independent events the last event satisfying a specific criterion (a ""specific event""). This identification must be done at the time of observation. No revisiting of preceding observations is permitted. Usually, a specific event is defined by the decision maker as an event that is of true interest in the view of ""stopping"" to take a well-defined action. Such problems are encountered in several situations."
Odlyzko–Schönhage algorithm,"In mathematics, the Odlyzko–Schönhage algorithm is a fast algorithm for evaluating the Riemann zeta function at many points, introduced by (Odlyzko & Schönhage 1988). The main point is the use of the fast Fourier transform to speed up the evaluation of a finite Dirichlet series of length N at O(N) equally spaced values from O(N2) to O(N1+ε) steps (at the cost of storing O(N1+ε) intermediate values). The Riemann–Siegel formula used for calculating the Riemann zeta function with imaginary part T uses a finite Dirichlet series with about N = T1/2 terms, so when finding about N values of the Riemann zeta function it is sped up by a factor of about T1/2. This reduces the time to find the zeros of the zeta function with imaginary part at most T from about T3/2+ε steps to about T1+ε steps.
The algorithm can be used not just for the Riemann zeta function, but also for many other functions given by Dirichlet series.
The algorithm was used by Gourdon (2004) to verify the Riemann hypothesis for the first 1013 zeros of the zeta function."
Online algorithm,"In computer science, an online algorithm is one that can process its input piece-by-piece in a serial fashion, i.e., in the order that the input is fed to the algorithm, without having the entire input available from the start.
In contrast, an offline algorithm is given the whole problem data from the beginning and is required to output an answer which solves the problem at hand.
As an example, consider the sorting algorithms selection sort and insertion sort: Selection sort repeatedly selects the minimum element from the unsorted remainder and places it at the front, which requires access to the entire input; it is thus an offline algorithm. On the other hand, insertion sort considers one input element per iteration and produces a partial solution without considering future elements. Thus insertion sort is an online algorithm.
Note that insertion sort produces the optimum result, i.e., a correctly sorted list. For many problems, online algorithms cannot match the performance of offline algorithms. If the ratio between the performance of an online algorithm and an optimal offline algorithm is bounded, the online algorithm is called competitive.
Not every online algorithm has an offline counterpart."
Operator-precedence parser,"In computer science, an operator precedence parser is a bottom-up parser that interprets an operator-precedence grammar. For example, most calculators use operator precedence parsers to convert from the human-readable infix notation relying on order of operations to a format that is optimized for evaluation such as Reverse Polish notation (RPN).
Edsger Dijkstra's shunting yard algorithm is commonly used to implement operator precedence parsers. Other algorithms include the precedence climbing method and the top down operator precedence method."
PPM compression algorithm,Prediction by partial matching (PPM) is an adaptive statistical data compression technique based on context modeling and prediction. PPM models use a set of previous symbols in the uncompressed symbol stream to predict the next symbol in the stream. PPM algorithms can also be used to cluster data into predicted groupings in cluster analysis.
Package-merge algorithm,"The package-merge algorithm is an O(nL)-time algorithm for finding an optimal length-limited Huffman code for a given distribution on a given alphabet of size n, where no code word is longer than L. It is a greedy algorithm, and a generalization of Huffman's original algorithm. Package-merge works by reducing the code construction problem to the binary coin collector's problem."
Packrat parser,"In computer science, a parsing expression grammar, or PEG, is a type of analytic formal grammar, i.e. it describes a formal language in terms of a set of rules for recognizing strings in the language. The formalism was introduced by Bryan Ford in 2004 and is closely related to the family of top-down parsing languages introduced in the early 1970s. Syntactically, PEGs also look similar to context-free grammars (CFGs), but they have a different interpretation: the choice operator selects the first match in PEG, while it is ambiguous in CFG. This is closer to how string recognition tends to be done in practice, e.g. by a recursive descent parser.
Unlike CFGs, PEGs cannot be ambiguous; if a string parses, it has exactly one valid parse tree. It is conjectured that there exist context-free languages that cannot be parsed by a PEG, but this is not yet proven. PEGs are well-suited to parsing computer languages, but not natural languages where their performance is comparable to general CFG algorithms such as the Earley algorithm."
PageRank,"PageRank is an algorithm used by Google Search to rank websites in their search engine results. PageRank was named after Larry Page, one of the founders of Google. PageRank is a way of measuring the importance of website pages. According to Google:

PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. The underlying assumption is that more important websites are likely to receive more links from other websites.

It is not the only algorithm used by Google to order search engine results, but it is the first algorithm that was used by the company, and it is the best-known."
Page replacement algorithms,"In a computer operating system that uses paging for virtual memory management, page replacement algorithms decide which memory pages to page out (swap out, write to disk) when a page of memory needs to be allocated. Paging happens when a page fault occurs and a free page cannot be used to satisfy the allocation, either because there are none, or because the number of free pages is lower than some threshold.
When the page that was selected for replacement and paged out is referenced again it has to be paged in (read in from disk), and this involves waiting for I/O completion. This determines the quality of the page replacement algorithm: the less time waiting for page-ins, the better the algorithm. A page replacement algorithm looks at the limited information about accesses to the pages provided by hardware, and tries to guess which pages should be replaced to minimize the total number of page misses, while balancing this with the costs (primary storage and processor time) of the algorithm itself.
The page replacing problem is a typical online problem from the competitive analysis perspective in the sense that the optimal deterministic algorithm is known."
Painter's algorithm,"The painter's algorithm, also known as a priority fill, is one of the simplest solutions to the visibility problem in 3D computer graphics. When projecting a 3D scene onto a 2D plane, it is necessary at some point to decide which polygons are visible, and which are hidden.
The name ""painter's algorithm"" refers to the technique employed by many painters of painting distant parts of a scene before parts which are nearer thereby covering some areas of distant parts. The painter's algorithm sorts all the polygons in a scene by their depth and then paints them in this order, farthest to closest. It will paint over the parts that are normally not visible — thus solving the visibility problem — at the cost of having painted invisible areas of distant objects. The ordering used by the algorithm is called a 'depth order', and does not have to respect the numerical distances to the parts of the scene: the essential property of this ordering is, rather, that if one object obscures part of another then the first object is painted after the object that it obscures. Thus, a valid ordering can be described as a topological ordering of a directed acyclic graph representing occlusions between objects.

The algorithm can fail in some cases, including cyclic overlap or piercing polygons. In the case of cyclic overlap, as shown in the figure to the right, Polygons A, B, and C overlap each other in such a way that it is impossible to determine which polygon is above the others. In this case, the offending polygons must be cut to allow sorting. Newell's algorithm, proposed in 1972, provides a method for cutting such polygons. Numerous methods have also been proposed in the field of computational geometry.
The case of piercing polygons arises when one polygon intersects another. As with cyclic overlap, this problem may be resolved by cutting the offending polygons.
In basic implementations, the painter's algorithm can be inefficient. It forces the system to render each point on every polygon in the visible set, even if that polygon is occluded in the finished scene. This means that, for detailed scenes, the painter's algorithm can overly tax the computer hardware.
A reverse painter's algorithm is sometimes used, in which objects nearest to the viewer are painted first — with the rule that paint must never be applied to parts of the image that are already painted (unless they are partially transparent). In a computer graphic system, this can be very efficient, since it is not necessary to calculate the colors (using lighting, texturing and such) for parts of the more distant scene that are hidden by nearby objects. However, the reverse algorithm suffers from many of the same problems as the standard version.
These and other flaws with the algorithm led to the development of Z-buffer techniques, which can be viewed as a development of the painter's algorithm, by resolving depth conflicts on a pixel-by-pixel basis, reducing the need for a depth-based rendering order. Even in such systems, a variant of the painter's algorithm is sometimes employed. As Z-buffer implementations generally rely on fixed-precision depth-buffer registers implemented in hardware, there is scope for visibility problems due to rounding error. These are overlaps or gaps at joins between polygons. To avoid this, some graphics engine implementations ""overrender"", drawing the affected edges of both polygons in the order given by painter's algorithm. This means that some pixels are actually drawn twice (as in the full painter's algorithm) but this happens on only small parts of the image and has a negligible performance effect."
Pancake sorting,"Pancake sorting is the colloquial term for the mathematical problem of sorting a disordered stack of pancakes in order of size when a spatula can be inserted at any point in the stack and used to flip all pancakes above it. A pancake number is the maximum number of flips required for a given number of pancakes. In this form, the problem was first discussed by American geometer Jacob E. Goodman. It is a variation of the sorting problem in which the only allowed operation is to reverse the elements of some prefix of the sequence. Unlike a traditional sorting algorithm, which attempts to sort with the fewest comparisons possible, the goal is to sort the sequence in as few reversals as possible. A variant of the problem is concerned with burnt pancakes, where each pancake has a burnt side and all pancakes must, in addition, end up with the burnt side on bottom.

"
Parsing expression grammar,"In computer science, a parsing expression grammar, or PEG, is a type of analytic formal grammar, i.e. it describes a formal language in terms of a set of rules for recognizing strings in the language. The formalism was introduced by Bryan Ford in 2004 and is closely related to the family of top-down parsing languages introduced in the early 1970s. Syntactically, PEGs also look similar to context-free grammars (CFGs), but they have a different interpretation: the choice operator selects the first match in PEG, while it is ambiguous in CFG. This is closer to how string recognition tends to be done in practice, e.g. by a recursive descent parser.
Unlike CFGs, PEGs cannot be ambiguous; if a string parses, it has exactly one valid parse tree. It is conjectured that there exist context-free languages that cannot be parsed by a PEG, but this is not yet proven. PEGs are well-suited to parsing computer languages, but not natural languages where their performance is comparable to general CFG algorithms such as the Earley algorithm."
Particle swarm optimization,"In computer science, particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. PSO optimizes a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particle's position and velocity. Each particle's movement is influenced by its local best known position but, is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions.
PSO is originally attributed to Kennedy, Eberhart and Shi and was first intended for simulating social behaviour, as a stylized representation of the movement of organisms in a bird flock or fish school. The algorithm was simplified and it was observed to be performing optimization. The book by Kennedy and Eberhart describes many philosophical aspects of PSO and swarm intelligence. An extensive survey of PSO applications is made by Poli.
PSO is a metaheuristic as it makes few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. However, metaheuristics such as PSO do not guarantee an optimal solution is ever found. More specifically, PSO does not use the gradient of the problem being optimized, which means PSO does not require that the optimization problem be differentiable as is required by classic optimization methods such as gradient descent and quasi-newton methods. PSO can therefore also be used on optimization problems that are partially irregular, noisy, change over time, etc."
Path-based strong component algorithm,"In graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, one to keep track of the vertices in the current component and the second to keep track of the current search path. Versions of this algorithm have been proposed by Purdom (1970), Munro (1971), Dijkstra (1976), Cheriyan & Mehlhorn (1996), and Gabow (2000); of these, Dijkstra's version was the first to achieve linear time."
Path tracing,"Path tracing is a computer graphics Monte Carlo method of rendering images of three-dimensional scenes such that the global illumination is faithful to reality. Fundamentally, the algorithm is integrating over all the illuminance arriving to a single point on the surface of an object. This illuminance is then reduced by a surface reflectance function (BRDF) to determine how much of it will go towards the viewpoint camera. This integration procedure is repeated for every pixel in the output image. When combined with physically accurate models of surfaces, accurate models of real light sources (light bulbs), and optically-correct cameras, path tracing can produce still images that are indistinguishable from photographs.
Path tracing naturally simulates many effects that have to be specifically added to other methods (conventional ray tracing or scanline rendering), such as soft shadows, depth of field, motion blur, caustics, ambient occlusion, and indirect lighting. Implementation of a renderer including these effects is correspondingly simpler. An extended version of the algorithm is realized by volumetric path tracing, which considers the light scattering of a scene.
Due to its accuracy and unbiased nature, path tracing is used to generate reference images when testing the quality of other rendering algorithms. In order to get high quality images from path tracing, a large number of rays must be traced to avoid visible noisy artifacts."
Paxos algorithm,"Paxos is a family of protocols for solving consensus in a network of unreliable processors. Consensus is the process of agreeing on one result among a group of participants. This problem becomes difficult when the participants or their communication medium may experience failures.
Consensus protocols are the basis for the state machine replication approach to distributed computing, as suggested by Leslie Lamport and surveyed by Fred B. Schneider. State machine replication is a technique for converting an algorithm into a fault-tolerant, distributed implementation. Ad-hoc techniques may leave important cases of failures unresolved. The principled approach proposed by Lamport et al. ensures all cases are handled safely.
The Paxos protocol was first published in 1989 and named after a fictional legislative consensus system used on the Paxos island in Greece. It was later published as a journal article in 1998.
The Paxos family of protocols includes a spectrum of trade-offs between the number of processors, number of message delays before learning the agreed value, the activity level of individual participants, number of messages sent, and types of failures. Although no deterministic fault-tolerant consensus protocol can guarantee progress in an asynchronous network (a result proven in a paper by Fischer, Lynch and Paterson), Paxos guarantees safety (consistency), and the conditions that could prevent it from making progress are difficult to provoke.
Paxos is usually used where durability is required (for example, to replicate a file or a database), in which the amount of durable state could be large. The protocol attempts to make progress even during periods when some bounded number of replicas are unresponsive. There is also a mechanism to drop a permanently failed replica or to add a new replica."
Perceptron,"In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers: functions that can decide whether an input (represented by a vector of numbers) belongs to one class or another. It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time.
The perceptron algorithm dates back to the late 1950s; its first implementation, in custom hardware, was one of the first artificial neural networks to be produced."
Peterson's algorithm,"Peterson's algorithm (AKA Peterson's solution) is a concurrent programming algorithm for mutual exclusion that allows two processes to share a single-use resource without conflict, using only shared memory for communication. It was formulated by Gary L. Peterson in 1981. While Peterson's original formulation worked with only two processes, the algorithm can be generalized for more than two, as shown below."
Phonetic algorithm,"A phonetic algorithm is an algorithm for indexing of words by their pronunciation. Most phonetic algorithms were developed for use with the English language; consequently, applying the rules to words in other languages might not give a meaningful result.
They are necessarily complex algorithms with many rules and exceptions, because English spelling and pronunciation is complicated by historical changes in pronunciation and words borrowed from many languages.
Among the best-known phonetic algorithms are:
Soundex, which was developed to encode surnames for use in censuses. Soundex codes are four-character strings composed of a single letter followed by three numbers.
Daitch–Mokotoff Soundex, which is a refinement of Soundex designed to better match surnames of Slavic and Germanic origin. Daitch–Mokotoff Soundex codes are strings composed of six numeric digits.
Kölner Phonetik: This is similar to Soundex, but more suitable for German words.
Metaphone, Double Metaphone, and Metaphone 3 which are suitable for use with most English words, not just names. Metaphone algorithms are the basis for many popular spell checkers.
New York State Identification and Intelligence System (NYSIIS), which maps similar phonemes to the same letter. The result is a string that can be pronounced by the reader without decoding.
Match Rating Approach developed by Western Airlines in 1977 - this algorithm has an encoding and range comparison technique.
Caverphone, created to assist in data matching between late 19th century and early 20th century electoral rolls, optimized for accents present in parts of New Zealand."
Photon mapping,"In computer graphics, photon mapping is a two-pass global illumination algorithm developed by Henrik Wann Jensen that approximately solves the rendering equation. Rays from the light source and rays from the camera are traced independently until some termination criterion is met, then they are connected in a second step to produce a radiance value. It is used to realistically simulate the interaction of light with different objects. Specifically, it is capable of simulating the refraction of light through a transparent substance such as glass or water, diffuse interreflection between illuminated objects, the subsurface scattering of light in translucent materials, and some of the effects caused by particulate matter such as smoke or water vapor. It can also be extended to more accurate simulations of light such as spectral rendering.
Unlike path tracing, bidirectional path tracing, volumetric path tracing and Metropolis light transport, photon mapping is a ""biased"" rendering algorithm, which means that averaging many renders using this method does not converge to a correct solution to the rendering equation. However, since it is a consistent method, any desired accuracy can be achieved by increasing the number of photons."
Pigeonhole sort,"Pigeonhole sorting, also known as count sort (not to be confused with counting sort), is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) and the number of possible key values (N) are approximately the same. It requires O(n + N) time.
The pigeonhole algorithm works as follows:
Given an array of values to be sorted, set up an auxiliary array of initially empty ""pigeonholes,"" one pigeonhole for each key through the range of the original array.
Going over the original array, put each value into the pigeonhole corresponding to its key, such that each pigeonhole eventually contains a list of all values with that key.
Iterate over the pigeonhole array in order, and put elements from non-empty pigeonholes back into the original array."
Pohlig–Hellman algorithm,"In number theory, the Pohlig–Hellman algorithm sometimes credited as the Silver–Pohlig–Hellman algorithm is a special-purpose algorithm for computing discrete logarithms in a multiplicative group whose order is a smooth integer.
The algorithm was discovered by Roland Silver, but first published by Stephen Pohlig and Martin Hellman (independent of Silver).
We will explain the algorithm as it applies to the group Z*p consisting of all the elements of Zp which are coprime to p, and leave it to the advanced reader to extend the algorithm to other groups by using Lagrange's theorem.
Input Integers p, g, e.
Output An Integer x, such that e ≡ gx (mod p) (if one exists).

Determine the prime factorization of the order of the group  :
(All the pi are considered small since the group order is smooth.)
From the Chinese remainder theorem it will be sufficient to determine the values of x modulo each prime power dividing the group order. Suppose for illustration that p1 divides this order but p12 does not. Then we need to determine x mod p1, that is, we need to know the ending coefficient b1 in the base-p1 expansion of x, i.e. in the expansion x = a1 p1 + b1. We can find the value of b1 by examining all the possible values between 0 and p1-1. (We may also use a faster algorithm such as baby-step giant-step when the order of the group is prime.) The key behind the examination is that:

(using Euler's theorem). With everything else now known, we may try each value of b1 to see which makes the equation be true. If , then there is exactly one b1, and that b1 is the value of x modulo p1. (An exception arises if  since then the order of g is less than φ(p). The conclusion in this case depends on the value of  on the left: if this quantity is not 1, then no solution x exists; if instead this quantity is also equal to 1, there will be more than one solution for x less than φ(p), but since we are attempting to return only one solution x, we may use b1=0.)
The same operation is now performed for p2 through pn.
A minor modification is needed where a prime number is repeated. Suppose we are seeing pi for the (k + 1)st time. Then we already know ci in the equation x = ai pik+1 + bi pik + ci, and we find either bi or ci the same way as before, depending on whether .
With all the bi known, we have enough simultaneous congruences to determine x using the Chinese remainder theorem."
Pollard's kangaroo algorithm,"In computational number theory and computational algebra, Pollard's kangaroo algorithm (aka Pollard's lambda algorithm, see Naming below) is an algorithm for solving the discrete logarithm problem. The algorithm was introduced in 1978 by the number theorist J. M. Pollard, in the same paper  as his better-known ρ algorithm for solving the same problem. Although Pollard described the application of his algorithm to the discrete logarithm problem in the multiplicative group of units modulo a prime p, it is in fact a generic discrete logarithm algorithm—it will work in any finite cyclic group.

"
Pollard's p − 1 algorithm,"Pollard's p − 1 algorithm is a number theoretic integer factorization algorithm, invented by John Pollard in 1974. It is a special-purpose algorithm, meaning that it is only suitable for integers with specific types of factors; it is the simplest example of an algebraic-group factorisation algorithm.
The factors it finds are ones for which the number preceding the factor, p − 1, is powersmooth; the essential observation is that, by working in the multiplicative group modulo a composite number N, we are also working in the multiplicative groups modulo all of N's factors.
The existence of this algorithm leads to the concept of safe primes, being primes for which p − 1 is two times a Sophie Germain prime q and thus minimally smooth. These primes are sometimes construed as ""safe for cryptographic purposes"", but they might be unsafe — in current recommendations for cryptographic strong primes (e.g. ANSI X9.31), it is necessary but not sufficient that p − 1 has at least one large prime factor. Most sufficiently large primes are strong; if a prime used for cryptographic purposes turns out to be non-strong, it is much more likely to be through malice than through an accident of random number generation. This terminology is considered obsolete by the cryptography industry. [1]"
Pollard's rho algorithm,Pollard's rho algorithm is a special-purpose integer factorization algorithm. It was invented by John Pollard in 1975. It is particularly effective for a composite number having a small prime factor.
Pollard's rho algorithm for logarithms,"Pollard's rho algorithm for logarithms is an algorithm introduced by John Pollard in 1978 to solve the discrete logarithm problem, analogous to Pollard's rho algorithm to solve the integer factorization problem.
The goal is to compute  such that , where  belongs to a cyclic group  generated by . The algorithm computes integers , , , and  such that . Assuming, for simplicity, that the underlying group is cyclic of order , we can calculate  as a solution of the equation .
To find the needed , , , and  the algorithm uses Floyd's cycle-finding algorithm to find a cycle in the sequence , where the function  is assumed to be random-looking and thus is likely to enter into a loop after approximately  steps. One way to define such a function is to use the following rules: Divide  into three disjoint subsets of approximately equal size: , , and . If  is in  then double both  and ; if  then increment , if  then increment ."
Polynomial time,"In computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input. The time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms. When expressed this way, the time complexity is said to be described asymptotically, i.e., as the input size goes to infinity. For example, if the time required by an algorithm on all inputs of size n is at most 5n3 + 3n for any n (bigger than some n0), the asymptotic time complexity is O(n3).
Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, where an elementary operation takes a fixed amount of time to perform. Thus the amount of time taken and the number of elementary operations performed by the algorithm differ by at most a constant factor.
Since an algorithm's performance time may vary with different inputs of the same size, one commonly uses the worst-case time complexity of an algorithm, denoted as T(n), which is defined as the maximum amount of time taken on any input of size n. Less common, and usually specified explicitly, is the measure of average-case complexity. Time complexities are classified by the nature of the function T(n). For instance, an algorithm with T(n) = O(n) is called a linear time algorithm, and an algorithm with T(n) = O(Mn) and mn= O(T(n)) for some M ≥ m > 1 is said to be an exponential time algorithm."
Postman sort,"Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. It is a distribution sort, and is a cousin of radix sort in the most to least significant digit flavour. Bucket sort is a generalization of pigeonhole sort. Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. The computational complexity estimates involve the number of buckets.
Bucket sort works as follows:
Set up an array of initially empty ""buckets"".
Scatter: Go over the original array, putting each object in its bucket.
Sort each non-empty bucket.
Gather: Visit the buckets in order and put all elements back into the original array."
Prim's algorithm,"In computer science, Prim's algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. The algorithm operates by building this tree one vertex at a time, from an arbitrary starting vertex, at each step adding the cheapest possible connection from the tree to another vertex.
The algorithm was developed in 1930 by Czech mathematician Vojtěch Jarník and later rediscovered and republished by computer scientists Robert C. Prim in 1957 and Edsger W. Dijkstra in 1959. Therefore, it is also sometimes called the DJP algorithm, Jarník's algorithm, the Prim–Jarník algorithm, or the Prim–Dijkstra algorithm.
Other well-known algorithms for this problem include Kruskal's algorithm and Borůvka's algorithm. These algorithms find the minimum spanning forest in a possibly disconnected graph; in contrast, the most basic form of Prim's algorithm only finds minimum spanning trees in connected graphs. However, running Prim's algorithm separately for each connected component of the graph, it can also be used to find the minimum spanning forest. In terms of their asymptotic time complexity, these three algorithms are equally fast for sparse graphs, but slower than other more sophisticated algorithms. However, for graphs that are sufficiently dense, Prim's algorithm can be made to run in linear time, meeting or improving the time bounds for other algorithms.

"
Primality test,"A primality test is an algorithm for determining whether an input number is prime. Amongst other fields of mathematics, it is used for cryptography. Unlike integer factorization, primality tests do not generally give prime factors, only stating whether the input number is prime or not. Factorization is thought to be a computationally difficult problem, whereas primality testing is comparatively easy (its running time is polynomial in the size of the input). Some primality tests prove that a number is prime, while others like Miller–Rabin prove that a number is composite. Therefore, the latter might be called compositeness tests instead of primality tests."
Prime-factor FFT algorithm,"The prime-factor algorithm (PFA), also called the Good–Thomas algorithm (1958/1963), is a fast Fourier transform (FFT) algorithm that re-expresses the discrete Fourier transform (DFT) of a size N = N1N2 as a two-dimensional N1×N2 DFT, but only for the case where N1 and N2 are relatively prime. These smaller transforms of size N1 and N2 can then be evaluated by applying PFA recursively or by using some other FFT algorithm.
PFA should not be confused with the mixed-radix generalization of the popular Cooley–Tukey algorithm, which also subdivides a DFT of size N = N1N2 into smaller transforms of size N1 and N2. The latter algorithm can use any factors (not necessarily relatively prime), but it has the disadvantage that it also requires extra multiplications by roots of unity called twiddle factors, in addition to the smaller transforms. On the other hand, PFA has the disadvantages that it only works for relatively prime factors (e.g. it is useless for power-of-two sizes) and that it requires a more complicated re-indexing of the data based on the Chinese remainder theorem (CRT). Note, however, that PFA can be combined with mixed-radix Cooley–Tukey, with the former factorizing N into relatively prime components and the latter handling repeated factors.
PFA is also closely related to the nested Winograd FFT algorithm, where the latter performs the decomposed N1 by N2 transform via more sophisticated two-dimensional convolution techniques. Some older papers therefore also call Winograd's algorithm a PFA FFT.
(Although the PFA is distinct from the Cooley–Tukey algorithm, Good's 1958 work on the PFA was cited as inspiration by Cooley and Tukey in their famous 1965 paper, and there was initially some confusion about whether the two algorithms were different. In fact, it was the only prior FFT work cited by them, as they were not then aware of the earlier research by Gauss and others.)"
Prime factorization algorithm,"In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. If these integers are further restricted to prime numbers, the process is called prime factorization.
When the numbers are very large, no efficient, non-quantum integer factorization algorithm is known; an effort by several researchers concluded in 2009, factoring a 232-digit number (RSA-768), utilizing hundreds of machines over a span of two years. However, it has not been proven that no efficient algorithm exists. The presumed difficulty of this problem is at the heart of widely used algorithms in cryptography such as RSA. Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.
Not all numbers of a given length are equally hard to factor. The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, e.g., to avoid efficient factorization by Fermat's factorization method), even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.
Many cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure."
Push–relabel algorithm,"In mathematical optimization, the push–relabel algorithm (alternatively, preflow–push algorithm) is an algorithm for computing maximum flows. The name ""push–relabel"" comes from the two basic operations used in the algorithm. Throughout its execution, the algorithm maintains a ""preflow"" and gradually converts it into a maximum flow by moving flow locally between neighboring vertices using push operations under the guidance of an admissible network maintained by relabel operations. In comparison, the Ford–Fulkerson algorithm performs global augmentations that send flow following paths from the source all the way to the sink.
The push–relabel algorithm is considered one of the most efficient maximum flow algorithms. The generic algorithm has a strongly polynomial O(V2E) time complexity, which is asymptotically more efficient than the O(VE2) Edmonds–Karp algorithm. Specific variants of the algorithms achieve even lower time complexities. The variant based on the highest label vertex selection rule has O(V2√E) time complexity and is generally regarded as the benchmark for maximum flow algorithms. Subcubic O(VE log (V2/E)) time complexity can be achieved using dynamic trees, although in practice it is less efficient.
The push–relabel algorithm has been extended to compute minimum cost flows. The idea of distance labels has led to a more efficient augmenting path algorithm, which in turn can be incorporated back into the push–relabel algorithm to create a variant with even higher empirical performance."
Quadratic sieve,"The quadratic sieve algorithm (QS) is an integer factorization algorithm and, in practice, the second fastest method known (after the general number field sieve). It is still the fastest for integers under 100 decimal digits or so, and is considerably simpler than the number field sieve. It is a general-purpose factorization algorithm, meaning that its running time depends solely on the size of the integer to be factored, and not on special structure or properties. It was invented by Carl Pomerance in 1981 as an improvement to Schroeppel's linear sieve."
Quantum algorithm,"In quantum computing, a quantum algorithm is an algorithm which runs on a realistic model of quantum computation, the most commonly used model being the quantum circuit model of computation. A classical (or non-quantum) algorithm is a finite sequence of instructions, or a step-by-step procedure for solving a problem, where each step or instruction can be performed on a classical computer. Similarly, a quantum algorithm is a step-by-step procedure, where each of the steps can be performed on a quantum computer. Although all classical algorithms can also be performed on a quantum computer, the term quantum algorithm is usually used for those algorithms which seem inherently quantum, or use some essential feature of quantum computation such as quantum superposition or quantum entanglement.
All problems which can be solved on a quantum computer can be solved on a classical computer. In particular, problems which are undecidable using classical computers remain undecidable using quantum computers. What makes quantum algorithms interesting is that they might be able to solve some problems faster than classical algorithms.
The most well known algorithms are Shor's algorithm for factoring, and Grover's algorithm for searching an unstructured database or an unordered list. Shor's algorithms runs exponentially faster than the best known classical algorithm for factoring, the general number field sieve. Grover's algorithm runs quadratically faster than the best possible classical algorithm for the same task."
Quicksort,"Quicksort (sometimes called partition-exchange sort) is an efficient sorting algorithm, serving as a systematic method for placing the elements of an array in order. Developed by Tony Hoare in 1959, with his work published in 1961, it is still a commonly used algorithm for sorting. When implemented well, it can be about two or three times faster than its main competitors, merge sort and heapsort.
Quicksort is a comparison sort, meaning that it can sort items of any type for which a ""less-than"" relation (formally, a total order) is defined. In efficient implementations it is not a stable sort, meaning that the relative order of equal sort items is not preserved. Quicksort can operate in-place on an array, requiring small additional amounts of memory to perform the sorting.
Mathematical analysis of quicksort shows that, on average, the algorithm takes O(n log n) comparisons to sort n items. In the worst case, it makes O(n2) comparisons, though this behavior is rare.

"
RANSAC,"Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data which contains outliers. It is a non-deterministic algorithm in the sense that it produces a reasonable result only with a certain probability, with this probability increasing as more iterations are allowed. The algorithm was first published by Fischler and Bolles at SRI International in 1981.They used RANSAC to solve the Location Determination Problem (LDP), where the goal is to determine the points in the space that project onto an image into a set of landmarks with known locations.
A basic assumption is that the data consists of ""inliers"", i.e., data whose distribution can be explained by some set of model parameters, though may be subject to noise, and ""outliers"" which are data that do not fit the model. The outliers can come, e.g., from extreme values of the noise or from erroneous measurements or incorrect hypotheses about the interpretation of data. RANSAC also assumes that, given a (usually small) set of inliers, there exists a procedure which can estimate the parameters of a model that optimally explains or fits this data."
Rabin–Karp string search algorithm,"In computer science, the Rabin–Karp algorithm or Karp–Rabin algorithm is a string searching algorithm created by Richard M. Karp and Michael O. Rabin (1987) that uses hashing to find any one of a set of pattern strings in a text. For text of length n and p patterns of combined length m, its average and best case running time is O(n+m) in space O(p), but its worst-case time is O(nm). In contrast, the Aho–Corasick string matching algorithm has asymptotic worst-time complexity O(n+m) in space O(m).
A practical application of the algorithm is detecting plagiarism. Given source material, the algorithm can rapidly search through a paper for instances of sentences from the source material, ignoring details such as case and punctuation. Because of the abundance of the sought strings, single-string searching algorithms are impractical."
Rader's FFT algorithm,"Rader's algorithm (1968) is a fast Fourier transform (FFT) algorithm that computes the discrete Fourier transform (DFT) of prime sizes by re-expressing the DFT as a cyclic convolution (the other algorithm for FFTs of prime sizes, Bluestein's algorithm, also works by rewriting the DFT as a convolution).
Since Rader's algorithm only depends upon the periodicity of the DFT kernel, it is directly applicable to any other transform (of prime order) with a similar property, such as a number-theoretic transform or the discrete Hartley transform.
The algorithm can be modified to gain a factor of two savings for the case of DFTs of real data, using a slightly modified re-indexing/permutation to obtain two half-size cyclic convolutions of real data (Chu & Burrus, 1982); an alternative adaptation for DFTs of real data, using the discrete Hartley transform, was described by Johnson & Frigo (2007).
Winograd extended Rader's algorithm to include prime-power DFT sizes  (Winograd 1976; Winograd 1978), and today Rader's algorithm is sometimes described as a special case of Winograd's FFT algorithm, also called the multiplicative Fourier transform algorithm (Tolimieri et al., 1997), which applies to an even larger class of sizes. However, for composite sizes such as prime powers, the Cooley–Tukey FFT algorithm is much simpler and more practical to implement, so Rader's algorithm is typically only used for large-prime base cases of Cooley–Tukey's recursive decomposition of the DFT (Frigo and Johnson, 2005)."
Radiosity (3D computer graphics),"In 3D computer graphics, radiosity is an application of the finite element method to solving the rendering equation for scenes with surfaces that reflect light diffusely. Unlike rendering methods that use Monte Carlo algorithms (such as path tracing), which handle all types of light paths, typical radiosity only account for paths (represented by the code ""LD*E"") which leave a light source and are reflected diffusely some number of times (possibly zero) before hitting the eye. Radiosity is a global illumination algorithm in the sense that the illumination arriving on a surface comes not just directly from the light sources, but also from other surfaces reflecting light. Radiosity is viewpoint independent, which increases the calculations involved, but makes them useful for all viewpoints.
Radiosity methods were first developed in about 1950 in the engineering field of heat transfer. They were later refined specifically for the problem of rendering computer graphics in 1984 by researchers at Cornell University.
Notable commercial radiosity engines are Enlighten by Geomerics (used for games including Battlefield 3 and Need for Speed: The Run); 3ds Max; form•Z; LightWave 3D and the Electric Image Animation System."
Radix sort,"In computer science, radix sort is a non-comparative integer sorting algorithm that sorts data with integer keys by grouping keys by the individual digits which share the same significant position and value. A positional notation is required, but because integers can represent strings of characters (e.g., names or dates) and specially formatted floating point numbers, radix sort is not limited to integers. Radix sort dates back as far as 1887 to the work of Herman Hollerith on tabulating machines.
Most digital computers internally represent all of their data as electronic representations of binary numbers, so processing the digits of integer representations by groups of binary digit representations is most convenient. Two classifications of radix sorts are least significant digit (LSD) radix sorts and most significant digit (MSD) radix sorts. LSD radix sorts process the integer representations starting from the least digit and move towards the most significant digit. MSD radix sorts work the other way around.
LSD radix sorts typically use the following sorting order: short keys come before longer keys, and keys of the same length are sorted lexicographically. This coincides with the normal order of integer representations, such as the sequence 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11.
MSD radix sorts use lexicographic order, which is suitable for sorting strings, such as words, or fixed-length integer representations. A sequence such as ""b, c, d, e, f, g, h, i, j, ba"" would be lexicographically sorted as ""b, ba, c, d, e, f, g, h, i, j"". If lexicographic ordering is used to sort variable-length integer representations, then the representations of the numbers from 1 to 10 would be output as 1, 10, 2, 3, 4, 5, 6, 7, 8, 9, as if the shorter keys were left-justified and padded on the right with blank characters to make the shorter keys as long as the longest key for the purpose of determining sorted order."
Ramer–Douglas–Peucker algorithm,"The Ramer–Douglas–Peucker algorithm (RDP) is an algorithm for reducing the number of points in a curve that is approximated by a series of points. The initial form of the algorithm was independently suggested in 1972 by Urs Ramer and 1973 by David Douglas and Thomas Peucker and several others in the following decade. This algorithm is also known under the names Douglas–Peucker algorithm, iterative end-point fit algorithm and split-and-merge algorithm."
Random-restart hill climbing,"In computer science, hill climbing is a mathematical optimization technique which belongs to the family of local search. It is an iterative algorithm that starts with an arbitrary solution to a problem, then attempts to find a better solution by incrementally changing a single element of the solution. If the change produces a better solution, an incremental change is made to the new solution, repeating until no further improvements can be found.
For example, hill climbing can be applied to the travelling salesman problem. It is easy to find an initial solution that visits all the cities but will be very poor compared to the optimal solution. The algorithm starts with such a solution and makes small improvements to it, such as switching the order in which two cities are visited. Eventually, a much shorter route is likely to be obtained.
Hill climbing is good for finding a local optimum (a solution that cannot be improved by considering a neighbouring configuration) but it is not necessarily guaranteed to find the best possible solution (the global optimum) out of all possible solutions (the search space). In convex problems, hill-climbing is optimal. Examples of algorithms that solve convex problems by hill-climbing include the simplex algorithm for linear programming and binary search.
The characteristic that only local optima are guaranteed can be cured by using restarts (repeated local search), or more complex schemes based on iterations, like iterated local search, on memory, like reactive search optimization and tabu search, or memory-less stochastic modifications, like simulated annealing.
The relative simplicity of the algorithm makes it a popular first choice amongst optimizing algorithms. It is used widely in artificial intelligence, for reaching a goal state from a starting node. Choice of next node and starting node can be varied to give a list of related algorithms. Although more advanced algorithms such as simulated annealing or tabu search may give better results, in some situations hill climbing works just as well. Hill climbing can often produce a better result than other algorithms when the amount of time available to perform a search is limited, such as with real-time systems. It is an anytime algorithm: it can return a valid solution even if it's interrupted at any time before it ends."
Random forest,"Random forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random forests correct for decision trees' habit of overfitting to their training set.
The algorithm for inducing a random forest was developed by Leo Breiman and Adele Cutler, and ""Random Forests"" is their trademark. The method combines Breiman's ""bagging"" idea and the random selection of features, introduced independently by Ho and Amit and Geman in order to construct a collection of decision trees with controlled variance.
The selection of a random subset of features is an example of the random subspace method, which, in Ho's formulation, is a way to implement classification proposed by Eugene Kleinberg."
Rate-monotonic scheduling,"In computer science, rate-monotonic scheduling (RMS) is a scheduling algorithm used in real-time operating systems (RTOS) with a static-priority scheduling class. The static priorities are assigned on the basis of the cycle duration of the job: the shorter the cycle duration is, the higher is the job's priority.
These operating systems are generally preemptive and have deterministic guarantees with regard to response times. Rate monotonic analysis is used in conjunction with those systems to provide scheduling guarantees for a particular application.
^ Liu, C. L.; Layland, J. (1973), ""Scheduling algorithms for multiprogramming in a hard real-time environment"", Journal of the ACM 20 (1): 46–61, doi:10.1145/321738.321743 .
^ Bovet, Daniel P.; Cesati, Marco, Understanding the Linux Kernel , http://oreilly.com/catalog/linuxkernel/chapter/ch10.html#85347."
Raymond's Algorithm,"Raymond's Algorithm is a lock based algorithm for mutual exclusion on a distributed system. It imposes a logical structure (a K-ary tree) on distributed resources. As defined, each node has only a single parent, to which all requests to attain the token are made.

"
Reinforcement Learning,"Reinforcement learning is an area of machine learning inspired by behaviorist psychology, concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. The problem, due to its generality, is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics, and genetic algorithms. In the operations research and control literature, the field where reinforcement learning methods are studied is called approximate dynamic programming. The problem has been studied in the theory of optimal control, though most studies are concerned with the existence of optimal solutions and their characterization, and not with the learning or approximation aspects. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
In machine learning, the environment is typically formulated as a Markov decision process (MDP) as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical techniques and reinforcement learning algorithms is that the latter do not need knowledge about the MDP and they target large MDPs where exact methods become infeasible.
Reinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). The exploration vs. exploitation trade-off in reinforcement learning has been most thoroughly studied through the multi-armed bandit problem and in finite MDPs."
Relevance Vector Machine,"In mathematics, a relevance vector machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification. The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.
It is actually equivalent to a Gaussian process model with covariance function:

where  is the kernel function (usually Gaussian),'s as the variances of the prior on the weight vector  ,and  are the input vectors of the training set.
Compared to that of support vector machines (SVM), the Bayesian formulation of the RVM avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, which are guaranteed to find a global optimum (of the convex problem).
The relevance vector machine is patented in the United States by Microsoft."
Restoring division,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.
Discussion will refer to the form , where
N = Numerator (dividend)
D = Denominator (divisor)
is the input, and
Q = Quotient
R = Remainder
is the output."
Reverse-delete algorithm,"The reverse-delete algorithm is an algorithm in graph theory used to obtain a minimum spanning tree from a given connected, edge-weighted graph. It first appeared in Kruskal (1956), but it should not be confused with Kruskal's algorithm which appears in the same paper. If the graph is disconnected, this algorithm will find a minimum spanning tree for each disconnected part of the graph. The set of these minimum spanning trees is called a minimum spanning forest, which contains every vertex in the graph.
This algorithm is a greedy algorithm, choosing the best choice given any situation. It is the reverse of Kruskal's algorithm, which is another greedy algorithm to find a minimum spanning tree. Kruskal’s algorithm starts with an empty graph and adds edges while the Reverse-Delete algorithm starts with the original graph and deletes edges from it. The algorithm works as follows:
Start with graph G, which contains a list of edges E.
Go through E in decreasing order of edge weights.
For each edge, check if deleting the edge will further disconnect the graph.
Perform any deletion that does not lead to additional disconnection."
Ricart-Agrawala Algorithm,"The Ricart-Agrawala Algorithm is an algorithm for mutual exclusion on a distributed system. This algorithm is an extension and optimization of Lamport's Distributed Mutual Exclusion Algorithm, by removing the need for  messages. It was developed by Glenn Ricart and Ashok Agrawala.

"
Ridder's method,"In numerical analysis, Ridders' method is a root-finding algorithm based on the false position method and the use of an exponential function to successively approximate a root of a function f. The method is due to C. Ridders.
Ridders' method is simpler than Muller's method or Brent's method but with similar performance. The formula below converges quadratically when the function is well-behaved, which implies that the number of additional significant digits found at each step approximately doubles; but the function has to be evaluated twice for each step, so the overall order of convergence of the method is √2. If the function is not well-behaved, the root remains bracketed and the length of the bracketing interval at least halves on each iteration, so convergence is guaranteed. The algorithm also makes use of square roots, which are slower than basic floating point operations."
Root-finding algorithm,"A root-finding algorithm is a numerical method, or algorithm, for finding a value x such that f(x) = 0, for a given function f. Such an x is called a root of the function f.
This article is concerned with finding scalar, real or complex roots, approximated as floating point numbers. Finding integer roots or exact algebraic roots are separate problems, whose algorithms have little in common with those discussed here. (See: Diophantine equation for integer roots)
Finding a root of f(x) − g(x) = 0 is the same as solving the equation f(x) = g(x). Here, x is called the unknown in the equation. Conversely, any equation can take the canonical form f(x) = 0, so equation solving is the same thing as computing (or finding) a root of a function.
Numerical root-finding methods use iteration, producing a sequence of numbers that hopefully converge towards a limit, which is a root. The first values of this series are initial guesses. Many methods computes subsequent values by evaluating an auxiliary function on the preceding values. The limit is thus a fixed point of the auxiliary function, which is chosen for having the roots of the original equation as fixed points.
The behaviour of root-finding algorithms is studied in numerical analysis. Algorithms perform best when they take advantage of known characteristics of the given function. Thus an algorithm to find isolated real roots of a low-degree polynomial in one variable may bear little resemblance to an algorithm for complex roots of a ""black-box"" function which is not even known to be differentiable. Questions include ability to separate close roots, robustness against failures of continuity and differentiability, reliability despite inevitable numerical errors, and rate of convergence."
Round-robin scheduling,"Round-robin (RR) is one of the algorithms employed by process and network schedulers in computing. As the term is generally used, time slices are assigned to each process in equal portions and in circular order, handling all processes without priority (also known as cyclic executive). Round-robin scheduling is simple, easy to implement, and starvation-free. Round-robin scheduling can also be applied to other scheduling problems, such as data packet scheduling in computer networks. It is an Operating System concept.
The name of the algorithm comes from the round-robin principle known from other fields, where each person takes an equal share of something in turn."
SEQUITUR algorithm,Sequitur (or Nevill-Manning algorithm) is a recursive algorithm developed by Craig Nevill-Manning and Ian H. Witten in 1997 that infers a hierarchical structure (context-free grammar) from a sequence of discrete symbols. The algorithm operates in linear space and time. It can be used in data compression software applications.
SHA-1,"In cryptography, SHA-1 (Secure Hash Algorithm 1) is a cryptographic hash function designed by the United States National Security Agency and is a U.S. Federal Information Processing Standard published by the United States NIST. SHA-1 is considered insecure against well-funded opponents, and it is recommended to use SHA-2 or SHA-3 instead.
SHA-1 produces a 160-bit (20-byte) hash value known as a message digest. A SHA-1 hash value is typically rendered as a hexadecimal number, 40 digits long.
SHA-1 is a member of the Secure Hash Algorithm family. The four SHA algorithms are structured differently and are named SHA-0, SHA-1, SHA-2, and SHA-3. SHA-0 is the original version of the 160-bit hash function published in 1993 under the name SHA: it was not adopted by many applications. Published in 1995, SHA-1 is very similar to SHA-0, but alters the original SHA hash specification to correct weaknesses that were unknown to the public at that time. SHA-2, published in 2001, is significantly different from the SHA-1 hash function.
In 2005, cryptanalysts found attacks on SHA-1 suggesting that the algorithm might not be secure enough for ongoing use. NIST required many applications in federal agencies to move to SHA-2 after 2010 because of the weakness. Although no successful attacks have yet been reported on SHA-2, it is algorithmically similar to SHA-1. In 2012, following a long-running competition, NIST selected an additional algorithm, Keccak, for standardization under SHA-3.
Microsoft, Google and Mozilla have all announced that their respective browsers will stop accepting SHA-1 SSL certificates by 2017. Windows XP SP2 and earlier, and Android 2.2 and earlier, do not support SHA2 certificates."
SHA-2,"SHA-2 (Secure Hash Algorithm 2) is a set of cryptographic hash functions designed by the NSA. SHA stands for Secure Hash Algorithm. Cryptographic hash functions are mathematical operations run on digital data; by comparing the computed ""hash"" (the output from execution of the algorithm) to a known and expected hash value, a person can determine the data's integrity. For example, computing the hash of a downloaded file and comparing the result to a previously published hash result can show whether the download has been modified or tampered with. A key aspect of cryptographic hash functions is their collision resistance: nobody should be able to find two different input values that result in the same hash output.
SHA-2 includes significant changes from its predecessor, SHA-1. The SHA-2 family consists of six hash functions with digests (hash values) that are 224, 256, 384 or 512 bits: SHA-224, SHA-256, SHA-384, SHA-512, SHA-512/224, SHA-512/256.
SHA-256 and SHA-512 are novel hash functions computed with 32-bit and 64-bit words, respectively. They use different shift amounts and additive constants, but their structures are otherwise virtually identical, differing only in the number of rounds. SHA-224 and SHA-384 are simply truncated versions of the first two, computed with different initial values. SHA-512/224 and SHA-512/256 are also truncated versions of SHA-512, but the initial values are generated using the method described in FIPS PUB 180-4. SHA-2 was published in 2001 by the NIST as a U.S. federal standard (FIPS). The SHA-2 family of algorithms are patented in US 6829355 . The United States has released the patent under a royalty-free license.
In 2005, an algorithm emerged for finding SHA-1 collisions in about 2000-times fewer steps than was previously thought possible. Although (as of 2015) no example of a SHA-1 collision has been published yet, the security margin left by SHA-1 is weaker than intended, and its use is therefore no longer recommended for applications that depend on collision resistance, such as digital signatures. Although SHA-2 bears some similarity to the SHA-1 algorithm, these attacks have not been successfully extended to SHA-2.
Currently, the best public attacks break preimage resistance 52 rounds of SHA-256 or 57 rounds of SHA-512, and collision resistance for 46 rounds of SHA-256, as shown in the Cryptanalysis and validation section below."
SRT division,"A division algorithm is an algorithm which, given two integers N and D, computes their quotient and/or remainder, the result of division. Some are applied by hand, while others are employed by digital circuit designs and software.
Division algorithms fall into two main categories: slow division and fast division. Slow division algorithms produce one digit of the final quotient per iteration. Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. Newton–Raphson and Goldschmidt fall into this category.
Discussion will refer to the form , where
N = Numerator (dividend)
D = Denominator (divisor)
is the input, and
Q = Quotient
R = Remainder
is the output."
SSS*,"SSS* is a search algorithm, introduced by George Stockman in 1979, that conducts a state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithm.
SSS* is based on the notion of solution trees. Informally, a solution tree can be formed from any arbitrary game tree by pruning the number of branches at each MAX node to one. Such a tree represents a complete strategy for MAX, since it specifies exactly one MAX action for every possible sequence of moves might be made by the opponent. Given a game tree, SSS* searches through the space of partial solution trees, gradually analyzing larger and larger subtrees, eventually producing a single solution tree with the same root and Minimax value as the original game tree. SSS* never examines a node that alpha-beta pruning would prune, and may prune some branches that alpha-beta would not. Stockman speculated that SSS* may therefore be a better general algorithm than alpha-beta. However, Igor Roizen and Judea Pearl have shown that the savings in the number of positions that SSS* evaluates relative to alpha/beta is limited and generally not enough to compensate for the increase in other resources (e.g., the storing and sorting of a list of nodes made necessary by the best-first nature of the algorithm). However, Aske Plaat, Jonathan Schaeffer, Wim Pijls and Arie de Bruin have shown that a sequence of null-window alpha-beta calls is equivalent to SSS* (i.e., it expands the same nodes in the same order) when alpha-beta is used with a transposition table, as is the case in all game-playing programs for chess, checkers, etc. Now the storing and sorting of the OPEN list were no longer necessary. This allowed the implementation of (an algorithm equivalent to) SSS* in tournament quality game-playing programs. Experiments showed that it did indeed perform better than Alpha-Beta in practice, but that it did not beat NegaScout.
The reformulation of a best-first algorithm as a sequence of depth-first calls prompted the formulation of a class of null-window alpha-beta algorithms, of which MTD-f is the best known example."
SUBCLU,"SUBCLU is an algorithm for clustering high-dimensional data by Karin Kailing, Hans-Peter Kriegel and Peer Kröger. It is a subspace clustering algorithm that builds on the density-based clustering algorithm DBSCAN. SUBCLU can find clusters in axis-parallel subspaces, and uses a bottom-up, greedy strategy to remain efficient.

"
Samplesort,"Samplesort is a sorting algorithm that is a divide and conquer algorithm often used in parallel processing systems. Conventional divide and conquer sorting algorithms partitions the array into sub-intervals or buckets. The buckets are then sorted individually and then concatenated together. However, if the array is non-uniformly distributed, the performance of these sorting algorithms can be significantly throttled. Samplesort addresses this issue by selecting a sample of size s from the n-element sequence, and determining the range of the buckets by sorting the sample and choosing m -1 elements from the result. These elements (called splitters) then divide the sample into m equal-sized buckets. Samplesort is described in the 1970 paper, ""Samplesort: A Sampling Approach to Minimal Storage Tree Sorting"", by W D Frazer and A C McKellar. In recent years, the algorithm has been adapted to implement randomized sorting on parallel computers."
Scanline rendering,"Scanline rendering is an algorithm for visible surface determination, in 3D computer graphics, that works on a row-by-row basis rather than a polygon-by-polygon or pixel-by-pixel basis. All of the polygons to be rendered are first sorted by the top y coordinate at which they first appear, then each row or scanline of the image is computed using the intersection of a scanline with the polygons on the front of the sorted list, while the sorted list is updated to discard no-longer-visible polygons as the active scan line is advanced down the picture.
The main advantage of this method is that sorting vertices along the normal of the scanning plane reduces the number of comparisons between edges. Another advantage is that it is not necessary to translate the coordinates of all vertices from the main memory into the working memory—only vertices defining edges that intersect the current scan line need to be in active memory, and each vertex is read in only once. The main memory is often very slow compared to the link between the central processing unit and cache memory, and thus avoiding re-accessing vertices in main memory can provide a substantial speedup.
This kind of algorithm can be easily integrated with many other graphics techniques, such as the Phong reflection model or the Z-buffer algorithm."
Schensted algorithm,"In mathematics, the Robinson–Schensted correspondence is a bijective correspondence between permutations and pairs of standard Young tableaux of the same shape. It has various descriptions, all of which are of algorithmic nature, it has many remarkable properties, and it has applications in combinatorics and other areas such as representation theory. The correspondence has been generalized in numerous ways, notably by Knuth to what is known as the Robinson–Schensted–Knuth correspondence, and a further generalization to pictures by Zelevinsky.
The simplest description of the correspondence is using the Schensted algorithm (Schensted 1961), a procedure that constructs one tableau by successively inserting the values of the permutation according to a specific rule, while the other tableau records the evolution of the shape during construction. The correspondence had been described, in a rather different form, much earlier by Robinson (Robinson 1938), in an attempt to prove the Littlewood–Richardson rule. The correspondence is often referred to as the Robinson–Schensted algorithm, although the procedure used by Robinson is radically different from the Schensted–algorithm, and almost entirely forgotten. Other methods of defining the correspondence include a nondeterministic algorithm in terms of jeu de taquin.
The bijective nature of the correspondence relates it to the enumerative identity:

where  denotes the set of partitions of n (or of Young diagrams with n squares), and tλ denotes the number of standard Young tableaux of shape λ."
Schönhage–Strassen algorithm,"The Schönhage–Strassen algorithm is an asymptotically fast multiplication algorithm for large integers. It was developed by Arnold Schönhage and Volker Strassen in 1971. The run-time bit complexity is, in Big O notation, O(n log n log log n) for two n-digit numbers. The algorithm uses recursive Fast Fourier transforms in rings with 22n + 1 elements, a specific type of number theoretic transform.
The Schönhage–Strassen algorithm was the asymptotically fastest multiplication method known from 1971 until 2007, when a new method, Fürer's algorithm, was announced with lower asymptotic complexity; however, Fürer's algorithm currently only achieves an advantage for astronomically large values and is not used in practice.
In practice the Schönhage–Strassen algorithm starts to outperform older methods such as Karatsuba and Toom–Cook multiplication for numbers beyond 2215 to 2217 (10,000 to 40,000 decimal digits). The GNU Multi-Precision Library uses it for values of at least 1728 to 7808 64-bit words (33,000 to 150,000 decimal digits), depending on architecture. There is a Java implementation of Schönhage–Strassen which uses it above 74,000 decimal digits.
Applications of the Schönhage–Strassen algorithm include mathematical empiricism, such as the Great Internet Mersenne Prime Search and computing approximations of π, as well as practical applications such as Kronecker substitution, in which multiplication of polynomials with integer coefficients can be efficiently reduced to large integer multiplication; this is used in practice by GMP-ECM for Lenstra elliptic curve factorization."
Secant method,"In numerical analysis, the secant method is a root-finding algorithm that uses a succession of roots of secant lines to better approximate a root of a function f. The secant method can be thought of as a finite difference approximation of Newton's method. However, the method was developed independently of Newton's method, and predated the latter by over 3,000 years."
Selection algorithm,"In computer science, a selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. This includes the cases of finding the minimum, maximum, and median elements. There are O(n) (worst-case linear time) selection algorithms, and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. Selection is a subproblem of more complex problems like the nearest neighbor and shortest path problems. Many selection algorithms are derived by generalizing a sorting algorithm, and conversely some sorting algorithms can be derived as repeated application of selection.
The simplest case of a selection algorithm is finding the minimum (or maximum) element by iterating through the list, keeping track of the running minimum – the minimum so far – (or maximum) and can be seen as related to the selection sort. Conversely, the hardest case of a selection algorithm is finding the median, and this necessarily takes n/2 storage. In fact, a specialized median-selection algorithm can be used to build a general selection algorithm, as in median of medians. The best-known selection algorithm is quickselect, which is related to quicksort; like quicksort, it has (asymptotically) optimal average performance, but poor worst-case performance, though it can be modified to give optimal worst-case performance as well."
Selection sort,"In computer science, selection sort is a sorting algorithm, specifically an in-place comparison sort. It has O(n2) time complexity, making it inefficient on large lists, and generally performs worse than the similar insertion sort. Selection sort is noted for its simplicity, and it has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.
The algorithm divides the input list into two parts: the sublist of items already sorted, which is built up from left to right at the front (left) of the list, and the sublist of items remaining to be sorted that occupy the rest of the list. Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist, exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right."
Sethi-Ullman algorithm,"In computer science, the Sethi–Ullman algorithm is an algorithm named after Ravi Sethi and Jeffrey D. Ullman, its inventors, for translating abstract syntax trees into machine code that uses as few registers as possible."
Shamir's Secret Sharing,"Shamir's Secret Sharing is an algorithm in cryptography created by Adi Shamir. It is a form of secret sharing, where a secret is divided into parts, giving each participant its own unique part, where some of the parts or all of them are needed in order to reconstruct the secret.
Counting on all participants to combine the secret might be impractical, and therefore sometimes the threshold scheme is used where any  of the parts are sufficient to reconstruct the original secret."
Shifting nth-root algorithm,"The shifting nth root algorithm is an algorithm for extracting the nth root of a positive real number which proceeds iteratively by shifting in n digits of the radicand, starting with the most significant, and produces one digit of the root on each iteration, in a manner similar to long division."
Shoelace algorithm,"The shoelace formula or shoelace algorithm (also known as Gauss's area formula and the surveyor's formula) is a mathematical algorithm to determine the area of a simple polygon whose vertices are described by ordered pairs in the plane. The user cross-multiplies corresponding coordinates to find the area encompassing the polygon, and subtracts it from the surrounding polygon to find the area of the polygon within. It is called the shoelace formula because of the constant cross-multiplying for the coordinates making up the polygon, like tying shoelaces. It is also sometimes called the shoelace method. It has applications in surveying and forestry, among other areas.
The formula was described by Meister (1724-1788) in 1769 and by Gauss in 1795. It can be verified by dividing the polygon into triangles, but it can also be seen as a special case of Green's theorem.
The area formula is derived by taking each edge AB, and calculating the (signed) area of triangle ABO with a vertex at the origin O, by taking the cross-product (which gives the area of a parallelogram) and dividing by 2. As one wraps around the polygon, these triangles with positive and negative area will overlap, and the areas between the origin and the polygon will be cancelled out and sum to 0, while only the area inside the reference triangle remains. This is why the formula is called the Surveyor's Formula, since the ""surveyor"" is at the origin; if going counterclockwise, positive area is added when going from left to right and negative area is added when going from right to left, from the perspective of the origin.
The area formula is valid for any non-self-intersecting (simple) polygon, which can be convex or concave."
Shor's algorithm,"Shor's algorithm, named after mathematician Peter Shor, is a quantum algorithm (an algorithm that runs on a quantum computer) for integer factorization formulated in 1994. Informally it solves the following problem: given an integer N, find its prime factors.
On a quantum computer, to factor an integer N, Shor's algorithm runs in polynomial time (the time taken is polynomial in log N, which is the size of the input). Specifically it takes quantum gates of order O((log N)2(log log N)(log log log N)) using fast multiplication, demonstrating that the integer factorization problem can be efficiently solved on a quantum computer and is thus in the complexity class BQP. This is substantially faster than the most efficient known classical factoring algorithm, the general number field sieve, which works in sub-exponential time — about O(e1.9 (log N)1/3 (log log N)2/3). The efficiency of Shor's algorithm is due to the efficiency of the quantum Fourier transform, and modular exponentiation by repeated squarings.
If a quantum computer with a sufficient number of qubits could operate without succumbing to noise and other quantum decoherence phenomena, Shor's algorithm could be used to break public-key cryptography schemes such as the widely used RSA scheme. RSA is based on the assumption that factoring large numbers is computationally intractable. So far as is known, this assumption is valid for classical (non-quantum) computers; no classical algorithm is known that can factor in polynomial time. However, Shor's algorithm shows that factoring is efficient on an ideal quantum computer, so it may be feasible to defeat RSA by constructing a large quantum computer. It was also a powerful motivator for the design and construction of quantum computers and for the study of new quantum computer algorithms. It has also facilitated research on new cryptosystems that are secure from quantum computers, collectively called post-quantum cryptography.
In 2001, Shor's algorithm was demonstrated by a group at IBM, who factored 15 into 3 × 5, using an NMR implementation of a quantum computer with 7 qubits. After IBM's implementation, two independent groups, one at the University of Science and Technology of China, and the other one at the University of Queensland, have implemented Shor's algorithm using photonic qubits, emphasizing that multi-qubit entanglement was observed when running the Shor's algorithm circuits. In 2012, the factorization of 15 was repeated. Also in 2012, the factorization of 21 was achieved, setting the record for the largest number factored with a quantum computer. In April 2012, the factorization of 143 was achieved, although this used adiabatic quantum computation rather than Shor's algorithm. It was discovered in November 2014 that this adiabatic quantum computation in 2012 had in fact also factored larger numbers, the largest being 56153, which is currently the record for the largest integer factored on a quantum device."
Shortest job next,"Shortest job next (SJN), also known as Shortest Job First (SJF) or Shortest Process Next (SPN), is a scheduling policy that selects the waiting process with the smallest execution time to execute next. SJN is a non-preemptive algorithm. Shortest remaining time is a preemptive variant of SJN.
Shortest job next is advantageous because of its simplicity and because it minimizes the average amount of time each process has to wait until its execution is complete. However, it has the potential for process starvation for processes which will require a long time to complete if short processes are continually added. Highest response ratio next is similar but provides a solution to this problem.
Another disadvantage of using shortest job next is that the total execution time of a job must be known before execution. While it is not possible to perfectly predict execution time, several methods can be used to estimate the execution time for a job, such as a weighted average of previous execution times.
Shortest job next can be effectively used with interactive processes which generally follow a pattern of alternating between waiting for a command and executing it. If the execution burst of a process is regarded as a separate ""job"", past behaviour can indicate which process to run next, based on an estimate of its running time.
Shortest job next is used in specialized environments where accurate estimates of running time are available. Estimating the running time of queued processes is sometimes done using a technique called aging.
^ Arpaci-Dusseau, Remzi H.; Arpaci-Dusseau, Andrea C. (2014), Operating Systems: Three Easy Pieces [Chapter Scheduling Introduction] (PDF), Arpaci-Dusseau Books 
^ Silberschatz, A.; Galvin, P.B.; Gagne, G. (2005). Operating Systems Concepts (7th ed.). Wiley. p. 161. ISBN 0-471-69466-5. 
^ Tanenbaum, A. S. (2008). Modern Operating Systems (3rd ed.). Pearson Education, Inc. p. 156. ISBN 0-13-600663-9."
Shortest remaining time,"Shortest remaining time, also known as shortest remaining time first (SRTF), is a scheduling method that is a preemptive version of shortest job next scheduling. In this scheduling algorithm, the process with the smallest amount of time remaining until completion is selected to execute. Since the currently executing process is the one with the shortest amount of time remaining by definition, and since that time should only reduce as execution progresses, processes will always run until they complete or a new process is added that requires a smaller amount of time.
Shortest remaining time is advantageous because short processes are handled very quickly. The system also requires very little overhead since it only makes a decision when a process completes or a new process is added, and when a new process is added the algorithm only needs to compare the currently executing process with the new process, ignoring all other processes currently waiting to execute.
Like shortest job first, it has the potential for process starvation; long processes may be held off indefinitely if short processes are continually added. This threat can be minimal when process times follow a heavy-tailed distribution.
Like shortest job next scheduling, shortest remaining time scheduling is rarely used outside of specialized environments because it requires accurate estimations of the runtime of all processes that are waiting to execute."
Shortest seek first,"Shortest seek first (or shortest seek time first) is a secondary storage scheduling algorithm to determine the motion of the disk's arm and head in servicing read and write requests.

"
Shunting yard algorithm,"In computer science, the shunting-yard algorithm is a method for parsing mathematical expressions specified in infix notation. It can be used to produce output in Reverse Polish notation (RPN) or as an abstract syntax tree (AST). The algorithm was invented by Edsger Dijkstra and named the ""shunting yard"" algorithm because its operation resembles that of a railroad shunting yard. Dijkstra first described the Shunting Yard Algorithm in the Mathematisch Centrum report MR 34/61.
Like the evaluation of RPN, the shunting yard algorithm is stack-based. Infix expressions are the form of mathematical notation most people are used to, for instance 3+4 or 3+4*(2−1). For the conversion there are two text variables (strings), the input and the output. There is also a stack that holds operators not yet added to the output queue. To convert, the program reads each symbol in order and does something based on that symbol.
The shunting-yard algorithm has been later generalized into operator-precedence parsing."
Sieve of Eratosthenes,"In mathematics, the sieve of Eratosthenes (Ancient Greek: κόσκινον Ἐρατοσθένους, kóskinon Eratosthénous), one of a number of prime number sieves, is a simple, ancient algorithm for finding all prime numbers up to any given limit. It does so by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting with the multiples of 2.
The multiples of a given prime are generated as a sequence of numbers starting from that prime, with constant difference between them that is equal to that prime. This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime.
The sieve of Eratosthenes is one of the most efficient ways to find all of the smaller primes. It is named after Eratosthenes of Cyrene, a Greek mathematician; although none of his works have survived, the sieve was described and attributed to Eratosthenes in the Introduction to Arithmetic by Nicomachus.
The sieve may be used to find primes in arithmetic progressions."
Simon's algorithm,"In computational complexity theory and quantum computing, Simon's problem is a computational problem in the model of decision tree complexity or query complexity, conceived by Daniel Simon in 1994. Simon exhibited a quantum algorithm, usually called Simon's algorithm, that solves the problem exponentially faster than any (deterministic or probabilistic) classical algorithm.
Simon's algorithm uses  queries to the black box, whereas the best classical probabilistic algorithm necessarily needs at least  queries. It is also known that Simon's algorithm is optimal in the sense that any quantum algorithm to solve this problem requires  queries. This problem yields an oracle separation between BPP and BQP, unlike the separation provided by the Deutsch-Jozsa algorithm, which separates P and EQP.
Although the problem itself is of little practical value it is interesting because it provides an exponential speedup over any classical algorithm. Moreover, it was also the inspiration for Shor's algorithm. Both problems are special cases of the abelian hidden subgroup problem, which is now known to have efficient quantum algorithms."
Simple LR parser,"In computer science, a Simple LR or SLR parser is a type of LR parser with small parse tables and a relatively simple parser generator algorithm. As with other types of LR(1) parser, an SLR parser is quite efficient at finding the single correct bottom-up parse in a single left-to-right scan over the input stream, without guesswork or backtracking. The parser is mechanically generated from a formal grammar for the language.
SLR and the more-general methods LALR parser and Canonical LR parser have identical methods and similar tables at parse time; they differ only in the mathematical grammar analysis algorithms used by the parser generator tool. SLR and LALR generators create tables of identical size and identical parser states. SLR generators accept fewer grammars than do LALR generators like yacc and Bison. Many computer languages don't readily fit the restrictions of SLR, as is. Bending the language's natural grammar into SLR grammar form requires more compromises and grammar hackery. So LALR generators have become much more widely used than SLR generators, despite being somewhat more complicated tools. SLR methods remain a useful learning step in college classes on compiler theory.
SLR and LALR were both developed by Frank DeRemer as the first practical uses of Donald Knuth's LR parser theory. The tables created for real grammars by full LR methods were impractically large, larger than most computer memories of that decade, with 100 times or more parser states than the SLR and LALR methods.

"
Simplex algorithm,"In mathematical optimization, Dantzig's simplex algorithm (or simplex method) is a popular algorithm for linear programming. The journal Computing in Science and Engineering listed it as one of the top 10 algorithms of the twentieth century.
The name of the algorithm is derived from the concept of a simplex and was suggested by T. S. Motzkin. Simplices are not actually used in the method, but one interpretation of it is that it operates on simplicial cones, and these become proper simplices with an additional constraint. The simplicial cones in question are the corners (i.e., the neighborhoods of the vertices) of a geometric object called a polytope. The shape of this polytope is defined by the constraints applied to the objective function."
Simulated annealing,"Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic for approximate global optimization in a large search space. It is often used when the search space is discrete (e.g., all tours that visit a given set of cities). For problems where finding the precise global optimum is less important than finding an acceptable global optimum in a fixed amount of time, simulated annealing may be preferable to alternatives such as brute-force search or gradient descent.
The name and inspiration come from annealing in metallurgy, a technique involving heating and controlled cooling of a material to increase the size of its crystals and reduce defects. Both are attributes of the material that depend on its thermodynamic free energy. Heating and cooling the material affects both the temperature and the thermodynamic free energy. While the same amount of cooling brings the same decrease in temperature, the rate of cooling dictates the magnitude of decrease in the thermodynamic free energy, with slower cooling producing a bigger decrease. Simulated annealing interprets slow cooling as a slow decrease in the probability of accepting worse solutions as it explores the solution space. Accepting worse solutions is a fundamental property of metaheuristics because it allows for a more extensive search for the optimal solution.
The method was independently described by Scott Kirkpatrick, C. Daniel Gelatt and Mario P. Vecchi in 1983, and by Vlado Černý in 1985. The method is an adaptation of the Metropolis–Hastings algorithm, a Monte Carlo method to generate sample states of a thermodynamic system, invented by M.N. Rosenbluth and published by N. Metropolis et al. in 1953."
Smith–Waterman algorithm,"The Smith–Waterman algorithm performs local sequence alignment; that is, for determining similar regions between two strings or nucleotide or protein sequences. Instead of looking at the total sequence, the Smith–Waterman algorithm compares segments of all possible lengths and optimizes the similarity measure.
The algorithm was first proposed by Temple F. Smith and Michael S. Waterman in 1981. Like the Needleman–Wunsch algorithm, of which it is a variation, Smith–Waterman is a dynamic programming algorithm. As such, it has the desirable property that it is guaranteed to find the optimal local alignment with respect to the scoring system being used (which includes the substitution matrix and the gap-scoring scheme). The main difference to the Needleman–Wunsch algorithm is that negative scoring matrix cells are set to zero, which renders the (thus positively scoring) local alignments visible. Backtracking starts at the highest scoring matrix cell and proceeds until a cell with score zero is encountered, yielding the highest scoring local alignment. One does not actually implement the algorithm as described because improved alternatives are now available that have better scaling (Gotoh, 1982)  and are more accurate (Altschul and Erickson, 1986)."
Smoothsort,"Smoothsort is a comparison-based sorting algorithm. It is a variation of heapsort developed by Edsger Dijkstra in 1981. Like heapsort, smoothsort is an in-place algorithm with an upper bound of O(n log n), but it is not a stable sort. The advantage of smoothsort is that it comes closer to O(n) time if the input is already sorted to some degree, whereas heapsort averages O(n log n) regardless of the initial sorted state."
Snapshot algorithm,"The snapshot algorithm is an algorithm used in distributed systems for recording a consistent global state of an asynchronous system. The algorithm discussed here is also known as the Chandy–Lamport algorithm, after Leslie Lamport and K. Mani Chandy.

"
Sort-Merge Join,"The sort-merge join (also known as merge join) is a join algorithm and is used in the implementation of a relational database management system.
The basic problem of a join algorithm is to find, for each distinct value of the join attribute, the set of tuples in each relation which display that value. The key idea of the Sort-merge algorithm is to first sort the relations by the join attribute, so that interleaved linear scans will encounter these sets at the same time.
In practice, the most expensive part of performing a sort-merge join is arranging for both inputs to the algorithm to be presented in sorted order. This can be achieved via an explicit sort operation (often an external sort), or by taking advantage of a pre-existing ordering in one or both of the join relations. The latter condition can occur because an input to the join might be produced by an index scan of a tree-based index, another merge join, or some other plan operator that happens to produce output sorted on an appropriate key.
Let's say that we have two relations  and  and .  fits in  pages memory and  fits in  pages memory. So, in the worst case Sort-Merge Join will run in  I/Os. In the case that  and  are not ordered the worst case time cost will contain additional terms of sorting time: , which equals  (as linearithmic terms outweigh the linear terms, see Big O notation – Orders of common functions)."
Sorted list,"A sorting algorithm is an algorithm that puts elements of a list in a certain order. The most-used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the use of other algorithms (such as search and merge algorithms) which require input data to be in sorted lists; it is also often useful for canonicalizing data and for producing human-readable output. More formally, the output must satisfy two conditions:
The output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);
The output is a permutation (reordering) of the input.
Further, the data is often taken to be in an array, which allows random access, rather than a list, which only allows sequential access, though often algorithms can be applied with suitable modification to either type of data.
Since the dawn of computing, the sorting problem has attracted a great deal of research, perhaps due to the complexity of solving it efficiently despite its simple, familiar statement. For example, bubble sort was analyzed as early as 1956. A fundamental limit of comparison sorting algorithms is that they require linearithmic time – O(n log n) – in the worst case, though better performance is possible on real-world data (such as almost-sorted data), and algorithms not based on comparison, such as counting sort, can have better performance. Although many consider sorting a solved problem – asymptotically optimal algorithms have been known since the mid-20th century – useful new algorithms are still being invented, with the now widely used Timsort dating to 2002, and the library sort being first published in 2006.
Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, time-space tradeoffs, and upper and lower bounds."
Sorting algorithm,"A sorting algorithm is an algorithm that puts elements of a list in a certain order. The most-used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the use of other algorithms (such as search and merge algorithms) which require input data to be in sorted lists; it is also often useful for canonicalizing data and for producing human-readable output. More formally, the output must satisfy two conditions:
The output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);
The output is a permutation (reordering) of the input.
Further, the data is often taken to be in an array, which allows random access, rather than a list, which only allows sequential access, though often algorithms can be applied with suitable modification to either type of data.
Since the dawn of computing, the sorting problem has attracted a great deal of research, perhaps due to the complexity of solving it efficiently despite its simple, familiar statement. For example, bubble sort was analyzed as early as 1956. A fundamental limit of comparison sorting algorithms is that they require linearithmic time – O(n log n) – in the worst case, though better performance is possible on real-world data (such as almost-sorted data), and algorithms not based on comparison, such as counting sort, can have better performance. Although many consider sorting a solved problem – asymptotically optimal algorithms have been known since the mid-20th century – useful new algorithms are still being invented, with the now widely used Timsort dating to 2002, and the library sort being first published in 2006.
Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, time-space tradeoffs, and upper and lower bounds."
Sorting algorithms,"A sorting algorithm is an algorithm that puts elements of a list in a certain order. The most-used orders are numerical order and lexicographical order. Efficient sorting is important for optimizing the use of other algorithms (such as search and merge algorithms) which require input data to be in sorted lists; it is also often useful for canonicalizing data and for producing human-readable output. More formally, the output must satisfy two conditions:
The output is in nondecreasing order (each element is no smaller than the previous element according to the desired total order);
The output is a permutation (reordering) of the input.
Further, the data is often taken to be in an array, which allows random access, rather than a list, which only allows sequential access, though often algorithms can be applied with suitable modification to either type of data.
Since the dawn of computing, the sorting problem has attracted a great deal of research, perhaps due to the complexity of solving it efficiently despite its simple, familiar statement. For example, bubble sort was analyzed as early as 1956. A fundamental limit of comparison sorting algorithms is that they require linearithmic time – O(n log n) – in the worst case, though better performance is possible on real-world data (such as almost-sorted data), and algorithms not based on comparison, such as counting sort, can have better performance. Although many consider sorting a solved problem – asymptotically optimal algorithms have been known since the mid-20th century – useful new algorithms are still being invented, with the now widely used Timsort dating to 2002, and the library sort being first published in 2006.
Sorting algorithms are prevalent in introductory computer science classes, where the abundance of algorithms for the problem provides a gentle introduction to a variety of core algorithm concepts, such as big O notation, divide and conquer algorithms, data structures such as heaps and binary trees, randomized algorithms, best, worst and average case analysis, time-space tradeoffs, and upper and lower bounds."
Soundex,"Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English. The goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. Soundex is the most widely known of all phonetic algorithms (in part because it is a standard feature of popular database software such as DB2, PostgreSQL, MySQL, Ingres, MS SQL Server and Oracle) and is often used (incorrectly) as a synonym for ""phonetic algorithm"". Improvements to Soundex are the basis for many modern phonetic algorithms."
Spaghetti sort,"Spaghetti sort is a linear-time, analog algorithm for sorting a sequence of items, by Alexander Dewdney in his column, Scientific American. This algorithm sorts a sequence of items requiring O(n) stack space in a stable manner. It requires a parallel processor."
Special number field sieve,"In number theory, a branch of mathematics, the special number field sieve (SNFS) is a special-purpose integer factorization algorithm. The general number field sieve (GNFS) was derived from it.
The special number field sieve is efficient for integers of the form re ± s, where r and s are small (for instance Mersenne numbers).
Heuristically, its complexity for factoring an integer  is of the form:

in O and L-notations.
The SNFS has been used extensively by NFSNet (a volunteer distributed computing effort), NFS@Home and others to factorise numbers of the Cunningham project; for some time the records for integer factorisation have been numbers factored by SNFS."
Spectral layout,"Spectral layout is a class of algorithm for drawing graphs. The layout uses the eigenvectors of a matrix, such as the Laplace matrix of the graph, as Cartesian coordinates of the graph's vertices.

"
Spigot algorithm,"A spigot algorithm is an algorithm for computing the value of a mathematical constant such as π or e which generates output digits left to right, with limited intermediate storage.
The name comes from a ""spigot"", meaning a tap or valve controlling the flow of a liquid.
Interest in such algorithms was spurred in the early days of computational mathematics by extreme constraints on memory, and an algorithm for calculating the digits of e appears in a paper by Sale in 1968. The name ""Spigot algorithm"" appears to have been coined by Stanley Rabinowitz and Stan Wagon, whose algorithm for calculating the digits of π is sometimes referred to as ""the spigot algorithm for π"".
The spigot algorithm of Rabinowitz and Wagon is bounded, in the sense that the number of required digits must be specified in advance. Jeremy Gibbons (2004) uses the term ""streaming algorithm"" to mean one which can be run indefinitely, without a prior bound. A further refinement is an algorithm which can compute a single arbitrary digit, without first computing the preceding digits: an example is the Bailey-Borwein-Plouffe formula, a digit extraction algorithm for π which produces hexadecimal digits.

"
State-Action-Reward-State-Action,"State-Action-Reward-State-Action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was introduced in a technical note  where the alternative name SARSA was only mentioned as a footnote.
This name simply reflects the fact that the main function for updating the Q-value depends on the current state of the agent ""S1"", the action the agent chooses ""A1"", the reward ""R"" the agent gets for choosing this action, the state ""S2"" that the agent will now be in after taking that action, and finally the next action ""A2"" the agent will choose in its new state. Taking every letter in the quintuple (st, at, rt, st+1, at+1) yields the word SARSA.

"
Statistical classification,"In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. An example would be assigning a given email into ""spam"" or ""non-spam"" classes or assigning a diagnosis to a given patient as described by observed characteristics of the patient (gender, blood pressure, presence or absence of certain symptoms, etc.).
In the terminology of machine learning, classification is considered an instance of supervised learning, i.e. learning where a training set of correctly identified observations is available. The corresponding unsupervised procedure is known as clustering, and involves grouping data into categories based on some measure of inherent similarity or distance.
Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These properties may variously be categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type), ordinal (e.g. ""large"", ""medium"" or ""small""), integer-valued (e.g. the number of occurrences of a part word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.
An algorithm that implements classification, especially in a concrete implementation, is known as a classifier. The term ""classifier"" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.
Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes. There is also some argument over whether classification methods that do not involve a statistical model can be considered ""statistical"". Other fields may use different terminology: e.g. in community ecology, the term ""classification"" normally refers to cluster analysis, i.e. a type of unsupervised learning, rather than the supervised learning described in this article."
Steinhaus–Johnson–Trotter algorithm,"The Steinhaus–Johnson–Trotter algorithm or Johnson–Trotter algorithm, also called plain changes, is an algorithm named after Hugo Steinhaus, Selmer M. Johnson and Hale F. Trotter that generates all of the permutations of n elements. Each permutation in the sequence that it generates differs from the previous permutation by swapping two adjacent elements of the sequence. Equivalently, this algorithm finds a Hamiltonian path in the permutohedron.
This method was known already to 17th-century English change ringers, and Sedgewick (1977) calls it ""perhaps the most prominent permutation enumeration algorithm"". As well as being simple and computationally efficient, it has the advantage that subsequent computations on the permutations that it generates may be sped up because these permutations are so similar to each other."
Stochastic universal sampling,"Stochastic universal sampling (SUS) is a technique used in genetic algorithms for selecting potentially useful solutions for recombination. It was introduced by James Baker.
SUS is a development of fitness proportionate selection (FPS) which exhibits no bias and minimal spread. Where FPS chooses several solutions from the population by repeated random sampling, SUS uses a single random value to sample all of the solutions by choosing them at evenly spaced intervals. This gives weaker members of the population (according to their fitness) a chance to be chosen and thus reduces the unfair nature of fitness-proportional selection methods.
Other methods like roulette wheel can have bad performance when a member of the population has a really large fitness in comparison with other members. Using a comb-like ruler, SUS starts from a small random number, and chooses the next candidates from the rest of population remaining, not allowing the fittest members to saturate the candidate space.
Described as an algorithm, pseudocode for SUS looks like:

SUS(Population, N)
    F := total fitness of Population
    N := number of offspring to keep
    P := distance between the pointers (F/N)
    Start := random number between 0 and P
    Pointers := [Start + i*P | i in [0..(N-1)]]
    return RWS(Population,Pointers)

RWS(Population, Points)
    Keep = []
    i := 0
    for P in Points
        while fitness sum of Population[0..i] < P
            i++
        add Population[i] to Keep
    return Keep

Where Population[0..i] is the set of individuals with array-index 0 to (and including) i.
Here RWS() describes the bulk of fitness proportionate selection (also known as ""roulette wheel selection"") - in true fitness proportional selection the parameter Points is always a (sorted) list of random numbers from 0 to F. The algorithm above is intended to be illustrative rather than canonical."
Stooge sort,"Stooge sort is a recursive sorting algorithm with a time complexity of O(nlog 3 / log 1.5 ) = O(n2.7095...). The running time of the algorithm is thus slower compared to efficient sorting algorithms, such as Merge sort, and is even slower than Bubble sort, a canonical example of a fairly inefficient and simple sort.
The algorithm is defined as follows:
If the value at the end is smaller than the value at the start, swap them.
If there are 3 or more elements in the list, then:
Stooge sort the initial 2/3 of the list
Stooge sort the final 2/3 of the list
Stooge sort the initial 2/3 of the list again

else: exit the procedure
It is important to get the integer sort size used in the recursive calls by rounding the 2/3 upwards, e.g. rounding 2/3 of 5 should give 4 rather than 3, as otherwise the sort can fail on certain data. However, if the code is written to end on a base case of size 1, rather than terminating on either size 1 or size 2, rounding the 2/3 of 2 upwards gives an infinite number of calls.
The algorithm gets its name from slapstick routines of The Three Stooges, in which each stooge hits the other two."
Strand sort,"Strand sort is a sorting algorithm. It works by repeatedly pulling sorted sublists out of the list to be sorted and merging them with a result array. Each iteration through the unsorted list pulls out a series of elements which were already sorted, and merges those series together.
The name of the algorithm comes from the ""strands"" of sorted data within the unsorted list which are removed one at a time. It is a comparison sort due to its use of comparisons when removing strands and when merging them into the sorted array.
The strand sort algorithm is O(n2) in the average case. In the best case (a list which is already sorted) the algorithm is linear, or O(n). In the worst case (a list which is sorted in reverse order) the algorithm is O(n2).
Strand sort is most useful for data which is stored in a linked list, due to the frequent insertions and removals of data. Using another data structure, such as an array, would greatly increase the running time and complexity of the algorithm due to lengthy insertions and deletions. Strand sort is also useful for data which already has large amounts of sorted data, because such data can be removed in a single strand."
Strassen algorithm,"In linear algebra, the Strassen algorithm, named after Volker Strassen, is an algorithm used for matrix multiplication. It is faster than the standard matrix multiplication algorithm and is useful in practice for large matrices, but would be slower than the fastest known algorithms for extremely large matrices.

"
Strongly polynomial,"In computer science, the time complexity of an algorithm quantifies the amount of time taken by an algorithm to run as a function of the length of the string representing the input. The time complexity of an algorithm is commonly expressed using big O notation, which excludes coefficients and lower order terms. When expressed this way, the time complexity is said to be described asymptotically, i.e., as the input size goes to infinity. For example, if the time required by an algorithm on all inputs of size n is at most 5n3 + 3n for any n (bigger than some n0), the asymptotic time complexity is O(n3).
Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, where an elementary operation takes a fixed amount of time to perform. Thus the amount of time taken and the number of elementary operations performed by the algorithm differ by at most a constant factor.
Since an algorithm's performance time may vary with different inputs of the same size, one commonly uses the worst-case time complexity of an algorithm, denoted as T(n), which is defined as the maximum amount of time taken on any input of size n. Less common, and usually specified explicitly, is the measure of average-case complexity. Time complexities are classified by the nature of the function T(n). For instance, an algorithm with T(n) = O(n) is called a linear time algorithm, and an algorithm with T(n) = O(Mn) and mn= O(T(n)) for some M ≥ m > 1 is said to be an exponential time algorithm."
Substring search,"In computer science, string searching algorithms, sometimes called string matching algorithms, are an important class of string algorithms that try to find a place where one or several strings (also called patterns) are found within a larger string or text.
Let Σ be an alphabet (finite set). Formally, both the pattern and searched text are vectors of elements of Σ. The Σ may be a usual human alphabet (for example, the letters A through Z in the Latin alphabet). Other applications may use binary alphabet (Σ = {0,1}) or DNA alphabet (Σ = {A,C,G,T}) in bioinformatics.
In practice, how the string is encoded can affect the feasible string search algorithms. In particular if a variable width encoding is in use then it is slow (time proportional to N) to find the Nth character. This will significantly slow down many of the more advanced search algorithms. A possible solution is to search for the sequence of code units instead, but doing so may produce false matches unless the encoding is specifically designed to avoid it."
Sukhotin's algorithm,"Sukhotin's algorithm (introduced by Boris V. Sukhotin) is a statistical classification algorithm for classifying characters in a text as vowels or consonants. It may also be of use in some of substitution ciphers and has been considered in deciphering the Voynich manuscript, though one problem is to agree on the set of symbols the manuscript is written in."
Support Vector Machines,"In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data and recognize patterns, used for classification and regression analysis. Given a set of training examples, each marked for belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples into one category or the other, making it a non-probabilistic binary linear classifier. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall on.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces."
Swarm intelligence,"Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.
SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of ""intelligent"" global behavior, unknown to the individual agents. Examples in natural systems of SI include ant colonies, bird flocking, animal herding, bacterial growth, fish schooling and microbial intelligence.
The application of swarm principles to robots is called swarm robotics, while 'swarm intelligence' refers to the more general set of algorithms. 'Swarm prediction' has been used in the context of forecasting problems.

"
Sweep line algorithm,"In computational geometry, a sweep line algorithm or plane sweep algorithm is a type of algorithm that uses a conceptual sweep line or sweep surface to solve various problems in Euclidean space. It is one of the key techniques in computational geometry.
The idea behind algorithms of this type is to imagine that a line (often a vertical line) is swept or moved across the plane, stopping at some points. Geometric operations are restricted to geometric objects that either intersect or are in the immediate vicinity of the sweep line whenever it stops, and the complete solution is available once the line has passed over all objects.

"
Symmetric key algorithm,"Symmetric-key algorithms are algorithms for cryptography that use the same cryptographic keys for both encryption of plaintext and decryption of ciphertext. The keys may be identical or there may be a simple transformation to go between the two keys. The keys, in practice, represent a shared secret between two or more parties that can be used to maintain a private information link. This requirement that both parties have access to the secret key is one of the main drawbacks of symmetric key encryption, in comparison to public-key encryption."
Tabu search,"Tabu search, created by Fred W. Glover in 1986 and formalized in 1989, is a metaheuristic search method employing local search methods used for mathematical optimization.
Local (neighborhood) searches take a potential solution to a problem and check its immediate neighbors (that is, solutions that are similar except for one or two minor details) in the hope of finding an improved solution. Local search methods have a tendency to become stuck in suboptimal regions or on plateaus where many solutions are equally fit.
Tabu search enhances the performance of local search by relaxing its basic rule. First, at each step worsening moves can be accepted if no improving move is available (like when the search is stuck at a strict local mimimum). In addition, prohibitions (henceforth the term tabu) are introduced to discourage the search from coming back to previously-visited solutions.
The implementation of tabu search uses memory structures that describe the visited solutions or user-provided sets of rules. If a potential solution has been previously visited within a certain short-term period or if it has violated a rule, it is marked as ""tabu"" (forbidden) so that the algorithm does not consider that possibility repeatedly."
Tarjan's off-line least common ancestors algorithm,"In computer science, Tarjan's off-line lowest common ancestors algorithm is an algorithm for computing lowest common ancestors for pairs of nodes in a tree, based on the union-find data structure. The lowest common ancestor of two nodes d and e in a rooted tree T is the node g that is an ancestor of both d and e and that has the greatest depth in T. It is named after Robert Tarjan, who discovered the technique in 1979. Tarjan's algorithm is an offline algorithm; that is, unlike other lowest common ancestor algorithms, it requires that all pairs of nodes for which the lowest common ancestor is desired must be specified in advance. The simplest version of the algorithm uses the union-find data structure, which unlike other lowest common ancestor data structures can take more than constant time per operation when the number of pairs of nodes is similar in magnitude to the number of nodes. A later refinement by Gabow & Tarjan (1983) speeds the algorithm up to linear time."
Tarjan's strongly connected components algorithm,"Tarjan's Algorithm is an algorithm in graph theory for finding the strongly connected components of a graph. Although proposed earlier, it can be seen as an improved version of Kosaraju's algorithm, and is comparable in efficiency to the path-based strong component algorithm. Tarjan's Algorithm is named for its discoverer, Robert Tarjan."
Temporal difference learning,"Temporal difference (TD) learning is a prediction-based machine learning method. It has primarily been used for the reinforcement learning problem, and is said to be ""a combination of Monte Carlo ideas and dynamic programming (DP) ideas."" TD resembles a Monte Carlo method because it learns by sampling the environment according to some policy, and is related to dynamic programming techniques as it approximates its current estimate based on previously learned estimates (a process known as bootstrapping). The TD learning algorithm is related to the temporal difference model of animal learning.
As a prediction method, TD learning takes into account the fact that subsequent predictions are often correlated in some sense. In standard supervised predictive learning, one learns only from actually observed values: A prediction is made, and when the observation is available, the prediction is adjusted to better match the observation. As elucidated by Richard Sutton, the core idea of TD learning is that we adjust predictions to match other, more accurate, predictions about the future. This procedure is a form of bootstrapping, as illustrated with the following example:
Suppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday - and thus be able to change, say, Monday's model before Saturday arrives.
Mathematically speaking, both in a standard and a TD approach, we would try to optimize some cost function, related to the error in our predictions of the expectation of some random variable, E[z]. However, while in the standard approach we in some sense assume E[z] = z (the actual observed value), in the TD approach we use a model. For the particular case of reinforcement learning, which is the major application of TD methods, z is the total return and E[z] is given by the Bellman equation of the return."
Ternary search,"A ternary search algorithm is a technique in computer science for finding the minimum or maximum of a unimodal function. A ternary search determines either that the minimum or maximum cannot be in the first third of the domain or that it cannot be in the last third of the domain, then repeats on the remaining two-thirds. A ternary search is an example of a divide and conquer algorithm (see search algorithm)."
Timsort,"Timsort is a hybrid stable sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data. It was invented by Tim Peters in 2002 for use in the Python programming language. The algorithm finds subsets of the data that are already ordered, and uses that knowledge to sort the remainder more efficiently. This is done by merging an identified subset, called a run, with existing runs until certain criteria are fulfilled. Timsort has been Python's standard sorting algorithm since version 2.3. It is also used to sort arrays of non-primitive type in Java SE 7, on the Android platform, and in GNU Octave."
Tomasulo algorithm,"Tomasulo’s algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions that allows out-of-order execution, designed to efficiently utilize multiple execution units. It was developed by Robert Tomasulo at IBM in 1967, and first implemented in the IBM System/360 Model 91’s floating point unit.
The major innovations of Tomasulo’s algorithm include register renaming in hardware, reservation stations for all execution units, and a common data bus (CDB) on which computed values broadcast to all reservation stations that may need them. These developments allow for improved parallel execution of instructions that would otherwise stall under the use of scoreboarding or other earlier algorithms.
Robert Tomasulo received the Eckert-Mauchly Award in 1997 for his work on the algorithm."
Toom–Cook multiplication,"Toom–Cook, sometimes known as Toom-3, named after Andrei Toom, who introduced the new algorithm with its low complexity, and Stephen Cook, who cleaned the description of it, is a multiplication algorithm, a method of multiplying two large integers.
Given two large integers, a and b, Toom–Cook splits up a and b into k smaller parts each of length l, and performs operations on the parts. As k grows, one may combine many of the multiplication sub-operations, thus reducing the overall complexity of the algorithm. The multiplication sub-operations can then be computed recursively using Toom–Cook multiplication again, and so on. Although the terms ""Toom-3"" and ""Toom–Cook"" are sometimes incorrectly used interchangeably, Toom-3 is only a single instance of the Toom–Cook algorithm, where k = 3.
Toom-3 reduces 9 multiplications to 5, and runs in Θ(nlog(5)/log(3)), about Θ(n1.465). In general, Toom-k runs in Θ(c(k) ne), where e = log(2k − 1) / log(k), ne is the time spent on sub-multiplications, and c is the time spent on additions and multiplication by small constants. The Karatsuba algorithm is a special case of Toom–Cook, where the number is split into two smaller ones. It reduces 4 multiplications to 3 and so operates at Θ(nlog(3)/log(2)), which is about Θ(n1.585). Ordinary long multiplication is equivalent to Toom-1, with complexity Θ(n2).
Although the exponent e can be set arbitrarily close to 1 by increasing k, the function c unfortunately grows very rapidly. The growth rate for mixed-level Toom-Cook schemes was still an open research problem in 2005. An implementation described by Donald Knuth achieves the time complexity Θ(n 2√(2 log n) log n).
Due to its overhead, Toom–Cook is slower than long multiplication with small numbers, and it is therefore typically used for intermediate-size multiplications, before the asymptotically faster Schönhage–Strassen algorithm (with complexity Θ(n log n log log n)) becomes practical.
Toom first described this algorithm in 1963, and Cook published an improved (asymptotically equivalent) algorithm in his PhD thesis in 1966.

"
Top-nodes algorithm,"The top-nodes algorithm is an algorithm for managing a resource reservation calendar.
It is used when a resource is shared among lots of users (for example bandwidth in a telecommunication link, or disk capacity in a large data center).
The algorithm allows
to check if an amount of resource is available during a specific period of time,
to reserve an amount of resource for a specific period of time,
to delete a previous reservation,
to move the calendar forward (the calendar covers a defined duration, and it must be moved forward as time goes by)."
Topological sorting,"In the field of computer science, a topological sort (sometimes abbreviated toposort) or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks. A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time."
Tournament selection,"Tournament selection is a method of selecting an individual from a population of individuals in a genetic algorithm. Tournament selection involves running several ""tournaments"" among a few individuals (or 'chromosomes') chosen at random from the population. The winner of each tournament (the one with the best fitness) is selected for crossover. Selection pressure is easily adjusted by changing the tournament size. If the tournament size is larger, weak individuals have a smaller chance to be selected.
The tournament selection method may be described in pseudo code:

choose k (the tournament size) individuals from the population at random
choose the best individual from pool/tournament with probability p
choose the second best individual with probability p*(1-p)
choose the third best individual with probability p*((1-p)^2)
and so on...

Deterministic tournament selection selects the best individual (when p = 1) in any tournament. A 1-way tournament (k = 1) selection is equivalent to random selection. The chosen individual can be removed from the population that the selection is made from if desired, otherwise individuals can be selected more than once for the next generation. In comparison with the (stochastic) fitness proportionate selection method, tournament selection is often implemented in practice due to its lack of stochastic noise.
Tournament selection has several benefits over alternative selection methods for genetic algorithms (for example, fitness proportionate selection and reward-based selection): it is efficient to code, works on parallel architectures and allows the selection pressure to be easily adjusted. Tournament selection has also been shown to be independent of the scaling of the genetic algorithm fitness function (or 'objective function') in some classifier systems."
Traveling salesman problem,"The travelling salesman problem (TSP) asks the following question: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city? It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science.

TSP is a special case of the travelling purchaser problem and the Vehicle routing problem.
In the theory of computational complexity, the decision version of the TSP (where, given a length L, the task is to decide whether the graph has any tour shorter than L) belongs to the class of NP-complete problems. Thus, it is possible that the worst-case running time for any algorithm for the TSP increases superpolynomially (perhaps, specifically, exponentially) with the number of cities.
The problem was first formulated in 1930 and is one of the most intensively studied problems in optimization. It is used as a benchmark for many optimization methods. Even though the problem is computationally difficult, a large number of heuristics and exact methods are known, so that some instances with tens of thousands of cities can be solved completely and even problems with millions of cities can be approximated within a small fraction of 1%.
The TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing. In these applications, the concept city represents, for example, customers, soldering points, or DNA fragments, and the concept distance represents travelling times or cost, or a similarity measure between DNA fragments. The TSP also appears in astronomy, as astronomers observing many sources will want to minimise the time spent slewing the telescope between the sources. In many applications, additional constraints such as limited resources or time windows may be imposed."
Tree sort,"A tree sort is a sort algorithm that builds a binary search tree from the keys to be sorted, and then traverses the tree (in-order) so that the keys come out in sorted order. Its typical use is sorting elements adaptively: after each insertion, the set of elements seen so far is available in sorted order."
Tree traversal,"In computer science, tree traversal (also known as tree search) is a form of graph traversal and refers to the process of visiting (examining and/or updating) each node in a tree data structure, exactly once, in a systematic way. Such traversals are classified by the order in which the nodes are visited. The following algorithms are described for a binary tree, but they may be generalized to other trees as well."
Trial division,"Trial division is the most laborious but easiest to understand of the integer factorization algorithms. The essential idea behind trial division tests to see if an integer n, the integer to be factored, can be divided by each number in turn that is less than n. For example, for the integer n = 12, the only numbers that divide it are 1,2,3,4,6,12. Selecting only the largest powers of primes in this list gives that 12 = 3 × 4."
Truncated binary exponential backoff,"Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, in order to gradually find an acceptable rate."
Truncation selection,"Truncation selection is a selection method used in genetic algorithms to select potential candidate solutions for recombination.
In truncation selection the candidate solutions are ordered by fitness, and some proportion, p, (e.g. p = 1/2, 1/3, etc.), of the fittest individuals are selected and reproduced 1/p times. Truncation selection is less sophisticated than many other selection methods, and is not often used in practice. It is used in Muhlenbein's Breeder Genetic Algorithm."
UPGMA,"UPGMA (Unweighted Pair Group Method with Arithmetic Mean) is a simple agglomerative (bottom-up) hierarchical clustering method. It is one of the most popular methods in ecology for the classification of sampling units (such as vegetation plots) on the basis of their pairwise similarities in relevant descriptor variables (such as species composition). In bioinformatics, UPGMA is used for the creation of phenetic trees (phenograms). In a phylogenetic context, UPGMA assumes a constant rate of evolution (molecular clock hypothesis), and is not a well-regarded method for inferring relationships unless this assumption has been tested and justified for the data set being used. UPGMA was initially designed for use in protein electrophoresis studies, but is currently most often used to produce guide trees for more sophisticated phylogenetic reconstruction algorithms.
The UPGMA algorithm constructs a rooted tree (dendrogram) that reflects the structure present in a pairwise similarity matrix (or a dissimilarity matrix).
At each step, the nearest two clusters are combined into a higher-level cluster. The distance between any two clusters A and B is taken to be the average of all distances between pairs of objects ""x"" in A and ""y"" in B, that is, the mean distance between elements of each cluster:

The method is generally attributed to Sokal and Michener. Fionn Murtagh found a time optimal  time algorithm to construct the UPGMA tree."
Ukkonen's algorithm,"In computer science, Ukkonen's algorithm is a linear-time, online algorithm for constructing suffix trees, proposed by Esko Ukkonen in 1995.
The algorithm begins with an implicit suffix tree containing the first character of the string. Then it steps through the string adding successive characters until the tree is complete. This order addition of characters gives Ukkonen's algorithm its ""on-line"" property. The original algorithm presented by Peter Weiner proceeded backward from the last character to the first one from the shortest to the longest suffix. A simpler algorithm was found by Edward M. McCreight, going from the longest to the shortest suffix.
The naive implementation for generating a suffix tree going forward requires O(n2) or even O(n3) time complexity in big O notation, where n is the length of the string. By exploiting a number of algorithmic techniques, Ukkonen reduced this to O(n) (linear) time, for constant-size alphabets, and O(n log n) in general, matching the runtime performance of the earlier two algorithms."
Unicode Collation Algorithm,"The Unicode collation algorithm (UCA) is an algorithm defined in Unicode Technical Report #10, which defines a customizable method to compare two strings. These comparisons can then be used to collate or sort text in any writing system and language that can be represented with Unicode.
Unicode Technical Report #10 also specifies the Default Unicode Collation Element Table (DUCET). This datafile specifies the default collation ordering. The DUCET is customizable for different languages. Some such customisations can be found in Common Locale Data Repository (CLDR).
An important open source implementation of UCA is included with the International Components for Unicode, ICU. ICU also supports tailoring and the collation tailorings from CLDR are included in ICU. You can see the effects of tailoring and a large number of language specific tailorings in the on-line ICU Locale Explorer."
Uniform-cost search,"Dijkstra's algorithm is an algorithm for finding the shortest paths between nodes in a graph, which may represent, for example, road networks. It was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.
The algorithm exists in many variants; Dijkstra's original variant found the shortest path between two nodes, but a more common variant fixes a single node as the ""source"" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest path tree.
For a given source node in the graph, the algorithm finds the shortest path between that node and every other. It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road, Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. As a result, the shortest path algorithm is widely used in network routing protocols, most notably IS-IS and Open Shortest Path First (OSPF). It is also employed as a subroutine in other algorithms such as Johnson's.
Dijkstra's original algorithm does not use a min-priority queue and runs in time  (where  is the number of nodes). The idea of this algorithm is also given in (Leyzorek et al. 1957). The implementation based on a min-priority queue implemented by a Fibonacci heap and running in  (where  is the number of edges) is due to (Fredman & Tarjan 1984). This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights.
In some fields, artificial intelligence in particular, Dijkstra's algorithm or a variant of it is known as uniform-cost search and formulated as an instance of the more general idea of best-first search."
Uniform binary search,"Uniform binary search is an optimization of the classic binary search algorithm invented by Donald Knuth and given in Knuth's The Art of Computer Programming. It uses a lookup table to update a single array index, rather than taking the midpoint of an upper and a lower bound on each iteration; therefore, it is optimized for architectures (such as Knuth's MIX) on which
a table lookup is generally faster than an addition and a shift, and
many searches will be performed on the same array, or on several arrays of the same length"
VEGAS algorithm,"The VEGAS algorithm, due to G. P. Lepage, is a method for reducing error in Monte Carlo simulations by using a known or approximate probability distribution function to concentrate the search in those areas of the integrand that make the greatest contribution to the final integral.
The VEGAS algorithm is based on importance sampling. It samples points from the probability distribution described by the function , so that the points are concentrated in the regions that make the largest contribution to the integral.
In general, if the Monte Carlo integral of  is sampled with points distributed according to a probability distribution described by the function , we obtain an estimate ,
.
The variance of the new estimate is then

where  is the variance of the original estimate, .
If the probability distribution is chosen as  then it can be shown that the variance  vanishes, and the error in the estimate will be zero. In practice it is not possible to sample from the exact distribution g for an arbitrary function, so importance sampling algorithms aim to produce efficient approximations to the desired distribution.
The VEGAS algorithm approximates the exact distribution by making a number of passes over the integration region while histogramming the function f. Each histogram is used to define a sampling distribution for the next pass. Asymptotically this procedure converges to the desired distribution. In order to avoid the number of histogram bins growing like  with dimension d the probability distribution is approximated by a separable function:  so that the number of bins required is only Kd. This is equivalent to locating the peaks of the function from the projections of the integrand onto the coordinate axes. The efficiency of VEGAS depends on the validity of this assumption. It is most efficient when the peaks of the integrand are well-localized. If an integrand can be rewritten in a form which is approximately separable this will increase the efficiency of integration with VEGAS."
Vector clocks,"Vector clocks is an algorithm for generating a partial ordering of events in a distributed system and detecting causality violations. Just as in Lamport timestamps, interprocess messages contain the state of the sending process's logical clock. A vector clock of a system of N processes is an array/vector of N logical clocks, one clock per process; a local ""smallest possible values"" copy of the global clock-array is kept in each process, with the following rules for clock updates:

Initially all clocks are zero.
Each time a process experiences an internal event, it increments its own logical clock in the vector by one.
Each time a process prepares to send a message, it sends its entire vector along with the message being sent.
Each time a process receives a message, it increments its own logical clock in the vector by one and updates each element in its vector by taking the maximum of the value in its own vector clock and the value in the vector in the received message (for every element).
The vector clocks algorithm was independently developed by Colin Fidge and Friedemann Mattern in 1988."
Vector quantization,"Vector quantization (VQ) is a classical quantization technique from signal processing that allows the modeling of probability density functions by the distribution of prototype vectors. It was originally used for data compression. It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. Each group is represented by its centroid point, as in k-means and some other clustering algorithms.
The density matching property of vector quantization is powerful, especially for identifying the density of large and high-dimensioned data. Since data points are represented by the index of their closest centroid, commonly occurring data have low error, and rare data high error. This is why VQ is suitable for lossy data compression. It can also be used for lossy data correction and density estimation.
Vector quantization is based on the competitive learning paradigm, so it is closely related to the self-organizing map model and to sparse coding models used in deep learning algorithms such as autoencoder."
Velvet (algorithm),"Velvet is an algorithm package that has been designed to deal with de novo genome assembly and short read sequencing alignments. This is achieved through the manipulation of de Bruijn graphs for genomic sequence assembly via the removal of errors and the simplification of repeated regions. Velvet has also been implemented inside of commercial packages, such as Geneious, MacVector and BioNumerics."
Verhoeff algorithm,"The Verhoeff algorithm is a checksum formula for error detection developed by the Dutch mathematician Jacobus Verhoeff and was first published in 1969. It was the first decimal check digit algorithm which detects all single-digit errors, and all transposition errors involving two adjacent digits, which was at the time thought impossible with such a code."
Wang and Landau algorithm,"The Wang and Landau algorithm, proposed by Fugao Wang and David P. Landau, is a Monte Carlo method designed to calculate the density of states of a system. The method performs a non-markovian random walk to build the density of states by quickly visiting all the available energy spectrum. The Wang and Landau algorithm is an important method to obtain the density of states required to perform a multicanonical simulation.
The Wang–Landau algorithm can be applied to any system which is characterized by a cost (or energy) function. For instance, it has been applied to the solution of numerical integrals and the folding of proteins. The Wang-Landau Sampling is related to the Metadynamics algorithm."
Warnock algorithm,"The Warnock algorithm is a hidden surface algorithm invented by John Warnock that is typically used in the field of computer graphics. It solves the problem of rendering a complicated image by recursive subdivision of a scene until areas are obtained that are trivial to compute. In other words, if the scene is simple enough to compute efficiently then it is rendered; otherwise it is divided into smaller parts which are likewise tested for simplicity.
This is a divide and conquer algorithm with run-time of , where n is the number of polygons and p is the number of pixels in the viewport.
The inputs are a list of polygons and a viewport. The best case is that if the list of polygons is simple, then draw the polygons in the viewport. Simple is defined as one polygon (then the polygon or its part is drawn in appropriate part of a viewport) or a viewport that is one pixel in size (then that pixel gets a color of the polygon closest to the observer). The continuous step is to split the viewport into 4 equally sized quadrants and to recursively call the algorithm for each quadrant, with a polygon list modified such that it only contains polygons that are visible in that quadrant."
Winnow algorithm,"The winnow algorithm is a technique from machine learning for learning a linear classifier from labeled examples. It is very similar to the perceptron algorithm. However, the perceptron algorithm uses an additive weight-update scheme, while Winnow uses a multiplicative scheme that allows it to perform much better when many dimensions are irrelevant (hence its name). It is a simple algorithm that scales well to high-dimensional data. During training, Winnow is shown a sequence of positive and negative examples. From these it learns a decision hyperplane that can then be used to label novel examples as positive or negative. The algorithm can also be used in the online learning setting, where the learning and the classification phase are not clearly separated."
Xiaolin Wu's line algorithm,"Xiaolin Wu's line algorithm is an algorithm for line antialiasing, which was presented in the article An Efficient Antialiasing Technique in the July 1991 issue of Computer Graphics, as well as in the article Fast Antialiasing in the June 1992 issue of Dr. Dobb's Journal.
Bresenham's algorithm draws lines extremely quickly, but it does not perform anti-aliasing. In addition, it cannot handle any cases where the line endpoints do not lie exactly on integer points of the pixel grid. A naive approach to anti-aliasing the line would take an extremely long time. Wu's algorithm is comparatively fast, but is still slower than Bresenham's algorithm. The algorithm consists of drawing pairs of pixels straddling the line, each coloured according to its distance from the line. Pixels at the line ends are handled separately. Lines less than one pixel long are handled as a special case.
An extension to the algorithm for circle drawing was presented by Xiaolin Wu in the book Graphics Gems II. Just like the line drawing algorithm is a replacement for Bresenham's line drawing algorithm, the circle drawing algorithm is a replacement for Bresenham's circle drawing algorithm."
Xor swap algorithm,"In computer programming, the XOR swap is an algorithm that uses the XOR bitwise operation to swap values of distinct variables having the same data type without using a temporary variable. ""Distinct"" means that the variables are stored at different memory addresses; the actual values of the variables do not have to be different."
Yamartino method,"The Yamartino method (introduced by Robert J. Yamartino in 1984) is an algorithm for calculating an approximation to the standard deviation σθ of wind direction θ during a single pass through the incoming data. The standard deviation of wind direction is a measure of lateral turbulence, and is used in a method for estimating the Pasquill stability category.
The simple method for calculating standard deviation requires two passes through the list of values. The first pass determines the average of those values; the second pass determines the sum of the squares of the differences between the values and the average. This double-pass method requires access to all values. A single-pass method can be used for normal data but is unsuitable for angular data such as wind direction where the 0°/360° (or +180°/-180°) discontinuity forces special consideration. For example, the directions 1°, 0°, and 359° (or -1°) should not average to the direction 120°!
The Yamartino method solves both problems. The United States Environmental Protection Agency (EPA) has chosen it as the preferred way to compute the standard deviation of wind direction. A further discussion of the Yamartino method, along with other methods of estimating the standard deviation of wind direction can be found in Farrugia & Micallef.
It should be mentioned that it is also possible to calculate the exact standard deviation in one pass. However, that method needs slightly more calculation effort."
Zeller's congruence,Zeller's congruence is an algorithm devised by Christian Zeller to calculate the day of the week for any Julian or Gregorian calendar date. It can be considered to be based on the conversion between Julian day and the calendar date.
Zhu–Takaoka string matching algorithm,"In computer science, the Zhu–Takaoka string matching algorithm is a variant of the Boyer–Moore string search algorithm. It uses two consecutive text characters to compute the bad character shift. It is faster when the alphabet or pattern is small, but the skip table grows quickly, slowing the pre-processing phase."
Ziggurat algorithm,"The ziggurat algorithm is an algorithm for pseudo-random number sampling. Belonging to the class of rejection sampling algorithms, it relies on an underlying source of uniformly-distributed random numbers, typically from a pseudo-random number generator, as well as precomputed tables. The algorithm is used to generate values from a monotone decreasing probability distribution. It can also be applied to symmetric unimodal distributions, such as the normal distribution, by choosing a value from one half of the distribution and then randomly choosing which half the value is considered to have been drawn from. It was developed by George Marsaglia and others in the 1960s.
A typical value produced by the algorithm only requires the generation of one random floating-point value and one random table index, followed by one table lookup, one multiply operation and one comparison. Sometimes (2.5% of the time, in the case of a normal or exponential distribution when using typical table sizes) more computations are required. Nevertheless, the algorithm is computationally much faster than the two most commonly used methods of generating normally distributed random numbers, the Marsaglia polar method and the Box–Muller transform, which require at least one logarithm and one square root calculation for each pair of generated values. However, since the ziggurat algorithm is more complex to implement it is best used when large quantities of random numbers are required.
The term ziggurat algorithm dates from Marsaglia's paper with Wai Wan Tsang in 2000; it is so named because it is conceptually based on covering the probability distribution with rectangular segments stacked in decreasing order of size, resulting in a figure that resembles a ziggurat."
Nudity Detection,"This algorithm detects nudity in pictures. It is based on the article: An Algorithm for Nudity Detection. The idea behind the algorithm is based primarily on observations that in general, nude images contain large amounts of skin, people have different skin tones, and skin regions in nude images are relatively close to each other. In order to make the algorithm more robust, we have incorporated face detection for skin ratio tweaking and skin color detection for limiting the generic skin color value interval. The limits that decide on the skin regions are based on the values in RGB, HSV and normalized RGB color spaces in the book Human Computer Interaction Using Hand Gestures by Prashan Premaratne, derived from multiple papers as described in the book."
Get Recommendations,"This algorithm provides page recommendations for a domain. It is primarily geared for use with the Algorithmia recommendation system, which provides more specificity, but can be used independently of this as well.It takes as input a url and a required string. Only urls whose source contains the required string will be considered as recommendations. Note that the page source itself is inspected, not just the user-readable text. Examples include long UUIDs embedded in the page's java script, or simply the empty string """" to consider every page. Get Recommendations maintains a permanent algorithm collection (see the documentation). When a url is sent to it, it checks to see if any url from the same domain has ever been sent. If not, the domain is explored, for 200 urls or 2 minutes, whichever comes first, using /web/BreadthFirstSiteMap and filtering for urls containing the required string. These urls are processed into recommendations as described below.It also takes an optional third integer parameter, which dictates how many months in the past (relative to a given page) will be considered for recommendation. For example, if the parameter is 24, no page that is more than 24 months older than a given page will be recommended. Note however that recommendations are always being updated as new pages are added, and for any given page, anything published more recently than it is eligible as a recommendation assuming some nonzero similarity. If this parameter is not supplied or is equal to -1, all pages will be considered.For each explored domain, the algorithm maintains a word count summary for each url and for the domain as a whole. For every new url in an existing domain, it scrapes the url for content using /web/AnalyzeURL, processes this into word statistics, and generates keywords for each url using  /nlp/KeywordsForDocumentSet, and generates recommendations from these keywords using /nlp/KeywordSetSimilarity. All recommendations are stored in a table, and any time the algorithm is queried with a url that is in this table, it returns the corresponding recommendations. Thus, the first call to a domain takes a few minutes and a call with a novel url takes up to a few seconds, but any subsequent call with a url returns very quickly."
Dijkstra,"Dijkstra's Path Planning Algorithm.
Input: A graph, a start vertex and a goal vertex.
Output: Shortest path from start to goal vertices.Input:Map from vertex name to location [x,y]Map from vertex to neighborsStart vertexGoal vertex"
Auto-Tag URL,"This algorithm takes in a URL, retrieves the content, and produces candidate tags using LDA."
Get Links,"Given a url (as a string), scrapes the page for links to other pages and returns them as url strings. Links to documents, such as PDFs and PPTs, are ignored. "
Hello,Returns a greeting addressed to the input text.
Summarizer,"Takes in blocks of text, and extracts key topic sentences. "
Web Page Recommender,"This algorithm provides page recommendations for a domain. It is primarily geared for use with the Algorithmia recommendation system, which provides more specificity, but can be used independently of this as well.It takes as input a url and a required string. Only urls whose source contains the required string will be considered as recommendations. Note that the page source itself is inspected, not just the user-readable text. Examples include long UUIDs embedded in the page's java script, or simply the empty string """" to consider every page. Get Recommendations maintains a permanent algorithm collection (see the documentation). When a url is sent to it, it checks to see if any url from the same domain has ever been sent. If not, the domain is explored, for 200 urls or 2 minutes, whichever comes first, using /web/BreadthFirstSiteMap and filtering for urls containing the required string. These urls are processed into recommendations as described below.It also takes an optional third integer parameter, which dictates how many months in the past (relative to a given page) will be considered for recommendation. For example, if the parameter is 24, no page that is more than 24 months older than a given page will be recommended. Note however that recommendations are always being updated as new pages are added, and for any given page, anything published more recently than it is eligible as a recommendation assuming some nonzero similarity. If this parameter is not supplied or is equal to -1, all pages will be considered.For each explored domain, the algorithm maintains a word count summary for each url and for the domain as a whole. For every new url in an existing domain, it scrapes the url for content using /web/AnalyzeURL, processes this into word statistics, and generates keywords for each url using  /nlp/KeywordsForDocumentSet, and generates recommendations from these keywords using /nlp/KeywordSetSimilarity. All recommendations are stored in a table, and any time the algorithm is queried with a url that is in this table, it returns the corresponding recommendations. Thus, the first call to a domain takes a few minutes and a call with a novel url takes up to a few seconds, but any subsequent call with a url returns very quickly."
Html 2 Text,Takes in a url and extracts the content from the page.Makes an attempt to remove non-content text like navigation and footer text.
PorterStemmer,Lucene PorterStemmer 3.6.2
AutoTag,This class takes in text content as a set of documents and produces candidate tags using LDA. Pipe your input as a json array of strings.
Sentiment Analysis,"Identify and extract sentiment in given string. Sentiment analysis (also known as opinion mining) refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials. This algorithm takes an input string and assigns a sentiment rating in the range [0-4] (very negative, negative, neutral, positive, and very positive).For more information, please refer to http://nlp.stanford.edu/software/corenlp.shtml or Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60. "
Auto-Tag Github,"Provide this algorithm a github repository and it will generate topic tags of its README based on LDA. Input is in the format [user,repo], and the repository queried is of form ""http://github.com/"" + user + ""/"" + repo."
LDA,"In natural language processing, Latent Dirichlet Allocation (LDA) is a generative model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael Jordan in 2003. InputA list of strings representing observationsThe number of topics to generate"
Url 2 Text,Takes in a url and extracts the content from the page.Makes an attempt to remove non-content text like navigation and footer text.
Analyze URL,"Given a url and a target string, returns several pieces of information on the url, including ""text"" - the text of the page (similar to /util/Url2Text) , ""title"" - the page title, ""summary"" - a summary of the page text (using either the meta description of the page, or, if this is not available, the summary of the page text via /nlp/Summarizer), ""thumbnail"" - the url of the main image on the page, if it can be located, and a boolean denoting if the page source contains the target string. Note that all information is provided on a best effort basis, if something can be found it will not be included in the output.Input consists of:url: The URL of the page to analyzemarker: An optional search term to look for in the page"
LDA,"In natural language processing, Latent Dirichlet Allocation (LDA) is a generative model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael Jordan in 2003. InputA list of strings representing observationsThe number of topics to generate"
Scrape RSS,"Takes the url of an RSS feed and scrapes the RSS feed, extracting title, link url, and comments url."
Recommend GitHub From Tags,"Based on topic tags, this algorithm will recommend similar Github repositories. The input consists of tags with corresponding desired scores."
Maze Generator,Generate a maze with long winding pathways using Recursive Backtracking Search.Takes in as input width and height of the requested maze.
Scrape HN,"Scrapes the HackerNews RSS feed and extracts title, link url, and comments url."
PageRank,"This is a simple implementation of the PageRank algorithm.  It accepts an input in the form of an int[][] or a String which can define the d value and the number of loops. The default d value is 0.85, and the default number of loops is 20.

Input Descriptionpage 0 links to pages 1,2,3Page 1 links to page 0Page 2 links to page 0Page 3 links to pages 0,4,5,6,7page 4 has no linkspage 5 has no linkspage 6 has no linkspage 7 has no linksSample Input[[1,2,3],[0],[0],[0,4,5,6,7],[],[],[],[]]Sample Output[0.9142543787391981,0.4089334112608257,0.4089334112608257,0.4089334112608257,0.21947767079447258,0.21947767079447258,0.21947767079447258,0.21947767079447258]








"
levenshtein,"The Levenshtein Distance algorithm measures the distance between two strings, returning the minimum number of changes required to change one string to another."
Weka Classification,"This is the main algorithm that all of the Weka classification algorithms call. It is preferred that you do not call this algorithm directly, but by one of the classifiers on the platform. The Weka algorithms that are on the platform take in flat Json as input. It is assumed that any data that is going to be used with these algorithms will be large in size, so all interaction is done through Data collections. You can look at the DigitRecognition algorithm we made for a quick demo of how to use Weka algorithms. That algorithm takes in either a matrix or an array of doubles that represent a black and white picture. The picture is a hand-drawn digit, and we feed it to a pretrained model on the MNIST dataset that will classify which of the 10 digits this particular one is. "
Sentence Detection,"Takes a block of text and splits into sentences, returning each sentence as an entry in a String array.For more information visit http://opennlp.apache.org."
Digit Recognizer,"Applies a random forest model trained on the MNIST handwritten digit dataset to a 28 by 28 grayscale pixel grid, which is input as a double array of length 784. Supports the Mahout portion of the handwritten digit classification demo"
Random Forest Apply,"OverviewThis routine applies a previously learned Mahout Random Forest Classifier to a set of test data. It takes as input a JSON array of four items, the first three are Data API URLs, of the (unlabeled) test data, the model file, and the (labelled) data used to train the model file, respectively, followed by a descriptor that details the type of each field in the dataset. It outputs the predicted labels of each instance in the test set. Note that test and training files are assumed to be CSVs.Data Format and DescriptorWe assume that the first entry of any instance is the label, though Mahout does support other placement. The descriptor must be of form ""L X X X ..."", where each X designates the type of its respective field, either I (ignored), N (numerical), or C (categorical). L designates the label label. Think of the descriptor as a header for the data. As an example, a dataset with four attributes (beyond the label) might have the first two as categorical, the third numerical, and the last ignored, and its header would be ""L C C N I"".With the test data we don’t have a label and as a matter of convention Mahout expects us to pass a ""-"". Our code currently handles this as long as your data has the label as the first field, and this field is missing in test data."
Site Map,Returns the graph resulting in crawling a URL for all links. 
Product Hunt Recommender,"Takes a post id from Product Hunt and returns up to five recommended posts. This algorithm is based on FP-Growth, found here:https://algorithmia.com/algorithms/paranoia/FpGrowth"
ExtractDependencies,"Input: Text.Output: For each sentence, a List of the dependencies we think might be useful."
Echo,This algorithm returns whatever you give it. Seems pretty clear.
Keyword Set Similarity ,"Determines similarity between sets of weighted keywords. Each keyword set is represented as a Map<String,Double>, where the String is the keyword and the Double is it's weight. The similarity of two sets is the sum of the products of the weights of their shared keywords, for instance, if set A has keywords ""dog"", ""cat"", and ""mouse"" with weights 1,2, and 2, respectively and set B has keywords ""dog"", ""cat"", and ""moose"" with weights 1.5,3, and 4, their similarity by this metric is 1*1.5 + 2*3 = 7.5. This can be thought of as the inner product of word vectors.The most convenient input format for a set of keyword sets is Map<String,Map<String,Double>>, where the first String key is an identifier for the keyword set, and it's value, a Map<String,Double>, is the set of keywords with their respective weights as values. The algorithm also requires an int that determines the maximum number of similar sets to return for each keyword set. The output for this input form is a Map from Strings to Set<String>'s, where the Set<String> value is the set of most similar keyword sets (denoted by a String identifier)."
Othello,An AI othello player.Input is a board represented as a 2D array of integers. 0 = empty1 = black2 = white
Keywords For Document Set,"Given a set of documents (represented as a List<String>) and a maximum number of keywords to return per document, returns a list, each entry of which contains the most relevant (as measured by weight) keywords for the respective document. In a given document, a keyword receives a higher weight for the number of times it appears in the document and a lower weight for the number of other documents it appears in. This is a simple implementation of tf-idf scoring."
Product Hunt Vote Rings,This algorithm takes two parameters (filter and limit) and returns the latest Product Hunt posts having a collusion ratio higher than 'filter'. The 'limit' parameter is used to specify maximum number of records returned. This depends on 'SimpleVoteRingDetection' algorithm to calculate the collusion ratio.
FpGrowth,"Java implementation of the Frequent Pattern Growth (FP-Growth) algorithm, which is a scalable method for finding frequent patterns within large datasets. For example, it could be used to find Association Rules and develop collaborative-filtering  systems, such as ""Other people also bought""... The algorithm takes three arguments:Dataset: path to a local (data://...) dataset, where each line represents a single transaction and each item is separated by whitespace. (See Example file).Support: represents the minimum frequency for a pattern to be recognized. In most cases you'll want to increase this number to reduce the size of the output.MinItems: represents the minimum number of items (per association rule). Having the value of this argument as 1 will return each unique item in the dataset and the number of times they appeared. Most applications would require a number higher than 1.This algorithm was featured in the Algorithmia Blog Post: ""Mining Product Hunt, Part 2: Building a Recommendation Engine""."
GraphGen,Given an array of integers creates grid graph.
Tokenizer,"Tokenizes input text, returning each token in order as a string in a string array. By default uses English tokenization model en-token.bin. To use an alternate tokenization model, call with arguments [<input text>, ""data://ApacheOpenNLP/models/<tokenization model>""].For more information visit http://opennlp.apache.org."
DigitRecognition,The Digit Recognition algorithm that uses a Random Forest model trained on 50000 samples obtained from the MNIST digits dataset.
Language Identification,This is a language identifier based on Apache Tika. It can identify 18 different languages. It takes a string as input and returns its best guess as to what language the string is written in. Currently supported languages areda—Danishde—Germanet—Estonianel—Greeken—Englishes—Spanishfi—Finnishfr—Frenchhu—Hungarianis—Icelandicit—Italiannl—Dutchno—Norwegianpl—Polishpt—Portugueseru—Russiansv—Swedishth—Thai
Get Building Permit Data,"Aggregates Socrata Building Permit data based on city. Valid cities are: Boston, Santa Rosa, New York City, Edmonton, Los Angeles, Fort Worth"
Autocorrelate,Computes the  autocorrelation  plot of a time series.
Linear Detrend,"Takes a time series (either a double[][] for unevenly spaced data, with time in the first column, or a double[] for evenly spaced data), fits a linear trend to the data, and returns the data with the linear trend subtracted out."
Retrieve Tweets With Keyword,Retrieve tweets that include keyword anywhere int their text. Limited to 500 tweets per call. 
Top Periods,Returns the top periods present in the input double[].
Sentiment Analysis,"Sentiment analysis (also known as opinion mining) refers to the use of natural language processing, text analysis and computational linguistics to identify and extract subjective information in source materials. This algorithm takes an input string and assigns a sentiment rating in the range [0-4].For more information, please refer to http://nlp.stanford.edu/software/corenlp.shtml or Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60. "
Forecast,"Gives a forecast the next n steps of a given time series based on extrapolation of linear and seasonal trends. It takes as inputA time series as a double[].A number of time steps into the future to forecast.A maximum number of seasonal periods to consider.Alternatively it takes just a time series, and defaults to a forecast range equal to the length of the original series and  the single strongest seasonal trend.It returns the forecasted series as a double[]. This algorithm works by fitting a linear trend to the given data, extrapolating it into the future series interval, and then adjusting it based on the expected contributions of each detected seasonal component. Seasonality is detected using https://algorithmia.com/algorithms/TimeSeries/Autocorrelate.Note that this algorithm only deals with linear and seasonal trends. If other artifacts, such as non-seasonal spikes or super/sub-linear trends, are present in the data, results will suffer, and as a general rule the further you go in the future, the less accuracy you should expect."
Factor,"Takes an integer, and decomposes it into its prime factors."
Object Detection With Models,"Core algorithm for taking in an image and a pretrained model and detecting objects. Call this algorithm from another algorithm, by giving a pretrained model file link. For usage examples, see:FaceDetectionSmileDetectionEyeDetectionBodyDetection"
Face Recognition,"Performs face recognition using OpenCV.  This is the internal workings of the corresponding algorithms Train Face Recognizer and Recognize Faces.  Users will probably prefer to use those algorithms, and should refer to their documentation for guidance.  "
Weka Cluster,k-means clustering using the Weka machine learning library.
Digit Recognition,"This algorithm classifies an image of a digit, given as an array."
BIPM,
Recognize Faces,"Uses a trained face recognizer to identify faces. This is a wrapper for the FaceRecognition algorithm.  The FaceRecognition algorithm additionally requires a user to specify a ""classify"" mode, so this wrapper makes things slightly more convenient. Users must pass in a list of JSON objects, one for each image.  Each image JSON object must have a ""Path"" field specifying the image URL (e.g. the path to the image in an Algorithmia collection).  Optionally, it may have a ""ID"" field with an ID (or label) saying whose face is pictured.  This is useful mainly for academic purposes to evaluate the model; of course in many real-world situations the point of having a face recognizer is to recognize people whose identity is not explicitly labeled already.  (Note: if an image has multiple faces, each face will be classified.  As in the accompanying Train/Update Face Recognizer algorithms, any ID that is provided will be the ""ground truth"" ID for each face in the image.)Users must also specify the full path to a face recognizer in an Algorithmia data collection (including the name of the face recognizer XML file).  There must also be a map of human IDs to internal numeric labels in a file with the same name plus an ""_idList.txt"" tag and extension instead of a "".xml"" extension. Both files must have the same name and must be in the same directory (the default name is facerec.xml and facerec_idList.txt).  These would both have been created, say, by a run of the companion Train Face Recognizer algorithm.  The output will be in JSON format, where each image path will have a list of predictions--one prediction for each face in the image--in JSON format.  Each prediction will contain: the predicted ID, the actual ID (if known), and the uncertainty, which is actually a distance measure to the nearest match.  Note: the predicted ID will be ""Unknown"" if the uncertainty is too high (at this time the threshold is 100).  (For clarity, it is obviously highly recommended that ""Unknown"" not be used as a valid image ID.)"
SentenceTagger,"Input: TextOutput: List of JsonObjects with Sentence as string[] words, with int[] verbs and int[] nouns containing positions of the verbs and nouns in the sentence."
Face Detection,"Uses a pretrained model to detect faces in a given image. Send your input as a url to a picture that is either hosted on our data API or is a direct link, and the output is the same image, with rectangles around the detected faces, written to your collection. A text file that contains all the coordinates of the rectangles is also output to the same collection for convenience. The text file contains 4 integers: the x and y coordinates of the top left of the rectangle, the width of the rectangle and the height of the rectangle.If you like, you can give an arbitrary URL as the first input, the second input will still have to be a data collection because we will be writing the output of the algorithm: [""http://fluorineplus.com/wp-content/2012/07/Natural-Ways-To-Take-Care-300x293.jpg"", ""data://.algo/temp/result.jpg""]"
Tech Events Finder,"Returns a list of future tech events from from Meetup and Eventbrite. Takes three inputs: country, state, and city."
Prepare Input Array,"This algorithm separates input string by spaces and makes them into an array. It is a convenience algorithm that takes in a String of values that need to be passed in to an Algorithm via Json. It is most useful for ""Algorithmia.call"" calls that take in multiple values. By sticking all values that need to be added into a JsonArray in a String delimited by a space ("" "") and passing it into this algorithm, the user gets back the prepared JsonArray."
Extract Text,"Extracts text from a given file. Supported formats include Office documents (Word, Powerpoint, etc), HTML, XML, PDF, RTF, etc. For more information on supported formats, please take a look at Apache Tika's documentation page.This algorithm handles any URL, be it from the Data API or the URL to a file on the internet."
Receipt Recognition,This algorithm reads a computer printed or handwritten receipt (containing only the total or tip portions) and returns the value written.Please upload the image of the receipt to a Data collection and give the direct link to this algorithm.
Breadth First Site Map,"A more efficient site mapper that explores in a breadth-first fashion. Given a url and a max number of pages to return, it explores in a breadth-first fashion until it detects that is has passed the maximum allowed number of pages, or until it has run for two minutes, whichever comes first. Returns an adjacency map of urls. Optionally, you can provide a list of required terms as a third argument. If you do this, the algorithm will return a hashmap with two keys, ""map"", whose value is the site map as described above, and ""marked"", whose value is a list of URLs whose html (including human-readable text) contains at least one of the strings provided in the third argument. This is most useful for finding specific html tags, though be careful of very short or common words that might be present either in html or in human readable text."
Count Social Shares,"Given a URL, returns the number of times that URL has been shared on various social media sites, currently facebook, twitter, pinterest, and linkedin. Facebook has more detailed information, including shares, comments, and likes, all others are just number of shares.  Please use the full http://.. syntax to ensure the best possible results."
Lat Long Distance,"Returns the geographical distance between two points defined by latitude and longitude. The first two arguments are the coordinates of one points, the second two the coordinates of the second point. The final argument is string, either ""km"" or ""mile"", depending upon if units of kilometers or miles are desired. This is calculated using the haversine formula and may not be accurate for points that are very close."
SentimentAnalysis,Sentiment analysis based on Apache Open NLP and SentiWordNet.Result:     positive: > 0     neutral: 0    negative: < 0
Generate Random Love Letter,Generates a random letter from trigrams trained from http://archive.lovingyou.com/content/inspiration/loveletters.php. Inputs are your valentine's and your names!
Parallel For Each,"An algorithm to run another algorithm on a list of inputs, run in parallel.This algorithm takesThe name of an algorithm (eg- ""/util/Echo"")A list of inputs which will map each input to the output of the given algorithmNote: This algorithm may not correctly run all iterations if the input algorithm calls other algorithms."
Random Text From Trigram,"This algorithm generates random text from a trained trigram model. The inputs are the Data URL that contains the trained file, and a beginning and an end token that will specify legitimate sentence beginnings and ends. You can learn more about Data collections on the Trigram generator algorithm page."
Image Binarization,"This algorithm binarizes the input image, useful for cleaning background noise in a text image to be input to OCR. This algorithm uses the implementation described in http://liris.cnrs.fr/christian.wolf/software/binarize/. The parameters are identical to those explained in the website. Unless the user finds a compelling reason, in general, the newest algorithm, ""w"", is the best performing algorithm and it is the default. You may wish to experiment with different parameters to find out the one that best fits your image.         -n Niblack (1986)       needs white text on black background
         -s Sauvola et al. (1997) needs black text onwhite background
         -w Wolf et al. (2001)   needs black text on white background

Default version: w (Wolf et al. 2001)
Default value for ""k"": 0.5

example:
      [""data://opencv/SampleImages/sampleOCR.jpg"", 
       """", ""data://opencv/SampleImages/sampleOCRCleaned.jpg""]"
Skin Color Detection,"Gets possible values of skin color for the detected people in a picture using nose detection and face detection. It is used by Nudity Detection to supply more accurate results. The return result is an array of arrays. Different arrays are for different detected faces; the values in the arrays are red_min, red_max, green_min, green_max, blue_min, blue_max; respectively."
discreteDistribution,"This algorithm let you draw from a discrete probability distribution.Several possible inputs:A probability distributionA probability distribution and a number of samples to drawA probability distribution, a number of samples to draw and a seed for the randomizer"
Cardinales,"This algorithm translates an integer into its textual representation in Spanish:Input can be of type long or String.String inputs are sanitized to ignore whitespaces.This algorithm uses long scale output, which is the Spanish standard number representation: ""1 000 000 000"" (one short scale billion) is transcripted as ""mil millones"", and ""1 000 000 000 000"" (one short scale trillion) as ""un billón"".Output strings are lowercase, and the output gender can be controlled by an optional Boolean algorithm: when second argument is True, the returned number will be feminine:  [""1"", true] = ""una"", [""1"", false] = ""uno"". Default is masculine output."
sat,"Boolean Satisfiability (SAT)Finds a mapping from variable names to boolean values such that the expression evaluates to true, or proves that the expression is always false.While boolean SAT is known to be NP-hard, this algorithm uses the DPLL/CDCL approach and can find an answer quickly even on some very large problem sizes. Additionally, this algorithm has been enhanced in two ways over traditional DPLL/CDCL solvers: it  is parallelized and it can break some simple kinds of symmetries.Input Languageexp ::= true       | false      | <var>      | (<exp>)      | not <exp>      | <exp> <op> <exp>op ::= iff | implies | xor | or | andvar ::= any word except true, false, not, iff, implies, xor, or, and
Operator precedence: (most tightly binding first) not, and, or, xor, implies, iffExpressions are mostly whitespace-agnostic (although you will need whitespace to separate some tokens; for instance, whitespace is needed between the ""not"" and the ""a"" in the expression ""not a"").OutputThe output is simply a satisfying assignment (a complete map from variable names to values) or null to indicate that no such assignment exists. Future versions of this algorithm may feature a different output format to indicate timeouts, memory shortage problems, or other practical concerns."
Retrieve Tweets By User,Takes in a username as a string input and returns a list of most recent tweets for that user (sent or mentioned)Rate limited by Twitter.
Image Reflection,"This algorithm reflects images either horizontally (around the y-axis) or vertically (around the x-axis).  Images may be passed in as one of two forms:1) via a path to where they are stored in an Algorithmia data collection (note: the path must be to an Algorithmia data collection, i.e. start with ""data://""!  Users interested in passing in an image with an arbitrary URL should check out the URL 2 Data algorithm).  In this case, the image will be saved to the same data collection with the same name plus the addition of a ""_reflected_""  tag and the reflection option (see below for details).  2) as base64 encoding, which can be done, for example, with the base64 module in Python.  In this case, the base64 encoding of the reflected image will be returned.  This is intended to support the needs of algorithms that call this algorithm internally, which will likely use this format.  Additionally, the user must specify which direction reflection should be performed.  To reflect an image about the x-axis, the user can either type ""X"" or ""vertical""; to reflect an image about the y-axis, the user can either type ""Y"" or ""horizontal"".  Capitalization does not matter. "
GetNGramFrequencies,"Gets lists of N grams from an input text. Input is: size of N-gram (number of words), cutoff size for max # of results returned, whether or not to ignore capitalization, and whether or not to sort from most frequent to least."
Tweet N-Gram,"Generates random tweets from your existing tweets. The input is the Twitter username String, an integer (2 for bigrams or 3 for trigrams) and a String that contains the URL to a Data collection. The generated ""tweets"" might not adhere to the 140 character limit! "
Digit Recognition,Recognizes handwritten digits using a pre trained Convolutional Neural Network. Input has to be a representation of 28x28 image that contains a digit (784 pixel values).
ScrapeSubReddit,"Given a name of a subreddit, scrapes and returns it's RSS feed including post title, link and publish timestamp,"
Remove Seasonality,"Removes seasonal components from a time series. It uses autocorrelation to identify the periods of dominant seasonal components, then subtracts the seasonal average from each point to yield a series of the seasonal residuals. Because of the autocorrelation step it is recommended that the series be de-trended first using ""/TimeSeries/LinearDetrend"" as a linear trend may result in undesirable autocorrelation artifacts. For instance, it has been observed in test cases that removing seasonality from series with a noticeable linear trend and non-zero mean yields a series in which almost all entries are zero, except for points at the beginning and end.There are two options, ""givenPeriod"" and ""topNPeriods"". Calling [[series],k,""givenPeriod""] subtracts from each point it's period length k seasonal average, removing a seasonal trend with period k. [[series],k,""topNPeriods""] removes the top k strongest seasonal trends as identified by autocorrelation. Calling the algorithm with just the series means the default option, e.g. removing only the strongest seasonal pattern, will be used. Note that removing a period of length 1 is a linear detrending that subtracts out the mean value, resulting in a series with slope and mean 0. When removing the top N trends, we ignore periods of length 1, as doing so can cause unwanted artifacts."
Censor Face,This algorithm uses Face Detection to detect any faces in an image and blacks them out.
Nose Detection,"Uses a pretrained model to detect noses in a given image. Send your input as a url to a picture that is either hosted on our data API or is a direct link, and the output is the same image, with rectangles around the detected noses, written to your collection. A text file that contains all the coordinates of the rectangles is also output to the same collection for convenience.P.S. Please make sure that the second input to this algorithm is a data collection that YOU own. You can create data collections from the ""Data"" link above."
Random Text From Bigram,"This algorithm generates random text from a trained bigram model. The inputs are the Data URL that contains the trained file, and a beginning and an end token that will specify legitimate sentence beginnings and ends. If no tokens are given, the algorithm selects a random bigram to start with and ends the sentence randomly after 5-12 words.  You can learn more about Data collections on the Trigram generator algorithm page."
Linear SVM with Stochastic Gradient Descent,"Learns a linear SVM trained with stochastic gradient descent, based on the implementation in scikit-learn, for classification of binary-labeled data.   Can be used for training or testing, depending on the arguments passed in:For training: takes in a data collection file path to write a trained model to and two other data collection paths to read data and labels in from, along with hyperparameter settings (in order: regularization coefficient, initial learning rate, learning rate decay parameter, and number of iterations of stochastic gradient descent).  Writes the trained model to the file at the specified data collection path.  For testing: takes in one path to a file in a data collection that contains a trained model to use, another file path to a test data set, and a third file path to which to write predictions.  "
RecognizeCharacters,"Uses Tesseract to perform OCR on a given image. As the trained model, it uses the latest model uploaded to Tesseract's Google Drive.Suggestion: If your image has noise that you would like to get rid of, you may wish to try out the ImageBinarization algorithm first, before feeding it to this algorithm.The input should be an image and can be from an arbitrary url, the data api, or binary. The output is the recognized text.If you would like to write the output to the data api, you can provide a second input that contains the target data api url (with filename). The output is same as this second input."
Dijkstra,"Dijkstra's  Path Planning Algorithm based on /kenny/Dijkstra but using input that specifies absolute edge distances rather than computing euclidian distance.
Input:Nested Map of directed edges with distance, e.g., { ""a"": { ""b"": 3 }} implies the edge from ""a"" to ""b"" has a distance of 3Start vertexGoal vertex"
Sylllables,Breaks up a word into syllables. Still needs a bit of work for long/short vowel detection in order to break including / excluding consonant surrounded by vowels in this case.
Outlier Detection,"Detects outliers in time series data. Takes a double[] of time series data and a double t, returns an array which matches the input array on all outlier points and is zero on all non-outliers. An outlier is defined as a datapoint that differs from the mean of the data by more than t times the standard deviation. The default setting of t is 2."
Url Link List,Link extractor. On input URL the HTML is fetched and all of the distinct links (a[href]) are extracted and presented as Set<String>.Dependencies:jsoupApache commons validatorThe algorithm does a url validation check and connection timeout is set to 5 seconds.
Aggregate Socrata Data By Month,Aggregates Socrata Data by month.
Face Detection,"Based on the /opencv/FaceDetection, this algorithm takes a base64 encoding of an image and returns an array of rectangles (x, y, width, height) bounding each face identified.Input FormatBase64 representation of an image.Output FormatArray of rectangles. Example:[    {'y': 10, 'x': 10, 'height': 100, 'width': 100},    {'y': 20, 'x': 20, 'height': 100, 'width': 100},    {'y': 30, 'x': 30, 'height': 100, 'width': 100}]"
CenterPoint,"This algorithm finds the center (or mid) point for an array of Lat/Long positions. Input: [{""Latitude"": 1.0,""Longitude"": 2.0},{""Latitude"": 3.0,""Longitude"": 4.0}]Output:{""Latitude"":2.0003044085023722,""Longitude"":2.999390393801055}"
Smart Thumbnail,"Takes an image (data uri, base64 encoded, or web url) and dimensions for a thumbnail and returns the binary of the resulting PNG encoded image.If a face is detected in the given image, the resulting thumbnail will be centered around the face. Pipe your input to this algorithm as an array with first element the string containing the image in one of the formats mentioned above, an integer for desired thumbnail width and an integer for desired thumbnail height."
SVM-SGD Wrapper,"This is a wrapper function for the Linear SVM with Stochastic Gradient Descent, designed to be passed into Bayesian optimization.  It takes in an array of four hyperparameters: a regularization coefficient, initial learning rate, weight decay parameter, and number of iterations to perform stochastic gradient descent.  The data, which is a collection of SMS messages to be classified as ""spam"" or ""ham"", is supplied manually, broken into training, validation, and test data (training and validation are used here).  A model is trained with the given hyperparameters on the training data, and the output is the classification error on the validation data.  This algorithm is designed to be used as a demonstration for Bayesian optimization.  Stochastic gradient descent shines most on larger scale data, but this job will finish in enough time to allow Bayesian optimization with a ""reasonable"" number of jobs to finish in a ""reasonable"" amount of time.  "
Simple Moving Average,"Returns the Simple Moving Average of a time series (http://en.wikipedia.org/wiki/Moving_average#Simple_moving_average). The lag time may be specified as a second argument, or, if it is omitted, will be defaulted to 3. This parameter determines how far in the past the average is calculated for each point. This is, if there is a lag of k, each point is replaced by the average of itself and the k points that proceeded it, or, if the point is within the lag factor of the the start of the series, with the average of it and all earlier points. This can be used to clean or smooth a time series."
Bayesian Optimization,"This algorithm performs Bayesian optimization to automatically set the hyperparameters of a machine learning algorithm for good performance on a training dataset (as measured by performance on a validation set).  Users must pass in three or optionally four things: the name of an Algorithmia algorithm that takes in ONLY the hyperparameters to be optimized and returns a number that should be minimized (for supervised learning tasks, this could be the error on the validation data after training on the training data with the hyperparameters supplied).  All other parameters must be supplied manually.  For many algorithms on the system that probably return either a trained model or predictions for input data, this will necessitate the writing of a short wrapper function.  the number of jobs Bayesian optimization should run (i.e. times to call the Hyperparameters --> Function Value to Minimize algorithm).the path to a data collection containing a config file.  This is a json file with the format config_<ALGORITHM_NAME>.json (not including the username of the algorithm creator), which for each variable must contain: the variable name, type (int, float, etc.), minimum value, maximum value, and number (if the user has, say, 2 variables with the exact same specifications, they can make this number 2 (otherwise it should be 1) instead of making duplicate entries).  optionally, the data collection path to write a results file (which will be titled results_<ALGORITHM_NAME>.dat) containing the results (function value and time taken) and hyperparameter settings of all jobs run for further analysis.  If this value is not supplied, by default it will write to the same collection as the config file.  After Bayesian optimization is performed, the results of the best job (job number, time, and function value--plus hyperparameter settings) will be returned in JSON format, while as mentioned above the results of all jobs will be written in a file to a data collection.Note about the sample input: probably more jobs should be run to achieve better results.  This number of jobs was chosen so it would finish quickly while still illustrating the format of the sample output.  "
WebImageSorter,"This algorithm takes a the following parameters:URL, Minimum image heightMaximum image heightMinimum image width Maximum image widthSort order (""desc"" or ""asc"", case-insensitive)It returns a list of images (within the minimum/maximum height/width) with their complete qualified URLs, height, width, and total pixels; sorted by total pixels, in the order specified (ascending or descending).Note: image dimensions are determined by the actual file image dimensions, disregarding the markup of ""height"" and ""width"" in the image tag."
SpeechRecognition,"This algorithm uses CMU Sphinx open source library to recognize speech in audio files that are uploaded to the Data API or Youtube videos that are licensed under Creative Commons. The models that are used to perform speech recognition are the latest Generic English models published by CMU on their Sourceforge website.  After doing a Youtube search with keywords on www.youtube.com, you can open filters and select Creative Commons to see what videos are available.The first input to the algorithm is the link to the media file (either a Data API url or a Youtube video url). There is an optional second input that points to a .tar.gz folder in the Data API that includes a new language model that you trained. The folder structure should be flat, including the .lm.dmp file (language model file) and the .dict file (dictionary file). The files that are required for the acoustic model should also be there (means, mixture_weights, etc).The output is a Json object that contains the following fields:text: The transcribed text of the audio filewordtimes: When the actual words were spoken (or silences)best3: Best 3 guesses for all of the phrases from the fileWarning: Please note that depending on the length of the media file, if you are using the website console and the video is longer than 4 minutes, it might time out."
Change Image Format,Converts an image to a specified format and returns the URL that the new image was posted to.
Profanity Detection,"The implements a rudimentary profanity detector based on string matching. The default usage just checks the input string to see if any of its substrings match a list of known obscenities and profanities, and returns a Map with identified profanities as keys and the number of times that profanity appeared as value. Our default dictionary of profanity includes about 340 words drawn from http://www.noswearing.com on 01/28/2015. Note that for compound profanities this may over count. This is not as straight forward to use as a boolean return value, but provides additional information that might be useful - for instance, a single use of the word ""damn"", or references to genitalia in a medical context, may not be considered objectionable, whereas stronger profanity or large volumes of profanity might be. For the maximum strictness, just check for an empty Map.Note that this is word-based only. It may miss some words, miss certain misspellings, or double entendres and other material that is offensive in context.The algorithm may be customized with your own list of objectionable words. To do this, send three inputs, the text to assets, a String[] with each entry corresponding to an objectionable word of interest, and a boolean (which we internally call customizedExclusive). If the latter is true, it means that the custom set of objectionable words is to be used exclusively. If it is set to false, the set will be combined with our default list of obscenities."
Image Similarity,"Given two URLs that point to images, this algorithm outputs a similarity score between 0 and 1, 1 for images that are perceived to be the same and 0 for images that are utterly unrelated.Algorithm makes use of OpenCV's keypoint extraction and extracts matches between the two images' keypoints. "
SiteMatcher,"This algorithm takes a web address and returns a summary relevant structural details of the site. Specifically, it is intended to identify the relevant pages on a hotel website, returning selected metadata and the relative importance of various pages as measured by PageRank. The returned information includes:url - the original address given, assumed to be the main page of the website.language - the language of the main page. See https://algorithmia.com/algorithms/nlp/LanguageIdentification for a guide to the returned language symbols.tags - important terms from the website.important pages - we check to identify which pages on the site are used for rooms, reservations/booking, photos, and location. For this we currently support English, Spanish, Italian, German, and Portuguese.pageRanks - an ordered list of pages on the site by page rank, the higher the rank, the more likely the page is to be important."
isPrime,"Checks to see  if input integer is prime. If prime returns True , if not Prime returns false."
URL 2 Data,"This algorithm will download a binary URL file to a specified local path. If no path is given, it will return the Base64 representation of that remote file."
NGramCommentGenerator,Outputs a randomly generated comment based on 3-ngram data trained on FCC comments on net neturality(Input is ignored)
Java Add One,Takes a long and adds one to itUsed in Algorithmia documentation to demonstrate API requests
Twitter Rate Limit info,"Used for other Algorithmia demos. Returns [remaining bandwidth,rate limit,seconds tuntil reset] for Algorithmia's twitter access credentials."
Face Detection Box,
Dynamo DB,Demo algorithm for integration with DynamoDB. Calls NudityDetection with files in an S3 bucket and writes results to a table.
Generate Recommendations,
ExtractSoundFromVideo,This algorithm takes in two Data urls. It fetches the video file from the first one and writes the extracted sound to the next one. Please make sure that the sound file url has a valid extension since this is taken into account during conversion.
aemsecurity,Check AEM security configurationThis is a private version of https://algorithmia.com/algorithms/bonfatti81/CheckMyAEM.If you want to use this version of the API please contact Coresecure: http://www.coresecure.com/
BackgroundSearch,"This algorithm can be used to search the web specifically for people.In contrast to the SearchEngineAggregator algorithm, this algorithm searches Facebook and Twitter for people instead of pages and tweets, respectively."
Download File From Bucket,"This algorithm downloads a file from an Amazon S3 bucket to a Data API collection. Please specify the path to the credentials file, the region of the bucket, the key of the file to be downloaded and the Data API collection URL that the file should be uploaded to as an input array to this algorithm. The first argument, credentials file, should be a file in the Data API that contains name of the bucket, access key ID and the secret access key.Warning: You may get an exception that says ""The bucket you are attempting to access must be addressed using the specified endpoint."" This means the region you have entered is incorrect for that specific bucket.The possible regions are:AP_NORTHEAST_1 AP_SOUTHEAST_1 AP_SOUTHEAST_2 CN_NORTH_1 EU_CENTRAL_1 EU_WEST_1 GovCloud SA_EAST_1 US_EAST_1 US_WEST_1US_WEST_2 "
SimpleVoteRingDetection,"This algorithm computes a collusion ratio for a given set of posts and user votes. For example, this could be used to flag user activity in social collaboration websites, such as Product Hunt / Hacker News / Reddit. It works by finding the ratio of common votes (among a group of users) over the sum of uncommon votes and total votes.Input FormatInput must be a CSV file with comma delimited numbers (without header). Each line represents the votes a single post received. The first number in every line is the post_id, while the remaining numbers are the user ids of the users who voted for that post/item.The input string can be a path to a local CSV file (data://), remote file (http://), or straight-up CSV content.The second parameter is the LIMIT, which is a double value between 0 and 1. This tells the algorithm to only return ratios that are larger than the specified limit.Output FormatThe output is an array of CollusionRatio. Each CollusionRatio element has the following properties:Ratio: see computation aboveCommonVotes: # of votes in common among given users, a.k.a. Votes(U,P)OutsideVotes: # of votes not in common among given users, a.k.a. Votes(U, -P)AllVotes: # of votes by all users made on any post, a.k.a. Votes(U, *)UserCount: # of users part of this computationUsers: array with ids of users involved in the computationPostId: the post id that was used to identify the group of users"
airpressure_parser,"Generates a plot for the air pressure of a given city.It retrieves the data from the site: http://w1.weather.gov/obhistory/KARB.htmlKARB is the city code for Ann Arbor in this link. Output is a json with a key as ""png"" and value as the base64 encoded png file generated."
SocrataOpenDataQuery,A pass at creating an algorithm that uses soda-java (Socrata's Java SDK) to query data and pull down data.Â My initial thoughts are to have another algorithm that callsÂ https://algorithmia.com/algorithms/marksskram/SomeStats to calculate summary statistics for each number column.
Download Youtube Vid,This algorithm downloads the given Youtube video to the DataAPI. The video has to be licensed exclusively with Creative Commons license. The first link is the Youtube link and the second link is the target Data url.
Curl,This algorithm pulls a webpage via URL and returns as text
MachineTranslateWordByWord,"Translates word by word from one language (auto detected) to a target language. Accepts input as a string of text (space separated) to translate, and a string as the language (either language name in English, or language code). Valid languages listed below:Language NameLanguage codeAlbaniansqArabianarArmenianhyAzeriazBelarusianbeBosnianbsBulgarianbgCatalancaCroatianhrCzechcsChinesezhDanishdaDutchnlEnglishenEstonianetFinnishfiFrenchfrGeorgiankaGermandeGreekelHebrewheHungarianhuIcelandicisIndonesianidItalianitJapanesejaKoreankoLatvianlvLithuanianltMacedonianmkMalaymsMaltesemtNorwegiannoPolishplPortugueseptRomanianroRussianruSpanishesSerbiansrSlovakskSlovenianslSwedishsvThaithTurkishtrUkrainianukVietnamesevi"
WMD,"Implementation of techniques from Kusner et. al, 2015: From Word Embeddings to Document Distances.  Represents documents as list of word vectors using the celebrated word2vec embedding to represent the words in the documents.  Implements heuristic distance metrics described in the paper (word centroid distance and relaxed word mover's distance) to compute distances between documents for k-nearest neighbor classification.  (The original paper showed that rWMD outperformed a wide variety of other ways of representing text data on a number of benchmark document classification tasks).  This algorithm has a preprocess mode: takes in a text file where each line is a label followed by the text of a document on the same line (with the two separated by a tab).  Additionally, a user must pass in the path to an ID map file in a data collection.  This file will hold a map from (human-readable) IDs for documents (e.g. ""sports"", ""politics"") to numeric algorithm-readable labels (e.g. 0, 1).   If a file exists at this path, the labels will be preprocessed using this ID map.  (So test data can have its IDs converted the same way that the training data was, which is important.)  If no file exists at this path, one will be formed when preprocessing the data and written to that path.  Currently, the preprocess mode assumes that even test data has tab-separated labels as well as the document text on a line, though the labels need not be used when testing (unless the user wants to compute the accuracy for test format).  So if test data is unlabeled, random labels may be added to each line of the data file to satisfy the required data format.  When the data is preprocessed, it will be written to the same data collection as the original data and a message to this effect will be returned.Additionally, this algorithm may be run in performance mode, where it is used for kNN classification.  Here the user must pass in training data (in a pickle file as returned by the preprocess mode of this algorithm), training labels, test data, and the number of neighbors to use for kNN classification.  Optionally, the user may pass in the test labels, in which case the program will return the accuracy.  Otherwise, a message informing the user that the predicted labels for the test data have been written to the data collection will be returned (either way, the predicted labels will be written to the data collection).  "
SearchEngineAggregator,"A search engine wrapper that aggregates results from different search engines and adds benchmarks for all at the end. Basic use: Just input the query and the algorithm will search all of the supported engines and return the top 10 results!Alternative use: Format inputs as a simple Json with clients, query and the desired number of results, as in the example below.Clients: Specify engines desired with a space in between (e.g. ""google facebook"") or try ""all"" to see all of them.Query: The keyword that you would like to search on the clients.NumResults: The number of results that you would like to retrieve.Currently supported enginesGoogleTwitterBingWikipediaDuckDuckGoFacebookHint: Try using ':)' or ':(' with your Twitter search query to retrieve Tweets with these sentiments!"
CoordinatesToTimezone,"This algorithm takes a latitude-longitude pair and outputs the timezone information for that location.Depending on the time of the year and the matching timezone, the ""name"" property will be modified to represent the potential current daylight saving time (i.e: Standard vs Daylight Time), e.g.:{""id"":""America/Los_Angeles"",""name"":""Pacific Standard Time"",""offset"":-28800,""offset_dst"":0}{""id"":""America/Los_Angeles"",""name"":""Pacific Daylight Time"",""offset"":-28800,""offset_dst"":3600}To get the current local time, clients should perform the following operation:local_time = UTC_time() + offset + offset_dstBoth ""offset"" and ""offset_dst"" are provided in seconds.In locations where timezones are not applicable, UTC is returned:{""id"":""UTC"",""name"":""Coordinated Universal Time"",""offset"":0,""offset_dst"":0}"
Auto Correlate,Computes the auto-correlation of a time series
Image Foreground Extraction,Takes in an image (in the form of a Data API URL or byte[]) and extracts the foreground after detecting the face in the image. A single face should be present in the taken photo. The foreground is returned in the binary format.
Nudity Detection Binary Helper,
CURL2DataAPI,
Email Validator,Simple verifier that checks if input was a valid formed email address and returns True or False.Follows RFC Internet Message Format for validityhttps://tools.ietf.org/html/rfc2822
FFTAutocorrelation,"Takes a time series and returns the autocorrelation plot of that time series as an array. The size of the entry at i of the autocorrelation plot is a measure of how similar the signal is to a lagged version of itself. Note that the 0th entry of the output is the correlation of the signal with itself (no lag) and is always the largest value. Nonzero values at lat 1 are indicative of a linear trend. Beyond 1, positive values index i indicate a periodic signal with period i.  Thus, the most interested entry is that with the second largest value - specifically, the index of this entry is the period of the strongest seasonal trend in the signal. It is recommended that the signal have linear trends removed (as by application of the /TimeSeries/LinearDetrend algorithm) before applying this algorithm, as linear trends make the results of autocorrelation more difficult to interpret. See http://en.wikipedia.org/wiki/Autocorrelation for more information.This is based on an implementation of the Fast Fourier Transform (in this the real Fourier Transform in the JTranforms library - https://sites.google.com/site/piotrwendykier/software/jtransforms) Note that there are many different conventions used in signal processing, so be careful as the ones chosen here may not be the ones you need. "
Named Entity Recognition,"Identifies named (PERSON,LOCATION,ORGANIZATION,MISC) and numerical (MONEY,NUMBER,DATA,TIME,DURATION,SET) in text, outputs the text of each entity along with its identifier. Specifically, this algorithm takes a string as input, splits it into sentences, finds the named entities in each sentence, and for each sentence, outputs a list of the named entities along with their type as a two-element string array.For more information, please refer to http://nlp.stanford.edu/software/corenlp.shtml or Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60. "
Movement Detection Demo,"Usage:[""data://opencv/MLabsVideos/0018850277aa-defaultPrimary.flv"", 0]Input 1: URL to the video on the Data API (http://algorithmia.com/data).Input 2: If 1, frames of the video are flipped. If 0, no flip. It is important to give the correct value for this parameter because the algorithm adjusts the background subtraction parameters according to the position of objects in the frame and orientation affects this vastly.Output shows where the file that contains the results were written. The text file is going to contain ""Frame# [Rectangle1,Rectangle2,...,RectangeN]"" on each line where rectangles are going to have their x, y, width and height separated by spaces.Warning: This algorithm performs best on videos that resemble security video footage of humans moving through a stable background. It may not perform well on other types of video."
Make Buzzwords,Manufactures Buzzwords in the Cloud
Clickstream,"Analyzes clickstream data, looking for the most ""interesting"" transitions necessary to reach a given ""goal state""."
ParallelMSTinC,"This algorithm takes as input a Data API URL to a GML file that contains the input graph. It then computes the weight of the minimum spanning tree using Parallel Kruskal's algorithm.Based onBlelloch, Guy E., Fineman, Jeremy T., Gibbons, Phillip B. and Shun, Julian. ""Internally deterministic parallel algorithms can be fast.."" Paper presented at the meeting of the PPOPP, 2012."
RecoTest,
Recommend Github,"Given an existing public Github repo, recommend others based on similar content. First input is the Github username (e.g. ""rails"") and the second input is the name of the repository (e.g. ""rails"")."
Generate Trigram Frequencies,Generates trigram frequencies from given text (given as array of Strings). Needs beginning and end tags in the data. Writes the model to the Data API url that is passed in as the fourth parameter. 
Pixelate Face,"This algorithm uses Face Detection to detect any faces in an image and pixellates them. The first argument is the source image and the second is the destination. The third argument is the amount of pixelation, the higher this parameter, the more pixelated the face."
Geographic Spectral Clustering,"Spectral clustering for geographic (lat/long) data. Input is json for a python dictionary containing keys""data"" - whose value is a list of lat/long pairs""numClusters"" - whose value is an integer denoting the number of clusters that the data will be partitioned into.The output is an ordered list containing the cluster label of each point.We use inverse distance (in km, as calculated by the Haversine formula)  for similarity, so close points are more similar. Any points within about a meter of each other are counted as the same point. We cannot guarantee the accuracy of Haversine distances on very nearby points, so be careful. The advantage of spectral clustering is that is does not depend on cluster centers, like K-means, and so can resolve clusters that are naturally non-convex. This is based on scikit-learn's spectral clustering implementation. Read more about spectral clustering here"
Edit distance,"This algorithm finds editing distance of two given strings, which is based on insertions, deletions and substitutions. The algorithm takes 5 arguments:- first string;- second string;- price of insertion;- price of deletion;- price of substitution;The algorithm returns one integer, which corresponds to minimal editing distance price using given prices for operations from input.Time complexity O(n*m), space complexity O(n), where n and m are minimal and maximal sizes of input strings respectively.If all prices are set to 1, then the algorithm calculates Levenshtein distance, i.e. merely number of required editing operations in order to convert first string into second one. More details:http://en.wikipedia.org/wiki/Edit_distancehttp://en.wikipedia.org/wiki/Levenshtein_distanceUsage: natural language processing, bioinformatics..."
Maze Generation,"This is an implementation of Eller's Algorithm (http://www.neocomputer.org/projects/eller.html) for generating a square or rectangular 2D maze. It takes two integers - one for width and one for height - and returns a two-dimensional array of cells indexed by [column][row]. Each cell is in turn an array of four boolean values that, when true, indicates a wall. This array is indexed as follows: [north wall, east wall, south wall, west wall]."
Base64-Data Converter,"Use this algorithm to convert from base64 to DataAPI files and vise versa.Base64 to Local DataInput: content (String), path (String)Description: will convert the base64 content to binary and will save it to the specified path.Local Data to Base 64Input: path (String)Description: will read file in path and return a base64 representation."
Sentiment_history,This API returns news sentiment history for all equities as listed on InfoTrie
For Each,"An algorithm to run another algorithm on a list of inputs.This algorithm takesThe name of an algorithm (eg- ""/util/Echo"")A list of inputs which will map each input to the output of the given algorithm"
Classifier,"The file format given must contain 2 columns. The first column should contain sales, and the second columns should contain the future loyalty of the customers.Sales is given by a double, and loyalty by 0 (unloyal) or 1(loyal). "
quicksort,A quicksort algorithm.
LogisticRegression,"Train a Logistic Regression model and use it to predict unknown classifiers.  You supply data of arbitrary number of features to train the model.  Once trained, the model can be used to classify test data.  The predicted results, as well as an accuracy measure, are returned to the user.  The input must be formatted as a dictionary {'X_train' : [x_train_data], 'y_train' : [y_train_data], 'X_test'  : [x_test_data]} X_train should be a matrix of size (n_samples, n_features), y_train should be a list of size (n_samples,), X_test must have the same number of features as X_train"
FindMinMax,Finding min and max in 3n/2 (reference CLR Introduction to Algorithm)
Index Json for Clustering,"This algorithm can be used to cluster and visualize Socrata data presented as a CSV file. In theory, and CSV file that you would like to cluster and visualize according to a ""title"" and latitude and longitude can be fed into this algorithm. The resulting map is written to your Data collection, given as the last input to this algorithm.The example in the sample input below clusters Assault events taken from raw Seattle crime data.Takes as input:Dataset CSVcolumn index of crime typecolumn index of latitudecolumn index of longitudecrime type to clusternumber of clustersdata api destination"
customerPredictionAlgorithm,
Pinky,For when you want to know if Pinky is pondering what you're pondering...Go ahead... Ask Pinky...
Custom Map From Json,"Given an array of Double arrays that are of the form: ['latitude', 'longitude', 'cluster number'], draws these points on a map. The last value defines the color of the pinpoint."
BirthDeathProcess,
HMM,This algorithm let you generate a sequence of observations from an HMM.Input is:starting state probability distributiontransition functionobservation functionsize of the sequence to generate
SiteTags,"Scrapes a site for links, and then runs auto-tagging on the crawled sites, and aggregates the tags. "
SimpleKNN,"This is a simple KNN classifier which accepts a given number of training data points, classes for those points and sample points to evaluate.Inputs:int k - the number of points to be counted, default 5
String distanceCalculation - the distance calculation to use, currently only supports ""squaredEuclidean"" which is the default.
String votingType - the way to calculate the class given the classes of the K closest points, currently only supports ""majority"" voting, which is the default.
ArrayList<double[]> data - the training data
int[] classes - the class for each data points
ArrayList<double[]> points - the sample points"
Get Twitter Friends,"Takes a screenName as input and returns an array of TwitterProfile as output.TwitterProfile has the following: handle, name, description, image_small, image_large, followers, friends, favorites.At the moment the algorithm is not setup to complain about Twitter rate limit, so you might get a weird/unfriendly exception if that happens. "
AdaptiveSpeechRecognition,"This algorithm uses CMU Sphinx open source library to recognize speech in audio files that are uploaded to the Data API or Youtube videos that are licensed under Creative Commons. The difference from /sphinx/SpeechRecognition is that this algorithm makes an attempt to adapt the recognition stats to the specific speaker in the media file. Note that this causes this algorithm to run twice as slow. The models that are used to perform speech recognition are the latest Generic English models published by CMU on their Sourceforge website. The first input to the algorithm is the link to the media file (either a Data API url or a Youtube video url). There is an optional second input that points to a .tar.gz folder in the Data API that includes a new language model that you trained. The folder structure should be flat, including the .lm.dmp file (language model file) and the .dict file (dictionary file). The files that are required for the acoustic model should also be there (means, mixture_weights, etc).The output is a Json object that contains the following fields:text: The transcribed text of the audio filewordtimes: When the actual words were spoken (or silences)best3: Best 3 guesses for all of the phrases from the fileWarning: Please note that depending on the length of the media file, if you are using the website console, it might time out if the input is longer than 4 minutes."
Integer Programming,"This is an algorithm for integer programming (also known as integer linear programming) built on the JaCoP Constraint Programming Solver (https://github.com/radsz/jacop). Read more about integer programming at https://en.wikipedia.org/wiki/Integer_programming. It's arguments areA String array of variable namesAn int vector c defining the cost of the returned solution via c^(T)x.An int array of lower bounds for the corresponding variablesAn int array of upper bounds for the corresponding variablesAn int matrix (int[][]) that defines the matrix part of the linear constraintsA String array denoting the nature of each constraint, either ""<"","">"",""<="", or "">="".An int array (int[]) defining the right hand side of the linear constraints.The output is a Map<String,Int> containing each variable name as key and its value in the found solution as value.  Note that this implementation minimizes c^(T)x and uses branch-and-bound. To maximize, change the signs of the integers in c.  For more information consult the JaCoP documentation at http://jacopguide.osolpro.com/guideJaCoP.html#x1-510006."
isPrime,"Using the fact that all primes are in the form c#k+i, where c# is the c-th primordial and i represents the numbers that are coprime to c# the algorithm implements primality testing using Trial Division using divisors only in the form 210k+i, which makes it several times faster.Works for all numbers smaller than 9223372036854775807 (long max value). Sample input with 2147483647 also known as the 8-th mersenne prime."
Lemmatizer,"Maps all words to their canonical forms for easier analysis. For example, the canonical form of ""running"" and ""ran"" is ""run"". This uses the CoreAnnotations.LemmaAnnotation annotation from Stanford CoreNLP. For more information, please refer to http://nlp.stanford.edu/software/corenlp.shtml or Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60. "
SimpleEuclidean,"This is a simple algorithm which performs the euclidean distance calculation on input arrays.  Input: data is a set of arrays to perform the euclidean distance calculation on, and squared is a boolean which if true uses squared euclidean distance instead of euclidean distance. The default value is false if left unspecified.



In: 
""{data:[   [  [1,2,3] , [2,3,4]  ],   [  [1,2,3] , [2,3,4]  ]    ]}""

     


Out:
[1.7320508075688772,1.7320508075688772]



"
Find Polygons In Image,"Searches for regular polygons in an image.Given an image and a number of sides, searches for regular polygons with that number of sides in am image (for example 3 for triangles, 4 for a square, etc..)"
Affinity Analysis for Market Basket Recommendation (FP-Growth),"Affinity analysis is an analytical technique that aims to discover relationships between activities and preferences that pertain to specific individuals. Based on recorded information, after the analysis, future behaviour can be statistically predicted. For a general overview see http://en.wikipedia.org/wiki/Affinity_analysis. Specific applications include clickstream analysis and market basket analysis.One important area of application is market basket analysis, which has widespread use in planning promotions, designs and sales strategies. Market basket analysis is necessarily somewhat open-ended, but one of the more useful angles of attack is the extraction of association rules.This API uses this FP-Growth algorithm implementation from the Weka library.Input: [url, options]The program takes a DataAPI url to a file with one session per line. A session represents the entities that were bought/used/visited in a single recorded event. This could be the urls seen in a given browsing session or items bought in a single visit to a store. For example:bread milk eggsbeer diapersbread bottled_water hot_dogs lemonadeThe items are not ordered and there is no customer identification. The items are separated by whitespace, and the only constraint on the format is that items must be uniquely identifiable by the string and the string may contain no white spaces.You can also use a CSV file instead, in which values can contain any character except comma:bread,milk,eggsbeer,diapersbread,bottled water,hot dogs,lemonadeYou can customise the behaviour of the FP-Growth algorithm by supplying your own custom parameters:-I <max items> The maximum number of items to include in large items sets (and rules). (default = -1, i.e. no limit.)-N <require number of rules> The required number of rules. (default = 10)-T <0=confidence | 1=lift | 2=leverage | 3=Conviction> The metric by which to rank rules. (default = confidence)-C <minimum metric score of a rule> The minimum metric score of a rule. (default = 0.9)-U <upper bound for minimum support> Upper bound for minimum support. (default = 1.0)-M <lower bound for minimum support> The lower bound for the minimum support. (default = 0.1)-D <delta for minimum support> The delta by which the minimum support is decreased in each iteration. (default = 0.05)-S Find all rules that meet the lower bound on minimum support and the minimum metric constraint. Turning this mode on will disable the iterative support reduction procedure to find the specified number of rules.-transactions <comma separated list of attribute names> Only consider transactions that contain these items (default = no restriction)-rules <comma separated list of attribute names> Only print rules that contain these items. (default = no restriction)-use-or Use OR instead of AND for must contain list(s). Use in conjunction with -transactions and/or -rulesOutputAn array of rules, where each rule contains:confidence: the proportion of the examples covered by the premise that are also covered by the consequence. Alternatively, this can be described as the probability that a rule is correct for a new transaction.lift: confidence divided by the proportion of all examples that are covered by the consequence. This is a measure of the importance of the association that is independent of support.leverage: the proportion of additional examples covered by both the premise and consequence above those expected if the premise and consequence were independent of each other.conviction: another measure of departure from independence.premise: the 'premise' part of the rule.consequence: the 'consequence' part of the rule.premiseSupport: the number of transactions (sessions) in the data set which contain the premise items.consequenceSupport the number of transactions (sessions) in the data set which contain the consequence items.In the context of Market Basket Analysis, the rules should be interpreted like this: if a user buys an item in the premise item set, then the user will likely buy the item in the consequence item set too.The sample data set is taken from here."
Keyword Analysis,
List Objects,"This algorithm lists objects with a given prefix from Amazon S3. Please specify the path to the credentials file, the key of the file to be downloaded and the Data API collection URL that the file should be uploaded to as an input array to this algorithm. The first argument, credentials file, should be a file in the Data API that contains name of the bucket, access key ID and the secret access key. The second argument should be the prefix of keys you would like to list in the bucket."
Train Face Recognizer,"Trains a face recognizer from a labeled set of faces. This is a wrapper for the FaceRecognition algorithm.  The FaceRecognition algorithm additionally requires a user to specify a ""train"" mode, so this wrapper makes things slightly more convenient. Users must pass in a list of JSON objects, one for each image.  Each image JSON object must have a ""ID"" field with an ID (or label) saying whose face is pictured.  It must also have a ""Path"" field specifying the image URL (e.g. the path to the image in an Algorithmia collection).(Note: if an image has multiple faces, all faces will be detected and used in training, but each face will have the ID specified to the image as a whole.  For instance, in a multi-face picture of the Smith siblings, each face will be used for training and each will have the image ""Smith siblings"".   It is difficult to specify labels more precisely than that, so that is the greatest level of precision this algorithm supports.)After preprocessing the image data by using an OpenCV face detector to detect faces, this program trains a face recognizer using Local Binary Pattern Histograms, also as implemented in OpenCV.  This face recognizer will be saved in XML format to a data collection of the user's choosing, along with a file of the same name (but with an additional _idList tag and a .txt extension) that maps people's IDs to numeric labels used internally.  For use when updating the model or classifying with it, both files must have the same name and must be in the same directory.Users must also specify an Algorithmia data collection to which a face recognizer can be written as well as a file name for the face recognizer.  This is done by specifying the full path that the .xml face recognizer file will have when saved (including the file name and a .xml extension).  If a file with this path exists, the model in that file will be updated with the new training images.  Otherwise, a new model will be created from scratch.  Upon successful completion, a user will receive a notification: ""Successfully completed!""  In the specified directory will then be the face recognition model (face recognizer and ID list).  "
Covariance,"Calculates the covariance matrix cov(A) of a input matrix A.Possible inputs are:- A- [A, N]A is a n-times-d-matrix and N the type of covariance matrix. If N=0 (default), then the output is the unbiased estimate of the covariance matrix normalizing by n-1. If N=1, then the algorithm normalizes by N and produces the second moment matrix of the observations about their mean. "
NegativeWeightCycles,"Detects negative-weight cycles in a complete, directed graph, using the Floyd–Warshall algorithm.No guarantees are made about whether all negative cycles are detected, or which cycles, but that at least one will be detected if it exists. To improve on this lack-of-guarantee, see http://www.sciencedirect.com/science/article/pii/S0166218X01002013."
Tokenize By Sentence,"Takes in a string as input, splits into sentences and tokenizes each sentence, placing each token into a String array. This set of arrays is returned as a list. The default setting is English, splitting sentences according to the model en-sent.bin and tokenizing according to en-token.bin (both located in data://ApacheOpenNLP/models/).To use this with other sentence and tokenization models, call[<input text>,""data://ApacheOpenNLP/models/<sentence detection model>"",""data://ApacheOpenNLP/models/<tokenizer model>""]For more information visit http://opennlp.apache.org."
Text To Speech,"This algorithm takes in a text, an output data api url and an output file name and writes the text as a speech wav file to this location. "
Haar Wavelet,"This algorithm performs the Daubechies D4 Wavelet Transform (or inverse transform) on a one dimensional signal.

This algorithm expects a signal which whose length is divisible by 2, and will perform the transform for as long as the length can be divided by 2, while still being evenly divisible.  Unless a max number of iterations is specified by an int before the input signal.Example Input[1,[1,2,3,1,2,3,4,0]]Example Output{""transform"":[2.1213203435596424,2.82842712474619,3.5355339059327373,2.82842712474619,-0.7071067811865475,1.414213562373095,-0.7071067811865475,2.82842712474619],""i"":1}
Inverse Input
[[5.656854249492379,-0.7071067811865481,-0.49999999999999994,0.49999999999999994,-0.7071067811865475,1.414213562373095,-0.7071067811865475,2.82842712474619],3]


Inverse Output
[0.9999999999999992,1.9999999999999991,2.999999999999999,0.9999999999999992,1.9999999999999993,2.999999999999999,3.9999999999999996,-3.14018491736755e-16]
(rounded)
[1,2,3,1,2,3,4,0]
"
CacheTest,"Returns a random number, but with caching enabled.By running this algorithm repeatedly on the same input, this gives some idea of how the caching layer is working / distributing jobs to new workers."
point_inside_polygon,"If a point is inside of a polygon, return true and false in other case.The entry are numbers separated by commas, that represent the coordinates of the vertices of the polygon, arranged in one sense. The last pair of coordinates belong to the required point .Note: if a point is in the sides, it is considered out."
ReVerb,
BIPM-final,
Conway's Game Of Life,"This algorithm computes Conway's Game of Life.  The basic input is: [ boardWidth , boardHeight , ListOfLiveCells ]
Which will return the list of live cells in the next iteration of the game.  The list of live cells expects List(int[]) where each int[] in the list is a pair of int values corresponding to a position on the board.  The other inputs are [ ... , ... , ... , doesBoardWrap , numberOfIterations , returnAllIterations, returnEntireBoard].


Name
   
Type
   
Description

 

doesBoardWrap
   
boolean
   
Determines if the edges of the board wrap to the opposite side.


numberOfIterations 
   
int
   
The number of iterations to run.


returnAllIterations
   
boolean
   
If all iterations should be returned.


returnEntireBoard
   
boolean
   
If the return type should be a boolean[][] of the board.



If returnEntireBoard is used returnAllIterations is ignored, and if set to false will only return the last instance of the board, while set to true will return all iterations of the board."
Java Binary In And Out,Example Java algorithm that accepts binary input and echos it as binary output
Find Object In Image,"Given an image of an object, searches for that object in an image.Returns if the object was found and the center of the object if it was found."
BIPM,
FileExists,
SocrataOpenDataAnalysis,"SummaryThe goal here is to combine my first two algorithms: /marksskram/SocrataOpenDataQuery which queries data given a dataset ID, domain, and query (an empty string is equivalent to a SELECT *) and /marksskram/SomeStats which calculates mean, variance, median, and more.Â See https://gist.github.com/marks/5c1c41df93b9cfa0f681 for a pretty version of the JSON output for the sample belowFeedback is always welcome. You can tweet me at @Skram or email me at mark.silverberg@socrata.comExample Inputs to TryThe top 100 White House salaries (""salary"" column)[""open.whitehouse.gov"",""rcp4-3y7g"",""select * order by salary desc limit 100""]City of Chicago owned property (""sq_ft"" column)[""data.cityofchicago.org"",""aksk-kvfp"",""select * where ward = 20""]Known Issues- Will probably break if you do not do a ""select *""- Need to document what happens with null and non-numeric values"
JSON Diff,
URL-2-PNG,"Functionality:This algorithm takes a screenshot of the input URL with a 1416x732 viewport and returns the URL of the screenshot PNG.  The PNG will remain available at the returned URL for the next hour.Status:This algorithm should not be used for production purposes at the moment, as it relies upon a PhantomJS web service on one of my personal (virtual private) servers.  My plan is to migrate this algorithm into Algorithmia's scalable environment once they have incorporated PhantomJS, allowing for production usage.Processing Time:This algorithm typically takes about 10 seconds to run.  I have the timeout set at 30 seconds, as something is probably went wrong with the render, if the web service hasn't provided a response by then.Known Issues: Flash and WebGL are not supported.PhantomJS appears to have trouble establishing an SSL handshake with sites using certain older cryptography.  Please let me know in the comments, below, if an HTTPS page that you are trying to capture, fails to render."
RandomForest,"This is the RandomForest classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/RandomForest.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
piCalculation,Pi calculated using Nilakantha series.
Recommendation,"OverviewThe algorithm encapsulates most of Mahout's recommendation capabilities. It takes as argument a string array of length 2, the first entry being the url of the appropriately formatted data file, the second a recommender option flag, ""user"" for user-based recommendation, ""item"" for item-based recommendation, and ""matrix"" for a matrix factorization based recommendation (currently ALS with default parameters). Eventually we will expand this to allow different parameters and options for neighborhood and similarity functions.One of the more convenient datasets is the MovieLens 100k dataset at http://www.grouplens.org/system/files/ml-100k.zip. More detailed documentation can be found at https://mahout.apache.org/users/recommender/recommender-documentation.htmlUser-based recommendationIn this mode, the recommender returns a set of item recommendations for each user, along with the predicted rating for the item. Think of this as generating recommendations based on user similarity. Item-based recommendationThis returns, for each item, a list of similar items. Matrix Factorization recommendation (with Alternating Least Squares)An alternate and often more effective approach to recommendation that can be useful for uncovering latent explanatory factors. We plan to expose more of this soon, meanwhile, it acts as a user-based recommender.Upcoming FeaturesWe plan to add ALS on implicit feedback and weighted matrix factorization soon. "
PDFToText,"This algorithm takes a PDF file with two coordinates and returns the text bounded within the rectangle of the coordinates. Input #1 (url, page, x1, y1, x2, y2):URL for PDF filePage number - choose 0 for all pagesCoordinate X1 - top left of the rectangleCoordinate Y1 - top left of the rectangleCoordinate X2 - bottom right of the rectangleCoordinate Y2 - bottom right of the rectangleInput #2 (url, page, space, x1, y1, x2, y2):Same as above with an additional space parameter to specify the expected width of whitespace character. The default is 2.0.Output:Always an array of strings, one element for each page.Sample Document:https://algorithmia.com/v1/data/ANaimi/PDFtoText/sample.pdf"
DateTime Calcutator,"Like a alarm Calculate the next up DateTime for an initial DateTime, you can specify the valid days and interval (for example every 3 hour or every 45 minutes or combine both).The system return an array with the DateTimes for execution.the imput is:[[initHour], [interval], [lowerBound], [upperBound], [days], iterarions]initHour: is the Start Time, for example at 8:00 [8, 0]interval: is the repeated time for example repeat every 1:30 hours [1,0]lowerbound: when the alarm init for example sinse 7:00AM [7,0]upper bound: same as lower bound the valid until time like 23:59  [23, 59]days: the week days starting by monday must be a boolean [true, true ,true , true, true, false, false]iterations:  is the window beyond to calculate time.other options to call are:        [[initHour], [interval], [lowerBound], [upperBound], iterarions]    [[initHour], [interval], iterarions]    [[initHour], iterarions]    [initHour]"
RandomGreetings,"Generate Random Greetings!Just provide a ""Name"" and get a random greeting generated for you! :-)"
Gregory-Loredo Algorithm,"Computes the Gregory-Loredo algorithm on a list of arrival times This function computes the likelihood of a set of arrival times originating from a periodic process rather than constant rate (Poisson) process (e.g. background noise).based on Gregory, P. C. and Thomas. J. Loredo, 1992, ""A New Method For The Detection Of A Periodic Signal Of Unknown Shape And Period"" in The Astrophysical Journal, Astrophysical J., 398, p.146InputsTlist (required)- a list of arrival timesm_max  (optional, default m_max=12) - number of phase binsw_range (optional, default min(20,N/10)*pi/T, N= number of arrival times, T= max(Tlist)) - frequency range to scan for periodicityni (optional, default ni=10) - number of bins for numerical integration of the phase from 0:2pi/mparallel (optional, default parallel=False) - parallel execution flagOutputsO_period - odds ratio for periodic process p_period - probability of periodic process (0<=p_period<=1)m_opt - optimal bin size S - spectrum for m_optw - frequency range of Sw_peak - most likely frequencyw_mean - mean frequencyw_conf - 95% confidence interval of frequency  Other sample inputs: {""Tlist"" : [80,921,2281,5831,6095,7703,8205,8761,11010,11095,14201,22531], ""m_max"" : 5}{""Tlist"" : [80,921,2281,5831,6095,7703,8205,8761,11010,11095,14201,22531],""m_max"" :5, ""ni"" : 20}"
NGramsFromSite,"Return list of N grams from a particular URL, after the text has been extracted from it."
Boyer-Moore-Majority,
Body Detection,"Uses a pretrained model to detect bodies in a given image. Send your input as a url to a picture that is hosted on our data API, and the output is the same image, with rectangles around the detected bodies, written to the same collection. A text file that contains all the coordinates of the rectangles is also output to the same collection for convenience."
ConvertsClassifier,
Smile Detection,"Uses a pretrained model to detect smiles in a given image. Send your input as a url to a picture that is hosted on our data API, and the output is the same image, with rectangles around the detected smiles, written to the same collection. A text file that contains all the coordinates of the rectangles is also output to the same collection for convenience."
Scrable Algorithm,"Scrabble Anagram FinderGiven an input of ""tiles"" you can find every legal english word you can make with that combination.For example, if you have the letters ""atme"" it will return ""meat"",""meta"",""tame"",""team""This is perfect for any word game algorithm that needs to know all legal permutations. In future I may add a bigger dictionary or let the user define a dictionary."
ConvertCase,"ArgumentsAction - A StringInputs - Either a String or a String ArrayReturnString - if the input was a StringArray - if the input was an ArrayActionsSentence - Capitalize first letter of each sentence.Word - Capitalize First Letter Of Each Word.Random - RANdoM CASe fOR each LEtTer.Upper - ALL UPPER CASE.Lower - all lower caseActions are parsed by their first letter; therefore, it is sufficient enough to do:[""u"", ""uppercase!""]to get""UPPERCASE!""
toUpper, and toLower work as well!"
Addition,
DoWordsRhyme,"Takes two English words and returns true if they rhyme, false if not. Certainly not perfect yet, but works for many cases."
WorldLocations,"An API to retrieve a list of World locations.Currently it supports:""USA"": All US states""Australia"": 7 Australian states""TopWorldCities"": [""Los Angeles"",""New York"",""Chicago"",""San Francisco"",""Houston"",""Philadelphia"",""Phoenix"",""Pittsburg"",""San Diego"",""San Antonio"",""Austin"",""Dallas"",""Detroit"",""Portland"",""Miami"",""Utah"",""Seattle"",""Boston"",""Washington D.C."",""Las Vegas"",""Denver"",""Singapore"",""Jakarta"",""Melbourne"",""Sydney"",""HongKong"",""Shanghai"",""Beijing"",""Taipei"",""Tokyo"",""Seoul"",""Mumbay"",""Cairo"",""Jerusalem"",""Beirut"",""Stockholm"",""Copenhagen"",""Frankfurt"",""Munich"",""Stuttgart"",""Viena"",""Rome"",""Milan"",""Paris"",""Madrid"",""Barcelona"",""Porto"",""London"",""Liverpool"",""Manchester"",""Dublin"",""Glasgow""]
"
FordFulkersonMaxFlow,"Ford-Fulkerson (BFS-based aka Edmonds-Karp) Computes a maximum flow given a weighted directed graph. Uses the scaling variant of Edmonds-Karp for stronger polynomial bounds (c.f. Network Flows by Ahuja, et al.).Takes as input a list of node IDs, a list of edges, the source ID, and the sink ID. The edges should be input as a dictionary of the form:

{""start"": <start ID>, ""end"":  <end ID>, ""capacity"": <integer>}
Returns as output a dictionary with keys ""flow"", an integer denoting the total flow, and ""edges"", a list of edges in the form:
{""start"": <start ID>, ""end"":  <end ID>, ""flow"": <integer>}"
Eye Detection,"Uses a pretrained model to detect eyes in a given image. Send your input as a url to a picture that is hosted on our data API, and the output is the same image, with rectangles around the detected eyes, written to the same collection. A text file that contains all the coordinates of the rectangles is also output to the same collection for convenience."
Page Recommender,"Crawls the domain and returns, for each crawled page, a list of similar pages within the domain, where similar pages are those that share keywords that are otherwise rare within the pages of the domain. Input isA URL.The max number of pages to crawl (in breadth-first order).The maximum number of keywords to generate per page.The maximum number of similar pages to return per page.Output is an adjacency list of URLs in the form of a Map<String,Set<String>>. This algorithm is simply a composition of the following algorithms:BreadthFirstSiteMapUrl2TextKeywordsForDocumentSetKeywordSetSimilarity"
Traveling Salesman,"The travelling salesman problem (TSP) asks the following question: Given a list of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city? It is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science."
meanturns,"Return the expected number of turns to witness at least P% of a uniformly distributed set of N items.Input  is given as [N, P]"
MergeSort,MergeSort of integer array.
Cached Curl,"Curl a URL and cache the resultCurl a URL and store the result in a data collection. If there is a cached copy already, check the time that it was cached and only perform the curl if the cache is older than the specified `cache duration`.Input consists of 3 parameters:URL - the URL to curlCache Duration: Number of seconds to consider the cached copy validCollection: Data URI where you cache results (requires write access)"
SequentialMST,This algorithm takes as input a Data API URL to a GML file that contains the input graph. It then computes the weight of the minimum spanning tree using Kruskal's algorithm.
CombinatorialAuction,
Burrows-Wheeler,"An implementation of the Burrows-Wheeler Transform which accepts either a String containing the value to be transformed, or a String,int to be reversed.


Sample Input
    
Sample Output

[""nnbaaa"",3]
    
""banana""


"
SomeStats,Give this algorithm an array of numbers and it will tell you the:minmaxvariancestandardDeviationmeanmediansumn (number of items)This algorithm uses the The Apache Commons Mathematics LibraryInspired and some code borrowed from https://algorithmia.com/algorithms/kenny/SummaryStats
matrix multiplication,multiply two matrices
QR Code Generator,"Input has formA String to encode.A String file path in the Algorithmia Data API format, for example, ""data://util/GeneratedQRCodes/"".A filename without a file exentsion, such as ""myBarcode"".A file extension, such as ""jpg"" or ""png"".An int denoting the height of the barcode (in pixels) to produce. 200 seems to work as a default in most cases.An int denoting the width of the barcode (in pixels) to produce. 200 seems to work as a default in most cases.The algorithm returns the full path of the generated barcode (file path + name + ""."" + suffix).This is based on the zxing library (https://github.com/zxing/zxing)."
BigInt Primality Test,This is a simple primality test using the BigInteger class that I wrote during my lunch break. I'll improve efficiency once I figure out how to calculate the square root of a BigInteger in Java. 
Geographic Outlier Detection,"This algorithm finds the most unexpected events in a set of geographic events relative to some reference set of events. Specifically, it takes, in the form of a python dictionary""reference"" - a reference set of events as a list of latitude/longitude pairs""data"" - the evaluation set, which is also a set of lat/long pairs""n"" -  the maximum number of events to return as outliers.The algorithm uses the first set to train a probabilistic model event occurences, specifically, using density estimation  for gaussian kernels. It then evaluates the probability of every event in the evaluation set. It then returns a dictionary with the following entries'outliers' - whose value is a list of the indices of the n least probable events in the evaluation set'logprobs' - itself a dictionary whose keys are the indices returned above, with corresponding values being the log probabilities of the events.'all_lp' - a list of the log probabilities of the events in the evaluation set in the original order. The purpose of this is to allow the user to make their own determination of what counts as an outlier. For instance, if the log probabilities of the outliers do not differ significantly from those of the rest of the set, the user may decide to ignore the classification.This is based on scikit-learn's implementation of kernel density estimation."
Named Entity Recognition,"The Name Finder can detect named entities and numbers in text. It is currently set to detect persons (proper names), organizations, locations, times, dates, money, and percentages. To be able to detect entities the Name Finder needs a model. The model is dependent on the language and entity type it was trained for. The OpenNLP projects offers a number of pre-trained name finder models which are trained on various freely available corpora. They are available in data://ApacheOpenNLP/models/. To find names in raw text the text must be segmented into tokens and sentences, which this implementation handles by default for standard English. Its important that the tokenization for the training data and the input text is identical, so if working with something other than standard English, you need to pick different models from the models directory or train your own.For more information visit http://opennlp.apache.org."
Barcode Generator,"Input has formInformation to encode, which is a String but may have additional constraints depending upon the format, for instance EAN 8 requires an 8 digit number and will fail on other inputs. Consult the documentation on each individual standard to clarify.A String file path in the Algorithmia Data API format, for example, ""data://util/GeneratedQRCodes/"".A filename without a file exentsion, such as ""myBarcode"".A file extension, such as ""jpg"" or ""png"".A String denoting the desired barcode format. See below for options.An int denoting the height of the barcode (in pixels) to produce. 200 seems to work as a default in most cases.An int denoting the width of the barcode (in pixels) to produce. 200 seems to work as a default in most cases.The algorithm returns the full path of the generated barcode (file path + name + ""."" + suffix).This is based on the zxing library (https://github.com/zxing/zxing).Options for barcode format include""AZTEC"" for the 2D Aztec Code (http://en.wikipedia.org/wiki/Aztec_Code). Aztec is readable /util/BarcodeReader.""CODE_39"" for 1D Code 39 (http://en.wikipedia.org/wiki/Code_39). This only accepts a certain set of characters and will fail if given a wrong character.CODE_128 for 1D Code 128 (http://en.wikipedia.org/wiki/Code_128). CODE 128 is readable by /util/BarcodeReader.DATA_MATRIX for the 2D Data Matrix code (http://en.wikipedia.org/wiki/Data_Matrix). This format can be generated but cannot currently be read by /util/BarcodeReader.EAN_8 for 1D EAN 8 (http://en.wikipedia.org/wiki/EAN-8). This is not currently readable by /util/BarcodeReader.EAN-13 for 1D EAN 13 (). Must be 13 digits, the last of which is a checksum digit, for instance, ""0075678164125."" This format is not currently readable by /util/BarcodeReader.ITF for Interleaved Two of Five (http://en.wikipedia.org/wiki/Interleaved_2_of_5).""PDF_417"" for PDF 417 (http://en.wikipedia.org/wiki/PDF417). This is a stacked linear barcode that is readable by /util/BarcodeReader.""QR_CODE"" for 2D QR Codes (http://en.wikipedia.org/wiki/QR_code). This is readable by /util/BarcodeReader. For simplicity it is probably better to use /util/QRCodeGenerator and /util/QRCodeReader.""UPC_A"" for UPC A (http://en.wikipedia.org/wiki/Universal_Product_Code). This is not currently readable by /util/BarcodeReader."
JavaSort,Sort a list of numbers using the Java Collections class.
TimeBasedGreetings,"IntroductionThe API provides different greetings based on different timezones.Just provide ""Name"" and the Time Zone"" and get a Custom Greeting String!Available Time Zones Etc/GMT+12 Etc/GMT+11 MIT Pacific/Apia Pacific/Midway Pacific/Niue Pacific/Pago_Pago Pacific/Samoa US/Samoa America/Adak America/Atka Etc/GMT+10 HST Pacific/Fakaofo Pacific/Honolulu Pacific/Johnston Pacific/Rarotonga Pacific/Tahiti SystemV/HST10 US/Aleutian US/Hawaii Pacific/Marquesas AST America/Anchorage America/Juneau America/Nome America/Yakutat Etc/GMT+9 Pacific/Gambier SystemV/YST9 SystemV/YST9YDT US/Alaska America/Dawson America/Ensenada America/Los_Angeles America/Tijuana America/Vancouver America/Whitehorse Canada/Pacific Canada/Yukon Etc/GMT+8 Mexico/BajaNorte PST PST8PDT Pacific/Pitcairn SystemV/PST8 SystemV/PST8PDT US/Pacific US/Pacific-New America/Boise America/Cambridge_Bay America/Chihuahua America/Dawson_Creek America/Denver America/Edmonton America/Hermosillo America/Inuvik America/Mazatlan America/Phoenix America/Shiprock America/Yellowknife Canada/Mountain Etc/GMT+7 MST MST7MDT Mexico/BajaSur Navajo PNT SystemV/MST7 SystemV/MST7MDT US/Arizona US/Mountain America/Belize America/Cancun America/Chicago America/Costa_Rica America/El_Salvador America/Guatemala America/Managua America/Menominee America/Merida America/Mexico_City America/Monterrey America/North_Dakota/Center America/Rainy_River America/Rankin_Inlet America/Regina America/Swift_Current America/Tegucigalpa America/Winnipeg CST CST6CDT Canada/Central Canada/East-Saskatchewan Canada/Saskatchewan Chile/EasterIsland Etc/GMT+6 Mexico/General Pacific/Easter Pacific/Galapagos SystemV/CST6 SystemV/CST6CDT US/Central America/Bogota America/Cayman America/Detroit America/Eirunepe America/Fort_Wayne America/Grand_Turk America/Guayaquil America/Havana America/Indiana/Indianapolis America/Indiana/Knox America/Indiana/Marengo America/Indiana/Vevay America/Indianapolis America/Iqaluit America/Jamaica America/Kentucky/Louisville America/Kentucky/Monticello America/Knox_IN America/Lima America/Louisville America/Montreal America/Nassau America/New_York America/Nipigon America/Panama America/Pangnirtung America/Port-au-Prince America/Porto_Acre America/Rio_Branco America/Thunder_Bay America/Toronto Brazil/Acre Canada/Eastern Cuba EST EST5EDT Etc/GMT+5 IET Jamaica SystemV/EST5 SystemV/EST5EDT US/East-Indiana US/Eastern US/Indiana-Starke US/Michigan America/Anguilla America/Antigua America/Aruba America/Asuncion America/Barbados America/Boa_Vista America/Campo_Grande America/Caracas America/Cuiaba America/Curacao America/Dominica America/Glace_Bay America/Goose_Bay America/Grenada America/Guadeloupe America/Guyana America/Halifax America/La_Paz America/Manaus America/Martinique America/Montserrat America/Port_of_Spain America/Porto_Velho America/Puerto_Rico America/Santiago America/Santo_Domingo America/St_Kitts America/St_Lucia America/St_Thomas America/St_Vincent America/Thule America/Tortola America/Virgin Antarctica/Palmer Atlantic/Bermuda Atlantic/Stanley Brazil/West Canada/Atlantic Chile/Continental Etc/GMT+4 PRT SystemV/AST4 SystemV/AST4ADT America/St_Johns CNT Canada/Newfoundland AGT America/Araguaina America/Bahia America/Belem America/Buenos_Aires America/Catamarca America/Cayenne America/Cordoba America/Fortaleza America/Godthab America/Jujuy America/Maceio America/Mendoza America/Miquelon America/Montevideo America/Paramaribo America/Recife America/Rosario America/Sao_Paulo Antarctica/Rothera BET Brazil/East Etc/GMT+3 America/Noronha Atlantic/South_Georgia Brazil/DeNoronha Etc/GMT+2 America/Scoresbysund Atlantic/Azores Atlantic/Cape_Verde Etc/GMT+1 Africa/Abidjan Africa/Accra Africa/Bamako Africa/Banjul Africa/Bissau Africa/Casablanca Africa/Conakry Africa/Dakar Africa/El_Aaiun Africa/Freetown Africa/Lome Africa/Monrovia Africa/Nouakchott Africa/Ouagadougou Africa/Sao_Tome Africa/Timbuktu America/Danmarkshavn Atlantic/Canary Atlantic/Faeroe Atlantic/Madeira Atlantic/Reykjavik Atlantic/St_Helena Eire Etc/GMT Etc/GMT+0 Etc/GMT-0 Etc/GMT0 Etc/Greenwich Etc/UCT Etc/UTC Etc/Universal Etc/Zulu Europe/Belfast Europe/Dublin Europe/Lisbon Europe/London GB GB-Eire GMT GMT0 Greenwich Iceland Portugal UCT UTC Universal WET Zulu Africa/Algiers Africa/Bangui Africa/Brazzaville Africa/Ceuta Africa/Douala Africa/Kinshasa Africa/Lagos Africa/Libreville Africa/Luanda Africa/Malabo Africa/Ndjamena Africa/Niamey Africa/Porto-Novo Africa/Tunis Africa/Windhoek Arctic/Longyearbyen Atlantic/Jan_Mayen CET ECT Etc/GMT-1 Europe/Amsterdam Europe/Andorra Europe/Belgrade Europe/Berlin Europe/Bratislava Europe/Brussels Europe/Budapest Europe/Copenhagen Europe/Gibraltar Europe/Ljubljana Europe/Luxembourg Europe/Madrid Europe/Malta Europe/Monaco Europe/Oslo Europe/Paris Europe/Prague Europe/Rome Europe/San_Marino Europe/Sarajevo Europe/Skopje Europe/Stockholm Europe/Tirane Europe/Vaduz Europe/Vatican Europe/Vienna Europe/Warsaw Europe/Zagreb Europe/Zurich MET Poland ART Africa/Blantyre Africa/Bujumbura Africa/Cairo Africa/Gaborone Africa/Harare Africa/Johannesburg Africa/Kigali Africa/Lubumbashi Africa/Lusaka Africa/Maputo Africa/Maseru Africa/Mbabane Africa/Tripoli Asia/Amman Asia/Beirut Asia/Damascus Asia/Gaza Asia/Istanbul Asia/Jerusalem Asia/Nicosia Asia/Tel_Aviv CAT EET Egypt Etc/GMT-2 Europe/Athens Europe/Bucharest Europe/Chisinau Europe/Helsinki Europe/Istanbul Europe/Kaliningrad Europe/Kiev Europe/Minsk Europe/Nicosia Europe/Riga Europe/Simferopol Europe/Sofia Europe/Tallinn Europe/Tiraspol Europe/Uzhgorod Europe/Vilnius Europe/Zaporozhye Israel Libya Turkey Africa/Addis_Ababa Africa/Asmera Africa/Dar_es_Salaam Africa/Djibouti Africa/Kampala Africa/Khartoum Africa/Mogadishu Africa/Nairobi Antarctica/Syowa Asia/Aden Asia/Baghdad Asia/Bahrain Asia/Kuwait Asia/Qatar Asia/Riyadh EAT Etc/GMT-3 Europe/Moscow Indian/Antananarivo Indian/Comoro Indian/Mayotte W-SU Asia/Riyadh87 Asia/Riyadh88 Asia/Riyadh89 Mideast/Riyadh87 Mideast/Riyadh88 Mideast/Riyadh89 Asia/Tehran Iran Asia/Aqtau Asia/Baku Asia/Dubai Asia/Muscat Asia/Oral Asia/Tbilisi Asia/Yerevan Etc/GMT-4 Europe/Samara Indian/Mahe Indian/Mauritius Indian/Reunion NET Asia/Kabul Asia/Aqtobe Asia/Ashgabat Asia/Ashkhabad Asia/Bishkek Asia/Dushanbe Asia/Karachi Asia/Samarkand Asia/Tashkent Asia/Yekaterinburg Etc/GMT-5 Indian/Kerguelen Indian/Maldives PLT Asia/Calcutta IST Asia/Katmandu Antarctica/Mawson Antarctica/Vostok Asia/Almaty Asia/Colombo Asia/Dacca Asia/Dhaka Asia/Novosibirsk Asia/Omsk Asia/Qyzylorda Asia/Thimbu Asia/Thimphu BST Etc/GMT-6 Indian/Chagos Asia/Rangoon Indian/Cocos Antarctica/Davis Asia/Bangkok Asia/Hovd Asia/Jakarta Asia/Krasnoyarsk Asia/Phnom_Penh Asia/Pontianak Asia/Saigon Asia/Vientiane Etc/GMT-7 Indian/Christmas VST Antarctica/Casey Asia/Brunei Asia/Chongqing Asia/Chungking Asia/Harbin Asia/Hong_Kong Asia/Irkutsk Asia/Kashgar Asia/Kuala_Lumpur Asia/Kuching Asia/Macao Asia/Macau Asia/Makassar Asia/Manila Asia/Shanghai Asia/Singapore Asia/Taipei Asia/Ujung_Pandang Asia/Ulaanbaatar Asia/Ulan_Bator Asia/Urumqi Australia/Perth Australia/West CTT Etc/GMT-8 Hongkong PRC Singapore Asia/Choibalsan Asia/Dili Asia/Jayapura Asia/Pyongyang Asia/Seoul Asia/Tokyo Asia/Yakutsk Etc/GMT-9 JST Japan Pacific/Palau ROK ACT Australia/Adelaide Australia/Broken_Hill Australia/Darwin Australia/North Australia/South Australia/Yancowinna AET Antarctica/DumontDUrville Asia/Sakhalin Asia/Vladivostok Australia/ACT Australia/Brisbane Australia/Canberra Australia/Hobart Australia/Lindeman Australia/Melbourne Australia/NSW Australia/Queensland Australia/Sydney Australia/Tasmania Australia/Victoria Etc/GMT-10 Pacific/Guam Pacific/Port_Moresby Pacific/Saipan Pacific/Truk Pacific/Yap Australia/LHI Australia/Lord_Howe Asia/Magadan Etc/GMT-11 Pacific/Efate Pacific/Guadalcanal Pacific/Kosrae Pacific/Noumea Pacific/Ponape SST Pacific/Norfolk Antarctica/McMurdo Antarctica/South_Pole Asia/Anadyr Asia/Kamchatka Etc/GMT-12 Kwajalein NST NZ Pacific/Auckland Pacific/Fiji Pacific/Funafuti Pacific/Kwajalein Pacific/Majuro Pacific/Nauru Pacific/Tarawa Pacific/Wake Pacific/Wallis NZ-CHAT Pacific/Chatham Etc/GMT-13 Pacific/Enderbury Pacific/Tongatapu Etc/GMT-14 Pacific/Kiritimati "
Diameter,Returns the diameter of a directed graph. The diameter of a graph is the longest distance between any two connected vertices.Input:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings). 
Fuse NGrams,"This routine is used for processing text prior to tagging (with an algorithm such as https://algorithmia.com/algorithms/kenny/LDA or https://algorithmia.com/algorithms/nlp/KeywordsForDocumentSet). It turns important multi-word terms (n-grams) into a single word (by replacing spaces with underscores, so ""machine learning"" becomes ""machine_learning"") so tagging routines will ""recognize"" the importance of the term and not treat it as a collection of unrelated words. It can either do this for a designated set of terms or it can automatically detect which n-grams are likely to be important and process these.If you already know the terms you want to fuse, simply supply the text to process as well as a list of the terms to fuse, for example, if you enter [""Machine learning is the future of technology!"",[""machine learning"",""future of technology""]]you will get""machine_learning is the future_of_technology!""If you want to automatically discover terms that should be fused, inputThe input text, either a String or a String[].n for the desired type of n-gram. 2 is the most common, for example, ""machine learning"" and ""big data"" are important 2-grams/bigrams.A List<String> of known n-grams. If you don't know what these should be just use an empty list.An int for the maximum number of n-grams to consider (default is 5).An int for the minimum frequency of the n-gram in the text (the number of times it appears) that will be considered (default is 5). If the first argument of a String, frequency is the total number of times the n-gram appears in it, if it is a String[], counts are taken across all entries. The output is the input with all relevant n-grams fused. All strings are converted to lower case for both processing and output.As mentioned above, if you input just a String or String[], the algorithm will search for a maximum of 5 bigrams, counting only those that appear more than five times, and will return, respectively, a String or String[] with all eligible bigrams fused."
Digit Preprocessing,
RandomNumberGenerator,"Generates lists of pseduo random numbers, in either a uniform, normal, exponential, or binomial distribution.The first parameter is a string, saying which distribution to use.  It can either be the English ""normal,"" ""binomial"" etc. or the name of the corresponding R function, ""rnorm,"" ""rbinom,"" etc.  The second parameter is the number of pseudo random numbers to return.  The other numerical parameters vary based on which distribution it is, but generally they are similar to the R signatures.Examples:[""rnorm"", 1000, 0.0, 1.0] - a normal/Gaussian distribution, 1000 numbers, mean of 0 and standard deviation of 1[""runif"", 1000, 0.0, 1.0] - 1000 numbers distributed uniformally between 0 and 1[""rexp"", 10000, 2, 0.0] - 10000 numbers from an exponential distribution, with gamma of 2.  The 0 is a placeholder and is not used in this case.[""rbinom"",""10000"", "".5""] - 10000 numbers from a binomial distribution (think coin flips), with a probability of .5 of being either 1 or 0Note, due to the way the JSON formatting is handled, decimal numbers less than one must be enclosed in quotes.  So 1.0 is fine, but 0.5 will cause a JSON format error and must be written as ""0.5""."
Random Forest Train,"Trains a Mahout random forest classifier. Takes as input a string array of the training data, a destination url for the model, a string data descriptor, and a number of trees, returns the Data API URL of the trained model. The trained model can be applied to test data using ApplyRandomForest.We assume that the first entry of any instance is the label, though Mahout does support other placement. The descriptor must be of form ""L X X X ..."", where each X designates the type of its respective field, either I (ignored), N (numerical), or C (categorical). L designates the label label. Think of the descriptor as a header for the data. As an example, a dataset with four attributes (beyond the label) might have the first two as categorical, the third numerical, and the last ignored, and its header would be ""L C C N I""."
Digit Extraction,
IsIntegerPalindrome,Check if the given integer is a palindrome
LearnHMM,Learns a Hidden Markov Model (HMM) based on a series of example state transitions.
ParallelSiteMatcher,
Arbitrage,"Detects arbitrage opportunities in a directed, complete graph of currency conversions.Currency conversions are specified in a square matrix of edge weights. All conversion rates must be strictly positive; numbers on the diagonal are expected to be exactly 1.The output is a list of arbitrage opportunities, represented as lists of indices into the input matrix.The returned cycles will not overlap, but some cycles may be missed; currently, there is no guarantee that the opportunities returned are the best possible. For details, see also jtbandes/NegativeWeightCycles."
Grab Cut,"Performs the GrabCut image segmentation algorithm on an image (data uri, base64 encoded, or web url) and returns the foreground.The specified region is used to focus the algorithm on an object of interest in the image and must be smaller than the region of the whole image.Input consists of:The image to process, either as a Data API url (data://), a web url (http://), base64 encoded file, or as a raw POST body.Optionally accepts a 2nd argument which specifies the bounding box of the foreground (eg- {""x"": 1, ""y"": 1, ""width"": 150, ""height"": 150})Optionally the 2nd argument can be a single integer which specifies how many pixels to shrink the window by, relative to the full image"
TimezoneByLatLong,"Input a set of coordinates (latitude, longitude) and receive the following results:name: Timezone nameoffset: Timezone offset (against UTC)offset_dst: Daylight Savings Time offset (based on current exact date/time)"
Linear Algebra Solve Linear Equation,"Solver Linear Equation: 


Finds a solution of the linear equation A x = b where A is an n x m - matrix given as double [n][m], b is an m - vector given as double [m] and x is an n- vector given as double [n]."
Word Frequency Counter,"Takes in a string and returns a Map of [word,frequency]. Note that internal quotation marks must be escaped or removed."
Keyword Cipher,"This keyword cipher includes a space in order to encrypt and decrypt phrases with spaces in them. Keyword cipher is a form of monoalphabetic substitution. A keyword is used as the key, and it determines the letter matchings of the cipher alphabet to the plain alphabet. Repeats of letters in the word are removed, then the cipher alphabet is generated with the keyword matching to A,B,C etc. until the keyword is used up, whereupon the rest of the ciphertext letters are used in alphabetical order, excluding those already used in the key.Usage[""<String> encrypt/decrypt"", ""<String> input"", <String> key]Sample[""encrypt"", ""American Standard Code for Information Interchange"", ""bloob""][""decrypt"", ""LKCRGBLM STLMALRA BNAC DNR GMDNRKLTGNM GMTCRBFLMEC"", ""bloob""]"
BronKerbosch,
reddit image grabber,"Grab images from a subreddit's front page.subreddit: The name of the subreddit.category: Can be 'hot', 'new', 'rising', 'controversial', or 'top'.domains: The list of image host domains you want to grab images from. Defaults to a single domain: ['i.imgur.com']."
knapsack,"This is an attempt to solve the knapsack problem by adding items with the highest value to weight ratio. To lessen the effects of adding items with high ratios but little actual value, the max weight is increased while adding, then items are removed in order of least value, until the weight requirement is met.The expected input is a List<double[]>,double where the List<double[]> is a list of each item, each double[] is a separate item of a [weight,value] pair. The double is the maximum weight value of the sack. This does not check for negative weights or values, which may result in errors."
FactorMoneda,Obtiene Factor Moneda del Mes en Curso
primes_to,Find all the prime numbers less than or equal to a given integer.
GraphicsToHtml,
ScrapeHN2,"Scrapes just the ""best of"" posts on Hacker News, instead of all the posts.  Based on the ScrapeHN algorithm."
SimpleGaussianFilter,"A simple Python script that takes an image and a sigma, and applies Gaussian filter."
codon2aminoacids,"translation of a nucleotide (DNA/RNA) sequence to a protein sequence.Parameter Lead seperate with ':' to sequencesLead with '-' will use the first ATG(AUG) as Start.Otherwise use the number as frame start. Will ignore space, line break."
Sleep,Sleeps for {input} milliseconds
StoneDivision,"StoneDivision makes the fairest division in a divisible or indivisible item set (integer or float).Input explanation:p - Number of ""people"" which receive the items values;i - Items values separated by comma, can be integer or float;d - If the items can be divided, always set 0 if the items cannot be divided or 1 if they can."
PiecewiseLinear,"Split a multidimensional timeseries such that a piece linear reconstruction has minimal error.The input is first normalized such that each timeseries has a mean of 0 and a standard deviation of 0. This is to avoid overweighting one of the dimensions vs the others simply because of scaling differences.The resulting timeseries is then run through a dynamic programming algorithm that produces the split with minimal SSE, running in time proportional to the size of the timeseries input and the number of segments."
OrdinaryLeastSquares,
CaptchaGenerate,Algorithm for generating a Captcha to test if your user is human
Echo Json,
JSONtoCSV,"Takes a JSON array (as a single string) and outputs a CSV representation of that array. The column names are automatically generated from the first element.The input can be:1) Path to a local/data file, detected using the ""data://"" prefix2) URL of a remove file, detected using the ""http://"" or ""https://"" prefix3) Direct string of a JSON array representation."
Email Validator,Checks whether a string is a legal email address.
BubbleSort,Sorts a list of numbers using the Bubble Sort algorithm. O(n^2)
FrequentItemsets,Simple implementation of the Apriori algorithm for frequent itemset computation. Note that the Apriori algorithm is a far cry from a fast algorithm and will run into the timeout cap of 5 minutes fairly quickly. Has been tested with up to ~1400 docs of length 6 on average with threshold of 12. Increasing threshold improves run time. Decreasing the number of documents or pruning the documents can also improve run time.
Click Stream Transitions,"Returns the top 5 most ""interesting"" transitions learned by the Hidden Markov Model."
Get image links,"Given a url will return a list of all the image urls at the target url.Great for web crawling, image analysis at scale. "
Related Tags,"Takes as input a mapping from each object to its tags, and a set of starting tags to generate recommendations for.{""object"" -> [""tagA"", ""tagB""]}, [""tag1"", ""tag2""]This algorithm models the tags as a bipartite graph, and uses collaborative filtering to recommend new tags.This can be useful when paired with automatic tag generation, as it allows for generating new tags that don't necessarily appear in the auto-tagged documents, but may still be related."
isPrime,Evaluates up to 2^63 - 1 (Java long size limit).  Behavior beyond this limit is untested.
SMS Spam Detector,"Given a string representing a SMS message, classifies it as ""spam"" or ""ham"" (not spam) using a Linear SVM with Stochastic Gradient Descent trained on a publicly available dataset from the well known UCI machine learning repository whose hyperparameters were set with Bayesian Optimization.  Designed to be a simple demonstration machine learning task whose performance was optimized with Bayesian optimization.  "
RoundJSON,"Takes a JSON with two keys to_round and prec.to_round is an object like a dictionary/map, a list or a number or string. prec is the needed rounding precisionThe algorithm will find all the floating point numbers and round them to prec decimal points and return the object. If prec < 0, no rounding will happen."
Test,Add two numbers
CRF Tagger,
Geographic Distance,"Calculates the geographic distance between two sets of coordinates provided as input in format {""lat1"": XX,""long1"":XX,""lat2"":XX,""long2"":XX}

Distance returned in arc seconds."
TipoDeCambio,Obtiene el tipo de Cambio del Diario Oficial de la Federacion(D.O.F.)
ResiBatChrg,"Returns the optimal battery size for an imported list of historical loads for a building.Two ways to run it:1. enter location of csv data file as a String that contains only meter reads, one on each row.  Defaults to ""data://MadDenker/LCTest1/LC2.csv""  Default file contains hourly kWh reads for a small residential building.2. enter a one dimensional array of double values.  Ex: [1.25, 1.47, ...]Output is the optimal battery size in the same units as the input."
Soundex,Your basic soundex algorithm.
ReVerb,"Runs the ReVerb Open Information Extraction software on a blob of HTML. The output is a collection of scored (argument1, relation, argument2) triples extracted from the HTML's text. For more information about ReVerb, see http://reverb.cs.washington.edu/"
Sentiment By Term,"This algorithm analyzes a document to find the approximate sentiment associated with each of a given set of terms. It does this by splitting the document into sentences and computing, for each provided term, the average sentiment of all sentences containing that term. It takes as inputThe document (as a string).The list of terms as a List<String>. Terms can either be provided by hand or extracted automatically with a tagging (for instance, /tags/AutoTagURL) or entity recognition algorithm (/StanfordNLP/NamedEntityRecognition).(Optional) Coreferences in the form of a Map<String,List<String>>, where each key is a member of the list of terms and the associated value is a list of terms that refer to the same entity. This ensures that all references to a given entity are accounted for, even if they don't all use the same term. This can be supplied either by hand or by previous application of an algorithm like /StanfordNLP/DeterministicCoreferenceResolution.Make sure that all strings in the term list and coreference map are lower case.For more information, please refer to http://nlp.stanford.edu/software/corenlp.shtml or Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60. "
Thin QR Decomposition,"Computes the thin QR Decomposition of a matrix using Modified Gram-SchmidtInput # of rows,m, # of columns, n, and array of values. Values fill the matrix column by column. For matrix:[ 1 2 3 4; 5 6 7 8; 9 10 11 12; 13 14 15 16];Input: [4,4,[1,5,9,13,2,6,10,14,3,7,11,15,4,8,12,16]]"
Upload File to Bucket,"This algorithm uploads a file (specified by the last input, filePath) to an Amazon S3 bucket. You can give the file as a URL to a file or a Data API url to a file -the last input-. Please specify the region of the bucket, the url to the credentials file and the title you would like to give to the file. The second argument, credsFile, should be a file in a collection in the Data API with first line containing the bucket name, the second line your access key and the third line containing your secret key.Warning: You may get an exception that says ""The bucket you are attempting to access must be addressed using the specified endpoint."" This means the region you have entered is incorrect for that specific bucket.The possible regions are:AP_NORTHEAST_1 AP_SOUTHEAST_1 AP_SOUTHEAST_2 CN_NORTH_1 EU_CENTRAL_1 EU_WEST_1 GovCloud SA_EAST_1 US_EAST_1 US_WEST_1US_WEST_2 "
Fourier Clean,"This algorithm takes a time series and a parameter k. It takes the Fourier transform of the series, zeroes out the top k weakest frequencies, and maps the result back into a time series with an inverse Fourier transform.This is a speculative algorithm for removing cleaning and smoothing - use it for exploratory analysis but try not to draw too many conclusions unless you corroborate with other methods.This is based on an implementation of the Fast Fourier Transform (in this the real Fourier Transform in the JTranforms library - https://sites.google.com/site/piotrwendykier/software/jtransforms) Note that there are many different conventions used in signal processing, so be careful as the ones chosen here may not be the ones you need. "
PyWaveletTransform,"Run some discrete-wavelet-related operation over the provided input data.        Input supports the following options:data, a required array of numbers to operate overoperation, specifies what to do with the data, currently supports: - forward, run a forward DWT from the time domain to the frequency domain - reverse, run a reverse DWT from frequency to time domain - denoise, run a forward DWT, zero out small coefficients, then reverse the transformwavelet, the wavelet type to use for the transformation, see PyWavelets documentationmode, how to treat the boundaries of the finite dataset, see the PyWavelets documentationthreshold, the cutoff level for component magnitudes below which they're zeroed, required if denoisingFor input data that is not sized as a power of 2, the data will first be decomposed into sublists    of sizes that do meet that requirement, and then each component is manipulated as before."
Shift Cipher,"Also known as a Caesar cipher, takes a rotation count (n) and a string and returns the rot-n version of that string. Casing and non-alpha characters are maintained as-is. Not to be used for serious encryption.O(n)"
Part-of-speech Tagger,"Input a piece of text, get back the same piece of text with the parts of speech tagged according to the Stanford CoreNLP MaxEnt Tagger. You can find a large list of pretrained taggers in the ""data://StanfordNLP/models/"" directory, including English (multiple taggers), Arabic, German, Chinese, and Spanish. If you do not specify a tagger model a default English one will be used.Parts are tagged according to the conventions of the Penn Treebank Project (http://www.cis.upenn.edu/~treebank/). For example, a plural noun is denoted NNS, a singular or mass noun is NN, and a determiner (such as a/an, every, no, the, another, any, some, each, etc.) as DT.For more information, please refer tohttp://nlp.stanford.edu/software/corenlp.shtml or Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60. "
EMI Calculator,Calculates EMI using a double[] of 3 elementsinput[0] --> Principal amountinput[1] --> Annual interestinput[2] --> Loan tenure in monthsThe double[] must have exactly 3 elements and all elements must be greater than 0 otherwise an Exception would be thrown
Pairs Generator,"This algorithm generates a list of all possible pairs from items in a list. This is sometimes an important first step in analyzing and optimizing datasets.Please note this free version of Pairs Generator supports 10 items per dataset. To generate pairs for an unlimited number of items per dataset please see ""Pairs Generator - Unlimited Items Per List""."
Address Extraction,"Finds the imprint or contact 
page of a website (hostname supplied as argument) and extracts addresses, contact data, company names and other information.
Emphasis currently is on Germany, Austria and Switzerland. The 
quality for other countries varies but can be optimized on demand.'status' should usually be 'ok' - unless you supplied an empty hostname.'crawl_status' contains 'ok' or an error message if no data could be extracted for some reason.'url_imprint' is the URL of the imprint or contact page - if one was found.'results' contains the extracted data (may be empty). The algorithm may return several results and several contact persons 
within a result. All results are sorted by descending relevance. You can
 simply use the first result returned. Each result contains:country, zip, city, street, company, phone, fax, iban, bic: Should be self-explanatory.emailhash: SHA-1-Hash of ‘mailto:’ + mail address (to prevent abuse, plain addresses are not available)vatidnr: VAT IDregnrde: Trade register number and town of register court (Germany only)managers: List of contact persons with relevant catchword and distance of name to catchword (currently only supported on german websites)blz: German bank code (Germany only)See http://www.netestate.de/en/information-extraction/software/imprint-crawler/ for more information or to try the algorithm for free."
fix squares,"Fit squares in a rectangle for layouts.have option to fit width of the rectangle.parameters are:[rectWidth, rectHeight, countOfSquare, isMatchWidthOfRect]return:size of each square."
Throwing dice,"It's a simple API to get dice rolls for rol games or whatever you need.You must send a array of dictionaries with the max value of the dice and the number of throws.[
	{""max"" : <max number of the dice>, ""n"" : <number of throws>}, 
	{""max"" : <max number of the dice>, ""n"" : <number of throws>}, 
	{""max"" : <max number of the dice>, ""n"" : <number of throws>}
]"
Number to Base 3,Converts a Base10 number to Base3. 
LogitBoost,"This is the LogitBoost classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/LogitBoost.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Is Acyclic,Returns true if and only if a directed graph contains no cycles.Input:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings). 
Svir clustering,"Svir clustering algorithm for VRP. This algorithm orders market by angle in polar coordinates system, relative lo geometrical center and warehouse location. Then it splits all set of markets to the sets, that fit car capacity for one route. Results of this clustering could be used for solving vehicle routing problem for delivery."
mysq To JSON,Provides easy connection to a mysql datasource and returns resulting query as a Json object. 
GeneratePermutation,This algorithm will take a String as input and find all possible distinct permutations of the string and return the permutations as a collection.
GaleShapley,"This is an implementation of the Gale-Shapley  stable marriage algorithm. This does not check for that all of the input names are valid, it only checks that the number of rows are divisible by 2, and that each row contains 'n' names, where 'n' is the number of rows divided by 2."
GaleShapley,"Gale Shapley algorithm for stable matching.Input: Object of class Map<String,List<String>>, in which each key is a node name, and its value is an ordered list of names denoting preferences (the first element being the highest preference). The input contains 2n elements, n of which are “male” and include each of the n “female” nodes in their preference list, and vice versa. The order of the elements in the input map does not matter, and ""male"" vs. ""female"" elements will be determined based on the composition of the preference lists (a ""male"" element will have only ""female"" elements in their preference list while a ""female"" element will have only ""male"" elements in their preference list"").Output: Object of class Map<String,String> where each key is paired with its value.Suitor Bias: The Gale-Shapley algorithm is biased for 'suitor optimality'. The suitor in this implementation will be the ""gender"" (group) opposite that of the first element. For more information on the Gale-Shapley algorithm see: http://en.wikipedia.org/wiki/Stable_marriage_problem.Sample: In the Sample Input and Sample Output below, ""Andre"" and ""Ritesh"" are male while ""Shikha"" and ""Joanne"" are female. The 'suitor' in this case will be the females, as the first element ""Andre"" is male. "
LeftRightTextJustification,"Produces left + right justified text for a specified line length, along with a left indent for the start of each paragraph. Final line of each paragraph is not right justified."
Find Broken Links,"Given the url of a webpage, returns a list of all the broken links on the page (links that cause an HTTP error when they are followed)."
POS Tagger,"Takes text input (as String), splits into sentences, identifies the part of speech of each word in each sentence, and outputs a list of the ordered lists of these parts for each sentence.Parts are tagged according to the conventions of the Penn Treebank Project (http://www.cis.upenn.edu/~treebank/). For example, a plural noun is denoted NNS, a singular or mass noun is NN, and a determiner (such as a/an, every, no, the,another, any, some, each, etc.) as DT.For more information, please refer to http://nlp.stanford.edu/software/corenlp.shtml or Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60."
1D Wavelet,"This is a wrapper algorithm for wavelet transforms Haar and Daubechies. With the option to specify which wavelet by passing in a string.""haar"" or ""d2"" -> Haar""daubechies"" or ""d4"" -> Daubechies D4"
bucketr,"Categorize numerical data in buckets given a list of numbers and a bucket size.Things to consider:If no bucket size is given, the largest number in the list is used as the default bucket size.The largest number in the list will be used as the bucket size if the given bucket size is greater than the largest number in the list."
EntityRecognition,
Stable Marriage Problem,"In mathematics, economics, and computer science, the stable marriage problem (SMP) is the problem of finding a stable matching between two sets of elements given a set of preferences for each element. A matching is a mapping from the elements of one set
  to the elements of the other set. A matching is stable whenever it is not the case that both:



some given element A of the first matched set prefers some given element B of the second matched set over the element to which A is already matched, and



B also prefers A over the element to which B is already matched




In other words, a matching is stable when there does not exist any alternative pairing (A, B) in which both A and B are individually better off than they would be with the element to which they are currently matched.

The stable marriage problem is commonly stated as:



Given n men and n women, where each person has ranked all members of the opposite sex with a unique number between 1 and n in order of preference, marry the men and women together such that there are no two people of opposite sex who would both rather
    have each other than their current partners. If there are no such people, all the marriages are ""stable"". (It is assumed that the participants are binary gendered and that marriages are not same-sex).

Source: http://en.wikipedia.org/wiki/Stable_marriage_problem

Input should be an object of class Map<String,List<String>>, in which each key is a node name, and its value is an ordered list of names denoting preferences (the first element being the highest preference). The structure of the problem
  implies that the input contain 2n elements, n of which are “male” and include each of the n “female” nodes in their preference list, and vice versa. Output Map<String,String> where each key is paired with its value.
Note that the algorithm is not symmetric in its optimality: as implemented,
  

it is optimal for the ""suitors"", but the stable, suitor-optimal solution



may or may not be optimal for the ""reviewers"".



"
StableMatching,An implementation of the Gale-Shapley algorithm for solving the Stable Marriage Problem.
Linear Regression,"In statistics, linear regression is an approach to model the relationship between a scalar dependent variable y and one or more explanatory variables denoted X.

This algorithm takes a list of points (floating point pairs), and returns coefficients to the linear equation of the form:

y = beta0 + beta1 * x

And also the r^2 correlation coefficient."
LinearRegression,Performs a least-squares regression with one independent variable.
NDTest,
Fibonacci Large,"Computes fibonacci numbers for large N (<=999) efficiently. Takes about 7 seconds on my machine to compute F(999) (probably a lot less here).Two variants of Fibonacci generator:1) N --- > Fib(N) (as String)N must be between 0 and 999 (both inclusive) otherwise an Exception will be thrown2) N, booleanValue ---> String[] of [Fib(0), Fib(1)...Fib(N)]N must be between 0 and 999 (both inclusive) otherwise an Exception will be thrown.booleanValue MUST be true (for compatibility reasons) otherwise an exception will be thrown"
HtmlToStringList,Return list of strings given a URLBreaks text depending on block level or inline level elements
Update Edmonton,
IP-to-Hostname,Performs a reverse lookup of the hostname for a given IP address
Sudoku,"Solves a 9x9 sudoku.

Input must be provided as a 9x9 array, with only numbers 0-9, where 0 indicates empty cell."
Daubechies D4 Wavelet,"This algorithm performs the Haar Wavelet Transform (or inverse transform) on a one dimensional signal. This algorithm expects a signal which whose length is divisible by 2, and will perform the transform for as long as the length can be divided by 2, while still being evenly divisible.  Unless a max number of iterations is specified by an int before the input signal.Sample Input[1,[1,2,3,1,2,3,4,0]]Sample Output{""transform"":[2.69901760217,2.34546421158,4.37205020963,1.8971764754999998,1.44888873942,-0.38822856765000013,2.4148145657,-0.64704761275],""i"":1}Inverse Input[[5.65685424933524,2.1516122217235534e-13,1.8660254037497057,0.1339745962132557,1.44888873942,-0.38822856765000013,2.4148145657,-0.64704761275],3]
Invers Output[0.9999999999129452,1.9999999998886333,2.999999999875018,0.9999999999147142,1.9999999998771734,2.9999999998447104,3.999999999853363,-5.548117520959295e-11]"
Spike Detection,"A simple anomaly detection algorithm that reports sudden changes in a time series. Takes as input a time series in the form of a double[] and a spike threshold. If any point differs from its neighbor by more than the threshold value, it is considered an anomaly. The array returned matches the input array on anomaly points, all other points are set to 0. Note that this only takes local information into account, so if the time series has, for instance, a high magnitude period in between low magnitude periods, only the start and end of the high magnitude period will be reported. "
Graph Coloring,"This is an algorithm for graph coloring (http://en.wikipedia.org/wiki/Graph_coloring) built on the JaCoP Constraint Programming Solver (https://github.com/radsz/jacop). It takes a hash table, with each node name a key having the list of neighbors of that node as the corresponding value (essentially an adjacency list) and outputs a hash table of node names with the corresponding (integer) color as value."
AccelerometerClassifier,"Given x,y,z accelerometer readings, will determine whther the user is Stationary, Walking, or Running. "
Alexa Ranking,Takes a domain name and returns the Alexa ranking of that domain.
LoadDataFromURL,Copy a file resource identified by URL in the Algorithmia data space
RetrieveStopWords,"Returns a common list of stop words. Based on Mallet's list of stop words and http://www.ranks.nl/stopwords. If you have any suggestions for adding to this list, comment below!"
WekaPreprocessing,"This is the main algorithm that all of the Weka preprocessing algorithms call. It is preferred that you do not call this algorithm directly, but by one of the preprocessors on the platform. The Weka algorithms that are on the platform take in flat Json as input. It is assumed that any data that is going to be used with these algorithms will be large in size, so all interaction is done through Data collections."
Principal Component Analysis,"Principal Component Analysis (PCA) using the covariance method (Wikipedia)The algorithm computes the principal components of a set of n-dimensional data points and transforms the points to a new coordinate system s.t. the first dimension contains the greatest variance, the second dimension contains the second greatest variance, etc.The algorithm takes as input a list of data points (which are themselves lists of doubles), and outputs an object with the following four fields:""transformed"": the transformed data points""eigenvectors"": the eigenvectors/principal components of the input data set (ordered by decreasing variance) ""eigenvalues"": the eigenvalues of the eigenvectors (in the same order)""mean"": the mean of the input data"
IsPrime,Evaluates if input (long integer) is a prime number. Returns boolean: true or false
Caesar Cipher,"Simple Caesar cipher with special characters.  Caesar cipher, also known as Caesar's cipher, the shift cipher, Caesar's code or Caesar shift, is one of the simplest and most widely known encryption techniques. It is a type of substitution cipher in which each letter in the plaintext is replaced by a letter some fixed number of positions down the alphabet. Usage[""<String> encrypt/decrypt"", ""<String> input"", <int> key]Sample[""encrypt"", ""American Standard Code for Information Interchange"", 24][""decrypt"", ""Y+#0'!Y,X12Y,\""Y0\""X!-\""#X$-0X',$-0+Y2'-,X',2#0!&Y,%#"", 24]"
eigenvalues,Compute the real eigenvalues of a square matrix.
StringEditDistance,"Levenshtein string edit distance. Returns the minimum number of character inserts, deletes, and substitutions required to transform one string into another."
Java2NER,"Identifies named (PERSON,LOCATION,ORGANIZATION,MISC) and numerical (MONEY,NUMBER,DATA,TIME,DURATION,SET) in text, outputs the text of each entity along with its identifier. Specifically, this algorithm takes a string as input, splits it into sentences, finds the named entities in each sentence, and for each sentence, outputs a list of the named entities along with their type as a two-element string array.For more information, please refer to http://nlp.stanford.edu/software/corenlp.shtml or Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60. "
Keyword Analysis For Reviews,"This algorithm can be used to identify the important terms that differentiate good reviews from bad reviews. It is based on tf-idf scoring (a version of which is implemented for general multiple-document keyword identification in /nlp/KeywordsForDocumentSet), but unlike these, it calculates term frequencies for the entire set of reviews sharing a given rating. It takes as inputThe set of reviews (as a String[])The ratings of each review (as Integer[]). The i-th review in the first argument corresponds to the i-th rating in the second.The number of rating options (general 5, for a 1-5 star system)The maximum number of keywords to return for each rating.It returns the set of keywords corresponding to weighting in order, from lowest rating to highest rating. Terms that appear frequently across all rating classes will be assigned low weight and thus will not likely appear as keywords. "
SendEmail,This algorithm simply sends an email.Warning: This algorithm is very simple and any message sent from this algorithm is likely to be sent to the Spam folder by major email providers. Use https://algorithmia.com/algorithms/email/SendEmailFromGmail if you would like a more robust email sender.
Lat Long To UTM,"Converts latitude/longitude coordinates to UTM (http://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system). Result is returned as a String[] of form  [<longitude zone>, <latitude zone>, <easting>, <northing>]  where easting is the projected distance from longitude zone's central meridian and northing is the projected distance from the equator. The values of both easting and northing are given in meters. For example, the latitude/longitude coordinates 61.44, 25.40 are presented in UTM as 35 V 414668 6812844; the latitude/longitude coordinates -47.04, -73.48 are 18 G 615471 4789269 in UTM.Based on coordinate conversion software by Sami Salkosuo at IBM, see https://www.ibm.com/developerworks/java/library/j-coordconvert/ for more details."
ParallelMST,"This algorithm computes the minimum spanning tree of a graph using a parallel Kruskal's algorithm implementation. Currently it operates on some sample graphs uploaded to the Data API in the GML format, new data can be uploaded and the path can be given to the readGMLData function inside the source code."
Primality,Returns true if number (64bit signed integer) is prime.
Website Summary,"This algorithm takes a web address and returns a summary relevant structural details of the site. Specifically, it is intended to identify the relevant pages on a hotel website, returning selected metadata and the relative importance of various pages as measured by PageRank. The returned information includes:url - the original address given, assumed to be the main page of the website.language - the language of the main page. See https://algorithmia.com/algorithms/nlp/LanguageIdentification for a guide to the returned language symbols.tags - important terms from the website.important pages - we check to identify which pages on the site are used for rooms, reservations/booking, photos, and location. For this we currently support English, Spanish, Italian, German, and Portuguese.pageRanks - an ordered list of pages on the site by page rank, the higher the rank, the more likely the page is to be important."
Random Allocator - Unlimited Items,This algorithm allocates random percentages to each item in a list where the sum of the percentages equals 100%.One example where this is useful is portfolio optimization where the many hypothetical portfolios are generated to find the optimal portfolio allocation for each level of risk. There are many other practical uses.An unlimited number of items can be used as inputs for this algorithm. There is also a free version that limits the number of inputs to 10 listed here: https://algorithmia.com/algorithms/chrisrobbins157/RandomAllocator
BayesianLogisticRegression,"This is the BayesianLogisticRegression classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/BayesianLogisticRegression.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
BranchAndBound,"This is an implementation of the Branch and Bound algorithm, used to solve an assignment problem.The expected input is an N by N matrix, where each row is assigned a task and each column a task. The values in the matrix is the cost of assigning task ""column"" to a ""row""."
RemoveImageSnow,"Removal/correction of image ""snow"" aka noise, inspired by: http://ostermiller.org/dilate_and_erode.htmlTakes a 2D array representing an image, and a radius of (max possible) snow, e.g., 1 (for 1 pixel). Resulting pixels determined to be snow are replaced with the average color of the non-zero colored pixels around them. Returns the cleaned up image 2D array."
Standard Deviation,Computes mean and standard deviation for a list of samples.
delphi,Just ask any plain text english question and delphi will tell you the truth.
LetterPressHint,Cheat at LetterPress.Given a LetterPress board (i.e. a list of letters) finds valid words using only those letters.Bonus: focus your search with preferred and required letters.
LuceneTextAnalyzer,"In general, any analyzer in Lucene is a combination of tokenizer => Stemmer => Stop-words filter.Tokenizer splits your text into chunks, and since different analyzers may use different tokenizers, you can get different output token streams, i.e. sequences of chunks of text. For example, KeywordAnalyzer you mentioned doesn't split the text at all and takes all the field as a single token. At the same time, StandardAnalyzer (and most other analyzers) use spaces and punctuation as a split points. For example, for phrase ""I am very happy"" it will produce list [""i"", ""am"", ""very"", ""happy""] (or something like that). For more information on specific analyzers/tokenizers see its Java Docs.Stemmers are used to get the base of a word in question. It heavily depends on the language used. For example, for previous phrase in English there will be something like [""i"", ""be"", ""veri"", ""happi""] produced, and for French ""Je suis très heureux"" some kind of French analyzer (like SnowballAnalyzer, initialized with ""French"") will produce [""je"", ""être"", ""tre"", ""heur""]. Of course, if you will use analyzer of one language to stem text in another, rules from the other language will be used and stemmer may produce incorrect results. It isn't fail of all the system, but search results then may be less accurate.KeywordAnalyzer do not use any stemmers, it passes all the field unmodified. So, if you are going to search some words in English text, it isn't a good idea to use this analyzer.Stop words are the most frequent and almost useless words. Again, it heavily depends on language. For English these words are ""a"", ""the"", ""I"", ""be"", ""have"", etc. Stop-words filters remove them from the token stream to lower noise in search results, so finally our phrase ""I'm very happy"" with StandardAnalyzer will be transformed to list [""veri"", ""happi""].And KeywordAnalyzer again do nothing. So, KeywordAnalyzer is used for things like ID or phone numbers, but not for usual text.]"
TopologicalSort,"In computer science, a topological sort (sometimes abbreviated topsort or toposort) or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; in this application, a topological ordering is just a valid sequence for the tasks. A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time."
poisson,Poisson Distribution Calculator
Arbitrage Detection,"Consider a market for financial transactions that is based on trading commodities. The table below shows conversion rates among currencies. USD 1      0.741  0.657  1.061  1.005EUR 1.349  1      0.888  1.433  1.366GBP 1.521  1.126  1      1.614  1.538CHF 0.942  0.698  0.619  1      0.953CAD 0.995  0.732  0.650  1.049  1The table has one line per currency, giving its name followed by the conversion rates to the other currencies. An arbitrage opportunity is a directed cycle such that the product of the exchange rates is greater than one. For example, our table says that 1,000 U.S. dollars will buy 1,000.00 × .741 = 741 euros, then we can buy 741 × 1.366 = 1,012.206 Canadian dollars with our euros, and finally, 1,012.206 × .995 = 1,007.14497 U.S. dollars with our Canadian dollars, a 7.14497-dollar profit!Output: a list of solutions, where each solution has the following fields: - amounts: how much 1 unit of the first currency is worth in the others (the ones in the 'currencies' field) - currencies: the currency conversions - profit: the amount of profit in the first currency - ratio: the profit ratio"
SortRowsOn,"Sorts a collection of rows alphabetically, based on column values, for the given specified column indices.First parameter: collection of rows to be sortedSecond parameter: list of columns to sort onThird parameter: flags for each sorted column whether or not the sort is ascending"
Arbitrage ,"Arbitrage finds the arbitrage combinations of currencies of a given currency rates.For the possible currency rates of:USD 1 0.741  0.6571.061 1.005
EUR 1.349 1 0.888 1.433 1.366
GBP 1.521 1.126 1 1.614 1.538
CHF 0.942 0.698 0.619 1 0.953
CAD 0.995 0.732 0.650 1.049 1the input shoul be:     [         [1, 0.741,  0.657,  1.061,  1.005],         [1.349,1, 0.888,  1.433,  1.366],         [1.521,  1.126,  1, 1.614, 1.538],         [0.942, 0.698, 0.619, 1,0.953],         [0.995,  0.732,0.650,1.049,1]     ]and the output would be:     [          {""combination"":[1,3],""profit"":1.000234},          {""combination"":[3,1],""profit"":1.000234},          {""combination"":[0,1,2],""profit"":1.000830168},          {""combination"":[0,1,3],""profit"":1.000265526},          {""combination"":[0,1,4],""profit"":1.0071449700000001},          {""combination"":[0,2,4],""profit"":1.00541367},          {""combination"":[0,3,4],""profit"":1.0060773349999999},          {""combination"":[1,2,0],""profit"":1.0008301679999998},          {""combination"":[1,2,3],""profit"":1.0003959359999999},          {""combination"":[1,3,0],""profit"":1.000265526},          ...    ]Output Explanation:The second combination tells changing USD -> EUR -> GBP -> USD will give a 1.000830168 profit."
BogoSort,"BogosortIf bogosort were used to sort a deck of cards, it would consist of checking if the deck were in order, and if it were not, throwing the deck into the air, picking the cards up at random, and repeating the process until the deck is sorted. Its name comes from the word bogus."
UpperCaseString,converts any string into all upper case
co-travelers,"This will tell you if two sets of ""travelers"" can possibly be traveling together.  A traveler consists of a set of coordinates and times.  Right now this will only check a pair of travelers, a future version may allow several travelers and pick the ones that potentially are co-travelers."
LoadDataFromBytes,
ModelCounting,"Model counting is the task of counting the number of satisfying assignments for a boolean expression. This implementation works by invoking an efficient SAT solver repeatedly, each time ruling out all previously-found models. It stops when the expression is no longer satisfiable.Input: the input is an expression over boolean variables. See cloncaric/sat for a description of the syntax.Output: the output is the number of satisfying assignments found."
Chartie,"See beyond the numbersThe new free web API that finds the trend event from your numerical arraysFor more detail : www.chartie.ioFeaturesEasy-to-useYou don't have to be a data scientist to catch it. Chartie thoroughly works with every kind of signal and allows you to high level programming producing clear code.Trend detectionYou can detect trend patterns such as ""a slight rise"" or ""a strong rise preceded by a moderate fall"". The program logics does not change if you change the signal.PerformanceChartie can be queried from every programming language without loading your server and it is fast: many thousands of values processed in a few tens of milliseconds.


                                Thanks to our technololgy, Chartie's output identifies the relevant trend in the input data.
                            

                                Chartie employs these commonly used concepts:
                            


Rise and Fall: representing the intuitive idea of signal increments and decrements
                                

Balanced: armonious combination of rises and falls. A constant signal is an extreme example of such a case.
                                


                                Rise and fall words are enriched with these adjectives: slight, moderate, strong, very strong.
                            
"
SVD,"Reduced Singular Value DecompositionFor more information on SVD, see: http://en.wikipedia.org/wiki/Singular_value_decompositionSample matrix comes from:http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htmThis algorithm computes the reduced singular value decomposition. For more information on the difference between reduced SVD and full SVD see:http://www.cs.cornell.edu/courses/cs322/2008sp/stuff/TrefethenBau_Lec4_SVD.pdfInput: Object of class double[][] representing the matrix to decomposeOutput: Object of class Map<String, double[][]> containing 4 keys:""U"" - m by n matrix with orthonormal columns, ""Sigma"" - n by n diagonal
matrix,""V"" - n by n unitary matrix with
orthonormal columns,""singularValues"" - the singular values "
Barcode Reader,"Takes a url in Algorithmia Data API format that identifies an image of a barcode (i.e. data://util/GeneratedQRCodes/AZTECTest.png) and returns the encoded data (generally text or a number) if it can recognize and read the format. Currently supported formats include PDF 417, QR Codes, Code 128, and AZTEC.This is based on the zxing library (https://github.com/zxing/zxing)."
BinarySearch,Binary search on sorted Integer arrays. Returns the index of the item in the array. Returns -1 if it is not found.
Exponential Moving Average,Takes a time series and returns the exponential moving average (http://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average). This can be used to smooth or denoise a time series. The user can optionally supply an alpha parameter (specifying how heavily the past is weighted) as second argument. If not supplied a default of 0.5 is used.
AssignmentProblem,"Solve the weighted bipartite matching problem, aka the ""assignment problem"""
EvalArithExpression,Evaluates arithmetic expressions with nested parens and operator precedence for / and *. Only lighted tested so far :)
PickRandomSubset,Picks a random subset of size k from a set of integers.
Hamiltonian Path,"A Hamiltonian path on a directed graph is a path that visits every node exactly once. This algorithm finds Hamiltonian paths on arbitrary graphs by converting the problem to an instance of SAT and invoking an efficient SAT solver.InputThis algorithm accepts a first argument (the graph) and an optional boolean second argument. If the second argument is true, a Hamiltonian cycle is returned. By default (or if the second argument is false), a Hamiltonian path is returned.The graph is specified as an adjacency list---that is, a map where keys are vertex names (strings) and values are lists of neighbor vertices (strings). For example, the map { ""a"": [""b"", ""c""] } represents a graph with three vertices (a, b, and c) and two edges (a->b and a->c).Note that the input graph is directed. To represent an undirected graph, just include both directions for every edge. Also note that self-edges and multi-edges are allowed, but for obvious reasons are never used in a Hamiltonian path.OutputThe output is a list of vertex names forming a path. If a Hamiltonian cycle was requested, the first vertex is repeated at the end of the list. If a Hamiltonian path/cycle does not exist on the graph, null is returned."
Theta Star,An implementation of the Theta* Any Angle Path Planning Algorithm.
ZipData,"Takes a zip code, returns various pieces of relevant data for that zipcode. If not found, returns null.The data includes zip code, zip code type (standard or PO box), city (beware that some zip codes span multiple cities), state, latitude, longitude, whether or not the code is decommissioned, and estimated population.Based on http://federalgovernmentzipcodes.us/, using data last updated 1/22/2012."
ConvertSoundFile,"This algorithm converts a given sound file to the specified format. First input is the location of the sound file (a Data url) and the second input is the target for saving the output (another Data url).Try this input:[""data://sound/MediaCollection/thank_you_sound.wav"",""data://sound/MediaCollection/thank_you_sound_converted.mp3"", ""pcm_s16le"", ""1"", ""16000""]"
whois,Extract whois information for a domain.
Remove_Duplicate_Words,Take in a duplicate string and return all unique words. (insensitive case)
Full Name Parser,"This algorithm extracts the the first name, middle name, last name and even the prefix and suffix from a given full name."
Binary Integer Programming,"This is an algorithm for binary integer programming (also known as 0-1 integer programming) built on the JaCoP Constraint Programming Solver (https://github.com/radsz/jacop). Binary integer programming is a restriction of integer programming in which variables are limited to being either 0 or 1. Read more about integer programming at https://en.wikipedia.org/wiki/Integer_programming. It's arguments areA String array of variable namesAn int vector c defining the cost of the returned solution via c^(T)x.An int matrix (int[][]) that defines the matrix part of the linear constraintsA String array denoting the nature of each constraint, either ""<"","">"",""<="", or "">="".An int array (in[]) defining the right hand side of the linear constraints.The output is a Map<String,Int> containing each variable name as key and its value in the found solution as value.  Note that this implementation is minimizing and uses branch-and-bound, which is to say, it minimizes c^(T)x. To maximize, change the signs of the integers in c.  For more information consult the JaCoP documentation at http://jacopguide.osolpro.com/guideJaCoP.html#x1-510006."
GenerateHistogram,"Creates a basic histogram with given number of bins, i.e., returns a data structure indicating how many data elements fell in which bucket, and the size of each bin."
Hash,Accepts any JSON input and returns a deterministic hash of the input.
Null,"This algorithm returns null, no matter what. Seems pretty clear."
MinimalBipartitePerfectMatching,"Given a bipartite graph, finds the perfect matching with minimal cost.Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)First partition (list of strings)Second partition (list of strings)"
JEdmondsBlossomShrinking,Constructs maximum matchings in a graph using Edmonds' Blossom algorithm.
testAlgor,
Pipeline,
Page Rank,"Given a url, crawls the domain of that url and computes a normalized PageRank for each page based on links within the domain. This is wrapper for https://algorithmia.com/algorithms/thatguy2048/PageRank."
Deterministic Coreference Resolution,"This is built from the Stanford Deterministic Coreference Resolution System (http://nlp.stanford.edu/software/dcoref.shtml). It takes text (as a String) as input and outputs a list of maps, in which each map contains the name of an entity (specifically, its most representative mention) as key and a list of other mentions of that entity as values. For instance, in the text ""Bob visited the university. He then enrolled."" yields input [{""Bob"":[""He""]}], meaning the ""Bob"" is the most representative mention of the person Bob, and the later word ""He"" refers to the same entity.For more information, please refer to http://nlp.stanford.edu/software/corenlp.shtml or Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60. "
SecretMessenger,"A simple encryption for messages. To decrypt a message, use the DecodeMessage Algorithm"
Calculator,"Calculator with arbitrary precisionAvailable operationsceil(x)floor(x)fabs(x)factorial(x)exp(x)log(x)log10(x)pow(x, y)sqrt(x)acos(x)asin(x)atan(x)atan2(y, x)cos(x)hypot(x, y)sin(x)tan(x)degrees(x)radians(x)acosh(x)asinh(x)atanh(x)cosh(x)sinh(x)tanh(x)erf(x)erfc(x)gamma(x)On decimals.ln().log10().sqrt()Exponention operatorx ** y"
BinaryMerge,"A single threaded algorithm to union merge two integer arrays. The arrays must be in sorted order, otherwise the behavior is undefined."
sumtest,
Colors From Image,"Uses ColorThief to extract a main color and a color palette from an image.Input the url and the number of colors to be extracted, returns the main color and a color palette accordingly."
Test Nudity Detection,
TimeSeriesSummary,"Returns various statistics of the given time series. Note that if given a double[] it treats the data as uniformly spaced, but it can handle non-uniformly spaced data if given a width 2 double[][]."
One Dimensional Midpoint Displacement Terrain,"This algorithm creates an array of unnormalized 32 bit float values that when rendered will resemble terrain. The inputs in the order they are to be sent:octaves : int -- the size of the array is 2^octaves + 1, so this value determines what power of two the array size will be. higher octaves interpolate more terrain, adding more detail.startPointVal : float -- the leftmost point in the array, along with endPointVal is used to determine other points values.endPointVal : float -- the rightmost point in the array. start and end points may be used to connect terrain pieces by matching end to start on subsequent terrains.smoothingFactor : float -- this determines the smoothness of the terrain. smoothingFactor < 0.5 -> rough, jagged, mountainous terrain. 0.5 < smoothingFactor < 0.9 -> mixed, mountainous terrain. smoothingFactor > 0.9 -> smooth, rolling hillsseed : long -- seed for the Java Random number generator. using the same seed will return the same terrain so this,  along with the same start and end points, can be used to 'save' a favorite terrain"
TurtleTreeDrawing,"Draw a simple tree in python, by giving it the depth value."
JOptimizer,"This is a quick demo of 2D Linear Programming example found on http://www.joptimizer.com/linearProgramming.html. The algorithm takes in the following parameters, with regard to the notation on the website linked above, where the problem definition:minimizex cTx+d  s.t.     Gx ≤ h     Ax = b,  where G ∈ RmXn and A ∈ RpXnis turned into:minimizex cTx+d  s.t.     Ax = b,       lb ≤ x ≤ ubSo the parameters are:[double[] c, double[][] G, double[] h, double[] lb, double[] ub]Try passing in:[[-1.0, -1.0], [[1.33, -1],[-0.5, 1.0], [-2.0, -1.0],[0.33, 1.0]],[2.0, 0.5, 2.0, 0.5],[0, 0],[10, 10]]"
WordFrequencies,"Computes word frequencies for a given input text. Takes text, max # of results, whether or not to ignore capitalization, and whether or not to sort from highest frequency to lowest or vice versa in resulting list of word frequencies. Strips off punctuation for all words in the text. For example: [""This is the test of the service."", 3, true, true] returns [{""word"":""the"",""frequency"":2},{""word"":""test"",""frequency"":1},{""word"":""service"",""frequency"":1}]."
CsvToArff,"Converts .csv type files to .arff type files. Please give two inputs, the first one a Data API url to a .csv file, the second the destination for the .arff to be written."
Auth Test,
Get Bucket Information,"This algorithm gets information about an AWS S3 bucket (size and ACL info). The input is a Data API URL pointing to a file that has the bucket name, access key ID and secret access key in separate lines."
JKShortestPaths,Determines the k shortest simple paths in increasing order of weight.Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)Start vertex (string)End vertex (string)k (int)
Yandex Translate,"This algorithm uses Yandex API to translate your text. Please give the input as a json object, 'to' and 'text' fields are mandatory. If 'from' is not specified, Yandex tries to detect the origin language before performing the translation. The supported languages are:LanguageCodeAlbaniansqArabianarArmenianhyAzeriazBelarusianbeBosnianbsBulgarianbgCatalancaCroatianhrCzechcsChinesezhDanishdaDutchnlEnglishenEstonianetFinnishfiFrenchfrGeorgiankaGermandeGreekelHebrewheHungarianhuIcelandicisIndonesianidItalianitJapanesejaKoreankoLatvianlvLithuanianltMacedonianmkMalaymsMaltesemtNorwegiannoPolishplPortugueseptRomanianroRussianruSpanishesSerbiansrSlovakskSlovenianslSwedishsvThaithTurkishtrUkrainianukVietnameseviPowered by Yandex.Translate: http://translate.yandex.com/"
Crawl And Page Rank,"Given a url, crawls the domain of that url and computes a normalized PageRank for each page based on links within the domain. This is wrapper for https://algorithmia.com/algorithms/thatguy2048/PageRank."
calcular-rfc,API to calculate Registro Federal de Contribuyentes (RFC)
LuhnCode,Return the luhn code of the  input
ExpectedRiskValue,"This class defines the expected rate value based of a risk. We use the next formula:E(R) = [(p(P))] - [(1-p)(L)]The expectative consist of certain probabilities of win or loss and the amount of the factors that you win or loss. If the rate of return if is > 0, We can win depending of the risk.E(R) is calculated in term of each individual game.p = probability of win.1-p = probability of loss.P = win factor.L = loss factor.Examples: 1.- I am going to flipping a coin, if the coin gets head I win $10, else I loss $10.     Will I take the risk?      E(R) = (0.5*10) - (1-0.5)(10) = 0       So I wont. because the rate of return is 0, there is no money!!      INPUT: [0.5, 1, 1] 2.- If I win I get $2, else I loss $1. I have 60% probability to loss and 40% of win.      E(R) = (0.4*2) - (1-0.4)(1) = 0.2       So the average rate of return is $0.20 each game. Maybe I will.      INPUT: [0.4, 2, 1]  3.- I win $2 if the thin wrestler win, else $1 if fat wrestler win.        If thin wrestler has 30% probability to win.       Will I take the risk for this wrestling?       E(R) = (0.3*2) - (1-0.3)(1) = 0.6 - 0.7 = -0.1      So I wont. I loss -0.1 each game, so If I play 100 times -> 100-0.1=10. I could loss $10       INPUT: [0.3, 2, 1]"
CarBrands,"A lookup service for car brands by categories:""popular"": [""Toyota"",""Honda"",""Mazda"",""Subaru"",""Nissan"",""KIA"",""Hyundai"",""Mitsubishi"",""VW"",""Audi"",""Mercedes-Benz"",""BMW"",""Tesla"",""Lexus"",""Porsche"",""Acura"",""Infiniti"",""Jaguar"",""Mini"",""Cadillac"",""Jeep"",""Ford"",""Chevrolet"",""Chrysler"",""Buick""]""japanese"":[""Daihatsu"",""Honda"",""Mazda"",""Mitsubishi"",""Nissan"",""Subaru"",""Suzuki"",""Toyota""]""premium"": [""Tesla"",""Ferrari"",""Maserati"",""Lamborghini"",""Lotus"",""Porsche"",""Lexus"",""Mercedes-Benz"",""BMW"",""Infiniti"",""Acura"",""Cadillac"",""Bentley"",""Jaguar""]""european"": [""Audi"",""BMW"",""Mercedes-Benz"",""Maserati"",""Ferrari"",""Bentley"",""Jaguar"",""Lotus"",""Lamborghini"",""Opel"",""VW""]"
CountSetBits,A function to count the number of set bits of a given integer.
LinearRegression,
GetNthLatticePoint,"Produces the Nth lattice point (starting at 0), for spiral square lattice points on an integer grid.Point 0 is (0,0)Point 1 is (1,0)Point 2 is (1, -1)Point 3 is (0, -1)Point 4 is (-1, -1)Point 5 is (-1, 0)Point 6 is (-1, 1)..."
Topic Extraction,"Topics extraction with Non-Negative Matrix FactorizationThis is a proof of concept application of Non Negative Matrix Factorization of the term frequency matrix of a corpus of documents so as to extract an additive model of the topic structure of the corpus. The output is a list of topics, each represented as a list of terms (weights are not shown).The default parameters (n_samples / n_features / n_topics) should make the example runnable in a couple of tens of seconds. You can copy this algorithm and try to increase the dimensions of the problem, but be aware than the time complexity is polynomial.Taken from: http://scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html"
prime_factorization,List of the integer's prime factors.
Fft,"Fast Fourier Transform for 1-dimensional, real input data (audio or other time series). Returns transformed complex data in interleaved format (Real_00, Imag_00, Real_01, Imag_01). For even-length input, output[1] equals Real[length/2], for odd-length input, output[1] equals Imag[(length-1)/2]."
Test,
RomanNumeral,"Returns the roman numeral for the given integer, up to 4000."
Summarizer,"Summarizer is an algorithm that takes in data and json format and returns a summary and key terms from the text. The json object the algorithm takes needs 4 fields:text: a string containing the text you want summarizedsummaryLength: a floating point number, less than or equal to one, which represents a percentage of the text length you want the summary to bekeyTermsWanted: a boolean, with true meaning you want the key terms from the algorithm, and false meaning you don'tnumKeyTerms: an integer, representing the number of key terms you want from the text.The output returns a json object with 1 or two fields, depending on if you want the key terms. The object contains:summary: a list, containing the strings that make up the summary. The sentences are ranked in order of importance; the first sentence is the most important. This is guaranteed to be part of the returned object.keyTerms: a list, containing the number of key terms requested, if they are requested. The key terms are ranked in order of importance; the first term is the most important. This is only part of the returned json object if 'keyTermsWanted' is true in the input json object.Below is an easy to use json object that you only need to paste the variables into to use.""{\""text\"": \""<TEXT>\"", \""summaryLength\"": <floatingPointPercentage>, \""keyTermsWanted\"": <boolean>, \""numKeyTerms\"": <numberOfKeyTerms>}"""
ZipToLatLong,"Takes a zip code, returns a central lat,long for that zipcode. If not found, returns null. Note that the returned lat,long is generally only to two decimal points, so can't be counted on to place with high accuracy (i.e. within about 70 miles).Based on http://federalgovernmentzipcodes.us/, using data last updated 1/22/2012."
EmailAddressGenerator,This is a email Address generator. which takes name and URL as user Input.Ex.Joe root as Name or Iker Casillas Fernández as Namewww. abc.com as URL and it returns a list of possible username
DiacriticsRemover,Remove diacritics from string.
Knapsack,
Move To Front,"This is an implementation of the move-to-front transform. This is useful when combined with Burrow-Wheeler Transform before compression of data. This transform only works on a string of ASCII characters.  



Sample Input
    
Sample Output

[110,0,99,99,0,0]
    
""nnbaaa""


"
CaptchaValidate,Algorithm for validating the input of the user for a given Captcha
FindNumbersThatSumTo,"Returns the indices of two numbers in an array that add to a specified number, or null if there are no such numbers. Indices are zero based."
Grown Up Decision Maker,Make a decision like a Grown Up
Finance Number Of Periods,"NPER(rate, PMT, PV, FV, type).
Returns the number of periods for an investment based on periodic,constant payments and a constant interest rate.  * Rate is the periodic interest rate.  * PMT is the constant annuity paid in each period.  * PV is the present value (cash value) in a sequence of payments.  * FV (optional) is the future value, which is reached at the end of the last period.  * Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period."
Summary Stats,
Fourier Filter,"This algorithm takes a time series and two parameter k and j. It takes the Fourier transform of the series, zeroes out the top k lowest frequencies and j highest freqencies, and maps the result back into a time series with an inverse Fourier transform.This is intended to act as a filter, high pass if j is 0, low pass is k is 0, and band pass if neither is 0. See http://en.wikipedia.org/wiki/Band-pass_filter for more information on the ideas behind filtering.This is a speculative algorithm for filtering - use it for exploratory analysis but try not to draw too many conclusions unless you corroborate with other methods.This is based on an implementation of the Fast Fourier Transform (in this the real Fourier Transform in the JTranforms library - https://sites.google.com/site/piotrwendykier/software/jtransforms) Note that there are many different conventions used in signal processing, so be careful as the ones chosen here may not be the ones you need. "
Bubblesort,
test,
QR Code Reader,Takes a url in Algorithmia Data API format that identifies an image of a barcode (i.e. data://util/GeneratedQRCodes/qrCodeTest.png) and returns the encoded data (generally text or a number) if it can recognize and read the format. This algorithm is designated for QR Codes but can also read some other formats.This is based on the zxing library (https://github.com/zxing/zxing).
Assignment Problem (Weighted Bipartite Matching),"The assignment problem is one of the fundamental combinatorial optimization problems in the branch of optimization or operations research in mathematics. It consists of finding a maximum weight matching in a weighted bipartite graph.In its most general form, the problem is as follows:There are a number of agents and a number of tasks. Any agent can be assigned to perform any task, incurring some cost that may vary depending on the agent-task assignment. It is required to perform all tasks by assigning exactly one agent to each task and exactly one task to each agent in such a way that the total cost of the assignment is minimized.If the numbers of agents and tasks are equal and the total cost of the assignment for all tasks is equal to the sum of the costs for each agent (or the sum of the costs for each task, which is the same thing in this case), then the problem is called the linear assignment problem. Commonly, when speaking of the assignment problem without any additional qualification, then the linear assignment problem is meant.This API is based on this Hungarian Algorithm implementation.The complexity is N^3.ExampleSay you have three workers: Jim, Steve, and Alan. You need to have one of them clean the bathroom, another sweep the floors, and the third wash the windows. What’s the best (minimum-cost) way to assign the jobs? First we need a matrix of the costs of the workers doing the jobs:Clean bathroomSweep floorsWash windowsJim$3$3$3Steve$3$2$3Alan$3$3$2When applied to the above table, the algorithm gives the minimum cost it can be done with: Jim washes the windows, Steve sweeps the floors, and Alan cleans the bathroom.InputA weighted graph in the form Map<String, Map<String, Double>>, in which each key is a node name, and its value is a map that links each neighbour to the weight (which may be positive, negative, or zero) of the edge connecting the two. If node B does not appear in the map corresponding to node A, the weight of the edge connecting them is assumed to be zero.Note: in the list you can include all the nodes (like in the example), or only those in one group of the bipartite graph (in the example this'd mean that it's enough to specify the Jim, Steve and Alan keys).OutputAn object with two keys:assignment: this is a Map<String, String>, in which each key is a node and its value is the name of the node it is assigned toweight: the total weight of the matches"
Ping,
LoadCurve1,"Returns Load Factor for a 8760 hour load curve.  In this case, it is essentially the asset utilization factor.Two versions:1. Input 1 for the stock data file LC1.csv.2. Input ""data://MadDenker/LCTest1/LC1.csv"" for same data fileInput data files should be a text file with 8760 measurements, each on a new line in the file."
SimplifyUnixPath,Simplifies unix paths.
TextTokenizer,"Lucene StandardTokenizerA grammar-based tokenizer constructed with JFlex.As of Lucene version 3.1, this class implements the Word Break rules from the Unicode Text Segmentation algorithm, as specified in Unicode Standard Annex #29.Many applications have specific tokenizer needs. If this tokenizer does not suit your application, please consider copying this source code directory to your project and maintaining your own grammar-based tokenizer."
Digitize,"Return digits of number in a specified baseInput: [number, base]number: the input numberbase: the specified base"
Connected Graphs with Restricted Number of Edges,"The algorithm gives the number of connected labeled graphs on `n` vertices restricted to `k` edges.def memoize(func):
    cache = {}
    def memof(*args, **kwargs):
        k = args
        if k in cache:
            return cache[k]
        else:
            result = func(*args, **kwargs)
            cache[k] = result
            return result
    memof._cache = cache
    return memof

@memoize
def binomial(n, k):
    if k < 0 or k > n:
        return 0
    if k == 0 or k == n:
        return 1
    k, c = min(k, n - k), 1
    for i in xrange(k):
        c = c * (n - i) / (i + 1)
    return c

@memoize
def c(n, k):
    if k < n - 1 or k > binomial(n, 2):
        return 0
    elif k == binomial(n, 2):
        return 1
    elif k == n - 1:
        return n**(n - 2)
    else:
        s, t = binomial(binomial(n, 2), k), 0
        for i in xrange(1, n):
            for j in xrange(i - 1, min(binomial(i, 2) + 1, k + 1)):
                t += (binomial(n - 1, i - 1) *
                      binomial(binomial(n - i, 2), k - j) *
                      c(i, j))
        return s - t

def apply(nk):
    assert len(nk) == 2 and set(map(type, nk)) == set([int])
    return str(c(*nk))
The implementation makes use of recurrences given by Marko Riedel (see below) and Brendan McKay et al (eqn 1.11 of [1]).For large `n` there will be integer wrap around. A simple way to use arbitrary precision is to convert the return value to the python `decimal.Decimal` type where the wraparound occurs:import decimal

c = decimal.Context(prec=256)
decimal.setcontext(c)

@memoize
def c(n, k):
    if k < n - 1 or k > binomial(n, 2):
        return 0
    elif k == binomial(n, 2):
        return 1
    elif k == n - 1:
        return Decimal(n)**(n - 2)
    else:
        s, t = binomial(binomial(n, 2), k), 0
        for i in xrange(1, n):
            for j in xrange(i - 1, min(binomial(i, 2) + 1, k + 1)):
                t += (binomial(n - 1, i - 1) *
                      binomial(binomial(n - i, 2), k - j) *
                      c(i, j))
    return s - t
This is OEIS A123527.A wide range of approaches with Maple and C implementations is discussed by Marko Riedel at these links:http://math.stackexchange.com/questions/689526/how-many-connected-graphs-over-v-vertices-and-e-edgeshttp://math.stackexchange.com/questions/1071564/how-many-good-graphs-of-size-n-are-thereReferences:[1] Bender, E. A., Canfield, E. R. and McKay, B. D. (1990), The asymptotic number of labeled connected graphs with a given number of vertices and edges. Random Struct. Alg., 1: 127–169. doi: 10.1002/rsa.3240010202[2] Flajolet and Sedgewick, Analytic Combinatorics[3] Harary and Palmer, Graphical Enumeration (helpful OEIS link)Table of values:  1    0 1
  2    1 1
  3    2 3
  3    3 1
  4    3 16
  4    4 15
  4    5 6
  4    6 1
  5    4 125
  5    5 222
  5    6 205
  5    7 120
  5    8 45
  5    9 10
  5   10 1
  6    5 1296
  6    6 3660
  6    7 5700
  6    8 6165
  6    9 4945
  6   10 2997
  6   11 1365
  6   12 455
  6   13 105
  6   14 15
  6   15 1
  7    6 16807
  7    7 68295
  7    8 156555
  7    9 258125
  7   10 331506
  7   11 343140
  7   12 290745
  7   13 202755
  7   14 116175
  7   15 54257
  7   16 20349
  7   17 5985
  7   18 1330
  7   19 210
  7   20 21
  7   21 1
  8    7 262144
  8    8 1436568
  8    9 4483360
  8   10 10230360
  8   11 18602136
  8   12 28044072
  8   13 35804384
  8   14 39183840
  8   15 37007656
  8   16 30258935
  8   17 21426300
  8   18 13112470
  8   19 6905220
  8   20 3107937
  8   21 1184032
  8   22 376740
  8   23 98280
  8   24 20475
  8   25 3276
  8   26 378
  8   27 28
  8   28 1
  9    8 4782969
  9    9 33779340
  9   10 136368414
  9   11 405918324
  9   12 974679363
  9   13 1969994376
  9   14 3431889000
  9   15 5228627544
  9   16 7032842901
  9   17 8403710364
  9   18 8956859646
  9   19 8535294180
  9   20 7279892361
  9   21 5557245480
  9   22 3792906504
  9   23 2309905080
  9   24 1251493425
  9   25 600775812
  9   26 254183454
  9   27 94143028
  9   28 30260331
  9   29 8347680
  9   30 1947792
  9   31 376992
  9   32 58905
  9   33 7140
  9   34 630
  9   35 36
  9   36 1
 10    9 100000000
 10   10 880107840
 10   11 4432075200
 10   12 16530124800
 10   13 50088981600
 10   14 128916045720
 10   15 288982989000
 10   16 573177986865
 10   17 1016662746825
 10   18 1624745199910
 10   19 2352103292070
 10   20 3096620034795
 10   21 3717889913655
 10   22 4078716030900
 10   23 4093594934220
 10   24 3761135471805
 10   25 3163862003211
 10   26 2435820178050
 10   27 1714943046390
 10   28 1102765999275
 10   29 646542946125
 10   30 344847947664
 10   31 166867565040
 10   32 73005619995
 10   33 28759950345
 10   34 10150589610
 10   35 3190186926
 10   36 886163125
 10   37 215553195
 10   38 45379620
 10   39 8145060
 10   40 1221759
 10   41 148995
 10   42 14190
 10   43 990
 10   44 45
 10   45 1

 ...

 20   19 262144000000000000000000
 20   20 8765006377126199463936000
 20   21 165302832533722012508160000
 20   22 2308679811324625848791040000
 20   23 26475493254511544166850560000
 20   24 262531375188086694499173888000
 20   25 2319409283218421983604361216000
 20   26 18609460830276453968696568576000
 20   27 137389515051337171399458435328000
 20   28 942286943592166927670520784780800
 20   29 6047449583948914817459539926604800
 20   30 36525524751295516417034996099750400
 20   31 208571200052782924641061209874483200
 20   32 1130300650993207566827148319301771880
 20   33 5831738934026048895826705179092277120
 20   34 28723888612157111882953788103273970340
 20   35 135376304331236127351814277357418826380
 20   36 611755203820630100932668139569655653840
 20   37 2655349100798421324674003273937347141440
 20   38 11088128409801591093117635255497606675800
 20   39 44606287696335732082661932475125008999720
 20   40 173093184087559388557367970317932248105360
 20   41 648634853134996172634225003614446597429320
 20   42 2349635408113437646955354971698533883712740
 20   43 8235373529177470880652460053802218808087980
 20   44 27952196835136685448917879439461869334410800
 20   45 91946325761953831526022842342180418988526640
 20   46 293323596830287357305947764536461608867858880
 20   47 908108519771992451439632625351722372571708480
 20   48 2730038400735615036002612889636029395562762940
 20   49 7974117537961181949550849258775727551874918300
 20   50 22641461465708071138697884315996625477663213040
 20   51 62523528991427060772830373925248588518522574480
 20   52 167994340448833310311473061857592690452647157960
 20   53 439379609944055570033573267043656746906954879560
 20   54 1119049560306988683013921063482443621234880458640
 20   55 2776400987013110803265071299784514459932820123440
 20   56 6712555103215350665474943224055436396502752517340
 20   57 15820003938512717669745201297125355031702332505620
 20   58 36355507329631394056324975255739939114129383525800
 20   59 81489719357083942168990372106284892841284143009440
 20   60 178205593469776626899179292865881830060256015277000
 20   61 380308445861969151464175305627521253582430617881200
 20   62 792229041509562288598344955587763758895313623436280
 20   63 1611253663378636285865154999161614458562821550550880
 20   64 3200134580885618414286699298034529266460857650169165
 20   65 6207996169249222497599923902980061119984393973560250
 20   66 11765121908406564458627012904714069001823741708809275
 20   67 21786207173226748678138947749938428180363721989737580
 20   68 39425860884511872102812365759471792027341500604140505
 20   69 69737314285580496469788594028337934658321806251330250
 20   70 120586868444914951946513467750933860571722138276133275
 20   71 203867609196437603533590129055793402004778274865911400
 20   72 337030365222188758030716978552941677096026008360126685
 20   73 544901842008654214368467275419586144121980197935690890
 20   74 861685216190423321548918666774119739779339168907460355
 20   75 1332937402373577118386130640579382271972565593132180356
 20   76 2017196958767237366974987508530256487182717131607867025
 20   77 2986813834341124743685220889482239558678124795956947690
 20   78 4327433786974669795474079704565108222943346091264022315
 20   79 6135551696813670525966684708099083381721260355072930000
 20   80 8513607726769030832716554358246595936380401384020592513
 20   81 11562291195436147058886026241015740143479912300177563970
 20   82 15370054806994526508193862134521986786696256523149495575
 20   83 20000313432783641272263388727030230896705466444525947180
 20   84 25477356873473639512476772908693816013722495702600032265
 20   85 31772556908599627440874194901507570438850872714755099978
 20   86 38792878749685698523603124984130516187889410469921596635
 20   87 46373890655353938344432935729914028056658332719926497720
 20   88 54279298250968782996315993072086727268668100407156688305
 20   89 62208459118903581653828763547308584092082785451805733650
 20   90 69812382963304726732245548458093917009834278334432962271
 20   91 76717506045668481889353643116268866438527555477778106580
 20   92 82555237341550578889317411945876816255247591325493788865
 20   93 86994146659895802590632485182513335241221849734798385290
 20   94 89770938575812625664252966870363776177858044360302789675
 20   95 90716210270590805577577419830401049909124355624633204320
 20   96 89771501959259118937676830983896956955582521303871633745
 20   97 86995261363941848917313195432844412346436616602276915330
 20   98 82556878852697422386984420202467010899908906906684189235
 20   99 76719636824248060278044888112048942753870048740116907380
 20  100 69814952030673113533958686801674332836149373773174947651
 20  101 62211402220234022386699832166692349370338847735769908830
 20  102 54282538963593584953787999506246943286043499332391242305
 20  103 46377342671349122394327736400967867346067152183569536520
 20  104 38796449351781662565196741539454848888798436467192045645
 20  105 31776151426229594201547195880717770354922595172775700058
 20  106 25480883701496407817663668469924390103410432416186941015
 20  107 20003689063008568275297739448428650099169952329309482060
 20  108 15373208289341682977649269463785287033042365636678730255
 20  109 11565167516449011402405138671907133965148772688639329910
 20  110 8516169751079597685524387014004417017329057631967548293
 20  111 6137780527785119153464081430114271153553168659165335280
 20  112 4329327623593330337833251497674502522149498526168623305
 20  113 2988385591700145044370076954621392876110910469340910090
 20  114 2018471048709153668977817177972559866040717903848861115
 20  115 1333946124145551613639320775313257003485862242518875116
 20  116 862465186021095880698295410618976586046560021104845675
 20  117 545490810771206073783501354122993645556725678544692350
 20  118 337464658482089079589796002636713575541484023559577465
 20  119 204180299547899591305891389147541363257662066887845000
 20  120 120806678106523419570559217427068210462880032100327765
 20  121 69888161263681650706591562873335577601622672740326850
 20  122 39526911035657022046321030330037891630371170361766175
 20  123 21852276083541036529943285114979161479083281732230260
 20  124 11807278231651869372879200108022171539044509244428375
 20  125 6234242915997603351903923705384889650448442799563206
 20  126 3216077698373482420749432886919342476289677315472285
 20  127 1620700573717262362712203729107789750445621786084480
 20  128 797688564086512130591907009078193504214907493183515
 20  129 383385201500820331780828273089260443941545849770710
 20  130 179896133063826131842513709857257062904264905996913
 20  131 82395175465721245931076845124914283097905692458260
 20  132 36828146614847099084196809802025358257033343217885
 20  133 16060394765923896864645582921613298110347681525090
 20  134 6831660460584231682152503775706989901188533361375
 20  135 2833873968962289754536610283559185323442863118728
 20  136 1146051972776748949124410694797914711532731704375
 20  137 451728514826216907648143959019480617455776185230
 20  138 173489936855856105192048869240464720776227201445
 20  139 64902710191244780984114223496919476839032288700
 20  140 23643130141229444309401933290813427173865172001
 20  141 8384088702593846915190463977517362799384283370
 20  142 2893101031183346045538679183398654219815743275
 20  143 971110835643346096476727486469808705931189200
 20  144 316959786633870808135160881728570717701504075
 20  145 100552759897695587573031656203698583528764070
 20  146 30992289009573660394831397048018733786528705
 20  147 9276603513071427256300896471050539487552100
 20  148 2695229399068347141922652342853832086738825
 20  149 759729092354883999355974194033869350173850
 20  150 207659285243675636633614664479122587930787
 20  151 55009082183756205442539852390164099951640
 20  152 14114172402411279476175267710544671473935
 20  153 3505480727396284813243037292390512525230
 20  154 842225889049759124780061125294485068365
 20  155 195613754876073363889867008010760639676
 20  156 43887701414503669503181582956431178405
 20  157 9504342981484874177139099402831015570
 20  158 1985084293601271480782780206077345495
 20  159 399513820095853405480892548907818560
 20  160 77405802643571599400105648837693511
 20  161 14423441486379801286095416614735710
 20  162 2581974093240828635856898201152485
 20  163 443529292090449091470400458532960
 20  164 73020066380744667533756230135455
 20  165 11506192278177947613104886925062
 20  166 1732860282858124640600590327845
 20  167 249033813105359229789524810400
 20  168 34093914889424180268881778375
 20  169 4438261109865869620803019350
 20  170 548255784159901541393346645
 20  171 64123483527473864490450280
 20  172 7083408064081415263479975
 20  173 737001995106736848223350
 20  174 72005942050658197814925
 20  175 6583400416060178085936
 20  176 561085262732401541415
 20  177 44379625300867918530
 20  178 3241208589389230005
 20  179 217287726662965140
 20  180 13278694407181203
 20  181 733629525258630
 20  182 36278383117185
 20  183 1585940245560
 20  184 60334683255
 20  185 1956800538
 20  186 52602165
 20  187 1125180
 20  188 17955
 20  189 190
 20  190 1

0.842158794403 sec to compute and printThe table was printed withimport time
ti = time.time()
for n in range(1, 21):
    for k in range(n - 1, binomial(n, 2) + 1):
        print '{:3d} {:4d} {}'.format(n, k, answer(n, k))
print time.time() - ti, 'sec'"
Copy All Files Between Buckets,"This algorithm copies all files in an AWS S3 bucket to another S3 bucket. If you specify a prefix, you can copy all of the files in a specific directory only.The credentials files seen in the sample input below contain the bucket name, the access key ID and the secret access key, separated by new lines."
TestRecursive,
Custom Map from Json,
CheckMyAEM,"Check your AEM Publish instance configuration.This service allows you to check your publish instance most common and dangerous vulnerability.For security reasons we don't provide the vulnerability details through the API and we don't allow scan of not owned websites.Before you start your scan, you must add the following meta tag to your site's homepage:<meta name=""CheckMyAEM"" content=""allow-scan"">

By adding the meta tag to your explicitly allow the API to run the following check list:- Check default passwords- Check most common entries that allow the attacker to gather juicy information- Check accessibility of private JCR sections (/home, /var, /apps...)No details about the vulnerability are retained by this algorithmThe test runs a limited set of checks, a ""NO-DETECTED-ISSUES"" response doesn't mean that the instance is completely secure.For further information about this tool or if you need any consulting on AEM instances configuration do not hesitate to contact us  (+1 617 621 3300), or visit http://www.coresecure.com.Contacts:- Coresecure Client Services:         Michael Cormio (michael.cormio@coresecure.com)- Software Engineer - Adobe AEM Solution Architect:          Alessandro Bonfatti (alessandro.bonfatti@coresecure.com)"
Max,Computes the maximum of an array of numbers
ListPermutations,Lists all permutations of a small set of integers (up to length 10 to limit excessive result size).
Sru,
ProductRecommender,"Product Recommender uses market basket analysis to recommend related products based on a user's cart.Makes use of https://algorithmia.com/algorithms/paranoia/FpGrowth The algorithm takes two:Dataset: path to a dataset in the Algorithmia DataAPI (data://...), where each line represents a single transaction and each item is separated by whitespace. (See Example file).Cart: A list of items currently in a user's cart, or the set of items that you would like to get related products for."
Extract Factor,"Return the power of the factor in a numberInput: [factor, number]factor: the factor to be extractednumber: the number to extract the factor from"
SimpleKMeansClusterer,"Uses SimpleKMeans clusterer, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/clusterers/SimpleKMeans.htmlInput Json should include:trainUrl: a dataset URL pointing to your dataset in the Data APIInput Json may include:modelUrl: a model URL pointing to a file uploaded to the Data API to save the model, if not specified, model is not savedmode: if load mode is specified, it loads an existing SimpleKMeans clusterer to use on the datasetoptions: specific to each clusterer, if not specified, uses default options. Please see the above documentation for different options for this algorithm."
Cache Algo,"Wrapper for calling another algorithm, but with aggressive caching enabled"
Chunk,"Text chunking consists of dividing a text in syntactically correlated parts of words, like noun groups, verb groups, but does not specify their internal structure, nor their role in the main sentence.Parts are tagged according to the conventions of the Penn Treebank Project (http://www.cis.upenn.edu/~treebank/). For example, a plural noun is denoted NNS, a singular or mass noun is NN, and a determiner (such as a/an, every, no, the,another, any, some, each, etc.) as DT.For more information visit http://opennlp.apache.org."
JBellmanFordShortestPath,Returns the shortest path from one vertex to another in a graph using Bellman Ford Shortest Path algorithm. Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)Start vertex (string)End vertex (string)
AODE,"This is the AODE classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/AODE.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
StringToWordVector,"This is the StringToWordVector filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/StringToWordVector.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Hello World,"A simple ""Hello World"" algorithm, which returns ""Hello "" + the algorithm input."
Color Palette from Image,Uses https://github.com/fengsp/color-thief-py to extract color palette from a given image. A Json input with the url and the number of colors that are to be extracted is necessary to run this algorithm.
Finance Black Scholes ATM Option Value,"Calculates the Black-Scholes option value of an atm call option.@param volatility The Black-Scholes volatility.@param optionMaturity The option maturity T.@param forward The forward, i.e., the expectation of the index under the measure associated with payoff unit. @param payoffUnit The payoff unit, i.e., the discount factor or the anuity associated with the payoff. @return Returns the value of a European at-the-money call option under the Black-Scholes model"
Static Json,Parses a Data API file into a JsonElement and returns it.Intended for testing purposes as a way to quickly replay specific results that might otherwise take much longer to repro.But perhaps one day I'll build an entire Open Data platform based on the idea of just returning static JSON files.
Python Add One,Takes a long and adds one to itUsed in Algorithmia documentation to demonstrate API requests
POS Tagger,"The Part of Speech Tagger marks tokens with their corresponding word type based on the token itself and the context of the token. A token might have multiple pos tags depending on the token and the context. The OpenNLP POS Tagger uses a probability model to predict the correct pos tag out of the tag set. To limit the possible tags for a token a tag dictionary can be used which increases the tagging and runtime performance of the tagger.Parts are tagged according to the conventions of the Penn Treebank Project (http://www.cis.upenn.edu/~treebank/). For example, a plural noun is denoted NNS, a singular or mass noun is NN, and a determiner (such as a/an, every, no, the,another, any, some, each, etc.) as DT.For more information visit http://opennlp.apache.org."
NaiveBayes,"This is the NaiveBayes classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/NaiveBayes.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
GaussianProcesses,"This is the GaussianProcesses classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/GaussianProcesses.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
quicksort,
crackmd5,
QuickSort,
Address Extraction From Text,"Extracts addresses, contact data, company names and other information from text.
Emphasis currently is on Germany, Austria and Switzerland. The 
quality for other countries varies but can be optimized on demand.'results' contains the extracted data (may be empty). The algorithm may return several results and several contact persons 
within a result. Be aware that no result will be returned when no postal code / town combination is found. Each result contains:country, zip, city, street, company, phone, fax, iban, bic: Should be self-explanatory.emailhash: SHA-1-Hash of ‘mailto:’ + mail address (to prevent abuse, plain addresses are not available)vatidnr: VAT IDregnrde: Trade register number and town of register court (Germany only)managers:
 List of contact persons with relevant catchword and distance of name to
 catchword (currently only supported on german websites)blz: German bank code (Germany only)See /brunni/AddressExtraction for a version that crawls websites."
FirstAlgorithm,
GenerateSurrogateKey,Generates an additional integer surrogate key row in a collection of columnar data.First parameter: array of data rowsSecond parameter: column index to create the surrogate keyThird parameter: starting value for surrogate key (default is 1)Fourth parameter: increment value for surrogate key (default is 1)
GetLevelOrderTraversal,Returns level order traversal of a binary tree.
RemoveDuplicateRows,"Removes duplicate concurrent rows. User can specify which subset of columns are used to determine a duplicate row.First parameter: array of row dataSecond parameter: list of column indices that constitute the set of column to use to compare to the previous row. If null, the entire row is compared to the previous one."
SplitColumnOn,Splits columnar data at a particular column index using a given split string.First parameter: list of rowsSecond parameter: column index to split onThird parameter: split stringThe first row's column value at the split column index is used to determine how many columns will be produced by the column split.
test,
Recommendations From Text,"Given a map from document name to text, generate a recommendation graph."
Word Operations,"word2vec is a celebrated word embedding that represents each word as a vector whose features are intended to maximize the predictive accuracy of the given word predicting the appearance of other words that do in fact occur in similar contexts.  This algorithm is based on the Gensim implementation of word2vec, trained on a Google News corpus of over 100 billion words (freely available and linked above).  It was observed on very large corpuses that words would demonstrate linguistic regularities, such as ""vec(king) - vec(man) + vec(woman) ≈ vec(queen)"".This algorithm allows the user to perform operations on words, either mathematically (word1 - word2 + word3 = ?), or in the popular academic format of analogies (word1 is to word2 as word3 is to ?).  In the format provided, mathematical symbols must be preceded by a double backslash (in case one of the words for some reason is a mathematical symbol). "
Resize Image,This algorithm lets you resize an image in your data collection.Input Parameters:Input CollectionInput FilenameOutput CollectionOutput FilenameOutput Width (Pixels)Output Height (Pixels)
GetWebpage,
Camel Case String,"Given a string, this function returns a camel case version of the same string."
NumberGuessingGame,
MySQLtoJSON,"Perform generic MySQL query with creds passed via Algorithmia DataAPI configuration file.Input:[""data://user/collection/configFile.json"",""select * on table limit 1;""]data://user/collection/configFile.json:{    ""url"":""db.example.com"",    ""user"":""test"",    ""password"":""blah""}"
conjugate,
UTM To Lat Long,"Converts UTM coordinates to latitude/longitude (http://en.wikipedia.org/wiki/Universal_Transverse_Mercator_coordinate_system). Input is a string of form  ""<longitude zone>  <latitude zone>  <easting>  <northing>""  where easting is the projected distance from longitude zone's central meridian and northing is the projected distance from the equator. The values of both easting and northing are given in meters. For example, the latitude/longitude coordinates 61.44, 25.40 are presented in UTM as 35 V 414668 6812844; the latitude/longitude coordinates -47.04, -73.48 are 18 G 615471 4789269 in UTM.Based on coordinate conversion software by Sami Salkosuo at IBM, see https://www.ibm.com/developerworks/java/library/j-coordconvert/ for more details."
TelemetryAnalysis,"Computes basic useful values from driver telemetry data, like max/average speeds, max/average acceleration/deceleration, number of stops, time stopped, apparent discontinuities, etc.Input data is a list of X,Y points per second in time. Output values are in meters (per second)."
ZipToState,"Takes a zip code, returns the state that contains that zipcode. If not found, returns ""Zip Code not found!""Based on http://federalgovernmentzipcodes.us/, using data last updated 1/22/2012."
IBAN Generator,"Calculates IBAN bank numbers, works for banks in the netherlands and probably some other countries as well (Not tested)"
zepback,
ValidParens,"Returns true if input string has balanced parens, curly braces, square brackets, or any combination of the three. False if they are not balanced."
Random Allocator,This algorithm allocates random percentages to each item in a list where the sum of the percentages equals 100%.One example where this is useful is portfolio optimization where the many hypothetical portfolios are generated to find the optimal portfolio allocation for each level of risk. There are many other practical uses.The input is limited to 10 items. For an unlimited number of items see Random Allocator - Unlimited Items listed here: https://algorithmia.com/algorithms/chrisrobbins157/RandomAllocatorUnlimitedItems
Finance Future Value,"The future value of money is how much it will be worth at some time in the future. The future value formula shows how much an investment will be worth after compounding for so many years.  FV(rate, NPER, PMT, PV, type)
Returns the future value of an investment based on periodic, constant payments and a constant interest rate.  Rate is the periodic interest rate.  NPER is the total number of periods.  PMT is the annuity paid regularly per period.  PV (optional) is the present cash value of an investment.  Type (optional) defines whether the payment is due at the beginning (1) or the end (0) of        a period."
Distance,Returns the distance between two vertices in a directed graph.Input:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings).The start vertex (string)The goal vertex (string)
ExtractImageListFromUrl,"Extract image url list ordered by size.Features-Support img tags with attributes ""src"" and ""data-original""-Get image size by not loading full image file(this effects when there are many and big images)https://algorithmia.com/bounties/61"
Get URL From Site,
LowpassFilter,"A single pole recursive lowpass filter for reducing high frequency audio content with 6 dB/octave. The filter frequency is expected to be normalized, ie. between 0 and 0.5. The normalized frequency is simply the target frequency divided by the sample rate (eg. 1000Hz / 44100Hz = 0.02267). To let the filter ring out, a number of samples can be specified that will be appended as zeros to the input, and extend the filter response by the same number of samples.The input is not modified."
GCD,Algorithm to calculate the GCD (Greatest Common Divisor) between two numbers.
Bubble_Sort,
RemoveRowsIfValue,"Filters out all rows from a collection that contain column values of a certain specified value. Useful for things like filtering out invalid / blank rows while doing ETL.First parameter is collection of rows, second is list of relevant indices to consider when looking for column values that would call for row removal, and the third parameter is the value to compare column values to. Can accept empty string and null."
shellsort,
Echo,
Disjoint Set,"Runs the Disjoint Set (or Union/Find) algorithm on a list of list of IDs, returning a list of the merged sets with at least one element in common.This implementation takes amortized time of pratically O(1) for inserting any element on the data structure.Based on https://en.wikipedia.org/wiki/Disjoint-set_data_structure"
Cluster Socrata Location Data,"This algorithm can be used to cluster and visualize Socrata data presented as a CSV file. In theory, and CSV file that you would like to cluster and visualize according to a ""title"" and latitude and longitude can be fed into this algorithm. The resulting map is written to your Data collection, given as the last input to this algorithm.The example in the sample input below clusters Assault events taken from raw Seattle crime data.Takes as input:Dataset CSVcolumn index of crime typecolumn index of latitudecolumn index of longitudecrime type to clusternumber of clustersdata api destination!!! Warning: Please make a new Data collection and pass that in instead of the last value in sample input."
Random Number,Returns a random number between 0 and 1.
NaiveBayesSimple,"This is the NaiveBayesSimple classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/NaiveBayesSimple.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
NumericToNominal,"This is the NumericToNominal filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/NumericToNominal.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Fourier Detrend,"This algorithm takes a time series and a parameter k. It takes the Fourier transform of the series, zeroes out the top k strongest frequencies, and maps the result back into a time series with an inverse Fourier transform.This is a speculative algorithm for removing seasonality - use it for exploratory analysis but try not to draw too many conclusions unless you corroborate with other methods.This is based on an implementation of the Fast Fourier Transform (in this the real Fourier Transform in the JTranforms library - https://sites.google.com/site/piotrwendykier/software/jtransforms) Note that there are many different conventions used in signal processing, so be careful as the ones chosen here may not be the ones you need. "
Cumulative Moving Average,"Returns the Cumulative Moving Average (http://en.wikipedia.org/wiki/Moving_average#Cumulative_moving_average) of a time series. That is, each point is replaced by the average of itself and all the points proceeding it. This can be used to smooth a time series."
Parse,"Takes a text string as input, splits the text into sentences, and parses each sentence, outputting the flattened parse tree into an object list. The parse tree is a decomposition of the sentence according to its grammatical structure, splitting out phrases, determining the subject or object of a verb, etc. The parser here is probabilistic and will output what it determines to be the most likely parse of a given sentence. You can read more about how it works http://nlp.stanford.edu/software/lex-parser.shtml.Parts are tagged according to the conventions of the Penn Treebank Project (http://www.cis.upenn.edu/~treebank/). For example, a plural noun is denoted NNS, a singular or mass noun is NN, and a determiner (such as a/an, every, no, the,another, any, some, each, etc.) as DT.For more information, please refer to http://nlp.stanford.edu/software/corenlp.shtml or Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60."
elo,
FrequencyRemover,"Removes a frequency component from an audio signal. This is using a recursive filter with a very narrow bandwith on the selected frequency.Input parameter:double[] inputArray, double frequency, double samplerateThe input is not modified."
ColumnSubset,"Selects a reordered subset from an array of arrays. Useful for ETL transformations (select a subset of columns, possibly in a different order, from a CSV file for example)."
ListAnagrams,Filters an array of strings to only those that are anagrams of some other word in the array.
MergeColumns,"Merge columns from from a collection of input rows into a designated column index, with an optional separator.First parameter: input row data (array of rows)Second parameter: list of column indices to merge (zero based)Third parameter: optional destination index of merged columns (if null, appends to end of row as a new column)Fourth parameter: optional string separator between merged column values"
ReverseLinkedList,Reverse a linked list in-place with no copy (iterative implementation).A->B->C->D   to D->C->B->A
Finance Black Scholes Generalized OptionValue,"Black-Scholes option value of a call i.e: the payoff max(S(T)-K,0)P, where S follows a log-normal process with constant log-volatility.
@param forward The forward of the underlying.
@param volatility The Black-Scholes volatility.
@param optionMaturity The option maturity T.
@param optionStrike The option strike. If the option strike is ≤ 0.0 the method returns the value of the forward contract paying S(T)-K in T.
@param payoffUnit The payoff unit (e.g., the discount factor)
@return Returns the value of a European call option under the Black-Scholes model."
Word Counter,"Receives a string , returns a count of words. Note that internal quotation marks must either be escaped or removed."
BasicThumbnail,"Takes an image (data uri, base64 encoded, or web url) and dimensions for a thumbnail and returns the binary of the resulting PNG encoded image.Pipe your input to this algorithm as an array with first element the string containing the image in one of the formats mentioned above, an integer for desired thumbnail width and an integer for desired thumbnail height."
Monte-Carlo Tree Search,
LatLonPathDistance,LatLonPathDistance computes the total distance in meters from a series of Latitude/Longitude coordinates.
DataAPITest,
Parse By Sentence,"Takes a block of text, splits into sentences, and returns an array of the parse trees of each sentence.Parts are tagged according to the conventions of the Penn Treebank Project (http://www.cis.upenn.edu/~treebank/). For example, a plural noun is denoted NNS, a singular or mass noun is NN, and a determiner (such as a/an, every, no, the,another, any, some, each, etc.) as DT.For more information visit http://opennlp.apache.org."
Logistic,"This is the Logistic classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/Logistic.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Sentence Split,"Splits input string into sentences which are returned as a list of strings.For more information, please refer tohttp://nlp.stanford.edu/software/corenlp.shtml or Manning, Christopher D., Surdeanu, Mihai, Bauer, John, Finkel, Jenny, Bethard, Steven J., and McClosky, David. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pp. 55-60. "
Yandex Language Identification,This algorithm uses Yandex API to detect the language of your text. The supported languages are:LanguageCodeAlbaniansqArabianarArmenianhyAzeriazBelarusianbeBosnianbsBulgarianbgCatalancaCroatianhrCzechcsChinesezhDanishdaDutchnlEnglishenEstonianetFinnishfiFrenchfrGeorgiankaGermandeGreekelHebrewheHungarianhuIcelandicisIndonesianidItalianitJapanesejaKoreankoLatvianlvLithuanianltMacedonianmkMalaymsMaltesemtNorwegiannoPolishplPortugueseptRomanianroRussianruSpanishesSerbiansrSlovakskSlovenianslSwedishsvThaithTurkishtrUkrainianukVietnameseviPowered by Yandex.Translate: http://translate.yandex.com/
Aggregate Directory,
GeoDist,"English / InglésThis code can be used for calculate the distance between two points in the world using their's coordinates (latitude, longitude) and the Earth radius (3958.75 for miles) or (6371 for kilometers) the result is a aproximated distance because the Earth isn't a perfect sphere.Español / SpanishEste código se puede utilizar para obtener la distancia aproximada existente entre dos puntos de la Tierra utilizando sus coordenadas (latitud y longitud) y haciendo uso del radio de la Tierra. (3958.75 para millas) o (6371 para kilómetros), el resultado es una distancia aproximada ya que la tierra no es completamente esférica."
Average ,Computes the average of an array of numbers:double apply(double[] input)
Matrix2String,
MaxGap,"Finds the maximum gap between sequential sorted elements from an unsorted input list, in linear time and space."
RemoveExcessSpaces,"Removes excess spaces from a list of words separated by whitespace - beginning, in between, and end."
SimplePolygonArea,"Computes area of simple polygon.Parameter: list of x,y points."
A Test,
hnscraper,
Passport Photo Maker,Takes in an image (in the form of a Data API URL or binary image) and extracts the foreground after detecting the face in the image. A single face should be present in the taken photo. The foreground is returned in the binary format.
Max-Sat,"This algorithm solves the unweighted maximum satisfiability problem by invoking a fast SAT solver repeatedly. The input consists of a hard formula which MUST be satisfied, and some soft formulas which MAY be satisfied. The output is a model which maximizes the number of satisfied soft formulas.Input format:{  ""hard"": <formula>  ""soft"": [<formula>, <formula>, ...]}The syntax for <formula>  is described in the SAT solver documentation.Output format:{  ""model"": <var-to-bool mapping>  ""satSoft"": <indices of satisfied soft formulas>}"
Word2VecRepresentation,"Takes in a word and gives its word2vec representation using the freely available word vectors trained on a >100 billion word Google News corpus, a 300-dimensional list of real numbers. "
Pairs Generator - Unlimited Items Per List,This algorithm generates a list of all possible pairs from items in a list. This is sometimes an important first step in analyzing and optimizing datasets.This version allows users to generate pairs for lists with an unlimited number of items. There is also a free version of this algorithm that allows users to generate pairs for a list of up to 10 items.
MLP,"An interface to OpenCV's ANN_MLP, as explained in http://docs.opencv.org/java/org/opencv/ml/CvANN_MLP.html. "
Eulerian Cycle,"This algorithm will check whether a graph is Eulerian (hence it contains an Eulerian cycle), and if so, return a list of vertices making up the Eulerian cycle.An Eulerian circuit is a circuit which traverses each edge exactly once."
FirstOpenSource,
MD5Generator,Simple MD5 Generator Algo
Artificial Neural Network for Differential Solving,A senorio of solving differential equations using artificial neural network
manhattanDistance,Returns Manhattan distance between two points
AssignmentBranchAndBound,"This is an algorithm with uses the branch and bound method to solve the assignment problem.Example: Assigning jobs to a group of people with the time to complete each job by each person known.


        
Job 0    Job 1    Job 2    Job 3


Person 0    
13    
4    
7    
6


Person 1    
1    
11    
5    
4


Person 2    
6    
7    
2    
8


Person 3    
1    
3    
5    
9


The input is the cost matrix for each person and job in the form of double[][]. The output is each persons job assignment in the form of int[].Output: [1,3,2,0]Meaning: Person 0 -> Job 1Person 1 -> Job 3Person 2 -> Job 2Person 3 -> Job 0"
DecodeMessage,a way to decrypt secret messages from the SecretMessager algorithm
RMUpper,
Execute Python Code Free Full Access,"Execute python code, for example to calculate the first 200 primes. Result is a JSON object with all the local variables."
Encode_String_for_URL,"Given an input string, will encode necessary characters within the string to make it ready to serve as URL params"
LCM,Algorithm to calculate the LCM (Least Common Multiple) between two numbers.
TimestampToDate,"Convert Unix timestamp to date1429593869 ->  ""2015-04-21 05:24:29"""
TrimAllExcessSpaces,"Trims all excessive whitespace in the input string: beginning, end, and in between words."
Cat,"Get a file from the Data API, and return its contents as a string."
SAX,
CSV To Matrix,"Takes a csv file and outputs a matrix. The csv file has to contain only numbers, please specify the number of lines to be skipped as below. The file should be hosted in the Data API."
Fast Fibonacci,Quickly compute ith fibonacci numbers.Input: array of number 
demo,
GetLinksFromText,Returns a List of links from text input.
JHopcroftKarpBipartiteMatching,Uses Hopcroft-Karp algorithm to find a maximum matching in an undirected simple bipartite graph.Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)First partition (set of strings)Second partition (set of strings)
JBronKerboschCliqueFinder,Detects the cliques in the input graph using Bron-Kerbosch clique detection algorithm.Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)
URLImagesSizeBasedSorter,This algorithm fetches URL and lists the images with specified dimensions constraints and in specified order.
Welcome,Prints a welcome message.Used in Algorithmia documentation to demonstrate API requests
BreakingNewsEntityRecognition,
Execute Python Code Free,"Execute python code, for example to calculate the first 200 primes. Result is a JSON object with all the local variables."
Test1,
FirstApp,
Threshold Anomaly Detection,Takes a time series as double[] and doubles for lower and upper thresholds. Returns an array that matches the input on all points that fall below the lower or exceed the upper thresholds and is zero on all other points.
ColumnValueMap,"Basic ETL transformation utility: replace one value with another in an array of arrays, using a map of desired substitution values."
ListCombinations,Lists all combinations of k integers from a given set of integers.
Simplex Linear Solver,"Basic Apaches Commons Math implementation of the simplex method for solving linear programming problems. We use the canonical representation, and seek to pick vector x such that c.x is minimized within the constraints that Ax <= b and each entry of x is non-negative, where c and b are vectors of doubles and A is a matrix of doubles. The algorithim is called as apply(c,A,b,i), where i is the maximum number of iterations the algorithm is allowed to run. Read more at http://en.wikipedia.org/wiki/Linear_programming."
PyTest1,
Delete File From Bucket,"This algorithm deletes a specified file from an Amazon S3 bucket in the region specified. The inputs are a String that represents the region your bucket is in, the Data API URL to a credentials file (that contains the bucket name in the first line, your AWS access key in the second line and your AWS secret key in the third line), and the key of the file that you would like to delete.The possible regions are:AP_NORTHEAST_1 AP_SOUTHEAST_1 AP_SOUTHEAST_2 CN_NORTH_1 EU_CENTRAL_1 EU_WEST_1 GovCloud SA_EAST_1 US_EAST_1 US_WEST_1US_WEST_2 "
Testing123,
Fibonacci,"En matemáticas, la sucesión de Fibonacci (a veces llamada erróneamente serie de Fibonacci) es la siguiente sucesión infinita de números naturales: 1,1,2,3,5,8,13... "
Left Eye Detection,"Uses a pretrained model to detect eye in a given image. Send your input as a url to a picture that is either hosted on our data API or is a direct link, and the output is the same image, with rectangles around the detected eye, written to your collection. A text file that contains all the coordinates of the rectangles is also output to the same collection for convenience.P.S. Please make sure that the second input to this algorithm is a data collection that YOU own. You can create data collections from the ""Data"" link above."
JConnectivityInspector,"Inspects the given graph for connectivity and returns the set of connected components. If there is only one set, the whole graph is connected. Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)"
JKruskalMinimumSpanningTree,"Uses Kruskal's algorithm. If the given graph is connected it computes the minimum spanning tree, otherwise it computes the minimum spanning forest. Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)"
NaiveBayesMultinomial,"This is the NaiveBayesMultinomial classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/NaiveBayesMultinomial.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
LeastMedSq,"This is the LeastMedSq classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/LeastMedSq.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
TimeSeriesTranslate,"This is the TimeSeriesTranslate filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/TimeSeriesTranslate.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
FirstRoyalty,
Hello World,"Enter you name, and the code will welcome you."
Remove_Duplicate_Integers,"Given a list of integer values, delimited by space, remove duplicate integers from the list.If non integer values detected, will return an error string."
Hello World,"A simple ""Hello World"" algorithm, which returns ""Hello "" + the algorithm input."
Execute Python Code,"Execute Python code, for example to calculate the first 200 primes. Result is a JSON object with all the local variables."
testing,
IFft,"Inverse Fast Fourier Transform for 1-dimensional, complex frequency domain data. Returns real time-domain data, rescaled by data length, so that IFft(Fft(x)) = x. Input is expected to be in interleaved format (Real_00, Imag_00, Real_01, Imag_01), satisfying the symmetry condition. For even-length input, input[1] should equal Real[length/2], for odd-length input, input[1] should equal Imag[(length-1)/2]."
Generate Clumped Sample,"Generates artificial clumped data in two dimensions using (radial) 2D Gaussian distributions. Takes as inputnumber of clusters (int) edge length (double) the size of the  square that clusters will be placed innumber of points (int) - the total number of points to generate. If a generated point lies to the right of (EdgeLength,0) or above (EdgedLength,0) it is not returned. standard deviation (double) the radial standard deviation of the the clustersThe cluster centers are picked uniformly at random from within the square. Then, we for each point, with pick from the clusters uniformly at random, then draw from the distribution of that cluster, returning the result if it is within the square."
SendEmailFromGmail,"Use this algorithm to send email from your Gmail account programmatically. You can pass in the credentials and the details of the message as a file to preserve privacy. Just pass in a Data file url that contains Json with the matching values for the following names:String username, String password, String recipientEmail, String ccEmail, String title, String messageccEmail is optional, the other fields are mandatory."
EmailValidator,
GetListLength,"Returns the length of an algorithmia data collectionInput: Collection URL, eg. data://username/collection"
MergeIntervals,"Merges a list of intervals into a non-overlapping list. For example: [[1, 5], [4,7], [8,10]] becomes [[1,7],[8,10]]."
Radix Sort (32 bit),"Sorts an array of 32-bit integers in-place using radix sort. The runtime of this algorithm is O(n).Note: while irrelevant for sorting integers, please not that this is NOT a stable sort algorithm."
testing1,
congestion control,
FarthestFirstClusterer,"Uses Farthest First clusterer, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/clusterers/FarthestFirst.htmlInput Json should include:trainUrl: a dataset URL pointing to your dataset in the Data APIInput Json may include:modelUrl: a model URL pointing to a file uploaded to the Data API to save the model, if not specified, model is not savedmode: if load mode is specified, it loads an existing Farthest First clusterer to use on the datasetoptions: specific to each clusterer, if not specified, uses default options. Please see the above documentation for different options for this algorithm."
Copy File Between Buckets,"This algorithm copies a file in an AWS S3 bucket to another S3 bucket. It takes in two sets of access keys so that you can copy the file between buckets that belong to different accounts.The credentials files seen in the sample input below contain the bucket name, the access key ID and the secret access key, separated by new lines."
Mouth Detection,"Uses a pretrained model to detect noses in a given image. Send your input as a url to a picture that is either hosted on our data API or is a direct link, and the output is the same image, with rectangles around the detected noses, written to your collection. A text file that contains all the coordinates of the rectangles is also output to the same collection for convenience.P.S. Please make sure that the second input to this algorithm is a data collection that YOU own. You can create data collections from the ""Data"" link above."
Helloworld,
Finance Payment,"PMT(rate, nper, pv, fv, type)
Returns the periodic payment for an annuity with constant interest rates.  rate is the periodic interest rate. nper is the number of periods in which annuity is paid. pv is the present value (cash value) in a sequence of payments. fv (optional) is the desired value (future value) to be reached at the end of the periodic payments.
*type (optional) defines whether a payment is due at the beginning (1) or the end (0) of a period."
JBiConnectivityInspector,"Inspects the given graph for biconnectivity and returns the set of biconnected components. If there is only one set, the whole graph is biconnected. Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)"
SimpleLogistic,"This is the SimpleLogistic classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/SimpleLogistic.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
CitationKNN,"This is the CitationKNN classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/mi/CitationKNN.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
DecisionTable,"This is the DecisionTable classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/rules/DecisionTable.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
J48,"This is the J48 classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/J48.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Obfuscate,"This is the Obfuscate filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/Obfuscate.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
PrincipalComponents,"This is the PrincipalComponents filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/PrincipalComponents.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Wavelet,"This is the Wavelet filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/Wavelet.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
SparseToNonSparse,"This is the SparseToNonSparse filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/instance/SparseToNonSparse.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Sleep,"Sleeps for N seconds, then returns."
Dijkstra,
BasicStatistic,"Basic statistic operationsInput: 2D array of points with x & y position, and a integer (range 0-6) to identify the operation to execute:0: Arithmetic average. The result is a Map with ""X"" value as the average of Xs and ""Y"" value as the average of Ys.1: Geometric average. The result is a Map with ""X"" value as the geometric average of Xs and ""Y"" value as the geometric average of Ys.2: Armonic average. The result is a Map with ""X"" value as the armonic average of Xs and ""Y"" value as the armonic average of Ys.3: RMS. The result is a Map with ""X"" value as the RMS of Xs and ""Y"" value as the RMS of Ys.4: Standard deviation. The result is a Map with ""X"" value as the standard deviation of Xs and ""Y"" value as the standard deviation of Ys.5: Linear regression. The result is a Map with ""m"" value as the slope and ""q"" value as the y offset at 0.6: Quadratic regression. The result is a Map with ""a"" value as the coefficient of x²"
test,
LowshelfBiquadFilter,"A musical lowshelf filter (two pole recursive, direct form 1). Filter frequency, gain (in decibel) and quality (resonance) must be specified. The filter frequency is expected to be normalized, ie. between 0 and 0.5. The normalized frequency is simply the target frequency divided by the sample rate (eg. 1000Hz / 44100Hz = 0.02267). To let the filter ring out, a number of samples can be specified that will be appended as zeros to the input, and extend the filter response by the same number of samples.Input parameter:double[] inputArray, double normalizedFrequency, double gainDb, double q (, numAppendedSamples)The input is not modified."
NotchBiquadFilter,"A notch filter (two pole recursive, direct form 1). Filter frequency, and filter quality must be specified. The filter frequency is expected to be normalized, ie. between 0 and 0.5. The normalized frequency is simply the target frequency divided by the sample rate (eg. 1000Hz / 44100Hz = 0.02267). To let the filter ring out, a number of samples can be specified that will be appended as zeros to the input, and extend the filter response by the same number of samples.Input parameter:double[] inputArray, double normalizedFrequency, double q (, numAppendedSamples)The input is not modified."
HishelfBiquadFilter,"A musical highshelf filter (two pole recursive, direct form 1). Filter frequency, gain (in decibel) and quality (resonance) must be specified. The filter frequency is expected to be normalized, ie. between 0 and 0.5. The normalized frequency is simply the target frequency divided by the sample rate (eg. 1000Hz / 44100Hz = 0.02267). To let the filter ring out, a number of samples can be specified that will be appended as zeros to the input, and extend the filter response by the same number of samples.Input parameter:double[] inputArray, double normalizedFrequency, double gainDb, double q (, numAppendedSamples)The input is not modified."
LowpassBiquadFilter,"A musical lowpass filter (two pole recursive, direct form 1). The filter frequency and quality (resonance) can be specified. The filter frequency is expected to be normalized, ie. between 0 and 0.5. The normalized frequency is simply the target frequency divided by the sample rate (eg. 1000Hz / 44100Hz = 0.02267). To let the filter ring out, a number of samples can be specified that will be appended as zeros to the input, and extend the filter response by the same number of samples.Input parameter:double[] inputArray, double normalizedFrequency, double gainDb (, int numAppendedSamples)The input is not modified."
Waveletdenoising,"Basic multilevel 1D wavelet denoising. Uses numpy and pywavelets. Third party packages are currently not supported on the Algorithmia platform, so I have made this open source."
Simple DDA Algorithm,this algorithm draws lines using the DDA Algorithm the file contains sample frame also so just compile it and run it to see what happens!
Bayes,
test,
Finance Black Scholes Digital Option Value,"Calculates the Black-Scholes option value of a call option.
@param initialStockValue The initial value of the underlying, i.e., the spot.
@param riskFreeRate The risk free rate of the bank account numerarie.
@param volatility The Black-Scholes volatility.
@param optionMaturity The option maturity T.
@param optionStrike The option strike.
@return Returns the value of a European call option under the Black-Scholes model

i.e {""initialStockValue"":23.3,""riskFreeRate"":0.05,""volatility"":1,""optionMaturity"":20,""optionStrike"":5}"
DemoAlgorithm,"Based on the /opencv/FaceDetection, this algorithm takes a base64 encoding of an image and returns an array of rectangles (x, y, width, height) bounding each face identified.Input FormatBase64 representation of an image or a URL of an image.Output FormatArray of rectangles. Example:[    {'y': 10, 'x': 10, 'height': 100, 'width': 100},    {'y': 20, 'x': 20, 'height': 100, 'width': 100},    {'y': 30, 'x': 30, 'height': 100, 'width': 100}]"
StringReverse,Reverse a given string.
havijoori,
Record Footage,Records the picture taken by a Seattle traffic camera every minute.
Boost,"A basic interface to OpenCV's Boost, as explained in http://docs.opencv.org/java/org/opencv/ml/CvBoost.html. It implements the boosted tree classifier. "
JStoerWagnerMinimumCut,Deterministically computes the minimum cut on the given graph.Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)
JEdmondsKarpMaximumFlow,"
Computes maximum flow in a network using Edmonds-Karp algorithm.Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)"
HNB,"This is the HNB classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/HNB.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
LibSVM,"This is the LibSVM classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/LibSVM.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
SimpleLinearRegression,"This is the SimpleLinearRegression classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/SimpleLinearRegression.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
GridSearch,"This is the GridSearch classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/GridSearch.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
SerializedClassifier,"This is the SerializedClassifier classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/misc/SerializedClassifier.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Id3,"This is the Id3 classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/Id3.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
NonSparseToSparse,"This is the NonSparseToSparse filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/instance/NonSparseToSparse.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
ClusterCrimes,
Security Key Generator,The best generator secure passwords for your accounts.Just enter the number of characters you want for your password.
md5crack,
Hello Python,"Hello World, written in Python"
Hello World,"A simple ""Hello World"" algorithm, which returns ""Hello "" + the algorithm input."
MultiplyIntegers,
holamundo,
Circle Area,Calculate the area of a circle.
prova,
PeakBiquadFilter,"A musical peak filter (two pole recursive, direct form 1). Filter frequency, gain (in decibel) and quality (resonance) must be specified. The filter frequency is expected to be normalized, ie. between 0 and 0.5. The normalized frequency is simply the target frequency divided by the sample rate (eg. 1000Hz / 44100Hz = 0.02267). To let the filter ring out, a number of samples can be specified that will be appended as zeros to the input, and extend the filter response by the same number of samples.Input parameter:double[] inputArray, double normalizedFrequency, double gainDb, double q (, numAppendedSamples)The input is not modified."
HipassFilter,"A single pole recursive highpass filter for reducing low frequency audio content with 6 dB/octave. The filter frequency is expected to be normalized, ie. between 0 and 0.5. The normalized frequency is simply the target frequency divided by the sample rate (eg. 1000Hz / 44100Hz = 0.02267). To let the filter ring out, a number of samples can be specified that will be appended as zeros to the input, and extend the filter response by the same number of samples.The input is not modified."
SqNum,
AreAnagrams,Determines whether or not two strings are anagrams of each other.
Delete Bucket,"This algorithm deletes the Amazon S3 bucket specified. The inputs are a String that represents the region your bucket is in, and the Data API URL to a credentials file that contains the bucket name in the first line, your AWS access key in the second line and your AWS secret key in the third line.The possible regions are:AP_NORTHEAST_1 AP_SOUTHEAST_1 AP_SOUTHEAST_2 CN_NORTH_1 EU_CENTRAL_1 EU_WEST_1 GovCloud SA_EAST_1 US_EAST_1 US_WEST_1US_WEST_2 "
Create Bucket,"This algorithm creates an Amazon S3 bucket in the region specified. The inputs are the Data API URL to a credentials file (that contains the bucket name in the first line, your AWS access key in the second line and your AWS secret key in the third line) and a String that represents the region you would like the bucket to be in.The possible regions are:AP_NORTHEAST_1 AP_SOUTHEAST_1 AP_SOUTHEAST_2 CN_NORTH_1 EU_CENTRAL_1 EU_WEST_1 GovCloud SA_EAST_1 US_EAST_1 US_WEST_1US_WEST_2 "
Conference Name Matching,The sample naive algorithm for the Conference Name Disambiguation bounty.
XMeansClusterer,"Uses XMeans clusterer, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/clusterers/XMeans.htmlInput Json should include:trainUrl: a dataset URL pointing to your dataset in the Data APIInput Json may include:modelUrl: a model URL pointing to a file uploaded to the Data API to save the model, if not specified, model is not savedmode: if load mode is specified, it loads an existing XMeans clusterer to use on the datasetoptions: specific to each clusterer, if not specified, uses default options. Please see the above documentation for different options for this algorithm."
sIBClusterer,"Uses sIB clusterer, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/clusterers/sIB.htmlInput Json should include:trainUrl: a dataset URL pointing to your dataset in the Data APIInput Json may include:modelUrl: a model URL pointing to a file uploaded to the Data API to save the model, if not specified, model is not savedmode: if load mode is specified, it loads an existing sIB clusterer to use on the datasetoptions: specific to each clusterer, if not specified, uses default options. Please see the above documentation for different options for this algorithm."
WekaMakeDensityBasedClusterer,"Uses MakeDensityBasedClusterer, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/clusterers/MakeDensityBasedClusterer.htmlInput Json should include:trainUrl: a dataset URL pointing to your dataset in the Data APIInput Json may include:modelUrl: a model URL pointing to a file uploaded to the Data API to save the model, if not specified, model is not savedmode: if load mode is specified, it loads an existing Make Density Based clusterer to use on the datasetoptions: specific to each clusterer, if not specified, uses default options. Please see the above documentation for different options for this algorithm."
WekaFilteredClusterer,"Uses Filtered clusterer, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/clusterers/FilteredClusterer.htmlInput Json should include:trainUrl: a dataset URL pointing to your dataset in the Data APIInput Json may include:modelUrl: a model URL pointing to a file uploaded to the Data API to save the model, if not specified, model is not savedmode: if load mode is specified, it loads an existing Filtered clusterer to use on the datasetoptions: specific to each clusterer, if not specified, uses default options. Please see the above documentation for different options for this algorithm."
EMClusterer,"Uses EM clusterer, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/clusterers/EM.htmlInput Json should include:trainUrl: a dataset URL pointing to your dataset in the Data APIInput Json may include:modelUrl: a model URL pointing to a file uploaded to the Data API to save the model, if not specified, model is not savedmode: if load mode is specified, it loads an existing EM clusterer to use on the datasetoptions: specific to each clusterer, if not specified, uses default options. Please see the above documentation for different options for this algorithm."
ObjectLostAlgorithm,
abc,
JVertexCover,Finds an approximate vertex cover for the given graph. The output is only an approximation since the actual calculation is NP-complete.Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)
JChromaticNumber,Finds an approximate for the chromatic number; minimal number of colors needed to color each vertex such that no two adjacent vertices share the same color. The output is only an approximation since the actual calculation is NP-complete.Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)
JBlockCutpointGraph,Finds the cutpoints in the given graph.Inputs:A graph represented as a map from vertices (strings) to a list of neighbors (list of strings) OR the url to the Data API file that holds the graph that you would like to be read (currently supported formats: GML)
DMNBtext,"This is the DMNBtext classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/DMNBtext.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
NaiveBayesMultinomialUpdateable,"This is the NaiveBayesMultinomialUpdateable classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/NaiveBayesMultinomialUpdateable.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
LinearRegression,"This is the LinearRegression classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/LinearRegression.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MultilayerPerceptron,"This is the MultilayerPerceptron classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/MultilayerPerceptron.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
SMO,"This is the SMO classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/SMO.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
SPegasos,"This is the SPegasos classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/SPegasos.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
ClassificationViaRegression,"This is the ClassificationViaRegression classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/ClassificationViaRegression.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MultiClassClassifier,"This is the MultiClassClassifier classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/MultiClassClassifier.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
ThresholdSelector,"This is the ThresholdSelector classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/ThresholdSelector.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Vote,"This is the Vote classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/Vote.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
HyperPipes,"This is the HyperPipes classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/misc/HyperPipes.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
SimpleCart,"This is the SimpleCart classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/SimpleCart.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
AbstractTimeSeries,"This is the AbstractTimeSeries filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/AbstractTimeSeries.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
MergeTwoValues,"This is the MergeTwoValues filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/MergeTwoValues.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
ReplaceMissingValues,"This is the ReplaceMissingValues filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/ReplaceMissingValues.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Randomize,"This is the Randomize filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/instance/Randomize.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
SubsetByExpression,"This is the SubsetByExpression filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/instance/SubsetByExpression.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
ImageElement,
Graph Inductive Scala,Inductive graph implementation based on Martin Erwig's paper:  * http://web.engr.oregonstate.edu/~erwig/papers/InductiveGraphs_JFP01.pdf
test2,the exponentiator
teste,
jogoDaVelha,
Calculadora,
reverse the words,
HipassBiquadFilter,"A musical highpass filter (two pole recursive, direct form 1). The filter frequency and quality (resonance) can be specified. The filter frequency is expected to be normalized, ie. between 0 and 0.5. The normalized frequency is simply the target frequency divided by the sample rate (eg. 1000Hz / 44100Hz = 0.02267). To let the filter ring out, a number of samples can be specified that will be appended as zeros to the input, and extend the filter response by the same number of samples.Input parameter:double[] inputArray, double normalizedFrequency, double gainDb (, int numAppendedSamples)The input is not modified."
canny edge detection,
Prim,
email,
svmxin,
test,
Strongly Connected Components,"Input: an array of directed Edges where each edge is given by a start and end vertices {""start"": ""vertexA"", ""end"": ""vertexB""}.Output: the algorithm returns an array which maps vertices to natural numbers describing to which connected component the vertex contains like {""vertexA"": 1, ""vertexB"": 4, ....}."
Html2TextMultiplefiles,keep required html files in same folder as py file
ddd,asdasd
Knuth-Morris-Pratt substring search,"The basic idea behind the algorithm discovered by Knuth, Morris, and Pratt is this: whenever we detect a mismatch, we already know some of the characters in the text (since they matched the pattern characters prior to the mismatch). We can take advantage of this information to avoid backing up the text pointer over all those known characters. As a specific example, suppose that we have a two-character alphabet and are searching for the pattern B A A A A A A A A A. Now, suppose that we match five characters in the pattern, with a mismatch on the sixth. When the mismatch is detected,we know that the six previous characters in the text must be B A A A A B (the first five match and the sixth does not), with the text pointernow pointing at the B at the end. The key observation is that we need not back up the text pointer i, since the previous four characters in the text are all As and do not match the first character in the pattern. Furthermore, the character currently pointed to by i is a B and does match the first character in the pattern, so we can increment i and compare the next character in the text with the second character in the pattern. This argument leads to the observation that, for this pattern, we can change the else clause in the alternate brute-force implementation to just set j = 1 (and not decrement i). Since the value of i does not change within the loop, this method does at most N character compares. The practical effect of this particular change is limited to this particular pattern, but the idea is worth thinking about—the Knuth-Morris-Pratt algorithm is a generalization of it. Surprisingly, it is always possible to find a value to set the j pointer to on a mismatch, so that the i pointer is never decremented. "
hima,
CND,
TestAlgo,
RoiTest,
WekaHierarchicalClusterer,"Uses Hierarchical clusterer, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/clusterers/HierarchicalClusterer.htmlInput Json should include:trainUrl: a dataset URL pointing to your dataset in the Data APIInput Json may include:modelUrl: a model URL pointing to a file uploaded to the Data API to save the model, if not specified, model is not savedmode: if load mode is specified, it loads an existing Hierarchical clusterer to use on the datasetoptions: specific to each clusterer, if not specified, uses default options. Please see the above documentation for different options for this algorithm."
CobwebClusterer,"Uses Cobweb clusterer, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/clusterers/Cobweb.htmlInput Json should include:trainUrl: a dataset URL pointing to your dataset in the Data APIInput Json may include:modelUrl: a model URL pointing to a file uploaded to the Data API to save the model, if not specified, model is not savedmode: if load mode is specified, it loads an existing Cobweb clusterer to use on the datasetoptions: specific to each clusterer, if not specified, uses default options. Please see the above documentation for different options for this algorithm."
CLOPEClusterer,"Uses CLOPE clusterer, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/clusterers/CLOPE.htmlInput Json should include:trainUrl: a dataset URL pointing to your dataset in the Data APIInput Json may include:modelUrl: a model URL pointing to a file uploaded to the Data API to save the model, if not specified, model is not savedmode: if load mode is specified, it loads an existing CLOPE clusterer to use on the datasetoptions: specific to each clusterer, if not specified, uses default options. Please see the above documentation for different options for this algorithm."
ll,
ggggg,tt
<script>alert(1)</script>,test
Finance PresentValue,"PV(rate, NPER, PMT, FV, type)
Returns the present value of an investment resulting from a series of regular payments.  rate defines the interest rate per period. nper is the total number of periods.
*pmt is the regular payment made per period. fv (optional) defines the future value remaining after the final installment has been made. type (optional) defines whether the payment is due at the beginning (1) or the end (0) of a period."
RTrees,"A basic interface to OpenCV's RTrees, as explained in http://docs.opencv.org/java/org/opencv/ml/CvRTrees.html. It implements the randomized trees algorithm. "
ERTrees,"A basic interface to OpenCV's ERTrees, as explained in http://docs.opencv.org/java/org/opencv/ml/CvERTrees.html. It implements the extremely randomized trees algorithm. "
DTree,"A basic interface to OpenCV's DTree, as explained in http://docs.opencv.org/java/org/opencv/ml/CvDTree.html. It implements the randomized trees algorithm. "
SVM,"A basic interface to OpenCV's SVM, as explained in http://docs.opencv.org/java/org/opencv/ml/CvSVM.html. It implements the randomized trees algorithm. "
Normal Bayes Classifier,"A basic interface to OpenCV's DTree, as explained in http://docs.opencv.org/java/org/opencv/ml/CvNormalBayesClassifier.html."
KNearest,"An interface to OpenCV's K nearest neighbor, as explained in http://docs.opencv.org/java/org/opencv/ml/CvKNearest.html."
GBTrees,"A basic interface to OpenCV's DTree, as explained in http://docs.opencv.org/java/org/opencv/ml/CvGBTrees.html. It implements the gradient boosted trees algorithm. "
AODEsr,"This is the AODEsr classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/AODEsr.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
BayesNet,"This is the BayesNet classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/BayesNet.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
ComplementNaiveBayes,"This is the ComplementNaiveBayes classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/ComplementNaiveBayes.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
NaiveBayesUpdateable,"This is the NaiveBayesUpdateable classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/NaiveBayesUpdateable.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
WAODE,"This is the WAODE classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/bayes/WAODE.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
IsotonicRegression,"This is the IsotonicRegression classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/IsotonicRegression.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
LibLINEAR,"This is the LibLINEAR classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/LibLINEAR.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
PaceRegression,"This is the PaceRegression classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/PaceRegression.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
PLSClassifier,"This is the PLSClassifier classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/PLSClassifier.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
RBFNetwork,"This is the RBFNetwork classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/RBFNetwork.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
SMOreg,"This is the SMOreg classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/SMOreg.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
VotedPerceptron,"This is the VotedPerceptron classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/VotedPerceptron.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Winnow,"This is the Winnow classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/functions/Winnow.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
IB1,"This is the IB1 classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/lazy/IB1.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
IBk,"This is the IBk classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/lazy/IBk.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
KStar,"This is the KStar classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/lazy/KStar.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
LBR,"This is the LBR classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/lazy/LBR.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
LWL,"This is the LWL classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/lazy/LWL.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
AdaBoostM1,"This is the AdaBoostM1 classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/AdaBoostM1.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
AdditiveRegression,"This is the AdditiveRegression classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/AdditiveRegression.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
AttributeSelectedClassifier,"This is the AttributeSelectedClassifier classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/AttributeSelectedClassifier.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Bagging,"This is the Bagging classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/Bagging.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
ClassificationViaClustering,"This is the ClassificationViaClustering classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/ClassificationViaClustering.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
CostSensitiveClassifier,"This is the CostSensitiveClassifier classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/CostSensitiveClassifier.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
CVParameterSelection,"This is the CVParameterSelection classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/CVParameterSelection.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Dagging,"This is the Dagging classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/Dagging.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Decorate,"This is the Decorate classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/Decorate.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
END,"This is the END classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/END.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
FilteredClassifier,"This is the FilteredClassifier classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/FilteredClassifier.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Grading,"This is the Grading classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/Grading.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MetaCost,"This is the MetaCost classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/MetaCost.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MultiBoostAB,"This is the MultiBoostAB classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/MultiBoostAB.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MultiScheme,"This is the MultiScheme classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/MultiScheme.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
ClassBalancedND,"This is the ClassBalancedND classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/nestedDichotomies/ClassBalancedND.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
DataNearBalancedND,"This is the DataNearBalancedND classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/nestedDichotomies/DataNearBalancedND.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
ND,"This is the ND classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/nestedDichotomies/ND.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
OrdinalClassClassifier,"This is the OrdinalClassClassifier classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/OrdinalClassClassifier.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
RacedIncrementalLogitBoost,"This is the RacedIncrementalLogitBoost classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/RacedIncrementalLogitBoost.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
RandomCommittee,"This is the RandomCommittee classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/RandomCommittee.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
RandomSubSpace,"This is the RandomSubSpace classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/RandomSubSpace.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
RegressionByDiscretization,"This is the RegressionByDiscretization classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/RegressionByDiscretization.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
RotationForest,"This is the RotationForest classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/RotationForest.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Stacking,"This is the Stacking classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/Stacking.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
StackingC,"This is the StackingC classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/meta/StackingC.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MDD,"This is the MDD classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/mi/MDD.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MIBoost,"This is the MIBoost classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/mi/MIBoost.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MIDD,"This is the MIDD classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/mi/MIDD.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MIEMDD,"This is the MIEMDD classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/mi/MIEMDD.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MILR,"This is the MILR classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/mi/MILR.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MINND,"This is the MINND classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/mi/MINND.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MIOptimalBall,"This is the MIOptimalBall classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/mi/MIOptimalBall.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MISMO,"This is the MISMO classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/mi/MISMO.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MISVM,"This is the MISVM classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/mi/MISVM.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
MIWrapper,"This is the MIWrapper classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/mi/MIWrapper.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
SimpleMI,"This is the SimpleMI classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/mi/SimpleMI.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
VFI,"This is the VFI classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/misc/VFI.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
ConjunctiveRule,"This is the ConjunctiveRule classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/rules/ConjunctiveRule.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
DTNB,"This is the DTNB classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/rules/DTNB.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
JRip,"This is the JRip classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/rules/JRip.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
M5Rules,"This is the M5Rules classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/rules/M5Rules.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
NNge,"This is the NNge classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/rules/NNge.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
OneR,"This is the OneR classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/rules/OneR.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
PART,"This is the PART classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/rules/PART.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Prism,"This is the Prism classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/rules/Prism.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Ridor,"This is the Ridor classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/rules/Ridor.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
ZeroR,"This is the ZeroR classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/rules/ZeroR.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
ADTree,"This is the ADTree classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/ADTree.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
BFTree,"This is the BFTree classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/BFTree.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
DecisionStump,"This is the DecisionStump classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/DecisionStump.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
FT,"This is the FT classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/FT.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
J48graft,"This is the J48graft classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/J48graft.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
LADTree,"This is the LADTree classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/LADTree.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
M5P,"This is the M5P classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/M5P.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
LMT,"This is the LMT classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/LMT.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
NBTree,"This is the NBTree classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/NBTree.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
REPTree,"This is the REPTree classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/REPTree.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
RandomTree,"This is the RandomTree classifier, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/classifiers/trees/RandomTree.htmlSend your input to the algorithm as a simple Json object, with the following options:mode: One of train/update/load. Use ""train"" to train a new classifier, use ""update"" if you have new instances to train an existing updateable classifier, use ""load"" to load an existing classifier for classifying test datatrainUrl: The path to the training data that you uploaded to our Data APItestUrl: The path to the test data that you would like to get the labels using this classifier (optional if you would like to use the cross-validation option)modelUrl: The path that you would like to either save the model that is trained or if you would like to load an already trained and saved modelcv: The number of cross-validation folds that you would like to useoptions: A string that contains any options that you would like to specifically set for this classifier (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this classifier, please see Wekadocs linked aboveclassIndex: Specify the index of the class values in the test set (caution: When using a test set, assumes that the class values are at the last index if the classIndex parameter is not specified, so please arrange accordingly)"
Add,"This is the Add filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/Add.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
AddCluster,"This is the AddCluster filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/AddCluster.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
AddExpression,"This is the AddExpression filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/AddExpression.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
AddID,"This is the AddID filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/AddID.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
AddNoise,"This is the AddNoise filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/AddNoise.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
AddValues,"This is the AddValues filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/AddValues.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Center,"This is the Center filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/Center.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
ChangeDateFormat,"This is the ChangeDateFormat filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/ChangeDateFormat.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
ClassAssigner,"This is the ClassAssigner filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/ClassAssigner.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
ClusterMembership,"This is the ClusterMembership filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/ClusterMembership.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Copy,"This is the Copy filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/Copy.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
FirstOrder,"This is the FirstOrder filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/FirstOrder.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
InterquartileRange,"This is the InterquartileRange filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/InterquartileRange.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
KernelFilter,"This is the KernelFilter filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/KernelFilter.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
MakeIndicator,"This is the MakeIndicator filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/MakeIndicator.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
MathExpression,"This is the MathExpression filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/MathExpression.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
MultiInstanceToPropositional,"This is the MultiInstanceToPropositional filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/MultiInstanceToPropositional.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
NominalToBinary,"This is the NominalToBinary filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/supervised/attribute/NominalToBinary.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
NominalToString,"This is the NominalToString filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/NominalToString.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Normalize,"This is the Normalize filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/Normalize.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
NumericCleaner,"This is the NumericCleaner filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/NumericCleaner.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
NumericToBinary,"This is the NumericToBinary filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/NumericToBinary.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
NumericTransform,"This is the NumericTransform filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/NumericTransform.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
PartitionedMultiFilter,"This is the PartitionedMultiFilter filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/PartitionedMultiFilter.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
PKIDiscretize,"This is the PKIDiscretize filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/PKIDiscretize.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
PotentialClassIgnorer,"This is the PotentialClassIgnorer filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/PotentialClassIgnorer.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
PropositionalToMultiInstance,"This is the PropositionalToMultiInstance filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/PropositionalToMultiInstance.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
RandomProjection,"This is the RandomProjection filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/RandomProjection.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
RandomSubset,"This is the RandomSubset filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/RandomSubset.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
RELAGGS,"This is the RELAGGS filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/RELAGGS.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Remove,"This is the Remove filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/Remove.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
RemoveType,"This is the RemoveType filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/RemoveType.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
RemoveUseless,"This is the RemoveUseless filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/RemoveUseless.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Reorder,"This is the Reorder filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/Reorder.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Standardize,"This is the Standardize filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/Standardize.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
StringToNominal,"This is the StringToNominal filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/StringToNominal.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
SwapValues,"This is the SwapValues filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/SwapValues.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
TimeSeriesDelta,"This is the TimeSeriesDelta filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/attribute/TimeSeriesDelta.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
AddClassification,"This is the AddClassification filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/supervised/attribute/AddClassification.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
AttributeSelection,"This is the AttributeSelection filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/supervised/attribute/AttributeSelection.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
ClassOrder,"This is the ClassOrder filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/supervised/attribute/ClassOrder.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Discretize,"This is the Discretize filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/supervised/attribute/Discretize.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
PLSFilter,"This is the PLSFilter filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/supervised/attribute/PLSFilter.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Resample,"This is the Resample filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/supervised/instance/Resample.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
SMOTE,"This is the SMOTE filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/supervised/instance/SMOTE.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
SpreadSubsample,"This is the SpreadSubsample filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/supervised/instance/SpreadSubsample.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
StratifiedRemoveFolds,"This is the StratifiedRemoveFolds filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/supervised/instance/StratifiedRemoveFolds.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
ReservoirSample,"This is the ReservoirSample filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/instance/ReservoirSample.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
RemoveWithValues,"This is the RemoveWithValues filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/instance/RemoveWithValues.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
RemoveRange,"This is the RemoveRange filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/instance/RemoveRange.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
RemovePercentage,"This is the RemovePercentage filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/instance/RemovePercentage.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
RemoveMisclassified,"This is the RemoveMisclassified filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/instance/RemoveMisclassified.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
RemoveFrequentValues,"This is the RemoveFrequentValues filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/instance/RemoveFrequentValues.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
RemoveFolds,"This is the RemoveFolds filter, as implemented in Weka:http://weka.sourceforge.net/doc.stable/weka/filters/unsupervised/instance/RemoveFolds.htmlSend your input to the algorithm as a simple Json object, with the following parameters:dataUrl: The path to the data to be manipulated (this path should point to the Data API)options: A string that contains any options that you would like to specifically set for this filter (format: param name followed by desired value separated by space: e.g. ""-C 5 -t 2""). For options specific to this filter, please see Weka docs linked abovewriteUrl: (optional) Default behavior is to overwrite the dataUrl, if you specify this parameter, your filtered data will be written to this url. (caution: Specify writeUrl if you do not want your data to be overwritten with the filtered data!)"
Dijkstra,
Test1,
mytest,
ElGamal Digital Signature,
Sample,
Hello-World,
